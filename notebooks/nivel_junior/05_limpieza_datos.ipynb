{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ff0e39",
   "metadata": {},
   "source": [
    "# Limpieza y Preparaci√≥n de Datos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Identificar y manejar datos faltantes\n",
    "- Detectar y tratar valores at√≠picos (outliers)\n",
    "- Eliminar duplicados y normalizar datos\n",
    "- Validar la calidad de los datos\n",
    "- Aplicar t√©cnicas de feature engineering b√°sico\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Librer√≠as cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c70a",
   "metadata": {},
   "source": [
    "## 1. Crear Dataset con Problemas de Calidad\n",
    "\n",
    "Vamos a crear un dataset con varios problemas t√≠picos de calidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f335ac4",
   "metadata": {},
   "source": [
    "### üìñ Detectando Problemas de Calidad de Datos\n",
    "\n",
    "**Los datos del mundo real son SUCIOS:**\n",
    "- **70-80% del tiempo** en proyectos de datos se gasta en limpieza\n",
    "- Sin calidad de datos ‚Üí an√°lisis incorrecto ‚Üí malas decisiones\n",
    "\n",
    "**Problemas comunes:**\n",
    "1. **Valores nulos** (NaN, None, NULL)\n",
    "2. **Duplicados** (registros repetidos)\n",
    "3. **Tipos incorrectos** (fechas como strings, n√∫meros como text)\n",
    "4. **Outliers** (valores extremos an√≥malos)\n",
    "5. **Inconsistencias** (Madrid vs madrid vs MADRID)\n",
    "6. **Formato incorrecto** (fechas en diferentes formatos)\n",
    "\n",
    "**Pipeline de inspecci√≥n:**\n",
    "```python\n",
    "df.info()                # Tipos y nulos\n",
    "df.describe()            # Estad√≠sticas (detectar outliers)\n",
    "df.isnull().sum()        # Contar nulos por columna\n",
    "df.duplicated().sum()    # Contar duplicados\n",
    "df.dtypes                # Verificar tipos\n",
    "```\n",
    "\n",
    "Este notebook te ense√±a a detectar y corregir cada uno de estos problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con problemas\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records = 1000\n",
    "\n",
    "df_dirty = pd.DataFrame({\n",
    "    'id': range(1, n_records + 1),\n",
    "    'nombre': [\n",
    "        np.random.choice(['  Juan  ', 'MAR√çA', 'pedro', 'Ana L√≥pez', None, 'Carlos', 'Laura '])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'email': [\n",
    "        np.random.choice([\n",
    "            'juan@email.com', 'maria@test.com', 'invalido', None, 'pedro@example.com',\n",
    "            'ANA@EMAIL.COM', 'carlos@test', '  laura@email.com  '\n",
    "        ])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'edad': np.random.choice([25, 30, None, -5, 35, 150, 28, 40], n_records),\n",
    "    'salario': np.random.choice(\n",
    "        [30000, 45000, None, -1000, 60000, 1000000, 55000, 70000],\n",
    "        n_records\n",
    "    ),\n",
    "    'fecha_registro': pd.date_range('2020-01-01', periods=n_records, freq='D'),\n",
    "    'ciudad': np.random.choice(['Madrid', 'madrid', 'BARCELONA', 'Barcelona', None], n_records),\n",
    "    'pais': np.random.choice(['Espa√±a', 'spain', 'ESPA√ëA', None], n_records)\n",
    "})\n",
    "\n",
    "# Agregar algunos duplicados\n",
    "df_dirty = pd.concat([df_dirty, df_dirty.iloc[:10]], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset creado con {len(df_dirty)} registros\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_dirty.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e45d1",
   "metadata": {},
   "source": [
    "## 2. An√°lisis Exploratorio de Problemas de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general\n",
    "print(\"=== INFORMACI√ìN DEL DATASET ===\")\n",
    "print(f\"\\nShape: {df_dirty.shape}\")\n",
    "print(f\"\\nColumnas: {df_dirty.columns.tolist()}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_dirty.dtypes)\n",
    "\n",
    "print(f\"\\n=== ESTAD√çSTICAS DESCRIPTIVAS ===\")\n",
    "df_dirty.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2a80b",
   "metadata": {},
   "source": [
    "### üìä An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "**Concepto:** Antes de limpiar datos, debemos entender qu√© problemas existen mediante un an√°lisis exploratorio.\n",
    "\n",
    "**T√©cnicas clave:**\n",
    "- `info()`: estructura, tipos de datos, valores no nulos\n",
    "- `describe()`: estad√≠sticas descriptivas de columnas num√©ricas\n",
    "- `shape`: dimensiones del dataset (filas, columnas)\n",
    "- `dtypes`: tipos de datos de cada columna\n",
    "\n",
    "**Objetivo:** Identificar patrones, anomal√≠as y problemas de calidad antes de aplicar transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores nulos\n",
    "print(\"=== VALORES NULOS ===\")\n",
    "null_counts = df_dirty.isnull().sum()\n",
    "null_percentages = (null_counts / len(df_dirty) * 100).round(2)\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'Nulos': null_counts,\n",
    "    'Porcentaje': null_percentages\n",
    "})\n",
    "\n",
    "print(null_summary[null_summary['Nulos'] > 0])\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "null_summary['Porcentaje'].plot(kind='bar', color='coral')\n",
    "plt.title('Porcentaje de Valores Nulos por Columna')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.xlabel('Columna')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b792c",
   "metadata": {},
   "source": [
    "### üîç Detecci√≥n de Valores Nulos\n",
    "\n",
    "**Concepto:** Los valores nulos (NaN, None, NULL) representan ausencia de datos y pueden afectar an√°lisis y modelos.\n",
    "\n",
    "**M√©todos importantes:**\n",
    "- `isnull()` / `isna()`: detecta valores nulos\n",
    "- `sum()`: cuenta nulos por columna\n",
    "- Porcentajes ayudan a decidir si eliminar o imputar\n",
    "\n",
    "**Buena pr√°ctica:** Visualizar el porcentaje de nulos por columna para priorizar acciones de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de duplicados\n",
    "print(\"=== AN√ÅLISIS DE DUPLICADOS ===\")\n",
    "duplicates = df_dirty.duplicated()\n",
    "print(f\"\\nTotal de filas duplicadas: {duplicates.sum()}\")\n",
    "print(f\"Porcentaje: {(duplicates.sum() / len(df_dirty) * 100):.2f}%\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"\\nEjemplos de duplicados:\")\n",
    "    print(df_dirty[duplicates].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074968a",
   "metadata": {},
   "source": [
    "### üîÑ Detecci√≥n de Duplicados\n",
    "\n",
    "**Concepto:** Registros duplicados distorsionan estad√≠sticas y an√°lisis, generando sesgo en los resultados.\n",
    "\n",
    "**M√©todo clave:**\n",
    "- `duplicated()`: retorna boolean indicando si una fila es duplicado\n",
    "- Por defecto considera todas las columnas\n",
    "- Puede configurarse con `subset=` para columnas espec√≠ficas\n",
    "\n",
    "**Decisi√≥n:** Antes de eliminar, verificar si los duplicados son errores reales o datos leg√≠timos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857df1f8",
   "metadata": {},
   "source": [
    "## 3. Limpieza de Datos Paso a Paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para limpiar\n",
    "df_clean = df_dirty.copy()\n",
    "\n",
    "print(f\"Dataset original: {len(df_dirty)} filas\")\n",
    "print(f\"Iniciando limpieza...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f73d9",
   "metadata": {},
   "source": [
    "### 3.1. Eliminar Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados completos\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after = len(df_clean)\n",
    "\n",
    "print(f\"Duplicados eliminados: {before - after}\")\n",
    "print(f\"Registros restantes: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d731990",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Eliminaci√≥n de Duplicados\n",
    "\n",
    "**Concepto:** `drop_duplicates()` elimina filas duplicadas, manteniendo la primera ocurrencia por defecto.\n",
    "\n",
    "**Par√°metros importantes:**\n",
    "- `subset=`: especifica columnas para evaluar duplicaci√≥n\n",
    "- `keep='first'|'last'|False`: qu√© duplicado mantener\n",
    "- `inplace=True`: modifica el DataFrame original\n",
    "\n",
    "**Impacto:** Reduce ruido y mejora calidad de datos, especialmente en agregaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3c63c",
   "metadata": {},
   "source": [
    "### 3.2. Limpiar Columnas de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar nombres\n",
    "print(\"=== LIMPIEZA DE NOMBRES ===\")\n",
    "print(\"\\nAntes:\")\n",
    "print(df_clean['nombre'].value_counts())\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_clean['nombre'] = df_clean['nombre'].str.strip().str.title()\n",
    "\n",
    "print(\"\\nDespu√©s:\")\n",
    "print(df_clean['nombre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482c59c",
   "metadata": {},
   "source": [
    "### üî§ Limpieza de Datos de Texto\n",
    "\n",
    "**Concepto:** Datos de texto requieren normalizaci√≥n para estandarizar formatos y facilitar an√°lisis.\n",
    "\n",
    "**Operaciones comunes:**\n",
    "- `str.strip()`: elimina espacios al inicio y final\n",
    "- `str.title()`: capitaliza primera letra de cada palabra\n",
    "- `str.lower()` / `str.upper()`: conversi√≥n de may√∫sculas/min√∫sculas\n",
    "- `str.replace()`: reemplaza patrones de texto\n",
    "\n",
    "**Uso:** Esencial para nombres, direcciones, categor√≠as y cualquier campo de texto libre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar emails\n",
    "print(\"=== LIMPIEZA DE EMAILS ===\")\n",
    "\n",
    "# Normalizar: quitar espacios y convertir a min√∫sculas\n",
    "df_clean['email'] = df_clean['email'].str.strip().str.lower()\n",
    "\n",
    "# Validar formato de email\n",
    "import re\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if pd.isna(email):\n",
    "        return False\n",
    "    return bool(re.match(email_pattern, email))\n",
    "\n",
    "df_clean['email_valido'] = df_clean['email'].apply(is_valid_email)\n",
    "\n",
    "print(f\"\\nEmails v√°lidos: {df_clean['email_valido'].sum()}\")\n",
    "print(f\"Emails inv√°lidos: {(~df_clean['email_valido']).sum()}\")\n",
    "\n",
    "print(\"\\nEjemplos de emails inv√°lidos:\")\n",
    "print(df_clean[~df_clean['email_valido']]['email'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509ca60",
   "metadata": {},
   "source": [
    "### üìß Validaci√≥n de Formatos con Regex\n",
    "\n",
    "**Concepto:** Las expresiones regulares (regex) permiten validar patrones complejos como emails, tel√©fonos, URLs.\n",
    "\n",
    "**Patr√≥n email:** `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`\n",
    "- `^`: inicio de string\n",
    "- `[...]+`: uno o m√°s caracteres permitidos\n",
    "- `@`: arroba obligatoria\n",
    "- `\\.`: punto literal (escapado)\n",
    "- `{2,}`: m√≠nimo 2 caracteres para dominio\n",
    "\n",
    "**Uso:** Crear columnas booleanas indicando validez facilita filtrado y an√°lisis de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd348517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar ciudades y pa√≠ses\n",
    "print(\"=== NORMALIZACI√ìN DE CIUDADES Y PA√çSES ===\")\n",
    "\n",
    "# Ciudades\n",
    "df_clean['ciudad'] = df_clean['ciudad'].str.strip().str.title()\n",
    "print(\"\\nCiudades √∫nicas:\")\n",
    "print(df_clean['ciudad'].value_counts())\n",
    "\n",
    "# Pa√≠ses\n",
    "df_clean['pais'] = df_clean['pais'].str.strip().str.title()\n",
    "print(\"\\nPa√≠ses √∫nicos:\")\n",
    "print(df_clean['pais'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303801d2",
   "metadata": {},
   "source": [
    "### 3.3. Limpiar Datos Num√©ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ad56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar edad\n",
    "print(\"=== AN√ÅLISIS DE EDAD ===\")\n",
    "print(f\"\\nEstad√≠sticas:\")\n",
    "print(df_clean['edad'].describe())\n",
    "\n",
    "# Visualizar distribuci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma\n",
    "df_clean['edad'].hist(bins=30, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Distribuci√≥n de Edad')\n",
    "axes[0].set_xlabel('Edad')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Boxplot\n",
    "df_clean.boxplot(column='edad', ax=axes[1])\n",
    "axes[1].set_title('Boxplot de Edad')\n",
    "axes[1].set_ylabel('Edad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar valores problem√°ticos\n",
    "print(f\"\\nValores negativos: {(df_clean['edad'] < 0).sum()}\")\n",
    "print(f\"Valores > 100: {(df_clean['edad'] > 100).sum()}\")\n",
    "print(f\"Valores nulos: {df_clean['edad'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891ec79",
   "metadata": {},
   "source": [
    "### üìà An√°lisis de Outliers en Datos Num√©ricos\n",
    "\n",
    "**Concepto:** Los outliers (valores at√≠picos) son observaciones que se desv√≠an significativamente del resto.\n",
    "\n",
    "**T√©cnicas de detecci√≥n:**\n",
    "- **Histograma:** muestra distribuci√≥n completa de valores\n",
    "- **Boxplot:** identifica valores fuera del rango IQR (Q1-1.5*IQR, Q3+1.5*IQR)\n",
    "- **Estad√≠sticas:** min, max, percentiles revelan valores extremos\n",
    "\n",
    "**Decisiones:**\n",
    "- ¬øSon errores de captura? ‚Üí Eliminar o corregir\n",
    "- ¬øSon valores leg√≠timos? ‚Üí Mantener pero documentar\n",
    "- ¬øAfectan el an√°lisis? ‚Üí Considerar transformaciones o tratamiento especial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar edad: establecer l√≠mites razonables\n",
    "print(\"=== LIMPIEZA DE EDAD ===\")\n",
    "\n",
    "# Convertir valores fuera de rango a NaN\n",
    "df_clean.loc[(df_clean['edad'] < 18) | (df_clean['edad'] > 100), 'edad'] = np.nan\n",
    "\n",
    "# Imputar valores faltantes con la mediana\n",
    "mediana_edad = df_clean['edad'].median()\n",
    "df_clean['edad'].fillna(mediana_edad, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: {mediana_edad}\")\n",
    "print(f\"\\nNueva distribuci√≥n:\")\n",
    "print(df_clean['edad'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca02c6",
   "metadata": {},
   "source": [
    "### üîß Imputaci√≥n de Valores Faltantes\n",
    "\n",
    "**Concepto:** Imputar significa reemplazar valores nulos con valores estimados para no perder datos.\n",
    "\n",
    "**Estrategias comunes:**\n",
    "- **Media:** `mean()` - sensible a outliers, buena para distribuciones normales\n",
    "- **Mediana:** `median()` - robusta a outliers, recomendada para datos sesgados\n",
    "- **Moda:** `mode()` - para datos categ√≥ricos\n",
    "- **Forward/Backward Fill:** para series temporales\n",
    "- **Modelos predictivos:** ML avanzado para imputaci√≥n\n",
    "\n",
    "**Elecci√≥n:** Depende de la distribuci√≥n de datos y del contexto del negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff06455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== AN√ÅLISIS DE SALARIO ===\")\n",
    "print(f\"\\nEstad√≠sticas:\")\n",
    "print(df_clean['salario'].describe())\n",
    "\n",
    "# Identificar outliers con IQR\n",
    "Q1 = df_clean['salario'].quantile(0.25)\n",
    "Q3 = df_clean['salario'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nRango IQR:\")\n",
    "print(f\"  L√≠mite inferior: {limite_inferior:,.2f}\")\n",
    "print(f\"  L√≠mite superior: {limite_superior:,.2f}\")\n",
    "\n",
    "outliers = df_clean[(df_clean['salario'] < limite_inferior) | (df_clean['salario'] > limite_superior)]\n",
    "print(f\"\\nOutliers detectados: {len(outliers)}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df_clean['salario'].hist(bins=30, color='lightgreen')\n",
    "plt.title('Distribuci√≥n de Salario')\n",
    "plt.xlabel('Salario')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_clean.boxplot(column='salario')\n",
    "plt.title('Boxplot de Salario')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9dd57",
   "metadata": {},
   "source": [
    "### üìä M√©todo IQR para Detecci√≥n de Outliers\n",
    "\n",
    "**Concepto:** El Rango Intercuart√≠lico (IQR) es una medida robusta para identificar valores at√≠picos.\n",
    "\n",
    "**C√°lculo:**\n",
    "- IQR = Q3 - Q1 (diferencia entre cuartil 75 y cuartil 25)\n",
    "- L√≠mite inferior = Q1 - 1.5 √ó IQR\n",
    "- L√≠mite superior = Q3 + 1.5 √ó IQR\n",
    "- Valores fuera de estos l√≠mites se consideran outliers\n",
    "\n",
    "**Ventaja:** Menos sensible a valores extremos que m√©todos basados en desviaci√≥n est√°ndar.\n",
    "\n",
    "**Uso:** Est√°ndar en data science para limpieza de datos num√©ricos antes de modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== LIMPIEZA DE SALARIO ===\")\n",
    "\n",
    "# Convertir valores negativos y outliers extremos a NaN\n",
    "df_clean.loc[(df_clean['salario'] < 0) | (df_clean['salario'] > 200000), 'salario'] = np.nan\n",
    "\n",
    "# Imputar con la mediana\n",
    "mediana_salario = df_clean['salario'].median()\n",
    "df_clean['salario'].fillna(mediana_salario, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: ${mediana_salario:,.2f}\")\n",
    "print(f\"\\nNueva distribuci√≥n:\")\n",
    "print(df_clean['salario'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca5222",
   "metadata": {},
   "source": [
    "### 3.4. Validaci√≥n de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear reporte de calidad\n",
    "print(\"=== REPORTE DE CALIDAD DE DATOS ===\")\n",
    "\n",
    "quality_report = pd.DataFrame({\n",
    "    'Columna': df_clean.columns,\n",
    "    'Tipo': df_clean.dtypes.values,\n",
    "    'Nulos': df_clean.isnull().sum().values,\n",
    "    '% Nulos': (df_clean.isnull().sum() / len(df_clean) * 100).values.round(2),\n",
    "    '√önicos': [df_clean[col].nunique() for col in df_clean.columns]\n",
    "})\n",
    "\n",
    "print(quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb89e05",
   "metadata": {},
   "source": [
    "### ‚úÖ Reporte de Calidad de Datos\n",
    "\n",
    "**Concepto:** Un reporte de calidad resume el estado del dataset tras la limpieza.\n",
    "\n",
    "**M√©tricas clave:**\n",
    "- **Tipo de dato:** verifica conversiones correctas\n",
    "- **Nulos:** identifica columnas con datos faltantes restantes\n",
    "- **√önicos:** eval√∫a cardinalidad y posibles errores de captura\n",
    "- **Porcentaje nulos:** prioriza columnas problem√°ticas\n",
    "\n",
    "**Objetivo:** Documentar la calidad final y detectar problemas residuales antes de an√°lisis o modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n antes y despu√©s\n",
    "print(\"=== COMPARACI√ìN ANTES Y DESPU√âS ===\")\n",
    "print(f\"\\nRegistros originales: {len(df_dirty)}\")\n",
    "print(f\"Registros limpios: {len(df_clean)}\")\n",
    "print(f\"Registros eliminados: {len(df_dirty) - len(df_clean)}\")\n",
    "\n",
    "print(f\"\\nValores nulos originales: {df_dirty.isnull().sum().sum()}\")\n",
    "print(f\"Valores nulos despu√©s: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a865b1b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering B√°sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas caracter√≠sticas\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Categor√≠a de edad\n",
    "df_clean['categoria_edad'] = pd.cut(\n",
    "    df_clean['edad'],\n",
    "    bins=[0, 25, 35, 50, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '51+']\n",
    ")\n",
    "\n",
    "# Categor√≠a de salario\n",
    "df_clean['categoria_salario'] = pd.cut(\n",
    "    df_clean['salario'],\n",
    "    bins=[0, 40000, 60000, 100000],\n",
    "    labels=['Bajo', 'Medio', 'Alto']\n",
    ")\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_clean['a√±o_registro'] = df_clean['fecha_registro'].dt.year\n",
    "df_clean['mes_registro'] = df_clean['fecha_registro'].dt.month\n",
    "df_clean['dias_desde_registro'] = (datetime.now() - df_clean['fecha_registro']).dt.days\n",
    "\n",
    "print(\"\\nNuevas caracter√≠sticas creadas:\")\n",
    "print(df_clean[[\n",
    "    'edad', 'categoria_edad',\n",
    "    'salario', 'categoria_salario',\n",
    "    'fecha_registro', 'dias_desde_registro'\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09911ac9",
   "metadata": {},
   "source": [
    "### üî® Feature Engineering: Creaci√≥n de Variables\n",
    "\n",
    "**Concepto:** Feature Engineering es crear nuevas caracter√≠sticas a partir de datos existentes para mejorar an√°lisis.\n",
    "\n",
    "**T√©cnicas aplicadas:**\n",
    "1. **Binning/Discretizaci√≥n:** `pd.cut()` convierte variables continuas en categor√≠as\n",
    "2. **Extracci√≥n temporal:** `.dt.year`, `.dt.month` extraen componentes de fechas\n",
    "3. **C√°lculo de diferencias:** d√≠as desde un evento, tiempo transcurrido\n",
    "\n",
    "**Beneficios:**\n",
    "- Facilita interpretaci√≥n de an√°lisis\n",
    "- Mejora rendimiento de modelos de ML\n",
    "- Descubre patrones ocultos en los datos\n",
    "\n",
    "**Nota:** Este es el puente entre limpieza de datos y an√°lisis avanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56ef12",
   "metadata": {},
   "source": [
    "## 5. Exportar Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio\n",
    "output_path = '../../datasets/processed/datos_limpios.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset limpio guardado en: {output_path}\")\n",
    "print(f\"\\nShape final: {df_clean.shape}\")\n",
    "print(f\"\\nColumnas finales:\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefa66e",
   "metadata": {},
   "source": [
    "### üíæ Exportaci√≥n de Datos Procesados\n",
    "\n",
    "**Concepto:** Guardar datos limpios para reutilizaci√≥n sin repetir todo el proceso de limpieza.\n",
    "\n",
    "**Formato CSV:**\n",
    "- Universal y legible por humanos\n",
    "- `index=False`: evita guardar √≠ndice como columna\n",
    "- Compresi√≥n opcional: `compression='gzip'` para datasets grandes\n",
    "\n",
    "**Buenas pr√°cticas:**\n",
    "- Guardar en carpeta `processed/` separada de datos `raw/`\n",
    "- Mantener datos originales intactos (trazabilidad)\n",
    "- Documentar transformaciones aplicadas (metadatos)\n",
    "\n",
    "**Pr√≥ximo paso:** Datos listos para an√°lisis exploratorio, visualizaci√≥n o modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5edd",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Pr√°cticas\n",
    "\n",
    "### Proceso de Limpieza:\n",
    "1. **An√°lisis exploratorio**: Identificar problemas de calidad\n",
    "2. **Eliminar duplicados**: Asegurar unicidad de registros\n",
    "3. **Normalizar texto**: Estandarizar formato de strings\n",
    "4. **Validar datos**: Verificar rangos y formatos\n",
    "5. **Manejar outliers**: Detectar y tratar valores at√≠picos\n",
    "6. **Imputar valores faltantes**: Estrategia seg√∫n el contexto\n",
    "7. **Feature engineering**: Crear caracter√≠sticas derivadas\n",
    "\n",
    "### Mejores Pr√°cticas:\n",
    "- Siempre trabajar en una copia del dataset original\n",
    "- Documentar cada paso de limpieza\n",
    "- Validar resultados despu√©s de cada transformaci√≥n\n",
    "- Considerar el contexto del negocio para decisiones de limpieza\n",
    "- Automatizar el proceso con funciones reutilizables\n",
    "- Mantener registro de cambios (auditor√≠a)\n",
    "\n",
    "### T√©cnicas de Imputaci√≥n:\n",
    "- **Media/Mediana**: Para datos num√©ricos\n",
    "- **Moda**: Para datos categ√≥ricos\n",
    "- **Forward/Backward fill**: Para series temporales\n",
    "- **Interpolaci√≥n**: Para datos secuenciales\n",
    "- **Valor constante**: Cuando tiene sentido de negocio\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Pandas Data Cleaning](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Data Quality Best Practices](https://www.dataversity.net/data-quality-best-practices/)\n",
    "- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
