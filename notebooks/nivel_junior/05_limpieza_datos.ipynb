{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ff0e39",
   "metadata": {},
   "source": [
    "# Limpieza y Preparación de Datos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Identificar y manejar datos faltantes\n",
    "- Detectar y tratar valores atípicos (outliers)\n",
    "- Eliminar duplicados y normalizar datos\n",
    "- Validar la calidad de los datos\n",
    "- Aplicar técnicas de feature engineering básico\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c70a",
   "metadata": {},
   "source": [
    "## 1. Crear Dataset con Problemas de Calidad\n",
    "\n",
    "Vamos a crear un dataset con varios problemas típicos de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con problemas\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records = 1000\n",
    "\n",
    "df_dirty = pd.DataFrame({\n",
    "    'id': range(1, n_records + 1),\n",
    "    'nombre': [\n",
    "        np.random.choice(['  Juan  ', 'MARÍA', 'pedro', 'Ana López', None, 'Carlos', 'Laura '])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'email': [\n",
    "        np.random.choice([\n",
    "            'juan@email.com', 'maria@test.com', 'invalido', None, 'pedro@example.com',\n",
    "            'ANA@EMAIL.COM', 'carlos@test', '  laura@email.com  '\n",
    "        ])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'edad': np.random.choice([25, 30, None, -5, 35, 150, 28, 40], n_records),\n",
    "    'salario': np.random.choice(\n",
    "        [30000, 45000, None, -1000, 60000, 1000000, 55000, 70000],\n",
    "        n_records\n",
    "    ),\n",
    "    'fecha_registro': pd.date_range('2020-01-01', periods=n_records, freq='D'),\n",
    "    'ciudad': np.random.choice(['Madrid', 'madrid', 'BARCELONA', 'Barcelona', None], n_records),\n",
    "    'pais': np.random.choice(['España', 'spain', 'ESPAÑA', None], n_records)\n",
    "})\n",
    "\n",
    "# Agregar algunos duplicados\n",
    "df_dirty = pd.concat([df_dirty, df_dirty.iloc[:10]], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset creado con {len(df_dirty)} registros\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_dirty.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e45d1",
   "metadata": {},
   "source": [
    "## 2. Análisis Exploratorio de Problemas de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general\n",
    "print(\"=== INFORMACIÓN DEL DATASET ===\")\n",
    "print(f\"\\nShape: {df_dirty.shape}\")\n",
    "print(f\"\\nColumnas: {df_dirty.columns.tolist()}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_dirty.dtypes)\n",
    "\n",
    "print(f\"\\n=== ESTADÍSTICAS DESCRIPTIVAS ===\")\n",
    "df_dirty.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores nulos\n",
    "print(\"=== VALORES NULOS ===\")\n",
    "null_counts = df_dirty.isnull().sum()\n",
    "null_percentages = (null_counts / len(df_dirty) * 100).round(2)\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'Nulos': null_counts,\n",
    "    'Porcentaje': null_percentages\n",
    "})\n",
    "\n",
    "print(null_summary[null_summary['Nulos'] > 0])\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "null_summary['Porcentaje'].plot(kind='bar', color='coral')\n",
    "plt.title('Porcentaje de Valores Nulos por Columna')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.xlabel('Columna')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de duplicados\n",
    "print(\"=== ANÁLISIS DE DUPLICADOS ===\")\n",
    "duplicates = df_dirty.duplicated()\n",
    "print(f\"\\nTotal de filas duplicadas: {duplicates.sum()}\")\n",
    "print(f\"Porcentaje: {(duplicates.sum() / len(df_dirty) * 100):.2f}%\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"\\nEjemplos de duplicados:\")\n",
    "    print(df_dirty[duplicates].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857df1f8",
   "metadata": {},
   "source": [
    "## 3. Limpieza de Datos Paso a Paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para limpiar\n",
    "df_clean = df_dirty.copy()\n",
    "\n",
    "print(f\"Dataset original: {len(df_dirty)} filas\")\n",
    "print(f\"Iniciando limpieza...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f73d9",
   "metadata": {},
   "source": [
    "### 3.1. Eliminar Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados completos\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after = len(df_clean)\n",
    "\n",
    "print(f\"Duplicados eliminados: {before - after}\")\n",
    "print(f\"Registros restantes: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3c63c",
   "metadata": {},
   "source": [
    "### 3.2. Limpiar Columnas de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar nombres\n",
    "print(\"=== LIMPIEZA DE NOMBRES ===\")\n",
    "print(\"\\nAntes:\")\n",
    "print(df_clean['nombre'].value_counts())\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_clean['nombre'] = df_clean['nombre'].str.strip().str.title()\n",
    "\n",
    "print(\"\\nDespués:\")\n",
    "print(df_clean['nombre'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar emails\n",
    "print(\"=== LIMPIEZA DE EMAILS ===\")\n",
    "\n",
    "# Normalizar: quitar espacios y convertir a minúsculas\n",
    "df_clean['email'] = df_clean['email'].str.strip().str.lower()\n",
    "\n",
    "# Validar formato de email\n",
    "import re\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if pd.isna(email):\n",
    "        return False\n",
    "    return bool(re.match(email_pattern, email))\n",
    "\n",
    "df_clean['email_valido'] = df_clean['email'].apply(is_valid_email)\n",
    "\n",
    "print(f\"\\nEmails válidos: {df_clean['email_valido'].sum()}\")\n",
    "print(f\"Emails inválidos: {(~df_clean['email_valido']).sum()}\")\n",
    "\n",
    "print(\"\\nEjemplos de emails inválidos:\")\n",
    "print(df_clean[~df_clean['email_valido']]['email'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd348517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar ciudades y países\n",
    "print(\"=== NORMALIZACIÓN DE CIUDADES Y PAÍSES ===\")\n",
    "\n",
    "# Ciudades\n",
    "df_clean['ciudad'] = df_clean['ciudad'].str.strip().str.title()\n",
    "print(\"\\nCiudades únicas:\")\n",
    "print(df_clean['ciudad'].value_counts())\n",
    "\n",
    "# Países\n",
    "df_clean['pais'] = df_clean['pais'].str.strip().str.title()\n",
    "print(\"\\nPaíses únicos:\")\n",
    "print(df_clean['pais'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303801d2",
   "metadata": {},
   "source": [
    "### 3.3. Limpiar Datos Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ad56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar edad\n",
    "print(\"=== ANÁLISIS DE EDAD ===\")\n",
    "print(f\"\\nEstadísticas:\")\n",
    "print(df_clean['edad'].describe())\n",
    "\n",
    "# Visualizar distribución\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma\n",
    "df_clean['edad'].hist(bins=30, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Distribución de Edad')\n",
    "axes[0].set_xlabel('Edad')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Boxplot\n",
    "df_clean.boxplot(column='edad', ax=axes[1])\n",
    "axes[1].set_title('Boxplot de Edad')\n",
    "axes[1].set_ylabel('Edad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar valores problemáticos\n",
    "print(f\"\\nValores negativos: {(df_clean['edad'] < 0).sum()}\")\n",
    "print(f\"Valores > 100: {(df_clean['edad'] > 100).sum()}\")\n",
    "print(f\"Valores nulos: {df_clean['edad'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar edad: establecer límites razonables\n",
    "print(\"=== LIMPIEZA DE EDAD ===\")\n",
    "\n",
    "# Convertir valores fuera de rango a NaN\n",
    "df_clean.loc[(df_clean['edad'] < 18) | (df_clean['edad'] > 100), 'edad'] = np.nan\n",
    "\n",
    "# Imputar valores faltantes con la mediana\n",
    "mediana_edad = df_clean['edad'].median()\n",
    "df_clean['edad'].fillna(mediana_edad, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: {mediana_edad}\")\n",
    "print(f\"\\nNueva distribución:\")\n",
    "print(df_clean['edad'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff06455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== ANÁLISIS DE SALARIO ===\")\n",
    "print(f\"\\nEstadísticas:\")\n",
    "print(df_clean['salario'].describe())\n",
    "\n",
    "# Identificar outliers con IQR\n",
    "Q1 = df_clean['salario'].quantile(0.25)\n",
    "Q3 = df_clean['salario'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nRango IQR:\")\n",
    "print(f\"  Límite inferior: {limite_inferior:,.2f}\")\n",
    "print(f\"  Límite superior: {limite_superior:,.2f}\")\n",
    "\n",
    "outliers = df_clean[(df_clean['salario'] < limite_inferior) | (df_clean['salario'] > limite_superior)]\n",
    "print(f\"\\nOutliers detectados: {len(outliers)}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df_clean['salario'].hist(bins=30, color='lightgreen')\n",
    "plt.title('Distribución de Salario')\n",
    "plt.xlabel('Salario')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_clean.boxplot(column='salario')\n",
    "plt.title('Boxplot de Salario')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== LIMPIEZA DE SALARIO ===\")\n",
    "\n",
    "# Convertir valores negativos y outliers extremos a NaN\n",
    "df_clean.loc[(df_clean['salario'] < 0) | (df_clean['salario'] > 200000), 'salario'] = np.nan\n",
    "\n",
    "# Imputar con la mediana\n",
    "mediana_salario = df_clean['salario'].median()\n",
    "df_clean['salario'].fillna(mediana_salario, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: ${mediana_salario:,.2f}\")\n",
    "print(f\"\\nNueva distribución:\")\n",
    "print(df_clean['salario'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca5222",
   "metadata": {},
   "source": [
    "### 3.4. Validación de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear reporte de calidad\n",
    "print(\"=== REPORTE DE CALIDAD DE DATOS ===\")\n",
    "\n",
    "quality_report = pd.DataFrame({\n",
    "    'Columna': df_clean.columns,\n",
    "    'Tipo': df_clean.dtypes.values,\n",
    "    'Nulos': df_clean.isnull().sum().values,\n",
    "    '% Nulos': (df_clean.isnull().sum() / len(df_clean) * 100).values.round(2),\n",
    "    'Únicos': [df_clean[col].nunique() for col in df_clean.columns]\n",
    "})\n",
    "\n",
    "print(quality_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación antes y después\n",
    "print(\"=== COMPARACIÓN ANTES Y DESPUÉS ===\")\n",
    "print(f\"\\nRegistros originales: {len(df_dirty)}\")\n",
    "print(f\"Registros limpios: {len(df_clean)}\")\n",
    "print(f\"Registros eliminados: {len(df_dirty) - len(df_clean)}\")\n",
    "\n",
    "print(f\"\\nValores nulos originales: {df_dirty.isnull().sum().sum()}\")\n",
    "print(f\"Valores nulos después: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a865b1b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas características\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Categoría de edad\n",
    "df_clean['categoria_edad'] = pd.cut(\n",
    "    df_clean['edad'],\n",
    "    bins=[0, 25, 35, 50, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '51+']\n",
    ")\n",
    "\n",
    "# Categoría de salario\n",
    "df_clean['categoria_salario'] = pd.cut(\n",
    "    df_clean['salario'],\n",
    "    bins=[0, 40000, 60000, 100000],\n",
    "    labels=['Bajo', 'Medio', 'Alto']\n",
    ")\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_clean['año_registro'] = df_clean['fecha_registro'].dt.year\n",
    "df_clean['mes_registro'] = df_clean['fecha_registro'].dt.month\n",
    "df_clean['dias_desde_registro'] = (datetime.now() - df_clean['fecha_registro']).dt.days\n",
    "\n",
    "print(\"\\nNuevas características creadas:\")\n",
    "print(df_clean[[\n",
    "    'edad', 'categoria_edad',\n",
    "    'salario', 'categoria_salario',\n",
    "    'fecha_registro', 'dias_desde_registro'\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56ef12",
   "metadata": {},
   "source": [
    "## 5. Exportar Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio\n",
    "output_path = '../../datasets/processed/datos_limpios.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset limpio guardado en: {output_path}\")\n",
    "print(f\"\\nShape final: {df_clean.shape}\")\n",
    "print(f\"\\nColumnas finales:\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5edd",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Prácticas\n",
    "\n",
    "### Proceso de Limpieza:\n",
    "1. **Análisis exploratorio**: Identificar problemas de calidad\n",
    "2. **Eliminar duplicados**: Asegurar unicidad de registros\n",
    "3. **Normalizar texto**: Estandarizar formato de strings\n",
    "4. **Validar datos**: Verificar rangos y formatos\n",
    "5. **Manejar outliers**: Detectar y tratar valores atípicos\n",
    "6. **Imputar valores faltantes**: Estrategia según el contexto\n",
    "7. **Feature engineering**: Crear características derivadas\n",
    "\n",
    "### Mejores Prácticas:\n",
    "- Siempre trabajar en una copia del dataset original\n",
    "- Documentar cada paso de limpieza\n",
    "- Validar resultados después de cada transformación\n",
    "- Considerar el contexto del negocio para decisiones de limpieza\n",
    "- Automatizar el proceso con funciones reutilizables\n",
    "- Mantener registro de cambios (auditoría)\n",
    "\n",
    "### Técnicas de Imputación:\n",
    "- **Media/Mediana**: Para datos numéricos\n",
    "- **Moda**: Para datos categóricos\n",
    "- **Forward/Backward fill**: Para series temporales\n",
    "- **Interpolación**: Para datos secuenciales\n",
    "- **Valor constante**: Cuando tiene sentido de negocio\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Pandas Data Cleaning](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Data Quality Best Practices](https://www.dataversity.net/data-quality-best-practices/)\n",
    "- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
