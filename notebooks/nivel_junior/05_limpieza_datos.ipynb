{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ff0e39",
   "metadata": {},
   "source": [
    "# Limpieza y Preparaci\u00f3n de Datos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Identificar y manejar datos faltantes\n",
    "- Detectar y tratar valores at\u00edpicos (outliers)\n",
    "- Eliminar duplicados y normalizar datos\n",
    "- Validar la calidad de los datos\n",
    "- Aplicar t\u00e9cnicas de feature engineering b\u00e1sico\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci\u00f3n de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci\u00f3n\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Librer\u00edas cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c70a",
   "metadata": {},
   "source": [
    "## 1. Crear Dataset con Problemas de Calidad\n",
    "\n",
    "Vamos a crear un dataset con varios problemas t\u00edpicos de calidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f335ac4",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 Detectando Problemas de Calidad de Datos\n",
    "\n",
    "**Los datos del mundo real son SUCIOS:**\n",
    "- **70-80% del tiempo** en proyectos de datos se gasta en limpieza\n",
    "- Sin calidad de datos \u2192 an\u00e1lisis incorrecto \u2192 malas decisiones\n",
    "\n",
    "**Problemas comunes:**\n",
    "1. **Valores nulos** (NaN, None, NULL)\n",
    "2. **Duplicados** (registros repetidos)\n",
    "3. **Tipos incorrectos** (fechas como strings, n\u00fameros como text)\n",
    "4. **Outliers** (valores extremos an\u00f3malos)\n",
    "5. **Inconsistencias** (Madrid vs madrid vs MADRID)\n",
    "6. **Formato incorrecto** (fechas en diferentes formatos)\n",
    "\n",
    "**Pipeline de inspecci\u00f3n:**\n",
    "```python\n",
    "df.info()                # Tipos y nulos\n",
    "df.describe()            # Estad\u00edsticas (detectar outliers)\n",
    "df.isnull().sum()        # Contar nulos por columna\n",
    "df.duplicated().sum()    # Contar duplicados\n",
    "df.dtypes                # Verificar tipos\n",
    "```\n",
    "\n",
    "Este notebook te ense\u00f1a a detectar y corregir cada uno de estos problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con problemas\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records = 1000\n",
    "\n",
    "df_dirty = pd.DataFrame({\n",
    "    'id': range(1, n_records + 1),\n",
    "    'nombre': [\n",
    "        np.random.choice(['  Juan  ', 'MAR\u00cdA', 'pedro', 'Ana L\u00f3pez', None, 'Carlos', 'Laura '])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'email': [\n",
    "        np.random.choice([\n",
    "            'juan@email.com', 'maria@test.com', 'invalido', None, 'pedro@example.com',\n",
    "            'ANA@EMAIL.COM', 'carlos@test', '  laura@email.com  '\n",
    "        ])\n",
    "        for _ in range(n_records)\n",
    "    ],\n",
    "    'edad': np.random.choice([25, 30, None, -5, 35, 150, 28, 40], n_records),\n",
    "    'salario': np.random.choice(\n",
    "        [30000, 45000, None, -1000, 60000, 1000000, 55000, 70000],\n",
    "        n_records\n",
    "    ),\n",
    "    'fecha_registro': pd.date_range('2020-01-01', periods=n_records, freq='D'),\n",
    "    'ciudad': np.random.choice(['Madrid', 'madrid', 'BARCELONA', 'Barcelona', None], n_records),\n",
    "    'pais': np.random.choice(['Espa\u00f1a', 'spain', 'ESPA\u00d1A', None], n_records)\n",
    "})\n",
    "\n",
    "# Agregar algunos duplicados\n",
    "df_dirty = pd.concat([df_dirty, df_dirty.iloc[:10]], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset creado con {len(df_dirty)} registros\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_dirty.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e45d1",
   "metadata": {},
   "source": [
    "## 2. An\u00e1lisis Exploratorio de Problemas de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci\u00f3n general\n",
    "print(\"=== INFORMACI\u00d3N DEL DATASET ===\")\n",
    "print(f\"\\nShape: {df_dirty.shape}\")\n",
    "print(f\"\\nColumnas: {df_dirty.columns.tolist()}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_dirty.dtypes)\n",
    "\n",
    "print(f\"\\n=== ESTAD\u00cdSTICAS DESCRIPTIVAS ===\")\n",
    "df_dirty.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2a80b",
   "metadata": {},
   "source": [
    "### \ud83d\udcca An\u00e1lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "**Concepto:** Antes de limpiar datos, debemos entender qu\u00e9 problemas existen mediante un an\u00e1lisis exploratorio.\n",
    "\n",
    "**T\u00e9cnicas clave:**\n",
    "- `info()`: estructura, tipos de datos, valores no nulos\n",
    "- `describe()`: estad\u00edsticas descriptivas de columnas num\u00e9ricas\n",
    "- `shape`: dimensiones del dataset (filas, columnas)\n",
    "- `dtypes`: tipos de datos de cada columna\n",
    "\n",
    "**Objetivo:** Identificar patrones, anomal\u00edas y problemas de calidad antes de aplicar transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis de valores nulos\n",
    "print(\"=== VALORES NULOS ===\")\n",
    "null_counts = df_dirty.isnull().sum()\n",
    "null_percentages = (null_counts / len(df_dirty) * 100).round(2)\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'Nulos': null_counts,\n",
    "    'Porcentaje': null_percentages\n",
    "})\n",
    "\n",
    "print(null_summary[null_summary['Nulos'] > 0])\n",
    "\n",
    "# Visualizaci\u00f3n\n",
    "plt.figure(figsize=(10, 6))\n",
    "null_summary['Porcentaje'].plot(kind='bar', color='coral')\n",
    "plt.title('Porcentaje de Valores Nulos por Columna')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.xlabel('Columna')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b792c",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d Detecci\u00f3n de Valores Nulos\n",
    "\n",
    "**Concepto:** Los valores nulos (NaN, None, NULL) representan ausencia de datos y pueden afectar an\u00e1lisis y modelos.\n",
    "\n",
    "**M\u00e9todos importantes:**\n",
    "- `isnull()` / `isna()`: detecta valores nulos\n",
    "- `sum()`: cuenta nulos por columna\n",
    "- Porcentajes ayudan a decidir si eliminar o imputar\n",
    "\n",
    "**Buena pr\u00e1ctica:** Visualizar el porcentaje de nulos por columna para priorizar acciones de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis de duplicados\n",
    "print(\"=== AN\u00c1LISIS DE DUPLICADOS ===\")\n",
    "duplicates = df_dirty.duplicated()\n",
    "print(f\"\\nTotal de filas duplicadas: {duplicates.sum()}\")\n",
    "print(f\"Porcentaje: {(duplicates.sum() / len(df_dirty) * 100):.2f}%\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"\\nEjemplos de duplicados:\")\n",
    "    print(df_dirty[duplicates].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074968a",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 Detecci\u00f3n de Duplicados\n",
    "\n",
    "**Concepto:** Registros duplicados distorsionan estad\u00edsticas y an\u00e1lisis, generando sesgo en los resultados.\n",
    "\n",
    "**M\u00e9todo clave:**\n",
    "- `duplicated()`: retorna boolean indicando si una fila es duplicado\n",
    "- Por defecto considera todas las columnas\n",
    "- Puede configurarse con `subset=` para columnas espec\u00edficas\n",
    "\n",
    "**Decisi\u00f3n:** Antes de eliminar, verificar si los duplicados son errores reales o datos leg\u00edtimos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857df1f8",
   "metadata": {},
   "source": [
    "## 3. Limpieza de Datos Paso a Paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para limpiar\n",
    "df_clean = df_dirty.copy()\n",
    "\n",
    "print(f\"Dataset original: {len(df_dirty)} filas\")\n",
    "print(f\"Iniciando limpieza...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f73d9",
   "metadata": {},
   "source": [
    "### 3.1. Eliminar Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados completos\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after = len(df_clean)\n",
    "\n",
    "print(f\"Duplicados eliminados: {before - after}\")\n",
    "print(f\"Registros restantes: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d731990",
   "metadata": {},
   "source": [
    "### \u2702\ufe0f Eliminaci\u00f3n de Duplicados\n",
    "\n",
    "**Concepto:** `drop_duplicates()` elimina filas duplicadas, manteniendo la primera ocurrencia por defecto.\n",
    "\n",
    "**Par\u00e1metros importantes:**\n",
    "- `subset=`: especifica columnas para evaluar duplicaci\u00f3n\n",
    "- `keep='first'|'last'|False`: qu\u00e9 duplicado mantener\n",
    "- `inplace=True`: modifica el DataFrame original\n",
    "\n",
    "**Impacto:** Reduce ruido y mejora calidad de datos, especialmente en agregaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3c63c",
   "metadata": {},
   "source": [
    "### 3.2. Limpiar Columnas de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar nombres\n",
    "print(\"=== LIMPIEZA DE NOMBRES ===\")\n",
    "print(\"\\nAntes:\")\n",
    "print(df_clean['nombre'].value_counts())\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_clean['nombre'] = df_clean['nombre'].str.strip().str.title()\n",
    "\n",
    "print(\"\\nDespu\u00e9s:\")\n",
    "print(df_clean['nombre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482c59c",
   "metadata": {},
   "source": [
    "### \ud83d\udd24 Limpieza de Datos de Texto\n",
    "\n",
    "**Concepto:** Datos de texto requieren normalizaci\u00f3n para estandarizar formatos y facilitar an\u00e1lisis.\n",
    "\n",
    "**Operaciones comunes:**\n",
    "- `str.strip()`: elimina espacios al inicio y final\n",
    "- `str.title()`: capitaliza primera letra de cada palabra\n",
    "- `str.lower()` / `str.upper()`: conversi\u00f3n de may\u00fasculas/min\u00fasculas\n",
    "- `str.replace()`: reemplaza patrones de texto\n",
    "\n",
    "**Uso:** Esencial para nombres, direcciones, categor\u00edas y cualquier campo de texto libre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar emails\n",
    "print(\"=== LIMPIEZA DE EMAILS ===\")\n",
    "\n",
    "# Normalizar: quitar espacios y convertir a min\u00fasculas\n",
    "df_clean['email'] = df_clean['email'].str.strip().str.lower()\n",
    "\n",
    "# Validar formato de email\n",
    "import re\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if pd.isna(email):\n",
    "        return False\n",
    "    return bool(re.match(email_pattern, email))\n",
    "\n",
    "df_clean['email_valido'] = df_clean['email'].apply(is_valid_email)\n",
    "\n",
    "print(f\"\\nEmails v\u00e1lidos: {df_clean['email_valido'].sum()}\")\n",
    "print(f\"Emails inv\u00e1lidos: {(~df_clean['email_valido']).sum()}\")\n",
    "\n",
    "print(\"\\nEjemplos de emails inv\u00e1lidos:\")\n",
    "print(df_clean[~df_clean['email_valido']]['email'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509ca60",
   "metadata": {},
   "source": [
    "### \ud83d\udce7 Validaci\u00f3n de Formatos con Regex\n",
    "\n",
    "**Concepto:** Las expresiones regulares (regex) permiten validar patrones complejos como emails, tel\u00e9fonos, URLs.\n",
    "\n",
    "**Patr\u00f3n email:** `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`\n",
    "- `^`: inicio de string\n",
    "- `[...]+`: uno o m\u00e1s caracteres permitidos\n",
    "- `@`: arroba obligatoria\n",
    "- `\\.`: punto literal (escapado)\n",
    "- `{2,}`: m\u00ednimo 2 caracteres para dominio\n",
    "\n",
    "**Uso:** Crear columnas booleanas indicando validez facilita filtrado y an\u00e1lisis de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd348517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar ciudades y pa\u00edses\n",
    "print(\"=== NORMALIZACI\u00d3N DE CIUDADES Y PA\u00cdSES ===\")\n",
    "\n",
    "# Ciudades\n",
    "df_clean['ciudad'] = df_clean['ciudad'].str.strip().str.title()\n",
    "print(\"\\nCiudades \u00fanicas:\")\n",
    "print(df_clean['ciudad'].value_counts())\n",
    "\n",
    "# Pa\u00edses\n",
    "df_clean['pais'] = df_clean['pais'].str.strip().str.title()\n",
    "print(\"\\nPa\u00edses \u00fanicos:\")\n",
    "print(df_clean['pais'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303801d2",
   "metadata": {},
   "source": [
    "### 3.3. Limpiar Datos Num\u00e9ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ad56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar edad\n",
    "print(\"=== AN\u00c1LISIS DE EDAD ===\")\n",
    "print(f\"\\nEstad\u00edsticas:\")\n",
    "print(df_clean['edad'].describe())\n",
    "\n",
    "# Visualizar distribuci\u00f3n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma\n",
    "df_clean['edad'].hist(bins=30, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Distribuci\u00f3n de Edad')\n",
    "axes[0].set_xlabel('Edad')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Boxplot\n",
    "df_clean.boxplot(column='edad', ax=axes[1])\n",
    "axes[1].set_title('Boxplot de Edad')\n",
    "axes[1].set_ylabel('Edad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar valores problem\u00e1ticos\n",
    "print(f\"\\nValores negativos: {(df_clean['edad'] < 0).sum()}\")\n",
    "print(f\"Valores > 100: {(df_clean['edad'] > 100).sum()}\")\n",
    "print(f\"Valores nulos: {df_clean['edad'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891ec79",
   "metadata": {},
   "source": [
    "### \ud83d\udcc8 An\u00e1lisis de Outliers en Datos Num\u00e9ricos\n",
    "\n",
    "**Concepto:** Los outliers (valores at\u00edpicos) son observaciones que se desv\u00edan significativamente del resto.\n",
    "\n",
    "**T\u00e9cnicas de detecci\u00f3n:**\n",
    "- **Histograma:** muestra distribuci\u00f3n completa de valores\n",
    "- **Boxplot:** identifica valores fuera del rango IQR (Q1-1.5*IQR, Q3+1.5*IQR)\n",
    "- **Estad\u00edsticas:** min, max, percentiles revelan valores extremos\n",
    "\n",
    "**Decisiones:**\n",
    "- \u00bfSon errores de captura? \u2192 Eliminar o corregir\n",
    "- \u00bfSon valores leg\u00edtimos? \u2192 Mantener pero documentar\n",
    "- \u00bfAfectan el an\u00e1lisis? \u2192 Considerar transformaciones o tratamiento especial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar edad: establecer l\u00edmites razonables\n",
    "print(\"=== LIMPIEZA DE EDAD ===\")\n",
    "\n",
    "# Convertir valores fuera de rango a NaN\n",
    "df_clean.loc[(df_clean['edad'] < 18) | (df_clean['edad'] > 100), 'edad'] = np.nan\n",
    "\n",
    "# Imputar valores faltantes con la mediana\n",
    "mediana_edad = df_clean['edad'].median()\n",
    "df_clean['edad'].fillna(mediana_edad, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: {mediana_edad}\")\n",
    "print(f\"\\nNueva distribuci\u00f3n:\")\n",
    "print(df_clean['edad'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca02c6",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 Imputaci\u00f3n de Valores Faltantes\n",
    "\n",
    "**Concepto:** Imputar significa reemplazar valores nulos con valores estimados para no perder datos.\n",
    "\n",
    "**Estrategias comunes:**\n",
    "- **Media:** `mean()` - sensible a outliers, buena para distribuciones normales\n",
    "- **Mediana:** `median()` - robusta a outliers, recomendada para datos sesgados\n",
    "- **Moda:** `mode()` - para datos categ\u00f3ricos\n",
    "- **Forward/Backward Fill:** para series temporales\n",
    "- **Modelos predictivos:** ML avanzado para imputaci\u00f3n\n",
    "\n",
    "**Elecci\u00f3n:** Depende de la distribuci\u00f3n de datos y del contexto del negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff06455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== AN\u00c1LISIS DE SALARIO ===\")\n",
    "print(f\"\\nEstad\u00edsticas:\")\n",
    "print(df_clean['salario'].describe())\n",
    "\n",
    "# Identificar outliers con IQR\n",
    "Q1 = df_clean['salario'].quantile(0.25)\n",
    "Q3 = df_clean['salario'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nRango IQR:\")\n",
    "print(f\"  L\u00edmite inferior: {limite_inferior:,.2f}\")\n",
    "print(f\"  L\u00edmite superior: {limite_superior:,.2f}\")\n",
    "\n",
    "outliers = df_clean[(df_clean['salario'] < limite_inferior) | (df_clean['salario'] > limite_superior)]\n",
    "print(f\"\\nOutliers detectados: {len(outliers)}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df_clean['salario'].hist(bins=30, color='lightgreen')\n",
    "plt.title('Distribuci\u00f3n de Salario')\n",
    "plt.xlabel('Salario')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_clean.boxplot(column='salario')\n",
    "plt.title('Boxplot de Salario')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9dd57",
   "metadata": {},
   "source": [
    "### \ud83d\udcca M\u00e9todo IQR para Detecci\u00f3n de Outliers\n",
    "\n",
    "**Concepto:** El Rango Intercuart\u00edlico (IQR) es una medida robusta para identificar valores at\u00edpicos.\n",
    "\n",
    "**C\u00e1lculo:**\n",
    "- IQR = Q3 - Q1 (diferencia entre cuartil 75 y cuartil 25)\n",
    "- L\u00edmite inferior = Q1 - 1.5 \u00d7 IQR\n",
    "- L\u00edmite superior = Q3 + 1.5 \u00d7 IQR\n",
    "- Valores fuera de estos l\u00edmites se consideran outliers\n",
    "\n",
    "**Ventaja:** Menos sensible a valores extremos que m\u00e9todos basados en desviaci\u00f3n est\u00e1ndar.\n",
    "\n",
    "**Uso:** Est\u00e1ndar en data science para limpieza de datos num\u00e9ricos antes de modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar salario\n",
    "print(\"=== LIMPIEZA DE SALARIO ===\")\n",
    "\n",
    "# Convertir valores negativos y outliers extremos a NaN\n",
    "df_clean.loc[(df_clean['salario'] < 0) | (df_clean['salario'] > 200000), 'salario'] = np.nan\n",
    "\n",
    "# Imputar con la mediana\n",
    "mediana_salario = df_clean['salario'].median()\n",
    "df_clean['salario'].fillna(mediana_salario, inplace=True)\n",
    "\n",
    "print(f\"\\nValores imputados con mediana: ${mediana_salario:,.2f}\")\n",
    "print(f\"\\nNueva distribuci\u00f3n:\")\n",
    "print(df_clean['salario'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca5222",
   "metadata": {},
   "source": [
    "### 3.4. Validaci\u00f3n de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear reporte de calidad\n",
    "print(\"=== REPORTE DE CALIDAD DE DATOS ===\")\n",
    "\n",
    "quality_report = pd.DataFrame({\n",
    "    'Columna': df_clean.columns,\n",
    "    'Tipo': df_clean.dtypes.values,\n",
    "    'Nulos': df_clean.isnull().sum().values,\n",
    "    '% Nulos': (df_clean.isnull().sum() / len(df_clean) * 100).values.round(2),\n",
    "    '\u00danicos': [df_clean[col].nunique() for col in df_clean.columns]\n",
    "})\n",
    "\n",
    "print(quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb89e05",
   "metadata": {},
   "source": [
    "### \u2705 Reporte de Calidad de Datos\n",
    "\n",
    "**Concepto:** Un reporte de calidad resume el estado del dataset tras la limpieza.\n",
    "\n",
    "**M\u00e9tricas clave:**\n",
    "- **Tipo de dato:** verifica conversiones correctas\n",
    "- **Nulos:** identifica columnas con datos faltantes restantes\n",
    "- **\u00danicos:** eval\u00faa cardinalidad y posibles errores de captura\n",
    "- **Porcentaje nulos:** prioriza columnas problem\u00e1ticas\n",
    "\n",
    "**Objetivo:** Documentar la calidad final y detectar problemas residuales antes de an\u00e1lisis o modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci\u00f3n antes y despu\u00e9s\n",
    "print(\"=== COMPARACI\u00d3N ANTES Y DESPU\u00c9S ===\")\n",
    "print(f\"\\nRegistros originales: {len(df_dirty)}\")\n",
    "print(f\"Registros limpios: {len(df_clean)}\")\n",
    "print(f\"Registros eliminados: {len(df_dirty) - len(df_clean)}\")\n",
    "\n",
    "print(f\"\\nValores nulos originales: {df_dirty.isnull().sum().sum()}\")\n",
    "print(f\"Valores nulos despu\u00e9s: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a865b1b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering B\u00e1sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas caracter\u00edsticas\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Categor\u00eda de edad\n",
    "df_clean['categoria_edad'] = pd.cut(\n",
    "    df_clean['edad'],\n",
    "    bins=[0, 25, 35, 50, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '51+']\n",
    ")\n",
    "\n",
    "# Categor\u00eda de salario\n",
    "df_clean['categoria_salario'] = pd.cut(\n",
    "    df_clean['salario'],\n",
    "    bins=[0, 40000, 60000, 100000],\n",
    "    labels=['Bajo', 'Medio', 'Alto']\n",
    ")\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_clean['a\u00f1o_registro'] = df_clean['fecha_registro'].dt.year\n",
    "df_clean['mes_registro'] = df_clean['fecha_registro'].dt.month\n",
    "df_clean['dias_desde_registro'] = (datetime.now() - df_clean['fecha_registro']).dt.days\n",
    "\n",
    "print(\"\\nNuevas caracter\u00edsticas creadas:\")\n",
    "print(df_clean[[\n",
    "    'edad', 'categoria_edad',\n",
    "    'salario', 'categoria_salario',\n",
    "    'fecha_registro', 'dias_desde_registro'\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09911ac9",
   "metadata": {},
   "source": [
    "### \ud83d\udd28 Feature Engineering: Creaci\u00f3n de Variables\n",
    "\n",
    "**Concepto:** Feature Engineering es crear nuevas caracter\u00edsticas a partir de datos existentes para mejorar an\u00e1lisis.\n",
    "\n",
    "**T\u00e9cnicas aplicadas:**\n",
    "1. **Binning/Discretizaci\u00f3n:** `pd.cut()` convierte variables continuas en categor\u00edas\n",
    "2. **Extracci\u00f3n temporal:** `.dt.year`, `.dt.month` extraen componentes de fechas\n",
    "3. **C\u00e1lculo de diferencias:** d\u00edas desde un evento, tiempo transcurrido\n",
    "\n",
    "**Beneficios:**\n",
    "- Facilita interpretaci\u00f3n de an\u00e1lisis\n",
    "- Mejora rendimiento de modelos de ML\n",
    "- Descubre patrones ocultos en los datos\n",
    "\n",
    "**Nota:** Este es el puente entre limpieza de datos y an\u00e1lisis avanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56ef12",
   "metadata": {},
   "source": [
    "## 5. Exportar Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio\n",
    "output_path = '../../datasets/processed/datos_limpios.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset limpio guardado en: {output_path}\")\n",
    "print(f\"\\nShape final: {df_clean.shape}\")\n",
    "print(f\"\\nColumnas finales:\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefa66e",
   "metadata": {},
   "source": [
    "### \ud83d\udcbe Exportaci\u00f3n de Datos Procesados\n",
    "\n",
    "**Concepto:** Guardar datos limpios para reutilizaci\u00f3n sin repetir todo el proceso de limpieza.\n",
    "\n",
    "**Formato CSV:**\n",
    "- Universal y legible por humanos\n",
    "- `index=False`: evita guardar \u00edndice como columna\n",
    "- Compresi\u00f3n opcional: `compression='gzip'` para datasets grandes\n",
    "\n",
    "**Buenas pr\u00e1cticas:**\n",
    "- Guardar en carpeta `processed/` separada de datos `raw/`\n",
    "- Mantener datos originales intactos (trazabilidad)\n",
    "- Documentar transformaciones aplicadas (metadatos)\n",
    "\n",
    "**Pr\u00f3ximo paso:** Datos listos para an\u00e1lisis exploratorio, visualizaci\u00f3n o modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5edd",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Pr\u00e1cticas\n",
    "\n",
    "### Proceso de Limpieza:\n",
    "1. **An\u00e1lisis exploratorio**: Identificar problemas de calidad\n",
    "2. **Eliminar duplicados**: Asegurar unicidad de registros\n",
    "3. **Normalizar texto**: Estandarizar formato de strings\n",
    "4. **Validar datos**: Verificar rangos y formatos\n",
    "5. **Manejar outliers**: Detectar y tratar valores at\u00edpicos\n",
    "6. **Imputar valores faltantes**: Estrategia seg\u00fan el contexto\n",
    "7. **Feature engineering**: Crear caracter\u00edsticas derivadas\n",
    "\n",
    "### Mejores Pr\u00e1cticas:\n",
    "- Siempre trabajar en una copia del dataset original\n",
    "- Documentar cada paso de limpieza\n",
    "- Validar resultados despu\u00e9s de cada transformaci\u00f3n\n",
    "- Considerar el contexto del negocio para decisiones de limpieza\n",
    "- Automatizar el proceso con funciones reutilizables\n",
    "- Mantener registro de cambios (auditor\u00eda)\n",
    "\n",
    "### T\u00e9cnicas de Imputaci\u00f3n:\n",
    "- **Media/Mediana**: Para datos num\u00e9ricos\n",
    "- **Moda**: Para datos categ\u00f3ricos\n",
    "- **Forward/Backward fill**: Para series temporales\n",
    "- **Interpolaci\u00f3n**: Para datos secuenciales\n",
    "- **Valor constante**: Cuando tiene sentido de negocio\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Pandas Data Cleaning](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Data Quality Best Practices](https://www.dataversity.net/data-quality-best-practices/)\n",
    "- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [SQL B\u00e1sico para Ingenier\u00eda de Datos](04_sql_basico.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83d\udcca Visualizaci\u00f3n de Datos en Ingenier\u00eda de Datos \u2192](06_visualizacion_datos.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Junior:**\n",
    "- [\ud83d\udcca Junior - 01. Introducci\u00f3n a la Ingenier\u00eda de Datos](01_introduccion_ingenieria_datos.ipynb)\n",
    "- [\ud83d\udc0d Junior - 02. Python para Manipulaci\u00f3n de Datos](02_python_manipulacion_datos.ipynb)\n",
    "- [Pandas: Fundamentos para An\u00e1lisis de Datos](03_pandas_fundamentos.ipynb)\n",
    "- [SQL B\u00e1sico para Ingenier\u00eda de Datos](04_sql_basico.ipynb)\n",
    "- [Limpieza y Preparaci\u00f3n de Datos](05_limpieza_datos.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83d\udcca Visualizaci\u00f3n de Datos en Ingenier\u00eda de Datos](06_visualizacion_datos.ipynb)\n",
    "- [\ud83d\udd04 Git y Control de Versiones para Ingenier\u00eda de Datos](07_git_control_versiones.ipynb)\n",
    "- [\ud83c\udf10 APIs REST y Web Scraping para Ingenier\u00eda de Datos](08_apis_web_scraping.ipynb)\n",
    "- [\ud83c\udfaf Proyecto Integrador 1: Pipeline ETL Completo](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
