{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5740a4",
   "metadata": {},
   "source": [
    "# 🚀 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas\n",
    "\n",
    "Este proyecto consolida el nivel Junior con un pipeline que monitorea una carpeta de entrada, valida y transforma nuevos archivos CSV en tiempo casi real, carga resultados, genera métricas, registra logs, implementa reintentos con backoff y envía notificaciones (stub).\n",
    "\n",
    "Objetivos:\n",
    "- ✅ Monitoreo de archivos nuevos (file watcher)\n",
    "- ✅ Validación de esquema y calidad\n",
    "- ✅ Transformación y enriquecimiento\n",
    "- ✅ Carga a SQLite y Parquet\n",
    "- ✅ Métricas y logging estructurado\n",
    "- ✅ Reintentos con backoff e idempotencia\n",
    "- ✅ Notificaciones (Slack/Email stub)\n",
    "- ✅ Scheduling simple sin dependencias externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c6ee1",
   "metadata": {},
   "source": [
    "## 1. Configuración y Estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, sqlite3, logging, traceback, hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "INPUT_DIR = os.path.join(BASE_DIR, 'ingest', 'incoming')\n",
    "CHKPT_DIR = os.path.join(BASE_DIR, 'ingest', 'checkpoints')\n",
    "OUT_DIR = os.path.join(BASE_DIR, 'outputs', 'proyecto_2')\n",
    "LOGS_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "\n",
    "for d in [INPUT_DIR, CHKPT_DIR, OUT_DIR, LOGS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "DB_PATH = os.path.join(OUT_DIR, 'near_rt_analytics.db')\n",
    "PARQUET_DIR = os.path.join(OUT_DIR, 'parquet')\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(LOGS_DIR, 'proyecto_integrador_2.log'), encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "logger = logging.getLogger('near_rt_pipeline')\n",
    "logger.info('✅ Entorno inicializado')\n",
    "print('BASE_DIR:', BASE_DIR)\n",
    "print('INPUT_DIR:', INPUT_DIR)\n",
    "print('OUT_DIR:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46440374",
   "metadata": {},
   "source": [
    "### 🏗️ Arquitectura de Pipeline Near Real-Time\n",
    "\n",
    "**Concepto:** Pipeline que procesa archivos automáticamente al detectar nuevos en carpeta monitoreada.\n",
    "\n",
    "**Estructura de directorios:**\n",
    "- **ingest/incoming/**: Archivos nuevos llegan aquí (trigger)\n",
    "- **ingest/checkpoints/**: Metadata de archivos procesados (evita duplicados)\n",
    "- **outputs/proyecto_2/**: Datos transformados (DB + Parquet)\n",
    "- **logs/**: Trazabilidad de ejecución\n",
    "\n",
    "**Patrón:**\n",
    "1. Watchdog monitorea incoming/\n",
    "2. Detecta nuevo archivo → valida → transforma → carga\n",
    "3. Guarda checkpoint para idempotencia\n",
    "\n",
    "**Uso:** Pipelines batch-streaming, ingesta continua sin colas MQ complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d977538",
   "metadata": {},
   "source": [
    "## 2. Utilidades: Hash, Checkpoints, Notificaciones (stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af116418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_md5(path: str) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chkpt_path(filename: str) -> str:\n",
    "    base = os.path.basename(filename)\n",
    "    return os.path.join(CHKPT_DIR, base + '.json')\n",
    "\n",
    "def is_processed(filename: str, md5sum: str) -> bool:\n",
    "    p = chkpt_path(filename)\n",
    "    if not os.path.exists(p):\n",
    "        return False\n",
    "    try:\n",
    "        data = json.load(open(p, 'r', encoding='utf-8'))\n",
    "        return data.get('md5') == md5sum\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def save_chkpt(filename: str, md5sum: str, rows: int, status: str, error: Optional[str]=None):\n",
    "    p = chkpt_path(filename)\n",
    "    payload = {\n",
    "        'filename': os.path.basename(filename),\n",
    "        'md5': md5sum,\n",
    "        'rows': rows,\n",
    "        'status': status,\n",
    "        'error': error,\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    json.dump(payload, open(p, 'w', encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "def notify(message: str, level: str = 'info') -> None:\n",
    "    \"\"\"\n",
    "    Stub de notificación: aquí iría Slack webhook o email SMTP.\n",
    "    Para Slack: POST a https://hooks.slack.com/services/... con {'text': message}\n",
    "    Para Email: usar smtplib con credenciales seguras (no hardcodear).\n",
    "    \"\"\"\n",
    "    levels = {'info': logging.INFO, 'warning': logging.WARNING, 'error': logging.ERROR}\n",
    "    logger.log(levels.get(level, logging.INFO), f'🔔 {message}')\n",
    "\n",
    "print('✅ Utilidades listas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6c01a",
   "metadata": {},
   "source": [
    "### 🔒 Idempotencia: Checkpoints y Hash MD5\n",
    "\n",
    "**Concepto:** Evitar procesar el mismo archivo múltiples veces mediante checksums y metadata.\n",
    "\n",
    "**Componentes:**\n",
    "- **MD5 Hash:** Firma única del contenido del archivo\n",
    "- **Checkpoint JSON:** Almacena {filename, md5, timestamp, status}\n",
    "- **is_processed():** Verifica si archivo ya fue procesado\n",
    "\n",
    "**Flujo:**\n",
    "1. Calcular MD5 del archivo nuevo\n",
    "2. Buscar checkpoint existente\n",
    "3. Si MD5 coincide → skip (ya procesado)\n",
    "4. Si no → procesar y guardar checkpoint\n",
    "\n",
    "**Importancia:** Pipeline idempotente permite reintentos sin duplicar datos (requisito producción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ab90b",
   "metadata": {},
   "source": [
    "## 3. Validación de Esquema y Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_COLUMNS = {\n",
    "    'venta_id': 'int64',\n",
    "    'cliente_id': 'int64',\n",
    "    'producto_id': 'int64',\n",
    "    'cantidad': 'int64',\n",
    "    'precio_unitario': 'float64',\n",
    "    'total': 'float64',\n",
    "    'fecha_venta': 'datetime64[ns]',\n",
    "    'metodo_pago': 'object',\n",
    "    'estado': 'object'\n",
    "}\n",
    "\n",
    "def read_csv_safe(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Normalización mínima\n",
    "    if 'fecha_venta' in df.columns:\n",
    "        df['fecha_venta'] = pd.to_datetime(df['fecha_venta'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_schema(df: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
    "    errors = []\n",
    "    for col, dtype in EXPECTED_COLUMNS.items():\n",
    "        if col not in df.columns:\n",
    "            errors.append(f\n",
    "        else:\n",
    "            # Validación de tipo (suave)\n",
    "            if dtype.startswith('datetime'):\n",
    "                if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "                    errors.append(f\"Tipo inválido en {col}: {df[col].dtype}\")\n",
    "            elif dtype == 'object':\n",
    "                pass\n",
    "            else:\n",
    "                if str(df[col].dtype) != dtype:\n",
    "                    errors.append(f\"Tipo inválido en {col}: {df[col].dtype} (esperado {dtype})\")\n",
    "    return (len(errors) == 0, errors)\n",
    "\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['anio'] = out['fecha_venta'].dt.year\n",
    "    out['mes'] = out['fecha_venta'].dt.to_period('M').astype(str)\n",
    "    out['ticket_unitario'] = out['total'] / out['cantidad']\n",
    "    return out\n",
    "\n",
    "print('✅ Validadores definidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f20685",
   "metadata": {},
   "source": [
    "### ✓ Validación de Esquema: Data Contract\n",
    "\n",
    "**Concepto:** Validar que datos entrantes cumplan esquema esperado antes de procesar.\n",
    "\n",
    "**Data Contract:**\n",
    "- Diccionario con columnas esperadas y tipos de datos\n",
    "- Valida presencia de columnas obligatorias\n",
    "- Verifica tipos de datos (int64, float64, datetime64, object)\n",
    "\n",
    "**Flujo:**\n",
    "1. Leer CSV con `pd.read_csv()`\n",
    "2. Normalizar fechas con `pd.to_datetime(errors='coerce')`\n",
    "3. Validar esquema contra contrato\n",
    "4. Si falla → logear errores y rechazar archivo\n",
    "5. Si pasa → aplicar transformaciones\n",
    "\n",
    "**Ventaja:** Detección temprana de problemas, evita corrupción downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe34a0f",
   "metadata": {},
   "source": [
    "## 4. Carga y Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f22f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(df: pd.DataFrame, table: str = 'ventas_near_rt') -> None:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    try:\n",
    "        df.to_sql(table, conn, if_exists='append', index=False)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def export_parquet(df: pd.DataFrame, base_name: str) -> str:\n",
    "    ts = datetime.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "    out_path = os.path.join(PARQUET_DIR, f\n",
    ")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame) -> Dict:\n",
    "    return {\n",
    "        'rows': len(df),\n",
    "        'total_sum': float(df['total'].sum()),\n",
    "        'ticket_promedio': float(df['total'].mean()),\n",
    "        'clientes_unicos': int(df['cliente_id'].nunique())\n",
    "    }\n",
    "\n",
    "print('✅ Funciones de carga y métricas listas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59238174",
   "metadata": {},
   "source": [
    "### 💾 Dual Output: SQLite + Parquet\n",
    "\n",
    "**Concepto:** Persistir datos en múltiples formatos para diferentes casos de uso.\n",
    "\n",
    "**SQLite:**\n",
    "- `if_exists='append'`: añade registros sin sobrescribir\n",
    "- Ideal para: queries SQL, análisis ad-hoc, dashboards pequeños\n",
    "- Limitación: no escala para Big Data\n",
    "\n",
    "**Parquet:**\n",
    "- Formato columnar comprimido\n",
    "- Timestamped: `ventas_20250101T120000.parquet`\n",
    "- Ideal para: data lakes, analytics distribuidos (Spark, Athena)\n",
    "- Ventaja: 10-100x más eficiente que CSV\n",
    "\n",
    "**Estrategia:** SQLite para operacional, Parquet para histórico y analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636c53d",
   "metadata": {},
   "source": [
    "## 5. Worker con Reintentos y Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(path: str, max_retries: int = 3) -> bool:\n",
    "    md5sum = file_md5(path)\n",
    "    if is_processed(path, md5sum):\n",
    "        logger.info(f'⏭️ Ya procesado: {os.path.basename(path)}')\n",
    "        return True\n",
    "\n",
    "    sleep = 1.0\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            logger.info(f'📥 Procesando {os.path.basename(path)} (intento {attempt})')\n",
    "            df = read_csv_safe(path)\n",
    "            ok, errs = validate_schema(df)\n",
    "            if not ok:\n",
    "                raise ValueError('Esquema inválido: ' + '; '.join(errs))\n",
    "            df_t = transform(df)\n",
    "            load_to_sqlite(df_t)\n",
    "            pq = export_parquet(df_t, base_name=os.path.splitext(os.path.basename(path))[0])\n",
    "            m = compute_metrics(df_t)\n",
    "            save_chkpt(path, md5sum, rows=len(df_t), status='ok')\n",
    "            notify(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cfe2c",
   "metadata": {},
   "source": [
    "### 🔄 Retry Logic con Exponential Backoff\n",
    "\n",
    "**Concepto:** Reintentar operaciones fallidas con delays incrementales para manejar errores transitorios.\n",
    "\n",
    "**Parámetros:**\n",
    "- **max_retries:** número máximo de intentos (ej: 3)\n",
    "- **base_delay:** delay inicial en segundos (ej: 2s)\n",
    "- **backoff:** multiplicador entre intentos (ej: 2x)\n",
    "\n",
    "**Ejemplo de delays:**\n",
    "- Intento 1: 2s\n",
    "- Intento 2: 4s (2 × 2)\n",
    "- Intento 3: 8s (4 × 2)\n",
    "\n",
    "**Cuándo usar:**\n",
    "- Errores de red/conexión\n",
    "- Rate limits de APIs\n",
    "- Locks de base de datos\n",
    "- Servicios externos temporalmente caídos\n",
    "\n",
    "**Nota:** No reintentar errores lógicos (validación fallida, datos corruptos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9dd599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be2bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98394fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b0342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48406a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac789f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77b4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768a932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5211605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12c999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36db79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f018cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
