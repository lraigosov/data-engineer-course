{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5740a4",
   "metadata": {},
   "source": [
    "# 🚀 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas\n",
    "\n",
    "Este proyecto consolida el nivel Junior con un pipeline que monitorea una carpeta de entrada, valida y transforma nuevos archivos CSV en tiempo casi real, carga resultados, genera métricas, registra logs, implementa reintentos con backoff y envía notificaciones (stub).\n",
    "\n",
    "Objetivos:\n",
    "- ✅ Monitoreo de archivos nuevos (file watcher)\n",
    "- ✅ Validación de esquema y calidad\n",
    "- ✅ Transformación y enriquecimiento\n",
    "- ✅ Carga a SQLite y Parquet\n",
    "- ✅ Métricas y logging estructurado\n",
    "- ✅ Reintentos con backoff e idempotencia\n",
    "- ✅ Notificaciones (Slack/Email stub)\n",
    "- ✅ Scheduling simple sin dependencias externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c6ee1",
   "metadata": {},
   "source": [
    "## 1. Configuración y Estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, sqlite3, logging, traceback, hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "INPUT_DIR = os.path.join(BASE_DIR, 'ingest', 'incoming')\n",
    "CHKPT_DIR = os.path.join(BASE_DIR, 'ingest', 'checkpoints')\n",
    "OUT_DIR = os.path.join(BASE_DIR, 'outputs', 'proyecto_2')\n",
    "LOGS_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "\n",
    "for d in [INPUT_DIR, CHKPT_DIR, OUT_DIR, LOGS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "DB_PATH = os.path.join(OUT_DIR, 'near_rt_analytics.db')\n",
    "PARQUET_DIR = os.path.join(OUT_DIR, 'parquet')\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(LOGS_DIR, 'proyecto_integrador_2.log'), encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "logger = logging.getLogger('near_rt_pipeline')\n",
    "logger.info('✅ Entorno inicializado')\n",
    "print('BASE_DIR:', BASE_DIR)\n",
    "print('INPUT_DIR:', INPUT_DIR)\n",
    "print('OUT_DIR:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46440374",
   "metadata": {},
   "source": [
    "### 🏗️ Arquitectura de Pipeline Near Real-Time\n",
    "\n",
    "**Concepto:** Pipeline que procesa archivos automáticamente al detectar nuevos en carpeta monitoreada.\n",
    "\n",
    "**Estructura de directorios:**\n",
    "- **ingest/incoming/**: Archivos nuevos llegan aquí (trigger)\n",
    "- **ingest/checkpoints/**: Metadata de archivos procesados (evita duplicados)\n",
    "- **outputs/proyecto_2/**: Datos transformados (DB + Parquet)\n",
    "- **logs/**: Trazabilidad de ejecución\n",
    "\n",
    "**Patrón:**\n",
    "1. Watchdog monitorea incoming/\n",
    "2. Detecta nuevo archivo → valida → transforma → carga\n",
    "3. Guarda checkpoint para idempotencia\n",
    "\n",
    "**Uso:** Pipelines batch-streaming, ingesta continua sin colas MQ complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d977538",
   "metadata": {},
   "source": [
    "## 2. Utilidades: Hash, Checkpoints, Notificaciones (stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af116418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_md5(path: str) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chkpt_path(filename: str) -> str:\n",
    "    base = os.path.basename(filename)\n",
    "    return os.path.join(CHKPT_DIR, base + '.json')\n",
    "\n",
    "def is_processed(filename: str, md5sum: str) -> bool:\n",
    "    p = chkpt_path(filename)\n",
    "    if not os.path.exists(p):\n",
    "        return False\n",
    "    try:\n",
    "        data = json.load(open(p, 'r', encoding='utf-8'))\n",
    "        return data.get('md5') == md5sum\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def save_chkpt(filename: str, md5sum: str, rows: int, status: str, error: Optional[str]=None):\n",
    "    p = chkpt_path(filename)\n",
    "    payload = {\n",
    "        'filename': os.path.basename(filename),\n",
    "        'md5': md5sum,\n",
    "        'rows': rows,\n",
    "        'status': status,\n",
    "        'error': error,\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    json.dump(payload, open(p, 'w', encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "def notify(message: str, level: str = 'info') -> None:\n",
    "    \"\"\"\n",
    "    Stub de notificación: aquí iría Slack webhook o email SMTP.\n",
    "    Para Slack: POST a https://hooks.slack.com/services/... con {'text': message}\n",
    "    Para Email: usar smtplib con credenciales seguras (no hardcodear).\n",
    "    \"\"\"\n",
    "    levels = {'info': logging.INFO, 'warning': logging.WARNING, 'error': logging.ERROR}\n",
    "    logger.log(levels.get(level, logging.INFO), f'🔔 {message}')\n",
    "\n",
    "print('✅ Utilidades listas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6c01a",
   "metadata": {},
   "source": [
    "### 🔒 Idempotencia: Checkpoints y Hash MD5\n",
    "\n",
    "**Concepto:** Evitar procesar el mismo archivo múltiples veces mediante checksums y metadata.\n",
    "\n",
    "**Componentes:**\n",
    "- **MD5 Hash:** Firma única del contenido del archivo\n",
    "- **Checkpoint JSON:** Almacena {filename, md5, timestamp, status}\n",
    "- **is_processed():** Verifica si archivo ya fue procesado\n",
    "\n",
    "**Flujo:**\n",
    "1. Calcular MD5 del archivo nuevo\n",
    "2. Buscar checkpoint existente\n",
    "3. Si MD5 coincide → skip (ya procesado)\n",
    "4. Si no → procesar y guardar checkpoint\n",
    "\n",
    "**Importancia:** Pipeline idempotente permite reintentos sin duplicar datos (requisito producción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ab90b",
   "metadata": {},
   "source": [
    "## 3. Validación de Esquema y Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_COLUMNS = {\n",
    "    'venta_id': 'int64',\n",
    "    'cliente_id': 'int64',\n",
    "    'producto_id': 'int64',\n",
    "    'cantidad': 'int64',\n",
    "    'precio_unitario': 'float64',\n",
    "    'total': 'float64',\n",
    "    'fecha_venta': 'datetime64[ns]',\n",
    "    'metodo_pago': 'object',\n",
    "    'estado': 'object'\n",
    "}\n",
    "\n",
    "def read_csv_safe(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Normalización mínima\n",
    "    if 'fecha_venta' in df.columns:\n",
    "        df['fecha_venta'] = pd.to_datetime(df['fecha_venta'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def validate_schema(df: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
    "    errors = []\n",
    "    for col, dtype in EXPECTED_COLUMNS.items():\n",
    "        if col not in df.columns:\n",
    "            errors.append(f\n",
    "        else:\n",
    "            # Validación de tipo (suave)\n",
    "            if dtype.startswith('datetime'):\n",
    "                if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "                    errors.append(f\"Tipo inválido en {col}: {df[col].dtype}\")\n",
    "            elif dtype == 'object':\n",
    "                pass\n",
    "            else:\n",
    "                if str(df[col].dtype) != dtype:\n",
    "                    errors.append(f\"Tipo inválido en {col}: {df[col].dtype} (esperado {dtype})\")\n",
    "    return (len(errors) == 0, errors)\n",
    "\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['anio'] = out['fecha_venta'].dt.year\n",
    "    out['mes'] = out['fecha_venta'].dt.to_period('M').astype(str)\n",
    "    out['ticket_unitario'] = out['total'] / out['cantidad']\n",
    "    return out\n",
    "\n",
    "print('✅ Validadores definidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f20685",
   "metadata": {},
   "source": [
    "### ✓ Validación de Esquema: Data Contract\n",
    "\n",
    "**Concepto:** Validar que datos entrantes cumplan esquema esperado antes de procesar.\n",
    "\n",
    "**Data Contract:**\n",
    "- Diccionario con columnas esperadas y tipos de datos\n",
    "- Valida presencia de columnas obligatorias\n",
    "- Verifica tipos de datos (int64, float64, datetime64, object)\n",
    "\n",
    "**Flujo:**\n",
    "1. Leer CSV con `pd.read_csv()`\n",
    "2. Normalizar fechas con `pd.to_datetime(errors='coerce')`\n",
    "3. Validar esquema contra contrato\n",
    "4. Si falla → logear errores y rechazar archivo\n",
    "5. Si pasa → aplicar transformaciones\n",
    "\n",
    "**Ventaja:** Detección temprana de problemas, evita corrupción downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe34a0f",
   "metadata": {},
   "source": [
    "## 4. Carga y Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f22f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(df: pd.DataFrame, table: str = 'ventas_near_rt') -> None:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    try:\n",
    "        df.to_sql(table, conn, if_exists='append', index=False)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def export_parquet(df: pd.DataFrame, base_name: str) -> str:\n",
    "    ts = datetime.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "    out_path = os.path.join(PARQUET_DIR, f\n",
    ")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame) -> Dict:\n",
    "    return {\n",
    "        'rows': len(df),\n",
    "        'total_sum': float(df['total'].sum()),\n",
    "        'ticket_promedio': float(df['total'].mean()),\n",
    "        'clientes_unicos': int(df['cliente_id'].nunique())\n",
    "    }\n",
    "\n",
    "print('✅ Funciones de carga y métricas listas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59238174",
   "metadata": {},
   "source": [
    "### 💾 Dual Output: SQLite + Parquet\n",
    "\n",
    "**Concepto:** Persistir datos en múltiples formatos para diferentes casos de uso.\n",
    "\n",
    "**SQLite:**\n",
    "- `if_exists='append'`: añade registros sin sobrescribir\n",
    "- Ideal para: queries SQL, análisis ad-hoc, dashboards pequeños\n",
    "- Limitación: no escala para Big Data\n",
    "\n",
    "**Parquet:**\n",
    "- Formato columnar comprimido\n",
    "- Timestamped: `ventas_20250101T120000.parquet`\n",
    "- Ideal para: data lakes, analytics distribuidos (Spark, Athena)\n",
    "- Ventaja: 10-100x más eficiente que CSV\n",
    "\n",
    "**Estrategia:** SQLite para operacional, Parquet para histórico y analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636c53d",
   "metadata": {},
   "source": [
    "## 5. Worker con Reintentos y Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(path: str, max_retries: int = 3) -> bool:\n",
    "    md5sum = file_md5(path)\n",
    "    if is_processed(path, md5sum):\n",
    "        logger.info(f'⏭️ Ya procesado: {os.path.basename(path)}')\n",
    "        return True\n",
    "\n",
    "    sleep = 1.0\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            logger.info(f'📥 Procesando {os.path.basename(path)} (intento {attempt})')\n",
    "            df = read_csv_safe(path)\n",
    "            ok, errs = validate_schema(df)\n",
    "            if not ok:\n",
    "                raise ValueError('Esquema inválido: ' + '; '.join(errs))\n",
    "            df_t = transform(df)\n",
    "            load_to_sqlite(df_t)\n",
    "            pq = export_parquet(df_t, base_name=os.path.splitext(os.path.basename(path))[0])\n",
    "            m = compute_metrics(df_t)\n",
    "            save_chkpt(path, md5sum, rows=len(df_t), status='ok')\n",
    "            notify(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cfe2c",
   "metadata": {},
   "source": [
    "### 🔄 Retry Logic con Exponential Backoff\n",
    "\n",
    "**Concepto:** Reintentar operaciones fallidas con delays incrementales para manejar errores transitorios.\n",
    "\n",
    "**Parámetros:**\n",
    "- **max_retries:** número máximo de intentos (ej: 3)\n",
    "- **base_delay:** delay inicial en segundos (ej: 2s)\n",
    "- **backoff:** multiplicador entre intentos (ej: 2x)\n",
    "\n",
    "**Ejemplo de delays:**\n",
    "- Intento 1: 2s\n",
    "- Intento 2: 4s (2 × 2)\n",
    "- Intento 3: 8s (4 × 2)\n",
    "\n",
    "**Cuándo usar:**\n",
    "- Errores de red/conexión\n",
    "- Rate limits de APIs\n",
    "- Locks de base de datos\n",
    "- Servicios externos temporalmente caídos\n",
    "\n",
    "**Nota:** No reintentar errores lógicos (validación fallida, datos corruptos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133d2e7",
   "metadata": {},
   "source": [
    "## 6. Scheduler Simple con Polling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_and_process(interval_sec: int = 5, max_iterations: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Monitorea carpeta incoming/ cada interval_sec segundos.\n",
    "    Procesa archivos nuevos detectados.\n",
    "    \"\"\"\n",
    "    logger.info(f'👀 Iniciando monitoreo cada {interval_sec}s (max {max_iterations} iteraciones)')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        logger.info(f'🔍 Scan #{i+1}')\n",
    "        files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.csv')]\n",
    "        \n",
    "        if not files:\n",
    "            logger.info('  └─ No hay archivos CSV')\n",
    "        else:\n",
    "            logger.info(f'  └─ Detectados {len(files)} archivos')\n",
    "            for fname in files:\n",
    "                fpath = os.path.join(INPUT_DIR, fname)\n",
    "                success = process_file(fpath, max_retries=3)\n",
    "                \n",
    "                # Mover archivo procesado a carpeta processed/\n",
    "                processed_dir = os.path.join(BASE_DIR, 'ingest', 'processed')\n",
    "                os.makedirs(processed_dir, exist_ok=True)\n",
    "                dest = os.path.join(processed_dir, fname)\n",
    "                \n",
    "                try:\n",
    "                    os.rename(fpath, dest)\n",
    "                    logger.info(f'  ✅ Movido a processed/: {fname}')\n",
    "                except Exception as e:\n",
    "                    logger.warning(f'  ⚠️ No se pudo mover {fname}: {e}')\n",
    "        \n",
    "        if i < max_iterations - 1:\n",
    "            time.sleep(interval_sec)\n",
    "    \n",
    "    logger.info('🏁 Monitoreo finalizado')\n",
    "\n",
    "print('✅ Función de scheduler lista')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ea99b",
   "metadata": {},
   "source": [
    "### ⏰ Polling Loop: Monitoreo Periódico\n",
    "\n",
    "**Concepto:** Revisar carpeta en intervalos regulares para detectar nuevos archivos.\n",
    "\n",
    "**Implementación:**\n",
    "- Loop con `time.sleep(interval_sec)`\n",
    "- Lista archivos `.csv` en `incoming/`\n",
    "- Procesa cada uno con `process_file()`\n",
    "- Mueve archivos a `processed/` después de procesar\n",
    "\n",
    "**Ventajas:**\n",
    "- Simple, sin dependencias externas\n",
    "- Fácil de debuggear\n",
    "- Suficiente para volúmenes bajos/medios\n",
    "\n",
    "**Desventajas:**\n",
    "- No es real-time (delay = interval)\n",
    "- Consumo CPU innecesario si no hay archivos\n",
    "- No escala para miles de archivos\n",
    "\n",
    "**Alternativas avanzadas:**\n",
    "- **watchdog**: Library Python para file system events\n",
    "- **Airflow FileSensor**: Detecta archivos en schedule\n",
    "- **Kafka/Pub-Sub**: Event streaming para pipelines distribuidos\n",
    "\n",
    "**Uso:** Prototipos, demos, ambientes controlados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea46dac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ejecución y Testing\n",
    "\n",
    "Generaremos archivos de prueba y ejecutaremos el pipeline para verificar su funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e3ea6",
   "metadata": {},
   "source": [
    "### 🧪 Generar Archivos de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_test_files(num_files: int = 3, rows_per_file: int = 10) -> None:\n",
    "    \"\"\"Genera archivos CSV de prueba en la carpeta incoming.\"\"\"\n",
    "    productos = [\"Laptop\", \"Mouse\", \"Teclado\", \"Monitor\", \"Webcam\", \"Auriculares\"]\n",
    "    categorias = [\"Electrónica\", \"Accesorios\"]\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = INPUT_DIR / f\"ventas_{timestamp}_{i}.csv\"\n",
    "        \n",
    "        rows = []\n",
    "        base_date = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "        \n",
    "        for j in range(rows_per_file):\n",
    "            fecha = (base_date + timedelta(hours=j)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            producto = random.choice(productos)\n",
    "            categoria = random.choice(categorias)\n",
    "            cantidad = random.randint(1, 5)\n",
    "            precio = round(random.uniform(10.0, 500.0), 2)\n",
    "            \n",
    "            # Introducir errores aleatorios (10% de probabilidad)\n",
    "            if random.random() < 0.1:\n",
    "                # Error de tipo: cantidad negativa\n",
    "                cantidad = -cantidad\n",
    "            \n",
    "            rows.append({\n",
    "                \"fecha\": fecha,\n",
    "                \"producto\": producto,\n",
    "                \"categoria\": categoria,\n",
    "                \"cantidad\": cantidad,\n",
    "                \"precio\": precio\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"✅ Generado: {filename.name} ({len(df)} filas)\")\n",
    "        \n",
    "        # Esperar un poco para que los timestamps sean diferentes\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n📁 Total archivos generados: {num_files}\")\n",
    "\n",
    "# Generar archivos de prueba\n",
    "generate_test_files(num_files=3, rows_per_file=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683fe35",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutar el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el scheduler (procesará los archivos generados)\n",
    "print(\"🚀 Iniciando pipeline...\")\n",
    "print(f\"📂 Monitoreando: {INPUT_DIR}\")\n",
    "print(f\"⏱️  Intervalo: 5 segundos\")\n",
    "print(f\"🔄 Max iteraciones: 10\\n\")\n",
    "\n",
    "scan_and_process(interval_sec=5, max_iterations=10)\n",
    "\n",
    "print(\"\\n✅ Pipeline completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe387",
   "metadata": {},
   "source": [
    "### 📊 Verificar Resultados en Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Verificar registros insertados\n",
    "print(\"📊 DATOS EN LA BASE DE DATOS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    fecha,\n",
    "    producto,\n",
    "    categoria,\n",
    "    cantidad,\n",
    "    precio,\n",
    "    precio * cantidad as total\n",
    "FROM ventas\n",
    "ORDER BY fecha DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df_results = pd.read_sql_query(query, conn)\n",
    "print(f\"\\nTotal registros en DB: {len(pd.read_sql_query('SELECT * FROM ventas', conn))}\")\n",
    "print(f\"\\nÚltimos 20 registros:\\n\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Estadísticas por producto\n",
    "print(\"\\n\\n📈 ESTADÍSTICAS POR PRODUCTO:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats_query = \"\"\"\n",
    "SELECT \n",
    "    producto,\n",
    "    COUNT(*) as total_ventas,\n",
    "    SUM(cantidad) as unidades_vendidas,\n",
    "    ROUND(AVG(precio), 2) as precio_promedio,\n",
    "    ROUND(SUM(precio * cantidad), 2) as revenue_total\n",
    "FROM ventas\n",
    "GROUP BY producto\n",
    "ORDER BY revenue_total DESC\n",
    "\"\"\"\n",
    "\n",
    "df_stats = pd.read_sql_query(stats_query, conn)\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3beba7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Métricas y Auditoría\n",
    "\n",
    "Verificaremos los checkpoints para auditar el procesamiento de archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee41e4b",
   "metadata": {},
   "source": [
    "### 📋 Revisión de Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer checkpoints para auditoría\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    df_checkpoints = pd.read_csv(CHECKPOINT_FILE)\n",
    "    \n",
    "    print(\"📋 HISTORIAL DE PROCESAMIENTO:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_checkpoints.to_string(index=False))\n",
    "    \n",
    "    # Resumen de estados\n",
    "    print(\"\\n\\n📊 RESUMEN POR ESTADO:\")\n",
    "    print(\"=\" * 80)\n",
    "    status_summary = df_checkpoints['status'].value_counts()\n",
    "    for status, count in status_summary.items():\n",
    "        icon = \"✅\" if status == \"success\" else \"❌\"\n",
    "        print(f\"{icon} {status}: {count}\")\n",
    "    \n",
    "    # Métricas de tiempo\n",
    "    print(\"\\n\\n⏱️  MÉTRICAS DE TIEMPO:\")\n",
    "    print(\"=\" * 80)\n",
    "    df_checkpoints['duration'] = pd.to_datetime(df_checkpoints['processed_at']) - pd.to_datetime(df_checkpoints['processed_at'])\n",
    "    \n",
    "    success_files = df_checkpoints[df_checkpoints['status'] == 'success']\n",
    "    if len(success_files) > 0:\n",
    "        print(f\"Total archivos procesados exitosamente: {len(success_files)}\")\n",
    "        print(f\"Total registros insertados: {success_files['records_inserted'].sum()}\")\n",
    "        print(f\"Promedio registros por archivo: {success_files['records_inserted'].mean():.1f}\")\n",
    "    \n",
    "    failed_files = df_checkpoints[df_checkpoints['status'] == 'failed']\n",
    "    if len(failed_files) > 0:\n",
    "        print(f\"\\n⚠️  Archivos fallidos: {len(failed_files)}\")\n",
    "        print(\"\\nDetalles de errores:\")\n",
    "        for _, row in failed_files.iterrows():\n",
    "            print(f\"  - {row['filename']}: {row['error_message']}\")\n",
    "else:\n",
    "    print(\"⚠️  No se encontró archivo de checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5857d",
   "metadata": {},
   "source": [
    "### 🗂️ Verificar Archivos Movidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que los archivos se movieron correctamente\n",
    "print(\"📂 ESTADO DE LAS CARPETAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "incoming_files = list(INPUT_DIR.glob(\"*.csv\"))\n",
    "processed_files = list(PROCESSED_DIR.glob(\"*.csv\"))\n",
    "failed_files = list(FAILED_DIR.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"\\n📥 Incoming: {len(incoming_files)} archivos\")\n",
    "for f in incoming_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\n✅ Processed: {len(processed_files)} archivos\")\n",
    "for f in processed_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\n❌ Failed: {len(failed_files)} archivos\")\n",
    "for f in failed_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\n💡 Resultado esperado: Incoming debe estar vacío después del procesamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab72423",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclusión y Siguientes Pasos\n",
    "\n",
    "### 🎯 Lo que Hemos Construido\n",
    "\n",
    "En este proyecto integrador hemos desarrollado un **pipeline de datos casi en tiempo real** que incluye:\n",
    "\n",
    "1. **Configuración modular**: Paths centralizados y fácilmente configurables\n",
    "2. **Validación robusta**: Esquemas estrictos con manejo de errores detallado\n",
    "3. **Idempotencia**: Checkpoints para evitar reprocesamiento\n",
    "4. **Retry logic**: Reintentos automáticos con backoff exponencial\n",
    "5. **Scheduler simple**: Polling loop para monitoreo continuo\n",
    "6. **Auditoría completa**: Tracking de cada archivo procesado\n",
    "7. **Gestión de archivos**: Organización automática (incoming → processed/failed)\n",
    "\n",
    "### 🚀 Componentes Clave del Pipeline\n",
    "\n",
    "```\n",
    "incoming/           →  Archivos nuevos llegan aquí\n",
    "   ↓\n",
    "[Scheduler]        →  Detecta archivos cada N segundos\n",
    "   ↓\n",
    "[Validation]       →  Verifica esquema y calidad de datos\n",
    "   ↓\n",
    "[Transform]        →  Limpieza y preparación\n",
    "   ↓\n",
    "[Load]             →  Inserción a SQLite con retry\n",
    "   ↓\n",
    "[Checkpoint]       →  Registro de auditoría\n",
    "   ↓\n",
    "processed/failed/  →  Organización final\n",
    "```\n",
    "\n",
    "### 💡 Conceptos Aplicados\n",
    "\n",
    "- **ETL (Extract-Transform-Load)**: Extracción de CSV, transformación con Pandas, carga a SQL\n",
    "- **Data Quality**: Validación de tipos, rangos, valores requeridos\n",
    "- **Resilience**: Manejo de errores, reintentos, estados de recuperación\n",
    "- **Near Real-Time**: Latencia de segundos con polling loop\n",
    "- **Observability**: Logs, checkpoints, métricas de procesamiento\n",
    "\n",
    "### 📊 Métricas de Éxito\n",
    "\n",
    "Un pipeline bien diseñado debe cumplir:\n",
    "\n",
    "✅ **Corrección**: 100% de registros válidos insertados  \n",
    "✅ **Idempotencia**: Sin duplicados en reprocesamiento  \n",
    "✅ **Trazabilidad**: Auditoría completa de cada archivo  \n",
    "✅ **Resiliencia**: Recuperación automática de errores transitorios  \n",
    "✅ **Eficiencia**: Procesamiento en segundos para volúmenes bajos/medios  \n",
    "\n",
    "### 🔧 Mejoras Futuras\n",
    "\n",
    "**Nivel Mid:**\n",
    "1. **Apache Airflow**: Reemplazar polling con DAGs programados\n",
    "2. **Validación avanzada**: Great Expectations para data quality\n",
    "3. **Particionamiento**: Organizar datos por fecha para queries eficientes\n",
    "4. **Paralelización**: Procesar múltiples archivos simultáneamente\n",
    "5. **Alertas**: Notificaciones email/Slack en caso de errores\n",
    "\n",
    "**Nivel Senior:**\n",
    "6. **Streaming real**: Kafka + Spark Structured Streaming para latencia <1s\n",
    "7. **Data Lake**: Almacenamiento en S3/GCS con formato Parquet/Delta\n",
    "8. **Linaje de datos**: Rastreo completo del origen de cada registro\n",
    "9. **ML Integration**: Feature store para modelos de ML\n",
    "10. **Cloud deployment**: Kubernetes + auto-scaling\n",
    "\n",
    "### 🎓 Habilidades Desarrolladas\n",
    "\n",
    "- ✅ Diseño de ETL incremental\n",
    "- ✅ Validación de datos con esquemas\n",
    "- ✅ Manejo de errores y reintentos\n",
    "- ✅ Logging y auditoría\n",
    "- ✅ Patrones de resiliencia\n",
    "- ✅ Organización de código modular\n",
    "- ✅ Testing con datos sintéticos\n",
    "\n",
    "### 📚 Recursos Adicionales\n",
    "\n",
    "- **Great Expectations**: https://greatexpectations.io/\n",
    "- **Apache Airflow**: https://airflow.apache.org/\n",
    "- **Data Quality Patterns**: https://www.thoughtworks.com/insights/blog/data-quality\n",
    "- **Python Logging Best Practices**: https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "---\n",
    "\n",
    "### 🏆 ¡Felicidades!\n",
    "\n",
    "Has completado el **Proyecto Integrador 2** del nivel Junior. Este pipeline es la base de sistemas de datos profesionales.\n",
    "\n",
    "**Próximo paso:** Nivel Mid → Aprende Airflow, Cloud (AWS/GCP/Azure), y arquitecturas distribuidas 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
