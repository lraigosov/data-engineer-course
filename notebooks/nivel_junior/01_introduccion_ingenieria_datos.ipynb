{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41da9f8",
   "metadata": {},
   "source": [
    "# üìä Junior - 01. Introducci√≥n a la Ingenier√≠a de Datos\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender qu√© es la Ingenier√≠a de Datos y su importancia\n",
    "- [ ] Diferenciar roles: Data Engineer vs Data Scientist vs Data Analyst\n",
    "- [ ] Entender el concepto de pipeline de datos\n",
    "- [ ] Conocer herramientas y tecnolog√≠as fundamentales\n",
    "- [ ] Realizar primeros ejercicios pr√°cticos con Python\n",
    "\n",
    "**Duraci√≥n Estimada:** 60-75 minutos  \n",
    "**Nivel de Dificultad:** Principiante  \n",
    "**Prerrequisitos:** Conocimientos b√°sicos de programaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7acc16",
   "metadata": {},
   "source": [
    "## üéØ ¬øQu√© es la Ingenier√≠a de Datos?\n",
    "\n",
    "La **Ingenier√≠a de Datos** es la disciplina que se encarga de:\n",
    "\n",
    "‚úÖ **Extraer** datos de m√∫ltiples fuentes  \n",
    "‚úÖ **Transformar** y limpiar los datos  \n",
    "‚úÖ **Cargar** datos en sistemas de almacenamiento  \n",
    "‚úÖ **Orquestar** procesos automatizados  \n",
    "‚úÖ **Monitorear** la calidad y performance  \n",
    "\n",
    "### üåü Analog√≠a: El Data Engineer como \"Plomero de Datos\"\n",
    "\n",
    "Imagina que los datos son como agua en una ciudad:\n",
    "\n",
    "- **Fuentes de agua** = Bases de datos, APIs, archivos\n",
    "- **Tuber√≠as** = Pipelines de datos\n",
    "- **Tratamiento** = Limpieza y transformaci√≥n\n",
    "- **Distribuci√≥n** = Data warehouses, dashboards\n",
    "- **Calidad** = Monitoreo y alertas\n",
    "\n",
    "El Data Engineer construye y mantiene toda esta \"infraestructura de datos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a4c2d",
   "metadata": {},
   "source": [
    "## üë• Comparaci√≥n de Roles en el Ecosistema de Datos\n",
    "\n",
    "| Aspecto | Data Engineer | Data Scientist | Data Analyst |\n",
    "|---------|---------------|----------------|---------------|\n",
    "| **Foco Principal** | Infraestructura y pipelines | Modelos y algoritmos | Reportes y insights |\n",
    "| **Herramientas** | Python, SQL, Airflow | Python, R, TensorFlow | SQL, Excel, Tableau |\n",
    "| **Output** | Datos limpios y accesibles | Modelos predictivos | Dashboards y reportes |\n",
    "| **Skills T√©cnicos** | ETL, Bases de datos, Cloud | Estad√≠stica, ML, Programaci√≥n | SQL, Visualizaci√≥n, Business |\n",
    "| **Tiempo en C√≥digo** | 80% | 60% | 30% |\n",
    "\n",
    "### üîÑ Flujo de Trabajo Colaborativo\n",
    "\n",
    "```\n",
    "Data Engineer ‚Üí Prepara los datos\n",
    "      ‚Üì\n",
    "Data Scientist ‚Üí Crea modelos\n",
    "      ‚Üì\n",
    "Data Analyst ‚Üí Genera insights de negocio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7a048",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Anatom√≠a de un Pipeline de Datos\n",
    "\n",
    "Un **pipeline de datos** es un conjunto de procesos que mueve y transforma datos desde su origen hasta su destino.\n",
    "\n",
    "### üìã Componentes Principales:\n",
    "\n",
    "1. **Extract (Extraer)** üîΩ\n",
    "   - APIs, bases de datos, archivos\n",
    "   - Web scraping\n",
    "   - Streams en tiempo real\n",
    "\n",
    "2. **Transform (Transformar)** ‚öôÔ∏è\n",
    "   - Limpiar datos (nulls, duplicados)\n",
    "   - Cambiar formatos\n",
    "   - Calcular m√©tricas\n",
    "   - Validar calidad\n",
    "\n",
    "3. **Load (Cargar)** üì§\n",
    "   - Data warehouses\n",
    "   - Bases de datos\n",
    "   - Data lakes\n",
    "   - APIs de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c40e76",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Stack Tecnol√≥gico del Data Engineer\n",
    "\n",
    "### üêç Lenguajes de Programaci√≥n\n",
    "- **Python** (m√°s popular)\n",
    "- **SQL** (fundamental)\n",
    "- **Scala** (para Spark)\n",
    "- **Java** (ecosistema big data)\n",
    "\n",
    "### üóÑÔ∏è Almacenamiento\n",
    "- **Relacionales**: PostgreSQL, MySQL\n",
    "- **NoSQL**: MongoDB, Cassandra\n",
    "- **Cloud**: BigQuery, Redshift, Snowflake\n",
    "\n",
    "### ‚ö° Procesamiento\n",
    "- **Batch**: Apache Spark, pandas\n",
    "- **Streaming**: Kafka, Apache Beam\n",
    "- **Orquestaci√≥n**: Airflow, Prefect\n",
    "\n",
    "### ‚òÅÔ∏è Cloud Platforms\n",
    "- **AWS**: S3, Glue, Lambda\n",
    "- **GCP**: BigQuery, Dataflow\n",
    "- **Azure**: Synapse, Data Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef8564",
   "metadata": {},
   "source": [
    "## üöÄ Ejercicio Pr√°ctico: Mi Primer Pipeline\n",
    "\n",
    "Vamos a crear un pipeline simple que:\n",
    "1. **Extraiga** datos de una API p√∫blica\n",
    "2. **Transforme** la informaci√≥n\n",
    "3. **Cargue** el resultado en un archivo CSV\n",
    "\n",
    "¬°Empecemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dc4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a71c7",
   "metadata": {},
   "source": [
    "### üîΩ Paso 1: Extract - Extraer datos de una API\n",
    "\n",
    "Usaremos la API p√∫blica **JSONPlaceholder** para obtener datos de usuarios ficticios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer datos de la API\n",
    "def extraer_datos_usuarios():\n",
    "    \"\"\"\n",
    "    Extrae datos de usuarios desde JSONPlaceholder API\n",
    "    Returns: dict con los datos o None si hay error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Conectando a la API...\")\n",
    "        \n",
    "        # Hacer petici√≥n a la API\n",
    "        url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Verificar que la petici√≥n fue exitosa\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Convertir respuesta a JSON\n",
    "        datos = response.json()\n",
    "        \n",
    "        print(f\"‚úÖ Datos extra√≠dos exitosamente: {len(datos)} usuarios\")\n",
    "        return datos\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error al conectar con la API: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Error al decodificar JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extraer los datos\n",
    "datos_raw = extraer_datos_usuarios()\n",
    "\n",
    "# Mostrar los primeros registros\n",
    "if datos_raw:\n",
    "    print(\"\\nüìã Primeros 2 registros:\")\n",
    "    for i, usuario in enumerate(datos_raw[:2]):\n",
    "        print(f\"Usuario {i+1}: {usuario['name']} ({usuario['email']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2b108",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Paso 2: Transform - Transformar y limpiar los datos\n",
    "\n",
    "Ahora vamos a:\n",
    "- Aplanar la estructura JSON\n",
    "- Seleccionar solo las columnas que necesitamos\n",
    "- Limpiar y validar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c89419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_datos_usuarios(datos_raw):\n",
    "    \"\"\"\n",
    "    Transforma los datos raw en un formato limpio y estructurado\n",
    "    Args: datos_raw (list): Lista de usuarios desde la API\n",
    "    Returns: pd.DataFrame con datos transformados\n",
    "    \"\"\"\n",
    "    if not datos_raw:\n",
    "        print(\"‚ùå No hay datos para transformar\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üîÑ Iniciando transformaci√≥n de datos...\")\n",
    "    \n",
    "    # Lista para almacenar usuarios transformados\n",
    "    usuarios_transformados = []\n",
    "    \n",
    "    for usuario in datos_raw:\n",
    "        # Extraer y aplanar informaci√≥n relevante\n",
    "        usuario_limpio = {\n",
    "            'id': usuario['id'],\n",
    "            'nombre': usuario['name'],\n",
    "            'username': usuario['username'],\n",
    "            'email': usuario['email'].lower(),  # Normalizar email\n",
    "            'telefono': usuario['phone'],\n",
    "            'website': usuario['website'],\n",
    "            'ciudad': usuario['address']['city'],\n",
    "            'codigo_postal': usuario['address']['zipcode'],\n",
    "            'latitud': float(usuario['address']['geo']['lat']),\n",
    "            'longitud': float(usuario['address']['geo']['lng']),\n",
    "            'empresa': usuario['company']['name'],\n",
    "            'empresa_sector': usuario['company']['bs'],\n",
    "            'fecha_procesamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        usuarios_transformados.append(usuario_limpio)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df = pd.DataFrame(usuarios_transformados)\n",
    "    \n",
    "    # Validaciones b√°sicas\n",
    "    print(f\"üìä Datos transformados: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "    print(f\"üìß Emails √∫nicos: {df['email'].nunique()}/{len(df)}\")\n",
    "    print(f\"üè¢ Empresas √∫nicas: {df['empresa'].nunique()}\")\n",
    "    \n",
    "    # Verificar datos faltantes\n",
    "    valores_nulos = df.isnull().sum().sum()\n",
    "    print(f\"‚ùì Valores nulos encontrados: {valores_nulos}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Transformar los datos\n",
    "df_usuarios = transformar_datos_usuarios(datos_raw)\n",
    "\n",
    "# Mostrar resumen de los datos transformados\n",
    "if df_usuarios is not None:\n",
    "    print(\"\\nüìã Muestra de datos transformados:\")\n",
    "    print(df_usuarios.head(3))\n",
    "    \n",
    "    print(\"\\nüìä Informaci√≥n del DataFrame:\")\n",
    "    print(df_usuarios.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18dce09",
   "metadata": {},
   "source": [
    "### üì§ Paso 3: Load - Cargar datos en destino final\n",
    "\n",
    "Finalmente, guardaremos los datos procesados en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa991b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(df, ruta_destino):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados en un archivo CSV\n",
    "    Args:\n",
    "        df (pd.DataFrame): Datos a guardar\n",
    "        ruta_destino (str): Ruta donde guardar el archivo\n",
    "    Returns:\n",
    "        bool: True si se guard√≥ exitosamente\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No hay datos para cargar\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Guardando datos en: {ruta_destino}\")\n",
    "        \n",
    "        # Crear directorio si no existe\n",
    "        directorio = os.path.dirname(ruta_destino)\n",
    "        os.makedirs(directorio, exist_ok=True)\n",
    "        \n",
    "        # Guardar CSV con encoding UTF-8\n",
    "        df.to_csv(ruta_destino, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Verificar que el archivo se cre√≥ correctamente\n",
    "        if os.path.exists(ruta_destino):\n",
    "            tama√±o_archivo = os.path.getsize(ruta_destino)\n",
    "            print(f\"‚úÖ Datos guardados exitosamente\")\n",
    "            print(f\"üìÅ Archivo: {ruta_destino}\")\n",
    "            print(f\"üìè Tama√±o: {tama√±o_archivo:,} bytes\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Error: El archivo no se cre√≥\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al guardar datos: {e}\")\n",
    "        return False\n",
    "\n",
    "# Definir ruta de destino\n",
    "ruta_output = \"../datasets/processed/usuarios_transformados.csv\"\n",
    "\n",
    "# Cargar los datos\n",
    "exito = cargar_datos(df_usuarios, ruta_output)\n",
    "\n",
    "if exito:\n",
    "    print(\"\\nüéâ ¬°Pipeline ejecutado exitosamente!\")\n",
    "    print(\"\\nüìä Resumen del pipeline:\")\n",
    "    print(f\"   ‚Ä¢ Extra√≠dos: {len(datos_raw)} registros\")\n",
    "    print(f\"   ‚Ä¢ Transformados: {len(df_usuarios)} registros\")\n",
    "    print(f\"   ‚Ä¢ Cargados: {len(df_usuarios)} registros\")\n",
    "    print(f\"   ‚Ä¢ Archivo generado: {ruta_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21591cd7",
   "metadata": {},
   "source": [
    "## üìä An√°lisis B√°sico de los Datos Procesados\n",
    "\n",
    "Ahora que tenemos nuestros datos procesados, hagamos un an√°lisis exploratorio b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6431a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis exploratorio b√°sico\n",
    "if df_usuarios is not None:\n",
    "    print(\"üîç AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas\n",
    "    print(f\"üìä Total de usuarios: {len(df_usuarios)}\")\n",
    "    print(f\"üè¢ Empresas √∫nicas: {df_usuarios['empresa'].nunique()}\")\n",
    "    print(f\"üåç Ciudades √∫nicas: {df_usuarios['ciudad'].nunique()}\")\n",
    "    \n",
    "    # Top 5 ciudades\n",
    "    print(\"\\nüèôÔ∏è Top 5 ciudades:\")\n",
    "    ciudades_top = df_usuarios['ciudad'].value_counts().head()\n",
    "    for ciudad, count in ciudades_top.items():\n",
    "        print(f\"   ‚Ä¢ {ciudad}: {count} usuarios\")\n",
    "    \n",
    "    # Dominios de email m√°s comunes\n",
    "    print(\"\\nüìß Dominios de email:\")\n",
    "    df_usuarios['dominio_email'] = df_usuarios['email'].str.split('@').str[1]\n",
    "    dominios_top = df_usuarios['dominio_email'].value_counts().head()\n",
    "    for dominio, count in dominios_top.items():\n",
    "        print(f\"   ‚Ä¢ {dominio}: {count} usuarios\")\n",
    "    \n",
    "    # Coordenadas geogr√°ficas (rango)\n",
    "    print(\"\\nüåê Rango geogr√°fico:\")\n",
    "    print(f\"   ‚Ä¢ Latitud: {df_usuarios['latitud'].min():.2f} a {df_usuarios['latitud'].max():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Longitud: {df_usuarios['longitud'].min():.2f} a {df_usuarios['longitud'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbfd83",
   "metadata": {},
   "source": [
    "## üìà Visualizaci√≥n B√°sica\n",
    "\n",
    "Vamos a crear algunas visualizaciones simples para entender mejor nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo de gr√°ficos\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if df_usuarios is not None:\n",
    "    # Crear figura con subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üìä An√°lisis de Datos de Usuarios', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 1: Distribuci√≥n por ciudades\n",
    "    ciudades_count = df_usuarios['ciudad'].value_counts()\n",
    "    axes[0, 0].bar(range(len(ciudades_count)), ciudades_count.values)\n",
    "    axes[0, 0].set_title('üèôÔ∏è Usuarios por Ciudad')\n",
    "    axes[0, 0].set_xlabel('Ciudad')\n",
    "    axes[0, 0].set_ylabel('N√∫mero de Usuarios')\n",
    "    axes[0, 0].set_xticks(range(len(ciudades_count)))\n",
    "    axes[0, 0].set_xticklabels(ciudades_count.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Gr√°fico 2: Distribuci√≥n de dominios de email\n",
    "    dominios_count = df_usuarios['dominio_email'].value_counts()\n",
    "    axes[0, 1].pie(dominios_count.values, labels=dominios_count.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('üìß Distribuci√≥n de Dominios de Email')\n",
    "    \n",
    "    # Gr√°fico 3: Distribuci√≥n geogr√°fica\n",
    "    scatter = axes[1, 0].scatter(df_usuarios['longitud'], df_usuarios['latitud'], \n",
    "                                c=range(len(df_usuarios)), cmap='viridis', alpha=0.7)\n",
    "    axes[1, 0].set_title('üåç Distribuci√≥n Geogr√°fica')\n",
    "    axes[1, 0].set_xlabel('Longitud')\n",
    "    axes[1, 0].set_ylabel('Latitud')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Longitud de nombres de empresa\n",
    "    df_usuarios['empresa_longitud'] = df_usuarios['empresa'].str.len()\n",
    "    axes[1, 1].hist(df_usuarios['empresa_longitud'], bins=8, alpha=0.7, color='skyblue')\n",
    "    axes[1, 1].set_title('üìè Longitud de Nombres de Empresa')\n",
    "    axes[1, 1].set_xlabel('Caracteres')\n",
    "    axes[1, 1].set_ylabel('Frecuencia')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajustar layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizaciones generadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbb82f",
   "metadata": {},
   "source": [
    "## üèÜ Ejercicio Pr√°ctico: ¬°Tu Turno!\n",
    "\n",
    "Ahora es tu turno de crear un pipeline. Vamos a trabajar con otra API p√∫blica.\n",
    "\n",
    "### üìù Instrucciones:\n",
    "1. Usa la API de **Posts**: `https://jsonplaceholder.typicode.com/posts`\n",
    "2. Extrae todos los posts\n",
    "3. Transforma los datos agregando:\n",
    "   - Longitud del t√≠tulo\n",
    "   - Longitud del cuerpo\n",
    "   - Categor√≠a basada en el userId (ej: \"usuario_1\", \"usuario_2\", etc.)\n",
    "4. Guarda el resultado en `../datasets/processed/posts_transformados.csv`\n",
    "\n",
    "¬°Usa el c√≥digo anterior como referencia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ EJERCICIO: Completa el c√≥digo siguiente\n",
    "\n",
    "def extraer_posts():\n",
    "    \"\"\"\n",
    "    Extrae posts desde JSONPlaceholder API\n",
    "    TODO: Implementar la funci√≥n\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠\n",
    "    pass\n",
    "\n",
    "def transformar_posts(posts_raw):\n",
    "    \"\"\"\n",
    "    Transforma los posts agregando m√©tricas adicionales\n",
    "    TODO: Implementar la funci√≥n\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠\n",
    "    pass\n",
    "\n",
    "# Ejecutar tu pipeline\n",
    "print(\"üöÄ Ejecutando tu pipeline de posts...\")\n",
    "\n",
    "# posts_raw = extraer_posts()\n",
    "# df_posts = transformar_posts(posts_raw)\n",
    "# exito_posts = cargar_datos(df_posts, \"../datasets/processed/posts_transformados.csv\")\n",
    "\n",
    "print(\"\\nüìù Descomenta las l√≠neas anteriores cuando hayas completado las funciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1c81e",
   "metadata": {},
   "source": [
    "## üí° Soluci√≥n del Ejercicio\n",
    "\n",
    "Aqu√≠ tienes una posible soluci√≥n. ¬°Comp√°rala con tu implementaci√≥n!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc982832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUCI√ìN DEL EJERCICIO\n",
    "\n",
    "def extraer_posts():\n",
    "    \"\"\"\n",
    "    Extrae posts desde JSONPlaceholder API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        posts = response.json()\n",
    "        print(f\"‚úÖ Posts extra√≠dos: {len(posts)}\")\n",
    "        return posts\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def transformar_posts(posts_raw):\n",
    "    \"\"\"\n",
    "    Transforma los posts agregando m√©tricas adicionales\n",
    "    \"\"\"\n",
    "    if not posts_raw:\n",
    "        return None\n",
    "    \n",
    "    posts_transformados = []\n",
    "    \n",
    "    for post in posts_raw:\n",
    "        post_limpio = {\n",
    "            'id': post['id'],\n",
    "            'userId': post['userId'],\n",
    "            'titulo': post['title'],\n",
    "            'cuerpo': post['body'],\n",
    "            'longitud_titulo': len(post['title']),\n",
    "            'longitud_cuerpo': len(post['body']),\n",
    "            'categoria_usuario': f\"usuario_{post['userId']}\",\n",
    "            'palabras_titulo': len(post['title'].split()),\n",
    "            'fecha_procesamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        posts_transformados.append(post_limpio)\n",
    "    \n",
    "    df = pd.DataFrame(posts_transformados)\n",
    "    print(f\"üìä Posts transformados: {len(df)} registros\")\n",
    "    return df\n",
    "\n",
    "# Ejecutar pipeline de posts\n",
    "posts_raw = extraer_posts()\n",
    "df_posts = transformar_posts(posts_raw)\n",
    "\n",
    "if df_posts is not None:\n",
    "    # Mostrar estad√≠sticas\n",
    "    print(\"\\nüìä Estad√≠sticas de Posts:\")\n",
    "    print(f\"   ‚Ä¢ Total posts: {len(df_posts)}\")\n",
    "    print(f\"   ‚Ä¢ Usuarios √∫nicos: {df_posts['userId'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Promedio palabras t√≠tulo: {df_posts['palabras_titulo'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Promedio caracteres cuerpo: {df_posts['longitud_cuerpo'].mean():.0f}\")\n",
    "    \n",
    "    # Guardar datos\n",
    "    exito_posts = cargar_datos(df_posts, \"../datasets/processed/posts_transformados.csv\")\n",
    "    \n",
    "    if exito_posts:\n",
    "        print(\"\\nüéâ ¬°Ejercicio completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ef12f",
   "metadata": {},
   "source": [
    "## üéØ Resumen y Pr√≥ximos Pasos\n",
    "\n",
    "### ‚úÖ Lo que Aprendiste Hoy:\n",
    "\n",
    "1. **Conceptos Fundamentales**:\n",
    "   - Qu√© es la Ingenier√≠a de Datos\n",
    "   - Diferencias entre roles de datos\n",
    "   - Componentes de un pipeline ETL\n",
    "\n",
    "2. **Habilidades Pr√°cticas**:\n",
    "   - Extracci√≥n de datos desde APIs\n",
    "   - Transformaci√≥n y limpieza b√°sica\n",
    "   - Carga de datos en archivos CSV\n",
    "   - An√°lisis exploratorio simple\n",
    "\n",
    "3. **Herramientas Utilizadas**:\n",
    "   - `requests` para APIs\n",
    "   - `pandas` para manipulaci√≥n\n",
    "   - `matplotlib/seaborn` para visualizaci√≥n\n",
    "\n",
    "### üîÆ Pr√≥ximos Notebooks:\n",
    "\n",
    "- **02_setup_entorno_desarrollo.ipynb**: Configuraci√≥n avanzada\n",
    "- **03_git_version_control.ipynb**: Control de versiones\n",
    "- **04_python_estructuras_datos.ipynb**: Python intermedio\n",
    "\n",
    "### üè† Tarea Opcional:\n",
    "\n",
    "1. Experimenta con otras APIs p√∫blicas:\n",
    "   - [GitHub API](https://api.github.com)\n",
    "   - [OpenWeather API](https://openweathermap.org/api)\n",
    "   - [REST Countries](https://restcountries.com)\n",
    "\n",
    "2. Modifica el pipeline para agregar validaciones de calidad de datos\n",
    "\n",
    "3. Investiga qu√© es Apache Airflow y c√≥mo se relaciona con lo que hicimos\n",
    "\n",
    "---\n",
    "\n",
    "## üéä ¬°Felicitaciones!\n",
    "\n",
    "Has completado tu primer notebook de Ingenier√≠a de Datos y creado tu primer pipeline ETL. \n",
    "\n",
    "**¬°Bienvenido al fascinante mundo de la Ingenier√≠a de Datos!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*¬øTienes preguntas o sugerencias? ¬°Abre un issue en el repositorio!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
