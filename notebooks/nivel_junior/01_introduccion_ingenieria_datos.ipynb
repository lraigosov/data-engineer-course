{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41da9f8",
   "metadata": {},
   "source": [
    "# üìä Junior - 01. Introducci√≥n a la Ingenier√≠a de Datos\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender qu√© es la Ingenier√≠a de Datos y su importancia\n",
    "- [ ] Diferenciar roles: Data Engineer vs Data Scientist vs Data Analyst\n",
    "- [ ] Entender el concepto de pipeline de datos\n",
    "- [ ] Conocer herramientas y tecnolog√≠as fundamentales\n",
    "- [ ] Realizar primeros ejercicios pr√°cticos con Python\n",
    "\n",
    "**Duraci√≥n Estimada:** 60-75 minutos  \n",
    "**Nivel de Dificultad:** Principiante  \n",
    "**Prerrequisitos:** Conocimientos b√°sicos de programaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27fda6",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANTE - LEE ESTO PRIMERO\n",
    "\n",
    "### üö® SOBRE EL USO DE JUPYTER NOTEBOOKS EN ESTE CURSO\n",
    "\n",
    "Este curso utiliza **Jupyter Notebooks exclusivamente con fines educativos** para facilitar el aprendizaje interactivo.\n",
    "\n",
    "**‚ùå EN PRODUCCI√ìN, LOS NOTEBOOKS NO SON UNA PR√ÅCTICA RECOMENDABLE**\n",
    "\n",
    "#### ‚úÖ Notebooks son excelentes para:\n",
    "- üìö Aprendizaje y ense√±anza (como este curso)\n",
    "- üî¨ Exploraci√≥n y an√°lisis de datos\n",
    "- üß™ Prototipado r√°pido\n",
    "- üìä Visualizaci√≥n interactiva\n",
    "\n",
    "#### ‚ùå En producci√≥n debes usar:\n",
    "- ‚úÖ **Scripts Python modulares** (`.py`) con estructura clara\n",
    "- ‚úÖ **Paquetes con testing** (pytest, unittest)\n",
    "- ‚úÖ **CI/CD pipelines** (GitHub Actions, GitLab CI)\n",
    "- ‚úÖ **Orquestadores** (Apache Airflow, Prefect, Dagster)\n",
    "- ‚úÖ **Contenedores** (Docker, Kubernetes)\n",
    "- ‚úÖ **Logging y monitoreo** (Datadog, Prometheus)\n",
    "\n",
    "#### üìñ Recursos adicionales:\n",
    "Lee el archivo `‚ö†Ô∏è_IMPORTANTE_LEER_PRIMERO.md` en la carpeta notebooks para detalles completos sobre:\n",
    "- Flujo de trabajo recomendado (exploraci√≥n ‚Üí prototipado ‚Üí producci√≥n)\n",
    "- Comparaci√≥n notebooks vs c√≥digo de producci√≥n\n",
    "- Ejemplos de conversi√≥n de notebooks a scripts\n",
    "\n",
    "**Recuerda:** *\"Learn with notebooks, deploy with production code.\"*\n",
    "\n",
    "---\n",
    "\n",
    "**Autor del Curso:** LuisRai (Luis J. Raigoso V.)  \n",
    "¬© 2024-2025 - Data Engineering Modular Course\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7acc16",
   "metadata": {},
   "source": [
    "## üéØ ¬øQu√© es la Ingenier√≠a de Datos?\n",
    "\n",
    "La **Ingenier√≠a de Datos** es la disciplina que se encarga de:\n",
    "\n",
    "‚úÖ **Extraer** datos de m√∫ltiples fuentes  \n",
    "‚úÖ **Transformar** y limpiar los datos  \n",
    "‚úÖ **Cargar** datos en sistemas de almacenamiento  \n",
    "‚úÖ **Orquestar** procesos automatizados  \n",
    "‚úÖ **Monitorear** la calidad y performance  \n",
    "\n",
    "### üåü Analog√≠a: El Data Engineer como \"Plomero de Datos\"\n",
    "\n",
    "Imagina que los datos son como agua en una ciudad:\n",
    "\n",
    "- **Fuentes de agua** = Bases de datos, APIs, archivos\n",
    "- **Tuber√≠as** = Pipelines de datos\n",
    "- **Tratamiento** = Limpieza y transformaci√≥n\n",
    "- **Distribuci√≥n** = Data warehouses, dashboards\n",
    "- **Calidad** = Monitoreo y alertas\n",
    "\n",
    "El Data Engineer construye y mantiene toda esta \"infraestructura de datos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a4c2d",
   "metadata": {},
   "source": [
    "## üë• Comparaci√≥n de Roles en el Ecosistema de Datos\n",
    "\n",
    "| Aspecto | Data Engineer | Data Scientist | Data Analyst |\n",
    "|---------|---------------|----------------|---------------|\n",
    "| **Foco Principal** | Infraestructura y pipelines | Modelos y algoritmos | Reportes y insights |\n",
    "| **Herramientas** | Python, SQL, Airflow | Python, R, TensorFlow | SQL, Excel, Tableau |\n",
    "| **Output** | Datos limpios y accesibles | Modelos predictivos | Dashboards y reportes |\n",
    "| **Skills T√©cnicos** | ETL, Bases de datos, Cloud | Estad√≠stica, ML, Programaci√≥n | SQL, Visualizaci√≥n, Business |\n",
    "| **Tiempo en C√≥digo** | 80% | 60% | 30% |\n",
    "\n",
    "### üîÑ Flujo de Trabajo Colaborativo\n",
    "\n",
    "```\n",
    "Data Engineer ‚Üí Prepara los datos\n",
    "      ‚Üì\n",
    "Data Scientist ‚Üí Crea modelos\n",
    "      ‚Üì\n",
    "Data Analyst ‚Üí Genera insights de negocio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7a048",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Anatom√≠a de un Pipeline de Datos\n",
    "\n",
    "Un **pipeline de datos** es un conjunto de procesos que mueve y transforma datos desde su origen hasta su destino.\n",
    "\n",
    "### üìã Componentes Principales:\n",
    "\n",
    "1. **Extract (Extraer)** üîΩ\n",
    "   - APIs, bases de datos, archivos\n",
    "   - Web scraping\n",
    "   - Streams en tiempo real\n",
    "\n",
    "2. **Transform (Transformar)** ‚öôÔ∏è\n",
    "   - Limpiar datos (nulls, duplicados)\n",
    "   - Cambiar formatos\n",
    "   - Calcular m√©tricas\n",
    "   - Validar calidad\n",
    "\n",
    "3. **Load (Cargar)** üì§\n",
    "   - Data warehouses\n",
    "   - Bases de datos\n",
    "   - Data lakes\n",
    "   - APIs de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c40e76",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Stack Tecnol√≥gico del Data Engineer\n",
    "\n",
    "### üêç Lenguajes de Programaci√≥n\n",
    "- **Python** (m√°s popular)\n",
    "- **SQL** (fundamental)\n",
    "- **Scala** (para Spark)\n",
    "- **Java** (ecosistema big data)\n",
    "\n",
    "### üóÑÔ∏è Almacenamiento\n",
    "- **Relacionales**: PostgreSQL, MySQL\n",
    "- **NoSQL**: MongoDB, Cassandra\n",
    "- **Cloud**: BigQuery, Redshift, Snowflake\n",
    "\n",
    "### ‚ö° Procesamiento\n",
    "- **Batch**: Apache Spark, pandas\n",
    "- **Streaming**: Kafka, Apache Beam\n",
    "- **Orquestaci√≥n**: Airflow, Prefect\n",
    "\n",
    "### ‚òÅÔ∏è Cloud Platforms\n",
    "- **AWS**: S3, Glue, Lambda\n",
    "- **GCP**: BigQuery, Dataflow\n",
    "- **Azure**: Synapse, Data Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef8564",
   "metadata": {},
   "source": [
    "## üöÄ Ejercicio Pr√°ctico: Mi Primer Pipeline\n",
    "\n",
    "Vamos a crear un pipeline simple que:\n",
    "1. **Extraiga** datos de una API p√∫blica\n",
    "2. **Transforme** la informaci√≥n\n",
    "3. **Cargue** el resultado en un archivo CSV\n",
    "\n",
    "¬°Empecemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab9d4c",
   "metadata": {},
   "source": [
    "### üìñ Tu Primer C√≥digo - Configuraci√≥n del Entorno Python\n",
    "\n",
    "**¬øQu√© estamos haciendo aqu√≠?**\n",
    "Este es el **punto de partida** de todo proyecto de Data Engineering en Python. Antes de procesar datos, necesitamos:\n",
    "1. Verificar que Python funciona correctamente\n",
    "2. Importar las bibliotecas esenciales\n",
    "3. Validar las versiones instaladas\n",
    "\n",
    "**¬øPor qu√© es importante?**\n",
    "- **Reproducibilidad**: Documentar versiones evita el \"en mi m√°quina funciona\"\n",
    "- **Compatibilidad**: Pandas 1.x vs 2.x tienen diferencias importantes\n",
    "- **Debugging**: Si algo falla, saber las versiones ayuda a investigar\n",
    "- **Documentaci√≥n**: Los stakeholders necesitan saber el stack t√©cnico\n",
    "\n",
    "**Las bibliotecas fundamentales del Data Engineer:**\n",
    "\n",
    "| Biblioteca | Prop√≥sito | Alternativas |\n",
    "|------------|-----------|--------------|\n",
    "| **pandas** | Manipulaci√≥n de datos tabulares | Polars, Dask |\n",
    "| **numpy** | C√°lculos num√©ricos, arrays | - |\n",
    "| **matplotlib** | Visualizaci√≥n b√°sica | Seaborn, Plotly |\n",
    "| **json** | Parsear/serializar JSON | - (built-in) |\n",
    "| **csv** | Leer/escribir CSV | - (built-in) |\n",
    "\n",
    "**Convenci√≥n de imports:**\n",
    "```python\n",
    "import pandas as pd        # pd es el alias est√°ndar\n",
    "import numpy as np         # np es el alias est√°ndar\n",
    "import matplotlib.pyplot as plt  # plt es el alias est√°ndar\n",
    "```\n",
    "\n",
    "**¬øPor qu√© usar alias?**\n",
    "- **Menos escritura**: `pd.DataFrame()` vs `pandas.DataFrame()`\n",
    "- **Convenci√≥n universal**: Cualquier desarrollador lo entiende\n",
    "- **Legibilidad**: C√≥digo m√°s limpio y compacto\n",
    "\n",
    "**Verificaci√≥n de versiones:**\n",
    "```python\n",
    "print(pd.__version__)  # Doble underscore antes y despu√©s\n",
    "```\n",
    "\n",
    "**Versiones recomendadas para este curso:**\n",
    "- Python: 3.8+\n",
    "- pandas: 1.5.0+ (o 2.0+ para mejor performance)\n",
    "- numpy: 1.20.0+\n",
    "- matplotlib: 3.5.0+\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Importar las bibliotecas esenciales con los alias est√°ndar\n",
    "2. Verificar versiones instaladas con `__version__`\n",
    "3. Por qu√© estas bibliotecas son fundamentales\n",
    "4. La convenci√≥n de nomenclatura en la comunidad Python\n",
    "5. C√≥mo detectar problemas de instalaci√≥n tempranamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dc4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSTALACI√ìN Y VERIFICACI√ìN DE BIBLIOTECAS =====\n",
    "# Este c√≥digo verifica que tengas las bibliotecas necesarias instaladas\n",
    "# Si no las tienes, ejecuta en terminal: pip install pandas numpy\n",
    "\n",
    "# Importar pandas: la biblioteca m√°s importante para manipulaci√≥n de datos\n",
    "# Pandas nos permite trabajar con tablas (DataFrames) de manera eficiente\n",
    "import pandas as pd\n",
    "\n",
    "# Importar numpy: biblioteca para operaciones num√©ricas y matrices\n",
    "# Es la base sobre la que se construye pandas\n",
    "import numpy as np\n",
    "\n",
    "# Verificar versiones instaladas\n",
    "print(\"‚úÖ Bibliotecas importadas exitosamente!\")\n",
    "print(f\"\udce6 Pandas versi√≥n: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy versi√≥n: {np.__version__}\")\n",
    "print(\"\\nüéâ ¬°Tu entorno est√° listo para comenzar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a71c7",
   "metadata": {},
   "source": [
    "### üîΩ Paso 1: Extract - Extraer datos de una API\n",
    "\n",
    "Usaremos la API p√∫blica **JSONPlaceholder** para obtener datos de usuarios ficticios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d26a3",
   "metadata": {},
   "source": [
    "### üìñ Paso 1: Extract (Extracci√≥n) - Obteniendo los Datos Crudos\n",
    "\n",
    "**¬øQu√© es la Extracci√≥n?**\n",
    "Es la **\"E\" de ETL** - el primer paso de cualquier pipeline. Consiste en:\n",
    "- Leer datos desde la fuente origen\n",
    "- Validar que existan y sean accesibles\n",
    "- Cargarlos en memoria (o procesarlos en streaming)\n",
    "\n",
    "**Fuentes comunes de datos:**\n",
    "1. **Archivos locales**: CSV, JSON, Excel, Parquet\n",
    "2. **APIs REST**: Endpoints HTTP que retornan JSON/XML\n",
    "3. **Bases de datos**: PostgreSQL, MySQL, MongoDB\n",
    "4. **Cloud Storage**: S3, Azure Blob, Google Cloud Storage\n",
    "5. **Streaming**: Kafka, Kinesis, Pub/Sub\n",
    "\n",
    "**En este ejemplo:**\n",
    "Estamos simulando datos de ventas que t√≠picamente vendr√≠an de:\n",
    "- Sistema POS (Point of Sale)\n",
    "- CRM (Salesforce, HubSpot)\n",
    "- ERP (SAP, Oracle)\n",
    "- Plataforma e-commerce (Shopify, WooCommerce)\n",
    "\n",
    "**La estructura del diccionario:**\n",
    "```python\n",
    "datos_ventas = {\n",
    "    'fecha': [...],         # Cu√°ndo ocurri√≥ la venta\n",
    "    'producto': [...],      # Qu√© se vendi√≥\n",
    "    'cantidad': [...],      # Cu√°ntas unidades\n",
    "    'precio_unitario': [...],  # Precio por unidad\n",
    "    'cliente': [...],       # Qui√©n compr√≥\n",
    "    'region': [...]         # D√≥nde se vendi√≥\n",
    "}\n",
    "```\n",
    "\n",
    "**¬øPor qu√© un diccionario de listas y no una lista de diccionarios?**\n",
    "\n",
    "**Opci√≥n 1 - Diccionario de listas (columnar - LO QUE USAMOS):**\n",
    "```python\n",
    "datos = {\n",
    "    'nombre': ['Ana', 'Luis'],\n",
    "    'edad': [25, 30]\n",
    "}\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- Estructura natural de pandas (orientado a columnas)\n",
    "- M√°s eficiente en memoria\n",
    "- Conversi√≥n directa a DataFrame\n",
    "\n",
    "**Opci√≥n 2 - Lista de diccionarios (orientado a filas):**\n",
    "```python\n",
    "datos = [\n",
    "    {'nombre': 'Ana', 'edad': 25},\n",
    "    {'nombre': 'Luis', 'edad': 30}\n",
    "]\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- M√°s intuitivo para algunos casos\n",
    "- Representa mejor registros individuales\n",
    "\n",
    "**Conversi√≥n a DataFrame:**\n",
    "```python\n",
    "df = pd.DataFrame(datos_ventas)\n",
    "```\n",
    "\n",
    "Esto crea una **tabla estructurada** donde:\n",
    "- Cada clave del dict ‚Üí Columna del DataFrame\n",
    "- Cada lista ‚Üí Valores de esa columna\n",
    "- Los √≠ndices se asignan autom√°ticamente (0, 1, 2, ...)\n",
    "\n",
    "**Conceptos clave:**\n",
    "- **DataFrame**: La estructura de datos fundamental de pandas (piensa en Excel programable)\n",
    "- **Columnas**: Atributos/caracter√≠sticas de los datos\n",
    "- **Filas**: Registros/observaciones individuales\n",
    "- **√çndice**: Identificador √∫nico de cada fila (auto-generado o personalizado)\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Crear datos simulados con diccionarios de listas\n",
    "2. Convertir diccionarios a DataFrames con `pd.DataFrame()`\n",
    "3. La diferencia entre orientaci√≥n columnar vs filas\n",
    "4. Estructura b√°sica de un DataFrame (√≠ndice, columnas, valores)\n",
    "5. Por qu√© pandas usa orientaci√≥n columnar por defecto\n",
    "6. Simular datos de un sistema real (ventas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PASO 1: EXTRAER (Extract) - Crear datos de ejemplo =====\n",
    "# En un escenario real, estos datos vendr√≠an de:\n",
    "# - Una base de datos (PostgreSQL, MySQL, MongoDB)\n",
    "# - Una API REST (endpoint de una aplicaci√≥n web)\n",
    "# - Archivos CSV, JSON, Excel\n",
    "# - Servicios en la nube (AWS S3, Google Cloud Storage)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Simulamos datos de ventas de una tienda online\n",
    "# Cada diccionario representa una transacci√≥n de venta\n",
    "datos_ventas = [\n",
    "    {\n",
    "        'fecha': '2024-01-15',           # Fecha de la transacci√≥n (formato YYYY-MM-DD)\n",
    "        'producto': 'Laptop',            # Nombre del producto vendido\n",
    "        'cantidad': 2,                   # Unidades compradas\n",
    "        'precio_unitario': 1200,         # Precio por unidad en USD\n",
    "        'cliente': 'Juan P√©rez'          # Identificaci√≥n del cliente\n",
    "    },\n",
    "    {\n",
    "        'fecha': '2024-01-16',\n",
    "        'producto': 'Mouse',\n",
    "        'cantidad': 5,\n",
    "        'precio_unitario': 25,\n",
    "        'cliente': 'Mar√≠a Garc√≠a'\n",
    "    },\n",
    "    {\n",
    "        'fecha': '2024-01-16',\n",
    "        'producto': 'Teclado',\n",
    "        'cantidad': 3,\n",
    "        'precio_unitario': 80,\n",
    "        'cliente': 'Carlos L√≥pez'\n",
    "    },\n",
    "    {\n",
    "        'fecha': '2024-01-17',\n",
    "        'producto': 'Monitor',\n",
    "        'cantidad': 1,\n",
    "        'precio_unitario': 350,\n",
    "        'cliente': 'Ana Mart√≠nez'\n",
    "    }\n",
    "]\n",
    "\n",
    "# ===== MOSTRAR LOS DATOS CRUDOS =====\n",
    "print(\"üì• DATOS CRUDOS (Raw Data):\")\n",
    "print(\"=\" * 60)\n",
    "# json.dumps(): convierte el diccionario Python a formato JSON legible\n",
    "# indent=2: agrega sangr√≠a para mejor visualizaci√≥n\n",
    "# ensure_ascii=False: permite mostrar caracteres especiales (tildes, √±)\n",
    "print(json.dumps(datos_ventas, indent=2, ensure_ascii=False))\n",
    "\n",
    "# ===== INFORMACI√ìN SOBRE LOS DATOS =====\n",
    "print(f\"\\nüìä INFORMACI√ìN B√ÅSICA:\")\n",
    "print(f\"   ‚Ä¢ Total de registros: {len(datos_ventas)}\")\n",
    "print(f\"   ‚Ä¢ Tipo de estructura: {type(datos_ventas).__name__}\")\n",
    "print(f\"   ‚Ä¢ Tipo de cada registro: {type(datos_ventas[0]).__name__}\")\n",
    "print(f\"   ‚Ä¢ Campos por registro: {len(datos_ventas[0])} columnas\")\n",
    "print(f\"   ‚Ä¢ Nombres de campos: {', '.join(datos_ventas[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2b108",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Paso 2: Transform - Transformar y limpiar los datos\n",
    "\n",
    "Ahora vamos a:\n",
    "- Aplanar la estructura JSON\n",
    "- Seleccionar solo las columnas que necesitamos\n",
    "- Limpiar y validar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d15302",
   "metadata": {},
   "source": [
    "### üìñ Paso 2: Transform (Transformaci√≥n) - Limpiando y Enriqueciendo Datos\n",
    "\n",
    "**¬øQu√© es la Transformaci√≥n?**\n",
    "Es la **\"T\" de ETL** - donde ocurre la **magia**. Aqu√≠ convertimos datos crudos en informaci√≥n √∫til:\n",
    "- Limpiar datos sucios (nulos, duplicados, formatos incorrectos)\n",
    "- Crear nuevas columnas calculadas (m√©tricas, KPIs)\n",
    "- Agregar, filtrar, enriquecer\n",
    "- Aplicar reglas de negocio\n",
    "\n",
    "**¬øPor qu√© es el paso m√°s complejo?**\n",
    "Porque aqu√≠ es donde:\n",
    "- **70-80% del tiempo se gasta** en proyectos de datos\n",
    "- Se aplica la **l√≥gica de negocio** espec√≠fica\n",
    "- Se descubren **problemas de calidad** de datos\n",
    "- Se generan **insights** y valor\n",
    "\n",
    "**En este ejemplo, vamos a:**\n",
    "\n",
    "**1. Crear columnas calculadas:**\n",
    "```python\n",
    "df['total'] = df['cantidad'] * df['precio_unitario']\n",
    "```\n",
    "- **Por qu√©**: El total no viene en los datos crudos, hay que calcularlo\n",
    "- **Beneficio**: Podemos analizar ingresos totales por venta\n",
    "- **Alternativa SQL**: `SELECT cantidad * precio_unitario AS total FROM ventas`\n",
    "\n",
    "**2. Calcular m√©tricas agregadas:**\n",
    "```python\n",
    "ingresos_por_region = df.groupby('region')['total'].sum()\n",
    "```\n",
    "- **groupby()**: Agrupa filas por una columna (como GROUP BY en SQL)\n",
    "- **sum()**: Suma los valores de cada grupo\n",
    "- **Resultado**: Una serie con el total de ingresos por regi√≥n\n",
    "\n",
    "**Anatom√≠a de groupby:**\n",
    "```\n",
    "Original DataFrame:\n",
    "region    | total\n",
    "----------|------\n",
    "Norte     | 100\n",
    "Norte     | 200\n",
    "Sur       | 150\n",
    "\n",
    "Despu√©s de groupby('region')['total'].sum():\n",
    "region    | total\n",
    "----------|------\n",
    "Norte     | 300\n",
    "Sur       | 150\n",
    "```\n",
    "\n",
    "**Operaciones comunes de transformaci√≥n:**\n",
    "\n",
    "| Operaci√≥n | Pandas | SQL Equivalente |\n",
    "|-----------|--------|-----------------|\n",
    "| Filtrar | `df[df['edad'] > 18]` | `WHERE edad > 18` |\n",
    "| Agregar | `df.groupby('ciudad').sum()` | `GROUP BY ciudad` |\n",
    "| Ordenar | `df.sort_values('fecha')` | `ORDER BY fecha` |\n",
    "| Joins | `pd.merge(df1, df2)` | `JOIN` |\n",
    "| Nuevas columnas | `df['nueva'] = df['a'] + df['b']` | `SELECT a + b AS nueva` |\n",
    "\n",
    "**Tipos de transformaciones:**\n",
    "\n",
    "**1. Element-wise (elemento por elemento):**\n",
    "```python\n",
    "df['precio_con_iva'] = df['precio'] * 1.21\n",
    "```\n",
    "\n",
    "**2. Agregaciones (reducen filas):**\n",
    "```python\n",
    "df.groupby('categoria').mean()\n",
    "```\n",
    "\n",
    "**3. Window functions (contexto de grupo):**\n",
    "```python\n",
    "df['rank'] = df.groupby('categoria')['precio'].rank()\n",
    "```\n",
    "\n",
    "**Mejores pr√°cticas:**\n",
    "- ‚úÖ **Inmutabilidad**: Crea nuevas columnas, no modifiques las originales (trazabilidad)\n",
    "- ‚úÖ **Nombres descriptivos**: `total_venta` mejor que `t` o `x`\n",
    "- ‚úÖ **Validaci√≥n**: Verifica que los c√°lculos tengan sentido (`total >= 0`)\n",
    "- ‚úÖ **Documentaci√≥n**: Comenta las transformaciones complejas\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Crear columnas calculadas con operaciones aritm√©ticas\n",
    "2. Usar `groupby()` para agregaciones (como GROUP BY en SQL)\n",
    "3. Aplicar funciones de agregaci√≥n (sum, mean, count, etc.)\n",
    "4. Entender la diferencia entre operaciones element-wise vs agregadas\n",
    "5. Estructura de un resultado de groupby\n",
    "6. Por qu√© las transformaciones son el coraz√≥n del ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c89419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PASO 2: TRANSFORMAR (Transform) - Convertir y Enriquecer Datos =====\n",
    "\n",
    "# 2.1 - Convertir lista de diccionarios a DataFrame de Pandas\n",
    "# DataFrame: estructura de datos tabular (filas y columnas) similar a Excel\n",
    "# Es la estructura fundamental para an√°lisis de datos en Python\n",
    "df = pd.DataFrame(datos_ventas)\n",
    "\n",
    "print(\"üîÑ TRANSFORMACI√ìN 1: Conversi√≥n a DataFrame\")\n",
    "print(\"=\" * 60)\n",
    "print(df)\n",
    "print(f\"\\n\udcd0 Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "\n",
    "# 2.2 - Convertir tipos de datos apropiados\n",
    "# Los datos crudos vienen como strings, necesitamos tipos correctos\n",
    "print(\"\\nüîÑ TRANSFORMACI√ìN 2: Conversi√≥n de Tipos de Datos\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Antes de la conversi√≥n, veamos los tipos actuales\n",
    "print(\"ANTES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convertir 'fecha' de string a tipo datetime\n",
    "# Esto permite operaciones como filtrar por rango de fechas, extraer mes/a√±o, etc.\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Convertir columnas num√©ricas a tipos num√©ricos apropiados\n",
    "# int64: enteros grandes (cantidad)\n",
    "# float64: n√∫meros decimales (precio)\n",
    "df['cantidad'] = df['cantidad'].astype('int64')\n",
    "df['precio_unitario'] = df['precio_unitario'].astype('float64')\n",
    "\n",
    "print(\"\\nDESPU√âS:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 2.3 - Crear nuevas columnas calculadas (Feature Engineering)\n",
    "print(\"\\nüîÑ TRANSFORMACI√ìN 3: Creaci√≥n de Columnas Calculadas\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular el total de cada venta (cantidad √ó precio_unitario)\n",
    "# Esta es una columna derivada que facilita el an√°lisis\n",
    "df['total'] = df['cantidad'] * df['precio_unitario']\n",
    "\n",
    "# Extraer el mes de la fecha\n",
    "# √ötil para an√°lisis de tendencias mensuales\n",
    "df['mes'] = df['fecha'].dt.month\n",
    "\n",
    "# Extraer el d√≠a de la semana (0=Lunes, 6=Domingo)\n",
    "df['dia_semana'] = df['fecha'].dt.dayofweek\n",
    "\n",
    "# Crear categor√≠as de productos (ejemplo de transformaci√≥n de negocio)\n",
    "# Podr√≠amos clasificar productos en categor√≠as como 'Electr√≥nica', 'Accesorios', etc.\n",
    "def categorizar_producto(producto):\n",
    "    \"\"\"\n",
    "    Funci√≥n que categoriza productos seg√∫n reglas de negocio\n",
    "    En producci√≥n, esto vendr√≠a de una tabla de referencia\n",
    "    \"\"\"\n",
    "    if producto in ['Laptop', 'Monitor']:\n",
    "        return 'Electr√≥nica Mayor'\n",
    "    elif producto in ['Mouse', 'Teclado']:\n",
    "        return 'Accesorios'\n",
    "    else:\n",
    "        return 'Otros'\n",
    "\n",
    "df['categoria'] = df['producto'].apply(categorizar_producto)\n",
    "\n",
    "# Mostrar el DataFrame transformado\n",
    "print(\"\\nüìä DATAFRAME TRANSFORMADO:\")\n",
    "print(df)\n",
    "\n",
    "# Mostrar estad√≠sticas descriptivas de las columnas num√©ricas\n",
    "print(\"\\n\udcc8 ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "print(df[['cantidad', 'precio_unitario', 'total']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18dce09",
   "metadata": {},
   "source": [
    "### üì§ Paso 3: Load - Cargar datos en destino final\n",
    "\n",
    "Finalmente, guardaremos los datos procesados en un archivo CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8b102",
   "metadata": {},
   "source": [
    "### üìñ Paso 3: Load (Carga) - Persistiendo los Resultados\n",
    "\n",
    "**¬øQu√© es la Carga?**\n",
    "Es la **\"L\" de ETL** - el destino final de los datos procesados. Consiste en:\n",
    "- Guardar los datos transformados en un sistema de almacenamiento\n",
    "- Hacerlos accesibles para consumo (analistas, dashboards, ML)\n",
    "- Garantizar durabilidad y disponibilidad\n",
    "\n",
    "**Destinos comunes:**\n",
    "1. **Archivos**: CSV, Parquet, JSON (para compartir/archivar)\n",
    "2. **Data Warehouses**: Snowflake, BigQuery, Redshift\n",
    "3. **Bases de datos**: PostgreSQL, MySQL, MongoDB\n",
    "4. **Data Lakes**: S3, Azure Data Lake, GCS\n",
    "5. **Dashboards**: Tableau, Power BI, Looker\n",
    "\n",
    "**En este ejemplo - Guardar a CSV:**\n",
    "```python\n",
    "df.to_csv('ventas_procesadas.csv', index=False)\n",
    "```\n",
    "\n",
    "**Par√°metros importantes:**\n",
    "\n",
    "| Par√°metro | Valor | ¬øPor qu√©? |\n",
    "|-----------|-------|-----------|\n",
    "| `index=False` | No guardar √≠ndice | El √≠ndice auto-generado (0,1,2...) no aporta valor |\n",
    "| `index=True` | Guardar √≠ndice | Si el √≠ndice es significativo (IDs, fechas) |\n",
    "| `encoding='utf-8'` | UTF-8 | Soporta caracteres especiales (√±, √°, ‚Ç¨) |\n",
    "| `sep=';'` | Cambiar delimitador | Com√∫n en Europa donde `,` es decimal |\n",
    "| `header=False` | Sin encabezado | Para sistemas que esperan solo datos |\n",
    "\n",
    "**¬øCu√°ndo usar qu√© formato?**\n",
    "\n",
    "**CSV (Comma-Separated Values):**\n",
    "```python\n",
    "df.to_csv('datos.csv', index=False)\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- Universal (Excel, SQL, cualquier lenguaje)\n",
    "- Legible por humanos\n",
    "- F√°cil de compartir\n",
    "\n",
    "‚ùå **Desventajas:**\n",
    "- Ineficiente en espacio (texto plano)\n",
    "- Lento de leer/escribir\n",
    "- No preserva tipos de datos perfectamente\n",
    "\n",
    "**Parquet (Columnar binary):**\n",
    "```python\n",
    "df.to_parquet('datos.parquet', compression='snappy')\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- **10-100x m√°s peque√±o** que CSV\n",
    "- **10-100x m√°s r√°pido** de leer\n",
    "- Preserva tipos de datos exactos\n",
    "- Compresi√≥n integrada\n",
    "- Ideal para Big Data (Spark, Hive)\n",
    "\n",
    "‚ùå **Desventajas:**\n",
    "- No legible por humanos\n",
    "- Requiere bibliotecas espec√≠ficas\n",
    "\n",
    "**JSON (JavaScript Object Notation):**\n",
    "```python\n",
    "df.to_json('datos.json', orient='records', indent=2)\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- Est√°ndar de APIs\n",
    "- Soporta estructuras anidadas\n",
    "- Legible\n",
    "\n",
    "‚ùå **Desventajas:**\n",
    "- M√°s pesado que CSV\n",
    "- M√°s lento que Parquet\n",
    "\n",
    "**Excel:**\n",
    "```python\n",
    "df.to_excel('datos.xlsx', sheet_name='Ventas', index=False)\n",
    "```\n",
    "‚úÖ **Ventajas:**\n",
    "- Stakeholders no t√©cnicos lo usan\n",
    "- Formato, estilos, m√∫ltiples hojas\n",
    "\n",
    "‚ùå **Desventajas:**\n",
    "- Limitado a ~1 mill√≥n de filas\n",
    "- Lento y pesado\n",
    "\n",
    "**Decisi√≥n seg√∫n caso de uso:**\n",
    "\n",
    "| Caso de uso | Formato recomendado | Raz√≥n |\n",
    "|-------------|---------------------|-------|\n",
    "| Compartir con analistas | Excel | Familiaridad |\n",
    "| Pipeline interno | Parquet | Performance |\n",
    "| Exportar a API | JSON | Est√°ndar web |\n",
    "| Intercambio universal | CSV | Compatibilidad |\n",
    "| Data Lake | Parquet + particiones | Escalabilidad |\n",
    "\n",
    "**Best practices:**\n",
    "- ‚úÖ **Nombra archivos con timestamps**: `ventas_2024_01_15.csv`\n",
    "- ‚úÖ **Usa carpetas organizadas**: `data/processed/2024/01/ventas.parquet`\n",
    "- ‚úÖ **Valida antes de guardar**: Verifica que el DataFrame no est√© vac√≠o\n",
    "- ‚úÖ **Logs de metadatos**: Registra filas guardadas, tama√±o, timestamp\n",
    "- ‚úÖ **Idempotencia**: Si re-ejecutas, sobrescribe o versionea\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Guardar DataFrames a CSV con `to_csv()`\n",
    "2. El par√°metro `index=False` y cu√°ndo usarlo\n",
    "3. Comparaci√≥n de formatos: CSV vs Parquet vs JSON vs Excel\n",
    "4. Cu√°ndo usar cada formato seg√∫n el caso de uso\n",
    "5. Best practices para nombrar y organizar archivos\n",
    "6. Por qu√© Parquet es el est√°ndar en Big Data\n",
    "7. C√≥mo hacer el pipeline reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa991b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PASO 3: CARGAR (Load) - Guardar Datos Procesados =====\n",
    "\n",
    "# En Ingenier√≠a de Datos, despu√©s de transformar los datos, debemos almacenarlos\n",
    "# para que otros sistemas o usuarios puedan consumirlos\n",
    "\n",
    "# 3.1 - Guardar a CSV (Comma-Separated Values)\n",
    "# CSV es un formato universal, f√°cil de leer para humanos y m√°quinas\n",
    "csv_filename = 'ventas_procesadas.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "print(f\"üíæ Datos guardados en CSV: {csv_filename}\")\n",
    "print(f\"   ‚Ä¢ Formato: Texto plano separado por comas\")\n",
    "print(f\"   ‚Ä¢ Tama√±o: ~{len(df) * 100} bytes\")\n",
    "print(f\"   ‚Ä¢ Uso t√≠pico: Reportes, intercambio entre sistemas\")\n",
    "\n",
    "# 3.2 - Guardar a JSON (JavaScript Object Notation)\n",
    "# JSON es ideal para APIs y sistemas web modernos\n",
    "json_filename = 'ventas_procesadas.json'\n",
    "df.to_json(json_filename, orient='records', indent=2, force_ascii=False)\n",
    "print(f\"\\nüíæ Datos guardados en JSON: {json_filename}\")\n",
    "print(f\"   ‚Ä¢ Formato: Texto estructurado con llaves y valores\")\n",
    "print(f\"   ‚Ä¢ Uso t√≠pico: APIs REST, aplicaciones web\")\n",
    "\n",
    "# 3.3 - Guardar a Parquet (formato columnar optimizado)\n",
    "# Parquet es MUY eficiente para Big Data, usado en Spark, AWS, etc.\n",
    "parquet_filename = 'ventas_procesadas.parquet'\n",
    "df.to_parquet(parquet_filename, index=False, engine='pyarrow')\n",
    "print(f\"\\nüíæ Datos guardados en Parquet: {parquet_filename}\")\n",
    "print(f\"   ‚Ä¢ Formato: Binario columnar comprimido\")\n",
    "print(f\"   ‚Ä¢ Ventajas: 10x m√°s r√°pido, 80% menos espacio que CSV\")\n",
    "print(f\"   ‚Ä¢ Uso t√≠pico: Data Lakes, Data Warehouses, Spark\")\n",
    "\n",
    "# 3.4 - Guardar a Excel (para usuarios de negocio)\n",
    "# Excel es familiar para usuarios no t√©cnicos\n",
    "excel_filename = 'ventas_procesadas.xlsx'\n",
    "df.to_excel(excel_filename, index=False, sheet_name='Ventas')\n",
    "print(f\"\\nüíæ Datos guardados en Excel: {excel_filename}\")\n",
    "print(f\"   ‚Ä¢ Formato: Binario de Microsoft Office\")\n",
    "print(f\"   ‚Ä¢ Uso t√≠pico: Reportes para usuarios de negocio\")\n",
    "\n",
    "# 3.5 - Comparaci√≥n de tama√±os de archivo\n",
    "import os\n",
    "\n",
    "print(\"\\nüìä COMPARACI√ìN DE FORMATOS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Formato':<12} {'Tama√±o (bytes)':<15} {'Relativo':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Obtener tama√±o de cada archivo\n",
    "csv_size = os.path.getsize(csv_filename)\n",
    "json_size = os.path.getsize(json_filename)\n",
    "parquet_size = os.path.getsize(parquet_filename)\n",
    "excel_size = os.path.getsize(excel_filename)\n",
    "\n",
    "# Usar CSV como baseline (100%)\n",
    "print(f\"CSV         {csv_size:<15} 100%\")\n",
    "print(f\"JSON        {json_size:<15} {int(json_size/csv_size*100)}%\")\n",
    "print(f\"Parquet     {parquet_size:<15} {int(parquet_size/csv_size*100)}%\")\n",
    "print(f\"Excel       {excel_size:<15} {int(excel_size/csv_size*100)}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline ETL completado exitosamente!\")\n",
    "print(f\"‚úÖ {len(df)} registros procesados y guardados en 4 formatos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21591cd7",
   "metadata": {},
   "source": [
    "## üìä An√°lisis B√°sico de los Datos Procesados\n",
    "\n",
    "Ahora que tenemos nuestros datos procesados, hagamos un an√°lisis exploratorio b√°sico:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d7c9a",
   "metadata": {},
   "source": [
    "### üìñ Explorando los Datos - Tu Primera Inspecci√≥n\n",
    "\n",
    "**¬øPor qu√© explorar los datos?**\n",
    "Antes de transformar o analizar, necesitas **entender qu√© tienes**:\n",
    "- ¬øCu√°ntas filas y columnas?\n",
    "- ¬øQu√© tipos de datos?\n",
    "- ¬øHay valores nulos?\n",
    "- ¬øC√≥mo lucen las primeras filas?\n",
    "\n",
    "**Esto es como revisar un paquete antes de abrirlo** - necesitas saber qu√© esperar.\n",
    "\n",
    "**Comandos esenciales de exploraci√≥n:**\n",
    "\n",
    "**1. `.head(n)` - Las primeras N filas**\n",
    "```python\n",
    "df.head(10)  # Primeras 10 filas (default = 5)\n",
    "```\n",
    "‚úÖ **Cu√°ndo usar:**\n",
    "- Quick check despu√©s de cargar datos\n",
    "- Verificar que la estructura es correcta\n",
    "- Ver ejemplos de datos reales\n",
    "\n",
    "**2. `.tail(n)` - Las √∫ltimas N filas**\n",
    "```python\n",
    "df.tail(5)  # √öltimas 5 filas\n",
    "```\n",
    "‚úÖ **Cu√°ndo usar:**\n",
    "- Verificar datos recientes (si est√°n ordenados por fecha)\n",
    "- Detectar si el archivo se cort√≥ a la mitad\n",
    "\n",
    "**3. `.info()` - Resumen estructural**\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "üìä **Muestra:**\n",
    "- N√∫mero de filas y columnas\n",
    "- Nombre de cada columna\n",
    "- Tipo de dato (int64, float64, object, datetime)\n",
    "- **Valores no-nulos** (detecta nulos)\n",
    "- Uso de memoria\n",
    "\n",
    "Salida ejemplo:\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1000 entries, 0 to 999\n",
    "Data columns (total 5 columns):\n",
    " #   Column    Non-Null Count  Dtype  \n",
    "---  ------    --------------  -----  \n",
    " 0   fecha     1000 non-null   object \n",
    " 1   producto  950 non-null    object  ‚Üê ¬°50 nulos!\n",
    " 2   cantidad  1000 non-null   int64  \n",
    " 3   precio    1000 non-null   float64\n",
    "dtypes: float64(1), int64(1), object(2)\n",
    "memory usage: 39.2+ KB\n",
    "```\n",
    "\n",
    "**4. `.describe()` - Estad√≠sticas descriptivas**\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "üìà **Para columnas num√©ricas muestra:**\n",
    "- **count**: Cantidad de valores no-nulos\n",
    "- **mean**: Promedio\n",
    "- **std**: Desviaci√≥n est√°ndar (dispersi√≥n)\n",
    "- **min**: Valor m√≠nimo\n",
    "- **25%**: Percentil 25 (Q1)\n",
    "- **50%**: Mediana (Q2)\n",
    "- **75%**: Percentil 75 (Q3)\n",
    "- **max**: Valor m√°ximo\n",
    "\n",
    "‚úÖ **Para qu√© sirve:**\n",
    "- Detectar **outliers**: Si max es 1,000,000 pero mean es 100\n",
    "- Ver **rangos**: ¬øLos precios van de 0.01 a 999?\n",
    "- Detectar **datos incorrectos**: Edades negativas, fechas futuras\n",
    "\n",
    "**5. `.shape` - Dimensiones**\n",
    "```python\n",
    "print(df.shape)  # (1000, 6) = 1000 filas, 6 columnas\n",
    "```\n",
    "\n",
    "**6. `.columns` - Nombres de columnas**\n",
    "```python\n",
    "print(df.columns)  # Index(['fecha', 'producto', 'cantidad', ...])\n",
    "```\n",
    "\n",
    "**7. `.dtypes` - Tipos de datos**\n",
    "```python\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "**Flujo t√≠pico de exploraci√≥n:**\n",
    "```python\n",
    "# 1. ¬øCu√°ntos datos tengo?\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# 2. ¬øC√≥mo lucen?\n",
    "df.head()\n",
    "\n",
    "# 3. ¬øQu√© estructura tienen?\n",
    "df.info()\n",
    "\n",
    "# 4. ¬øEstad√≠sticas b√°sicas?\n",
    "df.describe()\n",
    "\n",
    "# 5. ¬øValores nulos?\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "**Detecci√≥n de problemas comunes:**\n",
    "\n",
    "| S√≠ntoma | Problema | Soluci√≥n |\n",
    "|---------|----------|----------|\n",
    "| `object` en fecha | Fecha como string | Convertir: `pd.to_datetime()` |\n",
    "| `Non-Null Count < total` | Valores nulos | Rellenar o eliminar |\n",
    "| `max` muy alto vs `mean` | Outliers | Investigar y limpiar |\n",
    "| Columnas con typos | Naming inconsistente | Renombrar |\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Ver las primeras filas con `.head()`\n",
    "2. Obtener resumen estructural con `.info()`\n",
    "3. Calcular estad√≠sticas con `.describe()`\n",
    "4. Interpretar tipos de datos (int64, float64, object)\n",
    "5. Detectar valores nulos en Non-Null Count\n",
    "6. Entender memoria usada\n",
    "7. Por qu√© explorar antes de transformar es crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6431a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDACI√ìN: Verificar que los datos se guardaron correctamente =====\n",
    "\n",
    "# Es fundamental validar que el pipeline ETL funcion√≥ correctamente\n",
    "# Esto previene errores silenciosos que pueden propagar datos incorrectos\n",
    "\n",
    "print(\"üîç VALIDACI√ìN DE DATOS GUARDADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4.1 - Leer el archivo CSV que acabamos de crear\n",
    "df_from_csv = pd.read_csv('ventas_procesadas.csv')\n",
    "print(\"\\n1Ô∏è‚É£ Lectura desde CSV:\")\n",
    "print(f\"   ‚Ä¢ Registros le√≠dos: {len(df_from_csv)}\")\n",
    "print(f\"   ‚Ä¢ Columnas: {list(df_from_csv.columns)}\")\n",
    "print(f\"   ‚Ä¢ ¬øDatos coinciden?: {df_from_csv.shape == df.shape}\")\n",
    "\n",
    "# 4.2 - Leer el archivo JSON\n",
    "df_from_json = pd.read_json('ventas_procesadas.json')\n",
    "print(\"\\n2Ô∏è‚É£ Lectura desde JSON:\")\n",
    "print(f\"   ‚Ä¢ Registros le√≠dos: {len(df_from_json)}\")\n",
    "print(f\"   ‚Ä¢ ¬øDatos coinciden?: {df_from_json.shape == df.shape}\")\n",
    "\n",
    "# 4.3 - Leer el archivo Parquet\n",
    "df_from_parquet = pd.read_parquet('ventas_procesadas.parquet')\n",
    "print(\"\\n3Ô∏è‚É£ Lectura desde Parquet:\")\n",
    "print(f\"   ‚Ä¢ Registros le√≠dos: {len(df_from_parquet)}\")\n",
    "print(f\"   ‚Ä¢ ¬øDatos coinciden?: {df_from_parquet.shape == df.shape}\")\n",
    "\n",
    "# 4.4 - Validar integridad de los datos (Data Quality Check)\n",
    "print(\"\\n\udd0e VALIDACIONES DE CALIDAD:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Verificar que no hay valores nulos (cr√≠tico en datos de ventas)\n",
    "nulls_check = df.isnull().sum().sum() == 0\n",
    "print(f\"   ‚úì Sin valores nulos: {nulls_check}\")\n",
    "\n",
    "# Verificar que todos los totales est√°n correctamente calculados\n",
    "totals_check = all(df['total'] == df['cantidad'] * df['precio_unitario'])\n",
    "print(f\"   ‚úì Totales calculados correctamente: {totals_check}\")\n",
    "\n",
    "# Verificar que todas las cantidades son positivas\n",
    "positive_check = all(df['cantidad'] > 0)\n",
    "print(f\"   ‚úì Cantidades positivas: {positive_check}\")\n",
    "\n",
    "# Verificar que todos los precios son positivos\n",
    "prices_check = all(df['precio_unitario'] > 0)\n",
    "print(f\"   ‚úì Precios positivos: {prices_check}\")\n",
    "\n",
    "if nulls_check and totals_check and positive_check and prices_check:\n",
    "    print(\"\\nüéâ ¬°Todas las validaciones pasaron exitosamente!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Algunas validaciones fallaron. Revisar los datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbfd83",
   "metadata": {},
   "source": [
    "## üìà Visualizaci√≥n B√°sica\n",
    "\n",
    "Vamos a crear algunas visualizaciones simples para entender mejor nuestros datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f7d3b",
   "metadata": {},
   "source": [
    "### üìñ Calculando M√©tricas de Negocio - Del Dato al Insight\n",
    "\n",
    "**¬øQu√© son las m√©tricas de negocio?**\n",
    "Son **KPIs (Key Performance Indicators)** que responden preguntas clave:\n",
    "- ¬øCu√°nto vendimos en total?\n",
    "- ¬øCu√°l es el ticket promedio?\n",
    "- ¬øQu√© producto es el m√°s popular?\n",
    "- ¬øQu√© regi√≥n genera m√°s ingresos?\n",
    "\n",
    "**Estas preguntas NO vienen resueltas en los datos crudos** - hay que calcularlas.\n",
    "\n",
    "**En este ejemplo calcularemos:**\n",
    "\n",
    "**1. Ingresos Totales:**\n",
    "```python\n",
    "ingresos_totales = df['total'].sum()\n",
    "```\n",
    "- **`.sum()`**: Suma todos los valores de una columna\n",
    "- **Use case**: Reportar revenue total del periodo\n",
    "- **SQL equivalente**: `SELECT SUM(total) FROM ventas`\n",
    "\n",
    "**2. Ticket Promedio (Average Order Value - AOV):**\n",
    "```python\n",
    "ticket_promedio = df['total'].mean()\n",
    "```\n",
    "- **`.mean()`**: Promedio aritm√©tico\n",
    "- **Use case**: ¬øCu√°nto gasta un cliente en promedio?\n",
    "- **SQL equivalente**: `SELECT AVG(total) FROM ventas`\n",
    "\n",
    "**Por qu√© importa:**\n",
    "- Si AOV = $50 y quieres $1M de ingresos ‚Üí necesitas 20,000 transacciones\n",
    "- Sirve para proyecciones y metas\n",
    "\n",
    "**3. Producto M√°s Vendido:**\n",
    "```python\n",
    "producto_top = df['producto'].value_counts().head(1)\n",
    "```\n",
    "\n",
    "**Desglosando `.value_counts()`:**\n",
    "```\n",
    "Original DataFrame:\n",
    "producto\n",
    "--------\n",
    "Laptop\n",
    "Mouse\n",
    "Laptop\n",
    "Teclado\n",
    "Laptop\n",
    "\n",
    "Despu√©s de value_counts():\n",
    "producto    count\n",
    "--------    -----\n",
    "Laptop      3\n",
    "Mouse       1\n",
    "Teclado     1\n",
    "```\n",
    "\n",
    "- **`.value_counts()`**: Cuenta frecuencias de cada valor √∫nico\n",
    "- **`.head(1)`**: Toma el primero (el m√°s frecuente)\n",
    "- **Use case**: Optimizar inventario, marketing\n",
    "\n",
    "**4. Ingresos por Regi√≥n:**\n",
    "```python\n",
    "ingresos_region = df.groupby('region')['total'].sum()\n",
    "```\n",
    "\n",
    "**Visualizaci√≥n del groupby:**\n",
    "```\n",
    "Original:\n",
    "region    | total\n",
    "----------|------\n",
    "Norte     | 100\n",
    "Norte     | 200\n",
    "Sur       | 150\n",
    "Este      | 300\n",
    "\n",
    "Despu√©s de groupby('region')['total'].sum():\n",
    "region    | total\n",
    "----------|------\n",
    "Norte     | 300\n",
    "Sur       | 150\n",
    "Este      | 300\n",
    "```\n",
    "\n",
    "**Use case:**\n",
    "- ¬øD√≥nde invertir en marketing?\n",
    "- ¬øQu√© regiones est√°n bajo-performando?\n",
    "- Asignaci√≥n de recursos\n",
    "\n",
    "**Funciones de agregaci√≥n comunes:**\n",
    "\n",
    "| Funci√≥n | Qu√© hace | Ejemplo de uso |\n",
    "|---------|----------|----------------|\n",
    "| `.sum()` | Suma total | Ingresos totales |\n",
    "| `.mean()` | Promedio | Ticket promedio |\n",
    "| `.median()` | Mediana | Precio t√≠pico (resistente a outliers) |\n",
    "| `.count()` | Cantidad | N√∫mero de transacciones |\n",
    "| `.min()` | M√≠nimo | Venta m√°s peque√±a |\n",
    "| `.max()` | M√°ximo | Venta m√°s grande |\n",
    "| `.std()` | Desviaci√≥n est√°ndar | Variabilidad de ventas |\n",
    "| `.value_counts()` | Frecuencias | Productos m√°s vendidos |\n",
    "\n",
    "**Comparaci√≥n mean vs median:**\n",
    "```python\n",
    "# Con outliers\n",
    "ventas = [10, 10, 10, 10, 1000]\n",
    "mean = 208   # Sesgado por el outlier\n",
    "median = 10  # Valor t√≠pico real\n",
    "```\n",
    "**Regla:** Si hay outliers extremos, usa median.\n",
    "\n",
    "**M√©tricas compuestas:**\n",
    "```python\n",
    "# Conversi√≥n (porcentaje)\n",
    "tasa_conversion = (ventas / visitas) * 100\n",
    "\n",
    "# Growth rate (crecimiento)\n",
    "crecimiento = ((ventas_hoy - ventas_ayer) / ventas_ayer) * 100\n",
    "\n",
    "# Margen de ganancia\n",
    "margen = ((precio_venta - costo) / precio_venta) * 100\n",
    "```\n",
    "\n",
    "**De datos a decisiones:**\n",
    "1. **Data** (crudo): 500 filas de transacciones\n",
    "2. **Informaci√≥n** (procesada): Ingresos totales = $50,000\n",
    "3. **Insight** (interpretada): Norte genera 60% de ingresos\n",
    "4. **Acci√≥n** (decisi√≥n): Expandir equipo de ventas en Norte\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Calcular sumas totales con `.sum()`\n",
    "2. Calcular promedios con `.mean()`\n",
    "3. Encontrar valores m√°s frecuentes con `.value_counts()`\n",
    "4. Agrupar y agregar con `groupby().sum()`\n",
    "5. Diferencia entre mean y median\n",
    "6. C√≥mo traducir datos a insights accionables\n",
    "7. Funciones de agregaci√≥n fundamentales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== AN√ÅLISIS EXPLORATORIO: Entender patrones en los datos =====\n",
    "\n",
    "# El an√°lisis exploratorio es crucial para:\n",
    "# - Detectar anomal√≠as o datos at√≠picos\n",
    "# - Identificar tendencias de negocio\n",
    "# - Generar insights para la toma de decisiones\n",
    "\n",
    "print(\"üìä AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 5.1 - An√°lisis de ventas por producto\n",
    "print(\"\\n1Ô∏è‚É£ VENTAS POR PRODUCTO:\")\n",
    "ventas_por_producto = df.groupby('producto').agg({\n",
    "    'cantidad': 'sum',      # Total de unidades vendidas\n",
    "    'total': 'sum'          # Ingresos totales generados\n",
    "}).round(2)\n",
    "\n",
    "print(ventas_por_producto)\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(\"   ‚Ä¢ ¬øQu√© producto genera m√°s ingresos?\")\n",
    "print(\"   ‚Ä¢ ¬øCu√°l tiene mayor volumen de ventas?\")\n",
    "print(\"   ‚Ä¢ Esta informaci√≥n ayuda a optimizar inventario y estrategias de marketing\")\n",
    "\n",
    "# 5.2 - An√°lisis de ventas por categor√≠a\n",
    "print(\"\\n2Ô∏è‚É£ VENTAS POR CATEGOR√çA:\")\n",
    "ventas_por_categoria = df.groupby('categoria').agg({\n",
    "    'total': ['sum', 'mean', 'count']  # Total, promedio y cantidad de transacciones\n",
    "}).round(2)\n",
    "\n",
    "print(ventas_por_categoria)\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(\"   ‚Ä¢ ¬øQu√© categor√≠a domina las ventas?\")\n",
    "print(\"   ‚Ä¢ Categor√≠as 'alta' pueden indicar productos premium o clientes corporativos\")\n",
    "print(\"   ‚Ä¢ Este an√°lisis permite segmentar estrategias de pricing\")\n",
    "\n",
    "# 5.3 - Estad√≠sticas descriptivas completas\n",
    "print(\"\\n3Ô∏è‚É£ ESTAD√çSTICAS DESCRIPTIVAS DETALLADAS:\")\n",
    "print(\"-\" * 60)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n de las m√©tricas:\")\n",
    "print(\"   ‚Ä¢ count: N√∫mero total de registros (sin valores nulos)\")\n",
    "print(\"   ‚Ä¢ mean: Promedio (¬øcu√°l es el ticket promedio?)\")\n",
    "print(\"   ‚Ä¢ std: Desviaci√≥n est√°ndar (¬øqu√© tan variables son las ventas?)\")\n",
    "print(\"   ‚Ä¢ min/max: Valores extremos (¬øhay outliers?)\")\n",
    "print(\"   ‚Ä¢ 25%/50%/75%: Cuartiles (distribuci√≥n de datos)\")\n",
    "\n",
    "# 5.4 - Identificar el producto m√°s vendido (Best Seller)\n",
    "producto_mas_vendido = df.groupby('producto')['cantidad'].sum().idxmax()\n",
    "cantidad_max = df.groupby('producto')['cantidad'].sum().max()\n",
    "\n",
    "print(f\"\\nüèÜ BEST SELLER:\")\n",
    "print(f\"   ‚Ä¢ Producto: {producto_mas_vendido}\")\n",
    "print(f\"   ‚Ä¢ Unidades vendidas: {cantidad_max}\")\n",
    "\n",
    "# 5.5 - Calcular ingreso total generado\n",
    "ingreso_total = df['total'].sum()\n",
    "print(f\"\\nüí∞ INGRESO TOTAL GENERADO: ${ingreso_total:,.2f}\")\n",
    "print(\"   ‚Ä¢ Este KPI es fundamental para reportes financieros\")\n",
    "print(\"   ‚Ä¢ Compara con metas de ventas y proyecciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbb82f",
   "metadata": {},
   "source": [
    "## üèÜ Ejercicio Pr√°ctico: ¬°Tu Turno!\n",
    "\n",
    "Ahora es tu turno de crear un pipeline. Vamos a trabajar con otra API p√∫blica.\n",
    "\n",
    "### üìù Instrucciones:\n",
    "1. Usa la API de **Posts**: `https://jsonplaceholder.typicode.com/posts`\n",
    "2. Extrae todos los posts\n",
    "3. Transforma los datos agregando:\n",
    "   - Longitud del t√≠tulo\n",
    "   - Longitud del cuerpo\n",
    "   - Categor√≠a basada en el userId (ej: \"usuario_1\", \"usuario_2\", etc.)\n",
    "4. Guarda el resultado en `../datasets/processed/posts_transformados.csv`\n",
    "\n",
    "¬°Usa el c√≥digo anterior como referencia!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f3e85",
   "metadata": {},
   "source": [
    "### üìñ Visualizaci√≥n de Datos - Porque Una Imagen Vale M√°s Que Mil Filas\n",
    "\n",
    "**¬øPor qu√© visualizar?**\n",
    "Los humanos procesamos informaci√≥n visual **60,000 veces m√°s r√°pido** que texto:\n",
    "- ‚úÖ Detectar patrones instant√°neamente\n",
    "- ‚úÖ Comunicar insights a stakeholders no t√©cnicos\n",
    "- ‚úÖ Encontrar outliers visualmente\n",
    "- ‚úÖ Validar transformaciones\n",
    "- ‚úÖ Tomar decisiones m√°s r√°pido\n",
    "\n",
    "**Imagina explicar esto con n√∫meros vs un gr√°fico:**\n",
    "- \"La regi√≥n Norte gener√≥ $45,000, el Sur $30,000, el Este $25,000...\"\n",
    "- **VS** un gr√°fico de barras donde ves instant√°neamente que Norte domina\n",
    "\n",
    "**En este ejemplo - Gr√°fico de Barras:**\n",
    "```python\n",
    "ingresos_region.plot(kind='bar', title='Ingresos por Regi√≥n')\n",
    "plt.ylabel('Ingresos ($)')\n",
    "plt.xlabel('Regi√≥n')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Desglosando el c√≥digo:**\n",
    "\n",
    "**1. `ingresos_region.plot()`**\n",
    "- Pandas tiene integraci√≥n directa con matplotlib\n",
    "- Toma una Series o DataFrame y lo grafica\n",
    "- M√°s r√°pido que matplotlib puro para casos simples\n",
    "\n",
    "**2. `kind='bar'`**\n",
    "- Especifica el tipo de gr√°fico\n",
    "- Opciones: `'line'`, `'bar'`, `'barh'` (horizontal), `'hist'`, `'box'`, `'pie'`, `'scatter'`\n",
    "\n",
    "**3. `title='...'`**\n",
    "- T√≠tulo del gr√°fico\n",
    "- Crucial para contexto: ¬øQu√© estoy viendo?\n",
    "\n",
    "**4. `plt.ylabel()` y `plt.xlabel()`**\n",
    "- Etiquetas de ejes\n",
    "- **Siempre incl√∫yelas**: Sin unidades, los gr√°ficos no tienen significado\n",
    "- \"Ingresos\" ‚Üí ‚ùå Ambiguo\n",
    "- \"Ingresos ($)\" ‚Üí ‚úÖ Claro\n",
    "\n",
    "**5. `plt.show()`**\n",
    "- Renderiza el gr√°fico en pantalla\n",
    "- En notebooks Jupyter, a veces es opcional si usas `%matplotlib inline`\n",
    "\n",
    "**Tipos de gr√°ficos comunes:**\n",
    "\n",
    "| Tipo | Cu√°ndo usar | Ejemplo |\n",
    "|------|-------------|---------|\n",
    "| **Line** | Tendencias temporales | Ventas por mes |\n",
    "| **Bar** | Comparar categor√≠as | Ingresos por regi√≥n |\n",
    "| **Histogram** | Distribuci√≥n de valores | Distribuci√≥n de precios |\n",
    "| **Scatter** | Relaci√≥n entre 2 variables | Precio vs Cantidad |\n",
    "| **Pie** | Proporciones (evitar si >5 categor√≠as) | Market share |\n",
    "| **Box** | Dispersi√≥n y outliers | Salarios por departamento |\n",
    "\n",
    "**Ejemplos de otros tipos:**\n",
    "\n",
    "**Gr√°fico de l√≠nea (temporal):**\n",
    "```python\n",
    "df.groupby('fecha')['total'].sum().plot(kind='line')\n",
    "plt.title('Ventas Diarias')\n",
    "plt.ylabel('Ingresos ($)')\n",
    "plt.xlabel('Fecha')\n",
    "```\n",
    "\n",
    "**Histograma (distribuci√≥n):**\n",
    "```python\n",
    "df['total'].plot(kind='hist', bins=20)\n",
    "plt.title('Distribuci√≥n de Valores de Venta')\n",
    "plt.xlabel('Monto de Venta ($)')\n",
    "plt.ylabel('Frecuencia')\n",
    "```\n",
    "\n",
    "**Scatter (correlaci√≥n):**\n",
    "```python\n",
    "df.plot(kind='scatter', x='cantidad', y='total')\n",
    "plt.title('Cantidad vs Ingresos')\n",
    "```\n",
    "\n",
    "**Personalizaciones √∫tiles:**\n",
    "\n",
    "```python\n",
    "# Tama√±o de figura\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Rotaci√≥n de etiquetas (√∫til para fechas)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Grid para facilitar lectura\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Colores personalizados\n",
    "df.plot(kind='bar', color='steelblue')\n",
    "\n",
    "# M√∫ltiples gr√°ficos\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df1.plot(ax=ax1)\n",
    "df2.plot(ax=ax2)\n",
    "```\n",
    "\n",
    "**Reglas de oro para buenas visualizaciones:**\n",
    "1. ‚úÖ **T√≠tulo claro**: ¬øQu√© muestra el gr√°fico?\n",
    "2. ‚úÖ **Ejes etiquetados con unidades**: $, %, unidades, etc.\n",
    "3. ‚úÖ **Escala apropiada**: No empieces en 0 si distorsiona (cuidado con manipulaci√≥n)\n",
    "4. ‚úÖ **Colores significativos**: Rojo para negativo, verde para positivo\n",
    "5. ‚úÖ **No m√°s de 7 categor√≠as**: Si tienes 50 productos, agrupa\n",
    "6. ‚ùå **Evita pie charts 3D**: Son confusos y distorsionan proporciones\n",
    "7. ‚ùå **No uses colores estridentes**: Profesionalismo\n",
    "\n",
    "**Flujo t√≠pico:**\n",
    "1. **Explorar**: Histogramas y box plots para entender distribuciones\n",
    "2. **Analizar**: Line/bar charts para comparar y ver tendencias\n",
    "3. **Presentar**: Gr√°ficos pulidos con t√≠tulos, etiquetas, colores corporativos\n",
    "\n",
    "**En este bloque aprender√°s:**\n",
    "1. Crear gr√°ficos directamente desde pandas con `.plot()`\n",
    "2. Especificar tipos con `kind='bar'`\n",
    "3. Agregar t√≠tulos y etiquetas descriptivas\n",
    "4. Cu√°ndo usar cada tipo de gr√°fico\n",
    "5. Personalizaciones b√°sicas de matplotlib\n",
    "6. Por qu√© las etiquetas con unidades son cruciales\n",
    "7. Principios de visualizaci√≥n efectiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VISUALIZACI√ìN: Comunicar insights de forma efectiva =====\n",
    "\n",
    "# Los gr√°ficos transforman datos complejos en informaci√≥n comprensible\n",
    "# Un buen Data Engineer debe saber presentar resultados a stakeholders no t√©cnicos\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficos m√°s profesionales\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Estilo moderno y legible\n",
    "\n",
    "print(\"üìà VISUALIZACI√ìN DE DATOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 6.1 - Crear figura con m√∫ltiples subgr√°ficos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Dashboard de An√°lisis de Ventas', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 6.2 - Gr√°fico 1: Ventas totales por producto (Barras horizontales)\n",
    "ventas_producto = df.groupby('producto')['total'].sum().sort_values()\n",
    "axes[0, 0].barh(ventas_producto.index, ventas_producto.values, color='steelblue')\n",
    "axes[0, 0].set_title('üí∞ Ingresos por Producto')\n",
    "axes[0, 0].set_xlabel('Ingresos Totales ($)')\n",
    "# ¬øPor qu√© barras horizontales? Facilitan lectura de etiquetas largas\n",
    "\n",
    "# 6.3 - Gr√°fico 2: Distribuci√≥n de categor√≠as (Gr√°fico de pastel)\n",
    "categoria_counts = df['categoria'].value_counts()\n",
    "axes[0, 1].pie(categoria_counts.values, labels=categoria_counts.index, \n",
    "               autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "axes[0, 1].set_title('üìä Distribuci√≥n de Categor√≠as')\n",
    "# El gr√°fico de pastel muestra proporciones de forma intuitiva\n",
    "\n",
    "# 6.4 - Gr√°fico 3: Cantidad de unidades vendidas por producto\n",
    "cantidades = df.groupby('producto')['cantidad'].sum().sort_values()\n",
    "axes[1, 0].bar(range(len(cantidades)), cantidades.values, color='coral')\n",
    "axes[1, 0].set_xticks(range(len(cantidades)))\n",
    "axes[1, 0].set_xticklabels(cantidades.index, rotation=45, ha='right')\n",
    "axes[1, 0].set_title('üì¶ Unidades Vendidas por Producto')\n",
    "axes[1, 0].set_ylabel('Cantidad de Unidades')\n",
    "# Este gr√°fico ayuda a planificar inventario\n",
    "\n",
    "# 6.5 - Gr√°fico 4: Precio unitario promedio por producto\n",
    "precios_promedio = df.groupby('producto')['precio_unitario'].mean().sort_values()\n",
    "axes[1, 1].barh(precios_promedio.index, precios_promedio.values, color='lightgreen')\n",
    "axes[1, 1].set_title('üíµ Precio Unitario Promedio')\n",
    "axes[1, 1].set_xlabel('Precio ($)')\n",
    "# Identifica productos premium vs. econ√≥micos\n",
    "\n",
    "# Ajustar layout para evitar solapamiento\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar el dashboard como imagen (√∫til para reportes)\n",
    "plt.savefig('dashboard_ventas.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Dashboard generado y guardado como 'dashboard_ventas.png'\")\n",
    "print(\"   ‚Ä¢ Resoluci√≥n: 300 DPI (calidad de impresi√≥n)\")\n",
    "print(\"   ‚Ä¢ Formato: PNG (compatible con presentaciones)\")\n",
    "\n",
    "# Mostrar los gr√°ficos\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\udca1 ¬øQu√© insights obtenemos de estas visualizaciones?\")\n",
    "print(\"   1. ¬øQu√© producto tiene mayor margen de contribuci√≥n?\")\n",
    "print(\"   2. ¬øHay concentraci√≥n de ventas en pocas categor√≠as?\")\n",
    "print(\"   3. ¬øLos productos m√°s vendidos son los m√°s rentables?\")\n",
    "print(\"   4. ¬øExiste correlaci√≥n entre precio y volumen de ventas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1c81e",
   "metadata": {},
   "source": [
    "## üí° Soluci√≥n del Ejercicio\n",
    "\n",
    "Aqu√≠ tienes una posible soluci√≥n. ¬°Comp√°rala con tu implementaci√≥n!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1472084",
   "metadata": {},
   "source": [
    "### üìñ Ejercicio Pr√°ctico - Aplicando Todo lo Aprendido\n",
    "\n",
    "**¬øPor qu√© este ejercicio es importante?**\n",
    "Hasta ahora viste **ejemplos guiados**. Ahora es tu turno de:\n",
    "- Aplicar los conceptos sin ayuda paso a paso\n",
    "- Enfrentarte a decisiones de dise√±o\n",
    "- Cometer errores y aprender de ellos\n",
    "- Construir m√∫sculo memoria\n",
    "\n",
    "**Este es un dataset de productos de e-commerce** t√≠pico de:\n",
    "- Shopify\n",
    "- WooCommerce\n",
    "- Amazon Seller Central\n",
    "- Cualquier tienda online\n",
    "\n",
    "**Estructura del dataset:**\n",
    "```python\n",
    "{\n",
    "    'producto_id': [101, 102, ...],     # Identificador √∫nico\n",
    "    'nombre': ['Laptop', 'Mouse', ...],  # Nombre del producto\n",
    "    'categoria': ['Electr√≥nica', ...],   # Categor√≠a\n",
    "    'precio': [899.99, 25.50, ...],      # Precio unitario\n",
    "    'stock': [15, 45, ...],              # Inventario disponible\n",
    "    'ventas_mes': [23, 120, ...]         # Unidades vendidas\n",
    "}\n",
    "```\n",
    "\n",
    "**Tu misi√≥n - Responder estas preguntas de negocio:**\n",
    "\n",
    "**1. ¬øCu√°l es el valor total del inventario?**\n",
    "```python\n",
    "# Pista: stock * precio para cada producto, luego suma\n",
    "```\n",
    "- **Por qu√© importa**: Saber cu√°nto capital est√° inmovilizado en inventario\n",
    "- **M√©trica**: Inventory Value = Œ£ (stock √ó precio)\n",
    "\n",
    "**2. ¬øQu√© categor√≠a genera m√°s ingresos?**\n",
    "```python\n",
    "# Pista: ventas_mes * precio, luego groupby por categoria\n",
    "```\n",
    "- **Por qu√© importa**: Decidir d√≥nde enfocar esfuerzos de marketing\n",
    "- **Decisi√≥n**: Si \"Electr√≥nica\" genera 80% de ingresos, prioriza esa categor√≠a\n",
    "\n",
    "**3. ¬øCu√°les son los 3 productos m√°s rentables?**\n",
    "```python\n",
    "# Pista: Calcular ingresos_totales = ventas_mes * precio\n",
    "# Luego sort_values() y head(3)\n",
    "```\n",
    "- **Por qu√© importa**: Optimizar stock de productos estrella\n",
    "- **T√°ctica**: Nunca dejes que estos se agoten\n",
    "\n",
    "**4. ¬øQu√© productos tienen stock cr√≠tico (menos de 10 unidades)?**\n",
    "```python\n",
    "# Pista: Filtrar con df[df['stock'] < 10]\n",
    "```\n",
    "- **Por qu√© importa**: Prevenir p√©rdida de ventas por falta de inventario\n",
    "- **Alerta**: Estos necesitan reorden urgente\n",
    "\n",
    "**5. Visualiza las ventas por categor√≠a**\n",
    "```python\n",
    "# Pista: groupby('categoria')['ventas_mes'].sum().plot(kind='bar')\n",
    "```\n",
    "- **Por qu√© importa**: Comunicar insights al equipo comercial\n",
    "- **Impacto**: M√°s visual = m√°s acci√≥n\n",
    "\n",
    "**Habilidades que practicar√°s:**\n",
    "\n",
    "| Habilidad | Concepto | Aplicaci√≥n |\n",
    "|-----------|----------|------------|\n",
    "| **Crear columnas** | `df['nueva'] = df['a'] * df['b']` | Calcular ingresos |\n",
    "| **Agrupar** | `.groupby()` | Sumar por categor√≠a |\n",
    "| **Ordenar** | `.sort_values()` | Top productos |\n",
    "| **Filtrar** | `df[condici√≥n]` | Stock bajo |\n",
    "| **Visualizar** | `.plot()` | Gr√°ficos |\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "```python\n",
    "# 1. Crear DataFrame\n",
    "df = pd.DataFrame(datos_productos)\n",
    "\n",
    "# 2. Explorar\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 3. Crear columnas calculadas\n",
    "df['valor_inventario'] = df['stock'] * df['precio']\n",
    "df['ingresos_mes'] = df['ventas_mes'] * df['precio']\n",
    "\n",
    "# 4. Responder preguntas\n",
    "print(f\"Valor total inventario: ${df['valor_inventario'].sum()}\")\n",
    "\n",
    "# 5. Agrupar y analizar\n",
    "ingresos_categoria = df.groupby('categoria')['ingresos_mes'].sum()\n",
    "\n",
    "# 6. Filtrar\n",
    "stock_critico = df[df['stock'] < 10]\n",
    "\n",
    "# 7. Visualizar\n",
    "ingresos_categoria.plot(kind='bar')\n",
    "```\n",
    "\n",
    "**Errores comunes a evitar:**\n",
    "- ‚ùå Olvidar multiplicar (stock √ó precio)\n",
    "- ‚ùå Agrupar sin especificar funci√≥n de agregaci√≥n\n",
    "- ‚ùå Comparar con `=` en lugar de `==` en filtros\n",
    "- ‚ùå No verificar tipos de datos (¬øprecio es float o string?)\n",
    "\n",
    "**Validaciones recomendadas:**\n",
    "```python\n",
    "# ¬øTiene sentido el resultado?\n",
    "assert df['valor_inventario'].min() >= 0, \"Valores negativos!\"\n",
    "assert df['stock'].dtype == 'int64', \"Stock debe ser entero\"\n",
    "```\n",
    "\n",
    "**Pr√≥ximo paso despu√©s del ejercicio:**\n",
    "- Compara tu soluci√≥n con otros\n",
    "- ¬øHay formas m√°s eficientes?\n",
    "- ¬øQu√© m√©tricas adicionales podr√≠as calcular?\n",
    "\n",
    "**En este bloque practicar√°s:**\n",
    "1. Crear DataFrames desde cero con datos realistas\n",
    "2. Calcular columnas derivadas (valor_inventario, ingresos)\n",
    "3. Aplicar agregaciones por categor√≠a con groupby\n",
    "4. Ordenar y seleccionar top N con sort_values() + head()\n",
    "5. Filtrar con condiciones booleanas\n",
    "6. Visualizar resultados\n",
    "7. Responder preguntas de negocio con datos\n",
    "8. Validar que los resultados tengan sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc982832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUCI√ìN DEL EJERCICIO\n",
    "\n",
    "def extraer_posts():\n",
    "    \"\"\"\n",
    "    Extrae posts desde JSONPlaceholder API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        posts = response.json()\n",
    "        print(f\"‚úÖ Posts extra√≠dos: {len(posts)}\")\n",
    "        return posts\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def transformar_posts(posts_raw):\n",
    "    \"\"\"\n",
    "    Transforma los posts agregando m√©tricas adicionales\n",
    "    \"\"\"\n",
    "    if not posts_raw:\n",
    "        return None\n",
    "    \n",
    "    posts_transformados = []\n",
    "    \n",
    "    for post in posts_raw:\n",
    "        post_limpio = {\n",
    "            'id': post['id'],\n",
    "            'userId': post['userId'],\n",
    "            'titulo': post['title'],\n",
    "            'cuerpo': post['body'],\n",
    "            'longitud_titulo': len(post['title']),\n",
    "            'longitud_cuerpo': len(post['body']),\n",
    "            'categoria_usuario': f\"usuario_{post['userId']}\",\n",
    "            'palabras_titulo': len(post['title'].split()),\n",
    "            'fecha_procesamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        posts_transformados.append(post_limpio)\n",
    "    \n",
    "    df = pd.DataFrame(posts_transformados)\n",
    "    print(f\"üìä Posts transformados: {len(df)} registros\")\n",
    "    return df\n",
    "\n",
    "# Ejecutar pipeline de posts\n",
    "posts_raw = extraer_posts()\n",
    "df_posts = transformar_posts(posts_raw)\n",
    "\n",
    "if df_posts is not None:\n",
    "    # Mostrar estad√≠sticas\n",
    "    print(\"\\nüìä Estad√≠sticas de Posts:\")\n",
    "    print(f\"   ‚Ä¢ Total posts: {len(df_posts)}\")\n",
    "    print(f\"   ‚Ä¢ Usuarios √∫nicos: {df_posts['userId'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Promedio palabras t√≠tulo: {df_posts['palabras_titulo'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Promedio caracteres cuerpo: {df_posts['longitud_cuerpo'].mean():.0f}\")\n",
    "    \n",
    "    # Guardar datos\n",
    "    exito_posts = cargar_datos(df_posts, \"../datasets/processed/posts_transformados.csv\")\n",
    "    \n",
    "    if exito_posts:\n",
    "        print(\"\\nüéâ ¬°Ejercicio completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ef12f",
   "metadata": {},
   "source": [
    "## üéØ Resumen y Pr√≥ximos Pasos\n",
    "\n",
    "### ‚úÖ Lo que Aprendiste Hoy:\n",
    "\n",
    "1. **Conceptos Fundamentales**:\n",
    "   - Qu√© es la Ingenier√≠a de Datos\n",
    "   - Diferencias entre roles de datos\n",
    "   - Componentes de un pipeline ETL\n",
    "\n",
    "2. **Habilidades Pr√°cticas**:\n",
    "   - Extracci√≥n de datos desde APIs\n",
    "   - Transformaci√≥n y limpieza b√°sica\n",
    "   - Carga de datos en archivos CSV\n",
    "   - An√°lisis exploratorio simple\n",
    "\n",
    "3. **Herramientas Utilizadas**:\n",
    "   - `requests` para APIs\n",
    "   - `pandas` para manipulaci√≥n\n",
    "   - `matplotlib/seaborn` para visualizaci√≥n\n",
    "\n",
    "### üîÆ Pr√≥ximos Notebooks:\n",
    "\n",
    "- **02_setup_entorno_desarrollo.ipynb**: Configuraci√≥n avanzada\n",
    "- **03_git_version_control.ipynb**: Control de versiones\n",
    "- **04_python_estructuras_datos.ipynb**: Python intermedio\n",
    "\n",
    "### üè† Tarea Opcional:\n",
    "\n",
    "1. Experimenta con otras APIs p√∫blicas:\n",
    "   - [GitHub API](https://api.github.com)\n",
    "   - [OpenWeather API](https://openweathermap.org/api)\n",
    "   - [REST Countries](https://restcountries.com)\n",
    "\n",
    "2. Modifica el pipeline para agregar validaciones de calidad de datos\n",
    "\n",
    "3. Investiga qu√© es Apache Airflow y c√≥mo se relaciona con lo que hicimos\n",
    "\n",
    "---\n",
    "\n",
    "## üéä ¬°Felicitaciones!\n",
    "\n",
    "Has completado tu primer notebook de Ingenier√≠a de Datos y creado tu primer pipeline ETL. \n",
    "\n",
    "**¬°Bienvenido al fascinante mundo de la Ingenier√≠a de Datos!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*¬øTienes preguntas o sugerencias? ¬°Abre un issue en el repositorio!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
