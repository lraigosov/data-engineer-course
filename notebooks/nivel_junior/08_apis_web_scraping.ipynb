{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781eddc6",
   "metadata": {},
   "source": [
    "# üåê APIs REST y Web Scraping para Ingenier√≠a de Datos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar este notebook, ser√°s capaz de:\n",
    "\n",
    "1. ‚úÖ Consumir **APIs REST** con `requests`\n",
    "2. ‚úÖ Manejar **autenticaci√≥n** y headers\n",
    "3. ‚úÖ Implementar **paginaci√≥n** y rate limiting\n",
    "4. ‚úÖ Realizar **web scraping** con `BeautifulSoup`\n",
    "5. ‚úÖ Extraer datos de HTML estructurado\n",
    "6. ‚úÖ Aplicar **mejores pr√°cticas** y √©tica en scraping\n",
    "7. ‚úÖ Construir un **pipeline de ingesta** desde APIs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48e065",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n a APIs REST\n",
    "\n",
    "### ü§î ¬øQu√© es una API REST?\n",
    "\n",
    "**API REST** (Representational State Transfer) es un estilo de arquitectura para servicios web que usa HTTP.\n",
    "\n",
    "### M√©todos HTTP Comunes:\n",
    "\n",
    "| M√©todo | Prop√≥sito | Ejemplo |\n",
    "|--------|-----------|----------|\n",
    "| **GET** | Obtener datos | `/api/usuarios` |\n",
    "| **POST** | Crear recurso | `/api/usuarios` (con body) |\n",
    "| **PUT** | Actualizar completo | `/api/usuarios/123` |\n",
    "| **PATCH** | Actualizar parcial | `/api/usuarios/123` |\n",
    "| **DELETE** | Eliminar recurso | `/api/usuarios/123` |\n",
    "\n",
    "### C√≥digos de Estado HTTP:\n",
    "\n",
    "- **2xx**: √âxito (200 OK, 201 Created)\n",
    "- **4xx**: Error del cliente (400 Bad Request, 404 Not Found, 401 Unauthorized)\n",
    "- **5xx**: Error del servidor (500 Internal Server Error, 503 Service Unavailable)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793dd25d",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Para web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as cargadas correctamente\")\n",
    "print(f\"üì¶ Requests versi√≥n: {requests.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b4f51",
   "metadata": {},
   "source": [
    "### üìö Bibliotecas para APIs y Web Scraping\n",
    "\n",
    "**Librer√≠as principales:**\n",
    "- **requests:** Cliente HTTP para consumir APIs REST\n",
    "- **json:** Parsing y manipulaci√≥n de datos JSON\n",
    "- **BeautifulSoup:** Parser HTML/XML para web scraping\n",
    "- **pandas:** Estructurar datos extra√≠dos en DataFrames\n",
    "\n",
    "**Uso en ingenier√≠a de datos:**\n",
    "- Ingesta desde APIs externas (redes sociales, servicios cloud)\n",
    "- Extracci√≥n de datos de sitios web\n",
    "- Integraci√≥n con fuentes de datos sin APIs formales\n",
    "\n",
    "**Instalaci√≥n:** `pip install requests beautifulsoup4 lxml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c574e79",
   "metadata": {},
   "source": [
    "## 3. Consumir APIs REST P√∫blicas\n",
    "\n",
    "### 3.1 Ejemplo B√°sico: JSONPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02553358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API p√∫blica para testing\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Realizar petici√≥n GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar c√≥digo de estado\n",
    "print(f\"üì° Status Code: {response.status_code}\")\n",
    "print(f\"‚úÖ Exitoso: {response.ok}\")\n",
    "\n",
    "# Obtener datos JSON\n",
    "if response.ok:\n",
    "    posts = response.json()\n",
    "    print(f\"\\nüìä Total de posts: {len(posts)}\")\n",
    "    print(f\"\\nüîç Primer post:\")\n",
    "    print(json.dumps(posts[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6649e",
   "metadata": {},
   "source": [
    "### üåê Petici√≥n GET B√°sica con Requests\n",
    "\n",
    "**Concepto:** `requests.get()` env√≠a solicitud HTTP GET para obtener datos de una API.\n",
    "\n",
    "**Flujo:**\n",
    "1. **Enviar petici√≥n:** `response = requests.get(url)`\n",
    "2. **Verificar estado:** `response.status_code` (200 = √©xito)\n",
    "3. **Parsear JSON:** `response.json()` convierte a diccionario Python\n",
    "4. **Validar:** `response.ok` = True si status 200-299\n",
    "\n",
    "**Propiedades √∫tiles:**\n",
    "- `response.text`: contenido como string\n",
    "- `response.json()`: parsea JSON autom√°ticamente\n",
    "- `response.headers`: headers de respuesta\n",
    "\n",
    "**Buena pr√°ctica:** Siempre verificar `response.ok` antes de procesar datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088decaa",
   "metadata": {},
   "source": [
    "### 3.2 Convertir Respuesta a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8432a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame\n",
    "df_posts = pd.DataFrame(posts)\n",
    "\n",
    "print(f\"üìä Shape: {df_posts.shape}\")\n",
    "print(f\"\\nüîç Vista previa:\")\n",
    "display(df_posts.head())\n",
    "\n",
    "# Informaci√≥n del DataFrame\n",
    "print(\"\\nüìã Info:\")\n",
    "df_posts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dbc66",
   "metadata": {},
   "source": [
    "### üìä JSON a DataFrame: Estructurando Datos de API\n",
    "\n",
    "**Concepto:** `pd.DataFrame()` convierte lista de diccionarios JSON en tabla estructurada.\n",
    "\n",
    "**Ventajas:**\n",
    "- An√°lisis con pandas (filtros, agregaciones)\n",
    "- Exportaci√≥n a CSV, SQL, Parquet\n",
    "- Integraci√≥n con pipelines ETL\n",
    "\n",
    "**Conversi√≥n autom√°tica:**\n",
    "- Claves JSON ‚Üí columnas DataFrame\n",
    "- Lista de objetos ‚Üí filas\n",
    "- Tipos inferidos autom√°ticamente\n",
    "\n",
    "**Uso t√≠pico:** Primera transformaci√≥n tras ingestar datos desde API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66773074",
   "metadata": {},
   "source": [
    "### 3.3 Par√°metros de Consulta (Query Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por userId\n",
    "params = {'userId': 1}\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.ok:\n",
    "    posts_user1 = response.json()\n",
    "    print(f\"üìä Posts del usuario 1: {len(posts_user1)}\")\n",
    "    print(f\"\\nüîó URL completa: {response.url}\")\n",
    "    \n",
    "    # Ver primeros 3\n",
    "    for post in posts_user1[:3]:\n",
    "        print(f\"\\nüìù Post {post['id']}: {post['title'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90988da",
   "metadata": {},
   "source": [
    "### üîç Query Parameters: Filtrado en APIs\n",
    "\n",
    "**Concepto:** Los query parameters permiten filtrar, paginar y personalizar respuestas de APIs.\n",
    "\n",
    "**Sintaxis:**\n",
    "```python\n",
    "params = {'userId': 1, 'limit': 10}\n",
    "requests.get(url, params=params)\n",
    "```\n",
    "\n",
    "**URL generada:** `https://api.com/posts?userId=1&limit=10`\n",
    "\n",
    "**Casos comunes:**\n",
    "- Filtros: `?category=tech&status=active`\n",
    "- Paginaci√≥n: `?page=2&per_page=50`\n",
    "- Ordenamiento: `?sort=date&order=desc`\n",
    "\n",
    "**Ventaja:** Reduce transferencia de datos, obtiene solo lo necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff10d9",
   "metadata": {},
   "source": [
    "### 3.4 Headers Personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Data Pipeline v1.0)',\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(f\"‚úÖ Status: {response.status_code}\")\n",
    "print(f\"\\nüìÑ Headers de la respuesta:\")\n",
    "for key, value in list(response.headers.items())[:5]:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9baaaa",
   "metadata": {},
   "source": [
    "### üìÑ Headers HTTP: Metadata de Peticiones\n",
    "\n",
    "**Concepto:** Headers env√≠an informaci√≥n adicional sobre la petici√≥n (autenticaci√≥n, formato, identidad).\n",
    "\n",
    "**Headers importantes:**\n",
    "- **User-Agent:** Identifica tu aplicaci√≥n/script\n",
    "- **Accept:** Formato de respuesta deseado (application/json)\n",
    "- **Authorization:** Token de autenticaci√≥n (Bearer, API Key)\n",
    "- **Content-Type:** Tipo de datos enviados (POST/PUT)\n",
    "\n",
    "**Ejemplo autenticaci√≥n:**\n",
    "```python\n",
    "headers = {'Authorization': 'Bearer YOUR_TOKEN'}\n",
    "```\n",
    "\n",
    "**Importancia:** Muchas APIs requieren User-Agent v√°lido o bloquean peticiones sin headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c93be",
   "metadata": {},
   "source": [
    "## 4. Trabajar con APIs Reales\n",
    "\n",
    "### 4.1 GitHub API - Datos P√∫blicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener informaci√≥n de un repositorio p√∫blico\n",
    "repo_url = \"https://api.github.com/repos/pandas-dev/pandas\"\n",
    "\n",
    "response = requests.get(repo_url)\n",
    "\n",
    "if response.ok:\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    print(\"üì¶ INFORMACI√ìN DEL REPOSITORIO PANDAS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Nombre: {repo_data['name']}\")\n",
    "    print(f\"Descripci√≥n: {repo_data['description'][:80]}...\")\n",
    "    print(f\"‚≠ê Stars: {repo_data['stargazers_count']:,}\")\n",
    "    print(f\"üç¥ Forks: {repo_data['forks_count']:,}\")\n",
    "    print(f\"üëÅÔ∏è Watchers: {repo_data['watchers_count']:,}\")\n",
    "    print(f\"üêõ Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "    print(f\"üìÖ Creado: {repo_data['created_at']}\")\n",
    "    print(f\"üîÑ √öltima actualizaci√≥n: {repo_data['updated_at']}\")\n",
    "    print(f\"üìù Lenguaje principal: {repo_data['language']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6b58c",
   "metadata": {},
   "source": [
    "### 4.2 Paginaci√≥n en APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4642a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_todos_los_commits(owner: str, repo: str, max_pages: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Obtiene commits de un repositorio con paginaci√≥n.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    all_commits = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'page': page,\n",
    "            'per_page': 10  # Registros por p√°gina\n",
    "        }\n",
    "        \n",
    "        print(f\"üìÑ Obteniendo p√°gina {page}...\")\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.ok:\n",
    "            commits = response.json()\n",
    "            if not commits:  # No hay m√°s p√°ginas\n",
    "                break\n",
    "            all_commits.extend(commits)\n",
    "        else:\n",
    "            print(f\"‚ùå Error en p√°gina {page}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Respetar rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_commits\n",
    "\n",
    "# Ejemplo: Obtener commits de un repo peque√±o\n",
    "commits = obtener_todos_los_commits('octocat', 'Hello-World', max_pages=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Total commits obtenidos: {len(commits)}\")\n",
    "\n",
    "# Analizar commits\n",
    "if commits:\n",
    "    df_commits = pd.DataFrame([\n",
    "        {\n",
    "            'sha': c['sha'][:7],\n",
    "            'author': c['commit']['author']['name'],\n",
    "            'date': c['commit']['author']['date'],\n",
    "            'message': c['commit']['message'][:50]\n",
    "        }\n",
    "        for c in commits\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìä √öltimos commits:\")\n",
    "    display(df_commits.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d143287",
   "metadata": {},
   "source": [
    "### üìÑ Paginaci√≥n: Obtener Grandes Vol√∫menes de Datos\n",
    "\n",
    "**Concepto:** Las APIs limitan resultados por petici√≥n, requiriendo m√∫ltiples requests para datasets completos.\n",
    "\n",
    "**Estrategias comunes:**\n",
    "1. **Offset/Limit:** `?offset=100&limit=50`\n",
    "2. **Page-based:** `?page=3&per_page=100`\n",
    "3. **Cursor-based:** `?cursor=next_token` (m√°s eficiente)\n",
    "\n",
    "**Implementaci√≥n:**\n",
    "- Loop hasta que respuesta est√© vac√≠a o alcance m√°ximo\n",
    "- `time.sleep()` entre requests para respetar rate limits\n",
    "- Acumular resultados en lista\n",
    "\n",
    "**Buena pr√°ctica:** Implementar l√≠mite m√°ximo de p√°ginas para evitar loops infinitos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb6df0",
   "metadata": {},
   "source": [
    "### 4.3 Rate Limiting y Manejo de Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request_con_retry(url: str, max_retries: int = 3, delay: float = 2.0) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Realiza petici√≥n con reintentos autom√°ticos.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.ok:\n",
    "                return response.json()\n",
    "            \n",
    "            # Rate limit excedido\n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get('Retry-After', delay))\n",
    "                print(f\"‚è≥ Rate limit alcanzado. Esperando {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            # Otros errores\n",
    "            if response.status_code >= 500:\n",
    "                print(f\"‚ö†Ô∏è Error del servidor ({response.status_code}). Reintento {attempt}/{max_retries}\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            \n",
    "            # Error definitivo\n",
    "            print(f\"‚ùå Error {response.status_code}: {response.text[:100]}\")\n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚è±Ô∏è Timeout. Reintento {attempt}/{max_retries}\")\n",
    "            time.sleep(delay)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"üîå Error de conexi√≥n. Reintento {attempt}/{max_retries}\")\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inesperado: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"‚ùå M√°ximo de reintentos alcanzado ({max_retries})\")\n",
    "    return None\n",
    "\n",
    "# Probar funci√≥n\n",
    "data = api_request_con_retry(\"https://api.github.com/users/octocat\")\n",
    "\n",
    "if data:\n",
    "    print(f\"\\n‚úÖ Usuario obtenido: {data['login']}\")\n",
    "    print(f\"üë§ Nombre: {data['name']}\")\n",
    "    print(f\"üìç Ubicaci√≥n: {data.get('location', 'N/A')}\")\n",
    "    print(f\"üì¶ Repositorios p√∫blicos: {data['public_repos']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8886e1",
   "metadata": {},
   "source": [
    "### ‚ö° Rate Limiting y Reintentos Autom√°ticos\n",
    "\n",
    "**Concepto:** APIs limitan n√∫mero de peticiones por tiempo (ej: 100/hora) para prevenir abuso.\n",
    "\n",
    "**Manejo de rate limits:**\n",
    "- **Status 429:** Too Many Requests\n",
    "- **Header `Retry-After`:** segundos a esperar\n",
    "- **Backoff exponencial:** incrementar delay tras cada fallo\n",
    "\n",
    "**Errores comunes:**\n",
    "- **4xx:** Errores del cliente (request inv√°lido) ‚Üí no reintentar\n",
    "- **5xx:** Errores del servidor ‚Üí reintentar con delay\n",
    "- **Timeout/Connection:** problemas de red ‚Üí reintentar\n",
    "\n",
    "**Estrategia robusta:** 3 reintentos con delays incrementales + captura de excepciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4ac75",
   "metadata": {},
   "source": [
    "## 5. Autenticaci√≥n en APIs\n",
    "\n",
    "### 5.1 API Key en Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79300002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo gen√©rico (NO ejecutar sin API key real)\n",
    "def api_con_key(api_key: str, endpoint: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Ejemplo de autenticaci√≥n con API Key.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if response.ok:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "print(\"\"\"\n",
    "üí° MEJORES PR√ÅCTICAS PARA API KEYS:\n",
    "\n",
    "1. ‚ùå NUNCA hardcodear API keys en el c√≥digo\n",
    "2. ‚úÖ Usar variables de entorno:\n",
    "   import os\n",
    "   api_key = os.getenv('MI_API_KEY')\n",
    "\n",
    "3. ‚úÖ Archivo de configuraci√≥n (no versionado):\n",
    "   import yaml\n",
    "   with open('config/credentials.yaml') as f:\n",
    "       creds = yaml.safe_load(f)\n",
    "   api_key = creds['api_key']\n",
    "\n",
    "4. ‚úÖ Agregar credentials.yaml a .gitignore\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529f991",
   "metadata": {},
   "source": [
    "### 5.2 OAuth 2.0 (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f116fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üîê FLUJO OAUTH 2.0:\n",
    "\n",
    "1. Usuario autoriza aplicaci√≥n\n",
    "2. API devuelve c√≥digo de autorizaci√≥n\n",
    "3. Aplicaci√≥n intercambia c√≥digo por access token\n",
    "4. Usar access token en peticiones\n",
    "\n",
    "Ejemplo con Google API:\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "creds = Credentials.from_authorized_user_file('token.json')\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "üìö Librer√≠as √∫tiles:\n",
    "  ‚Ä¢ requests-oauthlib\n",
    "  ‚Ä¢ authlib\n",
    "  ‚Ä¢ httpx (async)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a352da",
   "metadata": {},
   "source": [
    "## 6. Web Scraping con BeautifulSoup\n",
    "\n",
    "### 6.1 Conceptos B√°sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7264eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con HTML simple\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Productos</title></head>\n",
    "<body>\n",
    "    <h1>Cat√°logo de Productos</h1>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Laptop</h2>\n",
    "        <p class=\"precio\">$1200</p>\n",
    "        <p class=\"stock\">En stock</p>\n",
    "    </div>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Mouse</h2>\n",
    "        <p class=\"precio\">$25</p>\n",
    "        <p class=\"stock\">Agotado</p>\n",
    "    </div>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Teclado</h2>\n",
    "        <p class=\"precio\">$75</p>\n",
    "        <p class=\"stock\">En stock</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parsear HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraer t√≠tulo\n",
    "titulo = soup.title.string\n",
    "print(f\"üìÑ T√≠tulo: {titulo}\")\n",
    "\n",
    "# Extraer encabezado\n",
    "h1 = soup.find('h1').text\n",
    "print(f\"üìå Encabezado: {h1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c2432",
   "metadata": {},
   "source": [
    "### üç≤ BeautifulSoup: Parser HTML para Scraping\n",
    "\n",
    "**Concepto:** BeautifulSoup convierte HTML en √°rbol navegable de objetos Python.\n",
    "\n",
    "**Parsers disponibles:**\n",
    "- `html.parser`: incluido en Python (est√°ndar)\n",
    "- `lxml`: m√°s r√°pido, requiere instalaci√≥n\n",
    "- `html5lib`: m√°s tolerante con HTML mal formado\n",
    "\n",
    "**Operaciones b√°sicas:**\n",
    "- `soup.find('tag')`: primer elemento que coincide\n",
    "- `soup.find_all('tag')`: todos los elementos\n",
    "- `soup.title.string`: acceso directo a elementos √∫nicos\n",
    "- `.text`: extraer texto sin tags HTML\n",
    "\n",
    "**Uso:** Extraer datos estructurados de p√°ginas web sin API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915e19b",
   "metadata": {},
   "source": [
    "### 6.2 Buscar Elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todos los productos\n",
    "productos = soup.find_all('div', class_='producto')\n",
    "\n",
    "print(f\"üì¶ Total productos encontrados: {len(productos)}\\n\")\n",
    "\n",
    "# Extraer informaci√≥n de cada producto\n",
    "datos_productos = []\n",
    "\n",
    "for producto in productos:\n",
    "    nombre = producto.find('h2').text\n",
    "    precio = producto.find('p', class_='precio').text\n",
    "    stock = producto.find('p', class_='stock').text\n",
    "    \n",
    "    datos_productos.append({\n",
    "        'nombre': nombre,\n",
    "        'precio': precio,\n",
    "        'stock': stock\n",
    "    })\n",
    "    \n",
    "    print(f\"üõí {nombre}\")\n",
    "    print(f\"   üí∞ Precio: {precio}\")\n",
    "    print(f\"   üì¶ Stock: {stock}\\n\")\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_productos = pd.DataFrame(datos_productos)\n",
    "display(df_productos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c030e77",
   "metadata": {},
   "source": [
    "### üîç Selectores CSS y B√∫squeda de Elementos\n",
    "\n",
    "**Concepto:** Localizar elementos espec√≠ficos en HTML usando clases, IDs, tags y atributos.\n",
    "\n",
    "**M√©todos principales:**\n",
    "- `find('tag')`: primer elemento\n",
    "- `find_all('tag')`: lista de todos los elementos\n",
    "- `find('tag', class_='nombre')`: buscar por clase CSS\n",
    "- `find('tag', id='nombre')`: buscar por ID\n",
    "- `find('tag', attrs={'data-id': '123'})`: atributos personalizados\n",
    "\n",
    "**Selectores CSS avanzados:**\n",
    "```python\n",
    "soup.select('.producto')  # por clase\n",
    "soup.select('#main')      # por ID\n",
    "soup.select('div > p')    # hijos directos\n",
    "```\n",
    "\n",
    "**Extracci√≥n:** `.text` para contenido, `.get('href')` para atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac557f",
   "metadata": {},
   "source": [
    "### 6.3 Selectores CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e028234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar selectores CSS (m√°s potente)\n",
    "precios = soup.select('.producto .precio')\n",
    "\n",
    "print(\"üí∞ Precios encontrados con selector CSS:\")\n",
    "for precio in precios:\n",
    "    print(f\"  {precio.text}\")\n",
    "\n",
    "# Productos en stock\n",
    "en_stock = [p for p in soup.select('.producto') \n",
    "            if 'En stock' in p.select_one('.stock').text]\n",
    "\n",
    "print(f\"\\n‚úÖ Productos en stock: {len(en_stock)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7c849",
   "metadata": {},
   "source": [
    "### 6.4 Scraping de Sitio Web Real (Ejemplo √âtico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e97cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_quotes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape de http://quotes.toscrape.com (sitio de prueba).\n",
    "    \"\"\"\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    \n",
    "    # Headers para simular navegador\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if not response.ok:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Encontrar todas las quotes\n",
    "    quotes_elements = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    quotes_data = []\n",
    "    \n",
    "    for quote_el in quotes_elements:\n",
    "        text = quote_el.find('span', class_='text').text\n",
    "        author = quote_el.find('small', class_='author').text\n",
    "        tags = [tag.text for tag in quote_el.find_all('a', class_='tag')]\n",
    "        \n",
    "        quotes_data.append({\n",
    "            'quote': text,\n",
    "            'author': author,\n",
    "            'tags': ', '.join(tags)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(quotes_data)\n",
    "\n",
    "# Ejecutar scraping\n",
    "df_quotes = scrape_quotes()\n",
    "\n",
    "if not df_quotes.empty:\n",
    "    print(f\"‚úÖ Quotes extra√≠das: {len(df_quotes)}\\n\")\n",
    "    display(df_quotes.head())\n",
    "    \n",
    "    # An√°lisis\n",
    "    print(\"\\nüìä Autores m√°s citados:\")\n",
    "    print(df_quotes['author'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191593e6",
   "metadata": {},
   "source": [
    "### üåê Scraping de Sitios Reales: Flujo Completo\n",
    "\n",
    "**Concepto:** Extraer datos de sitios web p√∫blicos siguiendo mejores pr√°cticas √©ticas y t√©cnicas.\n",
    "\n",
    "**Pasos esenciales:**\n",
    "1. **Revisar robots.txt:** `sitio.com/robots.txt` - qu√© est√° permitido\n",
    "2. **User-Agent v√°lido:** identificarse como navegador\n",
    "3. **Parsear HTML:** BeautifulSoup convierte respuesta en objeto navegable\n",
    "4. **Extraer datos:** usar selectores para localizar informaci√≥n\n",
    "5. **Estructurar:** convertir a DataFrame para an√°lisis\n",
    "\n",
    "**Sitios de pr√°ctica:**\n",
    "- quotes.toscrape.com (dise√±ado para aprender)\n",
    "- books.toscrape.com (cat√°logo de libros)\n",
    "\n",
    "**√âtica:** Respetar t√©rminos de servicio, no sobrecargar servidores, usar datos responsablemente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c827948",
   "metadata": {},
   "source": [
    "### 6.5 Extraer Tablas HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe19895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML con tabla\n",
    "html_tabla = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Pa√≠s</th>\n",
    "        <th>Capital</th>\n",
    "        <th>Poblaci√≥n (M)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>M√©xico</td>\n",
    "        <td>Ciudad de M√©xico</td>\n",
    "        <td>126</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Colombia</td>\n",
    "        <td>Bogot√°</td>\n",
    "        <td>51</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Argentina</td>\n",
    "        <td>Buenos Aires</td>\n",
    "        <td>45</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup_tabla = BeautifulSoup(html_tabla, 'html.parser')\n",
    "tabla = soup_tabla.find('table')\n",
    "\n",
    "# Extraer headers\n",
    "headers = [th.text for th in tabla.find_all('th')]\n",
    "\n",
    "# Extraer filas\n",
    "filas = []\n",
    "for tr in tabla.find_all('tr')[1:]:  # Saltar header\n",
    "    celdas = [td.text for td in tr.find_all('td')]\n",
    "    filas.append(celdas)\n",
    "\n",
    "# Crear DataFrame\n",
    "df_paises = pd.DataFrame(filas, columns=headers)\n",
    "df_paises['Poblaci√≥n (M)'] = df_paises['Poblaci√≥n (M)'].astype(int)\n",
    "\n",
    "print(\"üåç Tabla extra√≠da:\")\n",
    "display(df_paises)\n",
    "\n",
    "# Alternativa con pandas (m√°s simple)\n",
    "print(\"\\nüí° Con pandas.read_html():\")\n",
    "df_paises_pandas = pd.read_html(html_tabla)[0]\n",
    "display(df_paises_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c939fa",
   "metadata": {},
   "source": [
    "### üìä Extracci√≥n de Tablas HTML\n",
    "\n",
    "**Concepto:** Las tablas HTML (`<table>`) son estructuras ideales para convertir directamente a DataFrames.\n",
    "\n",
    "**Estrategia:**\n",
    "1. Localizar `<table>` con `soup.find('table')`\n",
    "2. Extraer headers de `<th>` en primera fila\n",
    "3. Iterar sobre `<tr>` (filas) extrayendo `<td>` (celdas)\n",
    "4. Crear DataFrame con headers y filas\n",
    "\n",
    "**Alternativa r√°pida:**\n",
    "```python\n",
    "pd.read_html(url)  # Pandas detecta tablas autom√°ticamente\n",
    "```\n",
    "\n",
    "**Uso com√∫n:** Datos estad√≠sticos, rankings, resultados deportivos, precios hist√≥ricos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72859f5a",
   "metadata": {},
   "source": [
    "## 7. Pipeline de Ingesta desde API\n",
    "\n",
    "### 7.1 Clase APIExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4931d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIExtractor:\n",
    "    \"\"\"\n",
    "    Extractor gen√©rico para APIs REST con manejo de errores y paginaci√≥n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, headers: Optional[Dict] = None, rate_limit: float = 1.0):\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers or {}\n",
    "        self.rate_limit = rate_limit\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "    \n",
    "    def get(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Dict]:\n",
    "        \"\"\"Realiza petici√≥n GET con manejo de errores.\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            time.sleep(self.rate_limit)  # Rate limiting\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"‚ùå HTTP Error: {e}\")\n",
    "            return None\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚è±Ô∏è Timeout al consultar {url}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inesperado: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_paginated(self, endpoint: str, page_param: str = 'page', \n",
    "                     max_pages: int = 10) -> List[Dict]:\n",
    "        \"\"\"Obtiene datos paginados.\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"üìÑ P√°gina {page}/{max_pages}\")\n",
    "            params = {page_param: page}\n",
    "            data = self.get(endpoint, params)\n",
    "            \n",
    "            if not data:\n",
    "                break\n",
    "            \n",
    "            # Asumiendo que data es una lista\n",
    "            if isinstance(data, list):\n",
    "                if not data:  # Lista vac√≠a\n",
    "                    break\n",
    "                all_data.extend(data)\n",
    "            else:\n",
    "                # Si es dict, agregar directamente\n",
    "                all_data.append(data)\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def to_dataframe(self, data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Convierte datos a DataFrame.\"\"\"\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "print(\"‚úÖ Clase APIExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dd22b",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Pipeline de Ingesta: Clase APIExtractor\n",
    "\n",
    "**Concepto:** Encapsular l√≥gica de extracci√≥n desde APIs en clase reutilizable.\n",
    "\n",
    "**Componentes clave:**\n",
    "- **Session:** `requests.Session()` reutiliza conexiones (m√°s eficiente)\n",
    "- **Rate limiting:** `time.sleep()` entre peticiones\n",
    "- **Manejo de errores:** try/except para HTTPError, Timeout, etc.\n",
    "- **Paginaci√≥n autom√°tica:** itera hasta encontrar datos vac√≠os\n",
    "- **Conversi√≥n:** m√©todo `.to_dataframe()` para an√°lisis\n",
    "\n",
    "**Ventajas:**\n",
    "- Reutilizable para m√∫ltiples APIs\n",
    "- C√≥digo m√°s limpio y mantenible\n",
    "- Configuraci√≥n centralizada (headers, rate limit)\n",
    "\n",
    "**Uso:** Base para pipelines ETL de ingesta desde APIs externas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922441e7",
   "metadata": {},
   "source": [
    "### 7.2 Usar APIExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cba0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor para JSONPlaceholder\n",
    "extractor = APIExtractor(\n",
    "    base_url=\"https://jsonplaceholder.typicode.com\",\n",
    "    rate_limit=0.5  # 0.5 segundos entre requests\n",
    ")\n",
    "\n",
    "# Extraer usuarios\n",
    "print(\"üîç Extrayendo usuarios...\\n\")\n",
    "usuarios = extractor.get('users')\n",
    "\n",
    "if usuarios:\n",
    "    df_usuarios = extractor.to_dataframe(usuarios)\n",
    "    \n",
    "    # Seleccionar columnas relevantes\n",
    "    df_usuarios_clean = df_usuarios[['id', 'name', 'email', 'phone', 'website']].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Usuarios extra√≠dos: {len(df_usuarios_clean)}\")\n",
    "    display(df_usuarios_clean.head())\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = '../../outputs/usuarios_api.csv'\n",
    "    df_usuarios_clean.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ Datos guardados en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1822f",
   "metadata": {},
   "source": [
    "## 8. Mejores Pr√°cticas y √âtica\n",
    "\n",
    "### 8.1 Consideraciones Legales y √âticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚öñÔ∏è MEJORES PR√ÅCTICAS EN WEB SCRAPING:\n",
    "\n",
    "1. üìú VERIFICAR LEGALIDAD\n",
    "   ‚Ä¢ Revisar T√©rminos de Servicio\n",
    "   ‚Ä¢ Consultar robots.txt\n",
    "   ‚Ä¢ Respetar copyright\n",
    "\n",
    "2. ü§ù SER RESPETUOSO\n",
    "   ‚Ä¢ Usar rate limiting (delays)\n",
    "   ‚Ä¢ No sobrecargar servidores\n",
    "   ‚Ä¢ Scraping en horarios de bajo tr√°fico\n",
    "\n",
    "3. üîí PRIVACIDAD\n",
    "   ‚Ä¢ No extraer datos personales sensibles\n",
    "   ‚Ä¢ Cumplir GDPR/CCPA si aplica\n",
    "   ‚Ä¢ Anonimizar datos cuando sea posible\n",
    "\n",
    "4. üéØ ALTERNATIVAS PREFERIBLES\n",
    "   ‚Ä¢ Usar API oficial si existe\n",
    "   ‚Ä¢ Contactar al propietario del sitio\n",
    "   ‚Ä¢ Considerar datasets p√∫blicos\n",
    "\n",
    "5. üõ°Ô∏è IDENTIFICARSE\n",
    "   ‚Ä¢ User-Agent descriptivo\n",
    "   ‚Ä¢ Email de contacto\n",
    "   ‚Ä¢ Documentar prop√≥sito\n",
    "\n",
    "6. üíæ CACH√â LOCAL\n",
    "   ‚Ä¢ No repetir requests innecesarios\n",
    "   ‚Ä¢ Almacenar resultados localmente\n",
    "   ‚Ä¢ Actualizar solo cuando sea necesario\n",
    "\n",
    "‚ùå NUNCA:\n",
    "  ‚Ä¢ Ignorar robots.txt\n",
    "  ‚Ä¢ Hacer requests masivos sin delays\n",
    "  ‚Ä¢ Usar datos para spam o fraude\n",
    "  ‚Ä¢ Redistribuir contenido con copyright\n",
    "  ‚Ä¢ Evadir CAPTCHAs o medidas de seguridad\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e4e44",
   "metadata": {},
   "source": [
    "### 8.2 Verificar robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac573d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_robots_txt(url: str) -> None:\n",
    "    \"\"\"\n",
    "    Verifica el archivo robots.txt de un sitio.\n",
    "    \"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.ok:\n",
    "            print(f\"ü§ñ robots.txt de {parsed.netloc}:\\n\")\n",
    "            print(response.text[:500])  # Primeros 500 caracteres\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ robots.txt (Status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al verificar robots.txt: {str(e)}\")\n",
    "\n",
    "# Ejemplo\n",
    "verificar_robots_txt(\"https://github.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f9997",
   "metadata": {},
   "source": [
    "## üéØ Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: API de GitHub\n",
    "Crea un script que:\n",
    "1. Obtenga los √∫ltimos 50 repositorios de un usuario de GitHub\n",
    "2. Extraiga: nombre, descripci√≥n, estrellas, lenguaje, fecha de creaci√≥n\n",
    "3. Convierta a DataFrame y guarde en CSV\n",
    "4. Genere estad√≠sticas (lenguaje m√°s usado, repo con m√°s estrellas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C√ìDIGO AQU√ç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1665c4",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Web Scraping\n",
    "Scrape http://books.toscrape.com:\n",
    "1. Extrae t√≠tulo, precio, disponibilidad y rating\n",
    "2. Navega m√∫ltiples p√°ginas (al menos 3)\n",
    "3. Limpia los datos (quitar s√≠mbolos de precio, convertir ratings)\n",
    "4. Crea visualizaci√≥n de precio promedio por rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C√ìDIGO AQU√ç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8d119",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Pipeline Completo\n",
    "Implementa un pipeline que:\n",
    "1. Extraiga datos de una API p√∫blica (elige una de [public-apis.io](https://public-apis.io))\n",
    "2. Implemente manejo de errores robusto\n",
    "3. Aplique transformaciones necesarias\n",
    "4. Guarde resultados en formato Parquet\n",
    "5. Genere un reporte con estad√≠sticas b√°sicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb252b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C√ìDIGO AQU√ç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d353414",
   "metadata": {},
   "source": [
    "## üìö Recursos Adicionales\n",
    "\n",
    "### APIs P√∫blicas para Practicar\n",
    "- [JSONPlaceholder](https://jsonplaceholder.typicode.com/) - API fake para testing\n",
    "- [GitHub API](https://docs.github.com/en/rest) - Datos de repositorios\n",
    "- [OpenWeatherMap](https://openweathermap.org/api) - Datos meteorol√≥gicos\n",
    "- [NewsAPI](https://newsapi.org/) - Noticias globales\n",
    "- [Public APIs](https://github.com/public-apis/public-apis) - Lista curada\n",
    "\n",
    "### Sitios para Practicar Scraping (Legalmente)\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/)\n",
    "- [Books to Scrape](http://books.toscrape.com/)\n",
    "- [Scrape This Site](https://www.scrapethissite.com/)\n",
    "\n",
    "### Documentaci√≥n\n",
    "- [Requests Documentation](https://requests.readthedocs.io/)\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Scrapy](https://scrapy.org/) - Framework avanzado de scraping\n",
    "\n",
    "### Alternativas Avanzadas\n",
    "- **Selenium**: Para sitios con JavaScript\n",
    "- **Playwright**: Automatizaci√≥n de navegadores\n",
    "- **httpx**: Cliente HTTP async\n",
    "- **aiohttp**: Requests as√≠ncronos\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resumen\n",
    "\n",
    "En este notebook aprendiste:\n",
    "\n",
    "1. ‚úÖ **APIs REST**: M√©todos HTTP, c√≥digos de estado, requests\n",
    "2. ‚úÖ **Biblioteca requests**: GET, POST, headers, par√°metros\n",
    "3. ‚úÖ **Paginaci√≥n**: Obtener datos en m√∫ltiples p√°ginas\n",
    "4. ‚úÖ **Rate limiting**: Respetar l√≠mites de APIs\n",
    "5. ‚úÖ **Autenticaci√≥n**: API keys, OAuth\n",
    "6. ‚úÖ **BeautifulSoup**: Parsear HTML, selectores CSS\n",
    "7. ‚úÖ **Web scraping √©tico**: Robots.txt, mejores pr√°cticas\n",
    "8. ‚úÖ **Pipeline de ingesta**: Clase reutilizable para APIs\n",
    "\n",
    "**üéØ Pr√≥ximo paso**: Proyecto Integrador 1\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
