{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781eddc6",
   "metadata": {},
   "source": [
    "# \ud83c\udf10 APIs REST y Web Scraping para Ingenier\u00eda de Datos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar este notebook, ser\u00e1s capaz de:\n",
    "\n",
    "1. \u2705 Consumir **APIs REST** con `requests`\n",
    "2. \u2705 Manejar **autenticaci\u00f3n** y headers\n",
    "3. \u2705 Implementar **paginaci\u00f3n** y rate limiting\n",
    "4. \u2705 Realizar **web scraping** con `BeautifulSoup`\n",
    "5. \u2705 Extraer datos de HTML estructurado\n",
    "6. \u2705 Aplicar **mejores pr\u00e1cticas** y \u00e9tica en scraping\n",
    "7. \u2705 Construir un **pipeline de ingesta** desde APIs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48e065",
   "metadata": {},
   "source": [
    "## 1. Introducci\u00f3n a APIs REST\n",
    "\n",
    "### \ud83e\udd14 \u00bfQu\u00e9 es una API REST?\n",
    "\n",
    "**API REST** (Representational State Transfer) es un estilo de arquitectura para servicios web que usa HTTP.\n",
    "\n",
    "### M\u00e9todos HTTP Comunes:\n",
    "\n",
    "| M\u00e9todo | Prop\u00f3sito | Ejemplo |\n",
    "|--------|-----------|----------|\n",
    "| **GET** | Obtener datos | `/api/usuarios` |\n",
    "| **POST** | Crear recurso | `/api/usuarios` (con body) |\n",
    "| **PUT** | Actualizar completo | `/api/usuarios/123` |\n",
    "| **PATCH** | Actualizar parcial | `/api/usuarios/123` |\n",
    "| **DELETE** | Eliminar recurso | `/api/usuarios/123` |\n",
    "\n",
    "### C\u00f3digos de Estado HTTP:\n",
    "\n",
    "- **2xx**: \u00c9xito (200 OK, 201 Created)\n",
    "- **4xx**: Error del cliente (400 Bad Request, 404 Not Found, 401 Unauthorized)\n",
    "- **5xx**: Error del servidor (500 Internal Server Error, 503 Service Unavailable)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793dd25d",
   "metadata": {},
   "source": [
    "## 2. Configuraci\u00f3n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Para web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\u2705 Librer\u00edas cargadas correctamente\")\n",
    "print(f\"\ud83d\udce6 Requests versi\u00f3n: {requests.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b4f51",
   "metadata": {},
   "source": [
    "### \ud83d\udcda Bibliotecas para APIs y Web Scraping\n",
    "\n",
    "**Librer\u00edas principales:**\n",
    "- **requests:** Cliente HTTP para consumir APIs REST\n",
    "- **json:** Parsing y manipulaci\u00f3n de datos JSON\n",
    "- **BeautifulSoup:** Parser HTML/XML para web scraping\n",
    "- **pandas:** Estructurar datos extra\u00eddos en DataFrames\n",
    "\n",
    "**Uso en ingenier\u00eda de datos:**\n",
    "- Ingesta desde APIs externas (redes sociales, servicios cloud)\n",
    "- Extracci\u00f3n de datos de sitios web\n",
    "- Integraci\u00f3n con fuentes de datos sin APIs formales\n",
    "\n",
    "**Instalaci\u00f3n:** `pip install requests beautifulsoup4 lxml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c574e79",
   "metadata": {},
   "source": [
    "## 3. Consumir APIs REST P\u00fablicas\n",
    "\n",
    "### 3.1 Ejemplo B\u00e1sico: JSONPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02553358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API p\u00fablica para testing\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Realizar petici\u00f3n GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar c\u00f3digo de estado\n",
    "print(f\"\ud83d\udce1 Status Code: {response.status_code}\")\n",
    "print(f\"\u2705 Exitoso: {response.ok}\")\n",
    "\n",
    "# Obtener datos JSON\n",
    "if response.ok:\n",
    "    posts = response.json()\n",
    "    print(f\"\\n\ud83d\udcca Total de posts: {len(posts)}\")\n",
    "    print(f\"\\n\ud83d\udd0d Primer post:\")\n",
    "    print(json.dumps(posts[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6649e",
   "metadata": {},
   "source": [
    "### \ud83c\udf10 Petici\u00f3n GET B\u00e1sica con Requests\n",
    "\n",
    "**Concepto:** `requests.get()` env\u00eda solicitud HTTP GET para obtener datos de una API.\n",
    "\n",
    "**Flujo:**\n",
    "1. **Enviar petici\u00f3n:** `response = requests.get(url)`\n",
    "2. **Verificar estado:** `response.status_code` (200 = \u00e9xito)\n",
    "3. **Parsear JSON:** `response.json()` convierte a diccionario Python\n",
    "4. **Validar:** `response.ok` = True si status 200-299\n",
    "\n",
    "**Propiedades \u00fatiles:**\n",
    "- `response.text`: contenido como string\n",
    "- `response.json()`: parsea JSON autom\u00e1ticamente\n",
    "- `response.headers`: headers de respuesta\n",
    "\n",
    "**Buena pr\u00e1ctica:** Siempre verificar `response.ok` antes de procesar datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088decaa",
   "metadata": {},
   "source": [
    "### 3.2 Convertir Respuesta a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8432a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame\n",
    "df_posts = pd.DataFrame(posts)\n",
    "\n",
    "print(f\"\ud83d\udcca Shape: {df_posts.shape}\")\n",
    "print(f\"\\n\ud83d\udd0d Vista previa:\")\n",
    "display(df_posts.head())\n",
    "\n",
    "# Informaci\u00f3n del DataFrame\n",
    "print(\"\\n\ud83d\udccb Info:\")\n",
    "df_posts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dbc66",
   "metadata": {},
   "source": [
    "### \ud83d\udcca JSON a DataFrame: Estructurando Datos de API\n",
    "\n",
    "**Concepto:** `pd.DataFrame()` convierte lista de diccionarios JSON en tabla estructurada.\n",
    "\n",
    "**Ventajas:**\n",
    "- An\u00e1lisis con pandas (filtros, agregaciones)\n",
    "- Exportaci\u00f3n a CSV, SQL, Parquet\n",
    "- Integraci\u00f3n con pipelines ETL\n",
    "\n",
    "**Conversi\u00f3n autom\u00e1tica:**\n",
    "- Claves JSON \u2192 columnas DataFrame\n",
    "- Lista de objetos \u2192 filas\n",
    "- Tipos inferidos autom\u00e1ticamente\n",
    "\n",
    "**Uso t\u00edpico:** Primera transformaci\u00f3n tras ingestar datos desde API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66773074",
   "metadata": {},
   "source": [
    "### 3.3 Par\u00e1metros de Consulta (Query Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por userId\n",
    "params = {'userId': 1}\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.ok:\n",
    "    posts_user1 = response.json()\n",
    "    print(f\"\ud83d\udcca Posts del usuario 1: {len(posts_user1)}\")\n",
    "    print(f\"\\n\ud83d\udd17 URL completa: {response.url}\")\n",
    "    \n",
    "    # Ver primeros 3\n",
    "    for post in posts_user1[:3]:\n",
    "        print(f\"\\n\ud83d\udcdd Post {post['id']}: {post['title'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90988da",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d Query Parameters: Filtrado en APIs\n",
    "\n",
    "**Concepto:** Los query parameters permiten filtrar, paginar y personalizar respuestas de APIs.\n",
    "\n",
    "**Sintaxis:**\n",
    "```python\n",
    "params = {'userId': 1, 'limit': 10}\n",
    "requests.get(url, params=params)\n",
    "```\n",
    "\n",
    "**URL generada:** `https://api.com/posts?userId=1&limit=10`\n",
    "\n",
    "**Casos comunes:**\n",
    "- Filtros: `?category=tech&status=active`\n",
    "- Paginaci\u00f3n: `?page=2&per_page=50`\n",
    "- Ordenamiento: `?sort=date&order=desc`\n",
    "\n",
    "**Ventaja:** Reduce transferencia de datos, obtiene solo lo necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff10d9",
   "metadata": {},
   "source": [
    "### 3.4 Headers Personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Data Pipeline v1.0)',\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(f\"\u2705 Status: {response.status_code}\")\n",
    "print(f\"\\n\ud83d\udcc4 Headers de la respuesta:\")\n",
    "for key, value in list(response.headers.items())[:5]:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9baaaa",
   "metadata": {},
   "source": [
    "### \ud83d\udcc4 Headers HTTP: Metadata de Peticiones\n",
    "\n",
    "**Concepto:** Headers env\u00edan informaci\u00f3n adicional sobre la petici\u00f3n (autenticaci\u00f3n, formato, identidad).\n",
    "\n",
    "**Headers importantes:**\n",
    "- **User-Agent:** Identifica tu aplicaci\u00f3n/script\n",
    "- **Accept:** Formato de respuesta deseado (application/json)\n",
    "- **Authorization:** Token de autenticaci\u00f3n (Bearer, API Key)\n",
    "- **Content-Type:** Tipo de datos enviados (POST/PUT)\n",
    "\n",
    "**Ejemplo autenticaci\u00f3n:**\n",
    "```python\n",
    "headers = {'Authorization': 'Bearer YOUR_TOKEN'}\n",
    "```\n",
    "\n",
    "**Importancia:** Muchas APIs requieren User-Agent v\u00e1lido o bloquean peticiones sin headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c93be",
   "metadata": {},
   "source": [
    "## 4. Trabajar con APIs Reales\n",
    "\n",
    "### 4.1 GitHub API - Datos P\u00fablicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener informaci\u00f3n de un repositorio p\u00fablico\n",
    "repo_url = \"https://api.github.com/repos/pandas-dev/pandas\"\n",
    "\n",
    "response = requests.get(repo_url)\n",
    "\n",
    "if response.ok:\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    print(\"\ud83d\udce6 INFORMACI\u00d3N DEL REPOSITORIO PANDAS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Nombre: {repo_data['name']}\")\n",
    "    print(f\"Descripci\u00f3n: {repo_data['description'][:80]}...\")\n",
    "    print(f\"\u2b50 Stars: {repo_data['stargazers_count']:,}\")\n",
    "    print(f\"\ud83c\udf74 Forks: {repo_data['forks_count']:,}\")\n",
    "    print(f\"\ud83d\udc41\ufe0f Watchers: {repo_data['watchers_count']:,}\")\n",
    "    print(f\"\ud83d\udc1b Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "    print(f\"\ud83d\udcc5 Creado: {repo_data['created_at']}\")\n",
    "    print(f\"\ud83d\udd04 \u00daltima actualizaci\u00f3n: {repo_data['updated_at']}\")\n",
    "    print(f\"\ud83d\udcdd Lenguaje principal: {repo_data['language']}\")\n",
    "else:\n",
    "    print(f\"\u274c Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6b58c",
   "metadata": {},
   "source": [
    "### 4.2 Paginaci\u00f3n en APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4642a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_todos_los_commits(owner: str, repo: str, max_pages: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Obtiene commits de un repositorio con paginaci\u00f3n.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    all_commits = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'page': page,\n",
    "            'per_page': 10  # Registros por p\u00e1gina\n",
    "        }\n",
    "        \n",
    "        print(f\"\ud83d\udcc4 Obteniendo p\u00e1gina {page}...\")\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.ok:\n",
    "            commits = response.json()\n",
    "            if not commits:  # No hay m\u00e1s p\u00e1ginas\n",
    "                break\n",
    "            all_commits.extend(commits)\n",
    "        else:\n",
    "            print(f\"\u274c Error en p\u00e1gina {page}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Respetar rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_commits\n",
    "\n",
    "# Ejemplo: Obtener commits de un repo peque\u00f1o\n",
    "commits = obtener_todos_los_commits('octocat', 'Hello-World', max_pages=2)\n",
    "\n",
    "print(f\"\\n\u2705 Total commits obtenidos: {len(commits)}\")\n",
    "\n",
    "# Analizar commits\n",
    "if commits:\n",
    "    df_commits = pd.DataFrame([\n",
    "        {\n",
    "            'sha': c['sha'][:7],\n",
    "            'author': c['commit']['author']['name'],\n",
    "            'date': c['commit']['author']['date'],\n",
    "            'message': c['commit']['message'][:50]\n",
    "        }\n",
    "        for c in commits\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca \u00daltimos commits:\")\n",
    "    display(df_commits.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d143287",
   "metadata": {},
   "source": [
    "### \ud83d\udcc4 Paginaci\u00f3n: Obtener Grandes Vol\u00famenes de Datos\n",
    "\n",
    "**Concepto:** Las APIs limitan resultados por petici\u00f3n, requiriendo m\u00faltiples requests para datasets completos.\n",
    "\n",
    "**Estrategias comunes:**\n",
    "1. **Offset/Limit:** `?offset=100&limit=50`\n",
    "2. **Page-based:** `?page=3&per_page=100`\n",
    "3. **Cursor-based:** `?cursor=next_token` (m\u00e1s eficiente)\n",
    "\n",
    "**Implementaci\u00f3n:**\n",
    "- Loop hasta que respuesta est\u00e9 vac\u00eda o alcance m\u00e1ximo\n",
    "- `time.sleep()` entre requests para respetar rate limits\n",
    "- Acumular resultados en lista\n",
    "\n",
    "**Buena pr\u00e1ctica:** Implementar l\u00edmite m\u00e1ximo de p\u00e1ginas para evitar loops infinitos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb6df0",
   "metadata": {},
   "source": [
    "### 4.3 Rate Limiting y Manejo de Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request_con_retry(url: str, max_retries: int = 3, delay: float = 2.0) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Realiza petici\u00f3n con reintentos autom\u00e1ticos.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.ok:\n",
    "                return response.json()\n",
    "            \n",
    "            # Rate limit excedido\n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get('Retry-After', delay))\n",
    "                print(f\"\u23f3 Rate limit alcanzado. Esperando {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            # Otros errores\n",
    "            if response.status_code >= 500:\n",
    "                print(f\"\u26a0\ufe0f Error del servidor ({response.status_code}). Reintento {attempt}/{max_retries}\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            \n",
    "            # Error definitivo\n",
    "            print(f\"\u274c Error {response.status_code}: {response.text[:100]}\")\n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\u23f1\ufe0f Timeout. Reintento {attempt}/{max_retries}\")\n",
    "            time.sleep(delay)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"\ud83d\udd0c Error de conexi\u00f3n. Reintento {attempt}/{max_retries}\")\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error inesperado: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"\u274c M\u00e1ximo de reintentos alcanzado ({max_retries})\")\n",
    "    return None\n",
    "\n",
    "# Probar funci\u00f3n\n",
    "data = api_request_con_retry(\"https://api.github.com/users/octocat\")\n",
    "\n",
    "if data:\n",
    "    print(f\"\\n\u2705 Usuario obtenido: {data['login']}\")\n",
    "    print(f\"\ud83d\udc64 Nombre: {data['name']}\")\n",
    "    print(f\"\ud83d\udccd Ubicaci\u00f3n: {data.get('location', 'N/A')}\")\n",
    "    print(f\"\ud83d\udce6 Repositorios p\u00fablicos: {data['public_repos']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8886e1",
   "metadata": {},
   "source": [
    "### \u26a1 Rate Limiting y Reintentos Autom\u00e1ticos\n",
    "\n",
    "**Concepto:** APIs limitan n\u00famero de peticiones por tiempo (ej: 100/hora) para prevenir abuso.\n",
    "\n",
    "**Manejo de rate limits:**\n",
    "- **Status 429:** Too Many Requests\n",
    "- **Header `Retry-After`:** segundos a esperar\n",
    "- **Backoff exponencial:** incrementar delay tras cada fallo\n",
    "\n",
    "**Errores comunes:**\n",
    "- **4xx:** Errores del cliente (request inv\u00e1lido) \u2192 no reintentar\n",
    "- **5xx:** Errores del servidor \u2192 reintentar con delay\n",
    "- **Timeout/Connection:** problemas de red \u2192 reintentar\n",
    "\n",
    "**Estrategia robusta:** 3 reintentos con delays incrementales + captura de excepciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4ac75",
   "metadata": {},
   "source": [
    "## 5. Autenticaci\u00f3n en APIs\n",
    "\n",
    "### 5.1 API Key en Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79300002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo gen\u00e9rico (NO ejecutar sin API key real)\n",
    "def api_con_key(api_key: str, endpoint: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Ejemplo de autenticaci\u00f3n con API Key.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if response.ok:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"\u274c Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "print(\"\"\"\n",
    "\ud83d\udca1 MEJORES PR\u00c1CTICAS PARA API KEYS:\n",
    "\n",
    "1. \u274c NUNCA hardcodear API keys en el c\u00f3digo\n",
    "2. \u2705 Usar variables de entorno:\n",
    "   import os\n",
    "   api_key = os.getenv('MI_API_KEY')\n",
    "\n",
    "3. \u2705 Archivo de configuraci\u00f3n (no versionado):\n",
    "   import yaml\n",
    "   with open('config/credentials.yaml') as f:\n",
    "       creds = yaml.safe_load(f)\n",
    "   api_key = creds['api_key']\n",
    "\n",
    "4. \u2705 Agregar credentials.yaml a .gitignore\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529f991",
   "metadata": {},
   "source": [
    "### 5.2 OAuth 2.0 (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f116fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\ud83d\udd10 FLUJO OAUTH 2.0:\n",
    "\n",
    "1. Usuario autoriza aplicaci\u00f3n\n",
    "2. API devuelve c\u00f3digo de autorizaci\u00f3n\n",
    "3. Aplicaci\u00f3n intercambia c\u00f3digo por access token\n",
    "4. Usar access token en peticiones\n",
    "\n",
    "Ejemplo con Google API:\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "creds = Credentials.from_authorized_user_file('token.json')\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "\ud83d\udcda Librer\u00edas \u00fatiles:\n",
    "  \u2022 requests-oauthlib\n",
    "  \u2022 authlib\n",
    "  \u2022 httpx (async)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a352da",
   "metadata": {},
   "source": [
    "## 6. Web Scraping con BeautifulSoup\n",
    "\n",
    "### 6.1 Conceptos B\u00e1sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7264eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con HTML simple\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Productos</title></head>\n",
    "<body>\n",
    "    <h1>Cat\u00e1logo de Productos</h1>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Laptop</h2>\n",
    "        <p class=\"precio\">$1200</p>\n",
    "        <p class=\"stock\">En stock</p>\n",
    "    </div>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Mouse</h2>\n",
    "        <p class=\"precio\">$25</p>\n",
    "        <p class=\"stock\">Agotado</p>\n",
    "    </div>\n",
    "    <div class=\"producto\">\n",
    "        <h2>Teclado</h2>\n",
    "        <p class=\"precio\">$75</p>\n",
    "        <p class=\"stock\">En stock</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parsear HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraer t\u00edtulo\n",
    "titulo = soup.title.string\n",
    "print(f\"\ud83d\udcc4 T\u00edtulo: {titulo}\")\n",
    "\n",
    "# Extraer encabezado\n",
    "h1 = soup.find('h1').text\n",
    "print(f\"\ud83d\udccc Encabezado: {h1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c2432",
   "metadata": {},
   "source": [
    "### \ud83c\udf72 BeautifulSoup: Parser HTML para Scraping\n",
    "\n",
    "**Concepto:** BeautifulSoup convierte HTML en \u00e1rbol navegable de objetos Python.\n",
    "\n",
    "**Parsers disponibles:**\n",
    "- `html.parser`: incluido en Python (est\u00e1ndar)\n",
    "- `lxml`: m\u00e1s r\u00e1pido, requiere instalaci\u00f3n\n",
    "- `html5lib`: m\u00e1s tolerante con HTML mal formado\n",
    "\n",
    "**Operaciones b\u00e1sicas:**\n",
    "- `soup.find('tag')`: primer elemento que coincide\n",
    "- `soup.find_all('tag')`: todos los elementos\n",
    "- `soup.title.string`: acceso directo a elementos \u00fanicos\n",
    "- `.text`: extraer texto sin tags HTML\n",
    "\n",
    "**Uso:** Extraer datos estructurados de p\u00e1ginas web sin API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915e19b",
   "metadata": {},
   "source": [
    "### 6.2 Buscar Elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todos los productos\n",
    "productos = soup.find_all('div', class_='producto')\n",
    "\n",
    "print(f\"\ud83d\udce6 Total productos encontrados: {len(productos)}\\n\")\n",
    "\n",
    "# Extraer informaci\u00f3n de cada producto\n",
    "datos_productos = []\n",
    "\n",
    "for producto in productos:\n",
    "    nombre = producto.find('h2').text\n",
    "    precio = producto.find('p', class_='precio').text\n",
    "    stock = producto.find('p', class_='stock').text\n",
    "    \n",
    "    datos_productos.append({\n",
    "        'nombre': nombre,\n",
    "        'precio': precio,\n",
    "        'stock': stock\n",
    "    })\n",
    "    \n",
    "    print(f\"\ud83d\uded2 {nombre}\")\n",
    "    print(f\"   \ud83d\udcb0 Precio: {precio}\")\n",
    "    print(f\"   \ud83d\udce6 Stock: {stock}\\n\")\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_productos = pd.DataFrame(datos_productos)\n",
    "display(df_productos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c030e77",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d Selectores CSS y B\u00fasqueda de Elementos\n",
    "\n",
    "**Concepto:** Localizar elementos espec\u00edficos en HTML usando clases, IDs, tags y atributos.\n",
    "\n",
    "**M\u00e9todos principales:**\n",
    "- `find('tag')`: primer elemento\n",
    "- `find_all('tag')`: lista de todos los elementos\n",
    "- `find('tag', class_='nombre')`: buscar por clase CSS\n",
    "- `find('tag', id='nombre')`: buscar por ID\n",
    "- `find('tag', attrs={'data-id': '123'})`: atributos personalizados\n",
    "\n",
    "**Selectores CSS avanzados:**\n",
    "```python\n",
    "soup.select('.producto')  # por clase\n",
    "soup.select('#main')      # por ID\n",
    "soup.select('div > p')    # hijos directos\n",
    "```\n",
    "\n",
    "**Extracci\u00f3n:** `.text` para contenido, `.get('href')` para atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac557f",
   "metadata": {},
   "source": [
    "### 6.3 Selectores CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e028234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar selectores CSS (m\u00e1s potente)\n",
    "precios = soup.select('.producto .precio')\n",
    "\n",
    "print(\"\ud83d\udcb0 Precios encontrados con selector CSS:\")\n",
    "for precio in precios:\n",
    "    print(f\"  {precio.text}\")\n",
    "\n",
    "# Productos en stock\n",
    "en_stock = [p for p in soup.select('.producto') \n",
    "            if 'En stock' in p.select_one('.stock').text]\n",
    "\n",
    "print(f\"\\n\u2705 Productos en stock: {len(en_stock)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7c849",
   "metadata": {},
   "source": [
    "### 6.4 Scraping de Sitio Web Real (Ejemplo \u00c9tico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e97cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_quotes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape de http://quotes.toscrape.com (sitio de prueba).\n",
    "    \"\"\"\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    \n",
    "    # Headers para simular navegador\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if not response.ok:\n",
    "        print(f\"\u274c Error: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Encontrar todas las quotes\n",
    "    quotes_elements = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    quotes_data = []\n",
    "    \n",
    "    for quote_el in quotes_elements:\n",
    "        text = quote_el.find('span', class_='text').text\n",
    "        author = quote_el.find('small', class_='author').text\n",
    "        tags = [tag.text for tag in quote_el.find_all('a', class_='tag')]\n",
    "        \n",
    "        quotes_data.append({\n",
    "            'quote': text,\n",
    "            'author': author,\n",
    "            'tags': ', '.join(tags)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(quotes_data)\n",
    "\n",
    "# Ejecutar scraping\n",
    "df_quotes = scrape_quotes()\n",
    "\n",
    "if not df_quotes.empty:\n",
    "    print(f\"\u2705 Quotes extra\u00eddas: {len(df_quotes)}\\n\")\n",
    "    display(df_quotes.head())\n",
    "    \n",
    "    # An\u00e1lisis\n",
    "    print(\"\\n\ud83d\udcca Autores m\u00e1s citados:\")\n",
    "    print(df_quotes['author'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191593e6",
   "metadata": {},
   "source": [
    "### \ud83c\udf10 Scraping de Sitios Reales: Flujo Completo\n",
    "\n",
    "**Concepto:** Extraer datos de sitios web p\u00fablicos siguiendo mejores pr\u00e1cticas \u00e9ticas y t\u00e9cnicas.\n",
    "\n",
    "**Pasos esenciales:**\n",
    "1. **Revisar robots.txt:** `sitio.com/robots.txt` - qu\u00e9 est\u00e1 permitido\n",
    "2. **User-Agent v\u00e1lido:** identificarse como navegador\n",
    "3. **Parsear HTML:** BeautifulSoup convierte respuesta en objeto navegable\n",
    "4. **Extraer datos:** usar selectores para localizar informaci\u00f3n\n",
    "5. **Estructurar:** convertir a DataFrame para an\u00e1lisis\n",
    "\n",
    "**Sitios de pr\u00e1ctica:**\n",
    "- quotes.toscrape.com (dise\u00f1ado para aprender)\n",
    "- books.toscrape.com (cat\u00e1logo de libros)\n",
    "\n",
    "**\u00c9tica:** Respetar t\u00e9rminos de servicio, no sobrecargar servidores, usar datos responsablemente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c827948",
   "metadata": {},
   "source": [
    "### 6.5 Extraer Tablas HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe19895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML con tabla\n",
    "html_tabla = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Pa\u00eds</th>\n",
    "        <th>Capital</th>\n",
    "        <th>Poblaci\u00f3n (M)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>M\u00e9xico</td>\n",
    "        <td>Ciudad de M\u00e9xico</td>\n",
    "        <td>126</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Colombia</td>\n",
    "        <td>Bogot\u00e1</td>\n",
    "        <td>51</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Argentina</td>\n",
    "        <td>Buenos Aires</td>\n",
    "        <td>45</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup_tabla = BeautifulSoup(html_tabla, 'html.parser')\n",
    "tabla = soup_tabla.find('table')\n",
    "\n",
    "# Extraer headers\n",
    "headers = [th.text for th in tabla.find_all('th')]\n",
    "\n",
    "# Extraer filas\n",
    "filas = []\n",
    "for tr in tabla.find_all('tr')[1:]:  # Saltar header\n",
    "    celdas = [td.text for td in tr.find_all('td')]\n",
    "    filas.append(celdas)\n",
    "\n",
    "# Crear DataFrame\n",
    "df_paises = pd.DataFrame(filas, columns=headers)\n",
    "df_paises['Poblaci\u00f3n (M)'] = df_paises['Poblaci\u00f3n (M)'].astype(int)\n",
    "\n",
    "print(\"\ud83c\udf0d Tabla extra\u00edda:\")\n",
    "display(df_paises)\n",
    "\n",
    "# Alternativa con pandas (m\u00e1s simple)\n",
    "print(\"\\n\ud83d\udca1 Con pandas.read_html():\")\n",
    "df_paises_pandas = pd.read_html(html_tabla)[0]\n",
    "display(df_paises_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c939fa",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Extracci\u00f3n de Tablas HTML\n",
    "\n",
    "**Concepto:** Las tablas HTML (`<table>`) son estructuras ideales para convertir directamente a DataFrames.\n",
    "\n",
    "**Estrategia:**\n",
    "1. Localizar `<table>` con `soup.find('table')`\n",
    "2. Extraer headers de `<th>` en primera fila\n",
    "3. Iterar sobre `<tr>` (filas) extrayendo `<td>` (celdas)\n",
    "4. Crear DataFrame con headers y filas\n",
    "\n",
    "**Alternativa r\u00e1pida:**\n",
    "```python\n",
    "pd.read_html(url)  # Pandas detecta tablas autom\u00e1ticamente\n",
    "```\n",
    "\n",
    "**Uso com\u00fan:** Datos estad\u00edsticos, rankings, resultados deportivos, precios hist\u00f3ricos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72859f5a",
   "metadata": {},
   "source": [
    "## 7. Pipeline de Ingesta desde API\n",
    "\n",
    "### 7.1 Clase APIExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4931d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIExtractor:\n",
    "    \"\"\"\n",
    "    Extractor gen\u00e9rico para APIs REST con manejo de errores y paginaci\u00f3n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, headers: Optional[Dict] = None, rate_limit: float = 1.0):\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers or {}\n",
    "        self.rate_limit = rate_limit\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "    \n",
    "    def get(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Dict]:\n",
    "        \"\"\"Realiza petici\u00f3n GET con manejo de errores.\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            time.sleep(self.rate_limit)  # Rate limiting\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"\u274c HTTP Error: {e}\")\n",
    "            return None\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\u23f1\ufe0f Timeout al consultar {url}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error inesperado: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_paginated(self, endpoint: str, page_param: str = 'page', \n",
    "                     max_pages: int = 10) -> List[Dict]:\n",
    "        \"\"\"Obtiene datos paginados.\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"\ud83d\udcc4 P\u00e1gina {page}/{max_pages}\")\n",
    "            params = {page_param: page}\n",
    "            data = self.get(endpoint, params)\n",
    "            \n",
    "            if not data:\n",
    "                break\n",
    "            \n",
    "            # Asumiendo que data es una lista\n",
    "            if isinstance(data, list):\n",
    "                if not data:  # Lista vac\u00eda\n",
    "                    break\n",
    "                all_data.extend(data)\n",
    "            else:\n",
    "                # Si es dict, agregar directamente\n",
    "                all_data.append(data)\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def to_dataframe(self, data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Convierte datos a DataFrame.\"\"\"\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "print(\"\u2705 Clase APIExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dd22b",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f Pipeline de Ingesta: Clase APIExtractor\n",
    "\n",
    "**Concepto:** Encapsular l\u00f3gica de extracci\u00f3n desde APIs en clase reutilizable.\n",
    "\n",
    "**Componentes clave:**\n",
    "- **Session:** `requests.Session()` reutiliza conexiones (m\u00e1s eficiente)\n",
    "- **Rate limiting:** `time.sleep()` entre peticiones\n",
    "- **Manejo de errores:** try/except para HTTPError, Timeout, etc.\n",
    "- **Paginaci\u00f3n autom\u00e1tica:** itera hasta encontrar datos vac\u00edos\n",
    "- **Conversi\u00f3n:** m\u00e9todo `.to_dataframe()` para an\u00e1lisis\n",
    "\n",
    "**Ventajas:**\n",
    "- Reutilizable para m\u00faltiples APIs\n",
    "- C\u00f3digo m\u00e1s limpio y mantenible\n",
    "- Configuraci\u00f3n centralizada (headers, rate limit)\n",
    "\n",
    "**Uso:** Base para pipelines ETL de ingesta desde APIs externas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922441e7",
   "metadata": {},
   "source": [
    "### 7.2 Usar APIExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cba0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor para JSONPlaceholder\n",
    "extractor = APIExtractor(\n",
    "    base_url=\"https://jsonplaceholder.typicode.com\",\n",
    "    rate_limit=0.5  # 0.5 segundos entre requests\n",
    ")\n",
    "\n",
    "# Extraer usuarios\n",
    "print(\"\ud83d\udd0d Extrayendo usuarios...\\n\")\n",
    "usuarios = extractor.get('users')\n",
    "\n",
    "if usuarios:\n",
    "    df_usuarios = extractor.to_dataframe(usuarios)\n",
    "    \n",
    "    # Seleccionar columnas relevantes\n",
    "    df_usuarios_clean = df_usuarios[['id', 'name', 'email', 'phone', 'website']].copy()\n",
    "    \n",
    "    print(f\"\u2705 Usuarios extra\u00eddos: {len(df_usuarios_clean)}\")\n",
    "    display(df_usuarios_clean.head())\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = '../../outputs/usuarios_api.csv'\n",
    "    df_usuarios_clean.to_csv(output_path, index=False)\n",
    "    print(f\"\\n\ud83d\udcbe Datos guardados en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1822f",
   "metadata": {},
   "source": [
    "## 8. Mejores Pr\u00e1cticas y \u00c9tica\n",
    "\n",
    "### 8.1 Consideraciones Legales y \u00c9ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\u2696\ufe0f MEJORES PR\u00c1CTICAS EN WEB SCRAPING:\n",
    "\n",
    "1. \ud83d\udcdc VERIFICAR LEGALIDAD\n",
    "   \u2022 Revisar T\u00e9rminos de Servicio\n",
    "   \u2022 Consultar robots.txt\n",
    "   \u2022 Respetar copyright\n",
    "\n",
    "2. \ud83e\udd1d SER RESPETUOSO\n",
    "   \u2022 Usar rate limiting (delays)\n",
    "   \u2022 No sobrecargar servidores\n",
    "   \u2022 Scraping en horarios de bajo tr\u00e1fico\n",
    "\n",
    "3. \ud83d\udd12 PRIVACIDAD\n",
    "   \u2022 No extraer datos personales sensibles\n",
    "   \u2022 Cumplir GDPR/CCPA si aplica\n",
    "   \u2022 Anonimizar datos cuando sea posible\n",
    "\n",
    "4. \ud83c\udfaf ALTERNATIVAS PREFERIBLES\n",
    "   \u2022 Usar API oficial si existe\n",
    "   \u2022 Contactar al propietario del sitio\n",
    "   \u2022 Considerar datasets p\u00fablicos\n",
    "\n",
    "5. \ud83d\udee1\ufe0f IDENTIFICARSE\n",
    "   \u2022 User-Agent descriptivo\n",
    "   \u2022 Email de contacto\n",
    "   \u2022 Documentar prop\u00f3sito\n",
    "\n",
    "6. \ud83d\udcbe CACH\u00c9 LOCAL\n",
    "   \u2022 No repetir requests innecesarios\n",
    "   \u2022 Almacenar resultados localmente\n",
    "   \u2022 Actualizar solo cuando sea necesario\n",
    "\n",
    "\u274c NUNCA:\n",
    "  \u2022 Ignorar robots.txt\n",
    "  \u2022 Hacer requests masivos sin delays\n",
    "  \u2022 Usar datos para spam o fraude\n",
    "  \u2022 Redistribuir contenido con copyright\n",
    "  \u2022 Evadir CAPTCHAs o medidas de seguridad\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e4e44",
   "metadata": {},
   "source": [
    "### 8.2 Verificar robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac573d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_robots_txt(url: str) -> None:\n",
    "    \"\"\"\n",
    "    Verifica el archivo robots.txt de un sitio.\n",
    "    \"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.ok:\n",
    "            print(f\"\ud83e\udd16 robots.txt de {parsed.netloc}:\\n\")\n",
    "            print(response.text[:500])  # Primeros 500 caracteres\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f No se encontr\u00f3 robots.txt (Status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error al verificar robots.txt: {str(e)}\")\n",
    "\n",
    "# Ejemplo\n",
    "verificar_robots_txt(\"https://github.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f9997",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Ejercicios Pr\u00e1cticos\n",
    "\n",
    "### Ejercicio 1: API de GitHub\n",
    "Crea un script que:\n",
    "1. Obtenga los \u00faltimos 50 repositorios de un usuario de GitHub\n",
    "2. Extraiga: nombre, descripci\u00f3n, estrellas, lenguaje, fecha de creaci\u00f3n\n",
    "3. Convierta a DataFrame y guarde en CSV\n",
    "4. Genere estad\u00edsticas (lenguaje m\u00e1s usado, repo con m\u00e1s estrellas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C\u00d3DIGO AQU\u00cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1665c4",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Web Scraping\n",
    "Scrape http://books.toscrape.com:\n",
    "1. Extrae t\u00edtulo, precio, disponibilidad y rating\n",
    "2. Navega m\u00faltiples p\u00e1ginas (al menos 3)\n",
    "3. Limpia los datos (quitar s\u00edmbolos de precio, convertir ratings)\n",
    "4. Crea visualizaci\u00f3n de precio promedio por rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C\u00d3DIGO AQU\u00cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8d119",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Pipeline Completo\n",
    "Implementa un pipeline que:\n",
    "1. Extraiga datos de una API p\u00fablica (elige una de [public-apis.io](https://public-apis.io))\n",
    "2. Implemente manejo de errores robusto\n",
    "3. Aplique transformaciones necesarias\n",
    "4. Guarde resultados en formato Parquet\n",
    "5. Genere un reporte con estad\u00edsticas b\u00e1sicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb252b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU C\u00d3DIGO AQU\u00cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d353414",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Recursos Adicionales\n",
    "\n",
    "### APIs P\u00fablicas para Practicar\n",
    "- [JSONPlaceholder](https://jsonplaceholder.typicode.com/) - API fake para testing\n",
    "- [GitHub API](https://docs.github.com/en/rest) - Datos de repositorios\n",
    "- [OpenWeatherMap](https://openweathermap.org/api) - Datos meteorol\u00f3gicos\n",
    "- [NewsAPI](https://newsapi.org/) - Noticias globales\n",
    "- [Public APIs](https://github.com/public-apis/public-apis) - Lista curada\n",
    "\n",
    "### Sitios para Practicar Scraping (Legalmente)\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/)\n",
    "- [Books to Scrape](http://books.toscrape.com/)\n",
    "- [Scrape This Site](https://www.scrapethissite.com/)\n",
    "\n",
    "### Documentaci\u00f3n\n",
    "- [Requests Documentation](https://requests.readthedocs.io/)\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Scrapy](https://scrapy.org/) - Framework avanzado de scraping\n",
    "\n",
    "### Alternativas Avanzadas\n",
    "- **Selenium**: Para sitios con JavaScript\n",
    "- **Playwright**: Automatizaci\u00f3n de navegadores\n",
    "- **httpx**: Cliente HTTP async\n",
    "- **aiohttp**: Requests as\u00edncronos\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Resumen\n",
    "\n",
    "En este notebook aprendiste:\n",
    "\n",
    "1. \u2705 **APIs REST**: M\u00e9todos HTTP, c\u00f3digos de estado, requests\n",
    "2. \u2705 **Biblioteca requests**: GET, POST, headers, par\u00e1metros\n",
    "3. \u2705 **Paginaci\u00f3n**: Obtener datos en m\u00faltiples p\u00e1ginas\n",
    "4. \u2705 **Rate limiting**: Respetar l\u00edmites de APIs\n",
    "5. \u2705 **Autenticaci\u00f3n**: API keys, OAuth\n",
    "6. \u2705 **BeautifulSoup**: Parsear HTML, selectores CSS\n",
    "7. \u2705 **Web scraping \u00e9tico**: Robots.txt, mejores pr\u00e1cticas\n",
    "8. \u2705 **Pipeline de ingesta**: Clase reutilizable para APIs\n",
    "\n",
    "**\ud83c\udfaf Pr\u00f3ximo paso**: Proyecto Integrador 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\udd04 Git y Control de Versiones para Ingenier\u00eda de Datos](07_git_control_versiones.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udfaf Proyecto Integrador 1: Pipeline ETL Completo \u2192](09_proyecto_integrador_1.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Junior:**\n",
    "- [\ud83d\udcca Junior - 01. Introducci\u00f3n a la Ingenier\u00eda de Datos](01_introduccion_ingenieria_datos.ipynb)\n",
    "- [\ud83d\udc0d Junior - 02. Python para Manipulaci\u00f3n de Datos](02_python_manipulacion_datos.ipynb)\n",
    "- [Pandas: Fundamentos para An\u00e1lisis de Datos](03_pandas_fundamentos.ipynb)\n",
    "- [SQL B\u00e1sico para Ingenier\u00eda de Datos](04_sql_basico.ipynb)\n",
    "- [Limpieza y Preparaci\u00f3n de Datos](05_limpieza_datos.ipynb)\n",
    "- [\ud83d\udcca Visualizaci\u00f3n de Datos en Ingenier\u00eda de Datos](06_visualizacion_datos.ipynb)\n",
    "- [\ud83d\udd04 Git y Control de Versiones para Ingenier\u00eda de Datos](07_git_control_versiones.ipynb)\n",
    "- [\ud83c\udf10 APIs REST y Web Scraping para Ingenier\u00eda de Datos](08_apis_web_scraping.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udfaf Proyecto Integrador 1: Pipeline ETL Completo](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
