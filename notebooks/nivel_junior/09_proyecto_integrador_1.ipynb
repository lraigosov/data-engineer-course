{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e10312",
   "metadata": {},
   "source": [
    "# ğŸ¯ Proyecto Integrador 1: Pipeline ETL Completo\n",
    "\n",
    "## DescripciÃ³n del Proyecto\n",
    "\n",
    "En este proyecto integrador, construirÃ¡s un **pipeline ETL completo** que integra todos los conceptos aprendidos:\n",
    "\n",
    "- âœ… ExtracciÃ³n de datos desde mÃºltiples fuentes (CSV, API, Base de Datos)\n",
    "- âœ… TransformaciÃ³n y limpieza de datos con Pandas\n",
    "- âœ… ValidaciÃ³n de calidad de datos\n",
    "- âœ… VisualizaciÃ³n de resultados\n",
    "- âœ… Almacenamiento en base de datos SQLite\n",
    "- âœ… GeneraciÃ³n de reportes\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Requisitos del Proyecto\n",
    "\n",
    "### Funcionalidades Requeridas:\n",
    "\n",
    "1. **ExtracciÃ³n** de datos desde:\n",
    "   - Archivos CSV locales\n",
    "   - API REST pÃºblica\n",
    "   - Base de datos SQLite\n",
    "\n",
    "2. **TransformaciÃ³n** que incluya:\n",
    "   - Limpieza de valores nulos\n",
    "   - NormalizaciÃ³n de formatos\n",
    "   - CÃ¡lculo de mÃ©tricas agregadas\n",
    "   - Enriquecimiento con datos externos\n",
    "\n",
    "3. **Carga** de datos transformados:\n",
    "   - Base de datos SQLite\n",
    "   - Archivos Parquet para anÃ¡lisis\n",
    "\n",
    "4. **VisualizaciÃ³n** de insights:\n",
    "   - Dashboard con matplotlib/seaborn\n",
    "   - MÃ©tricas clave (KPIs)\n",
    "\n",
    "5. **Logging** y manejo de errores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53950",
   "metadata": {},
   "source": [
    "## 1. ConfiguraciÃ³n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../../logs/proyecto_integrador_1.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs('../../outputs/proyecto_1', exist_ok=True)\n",
    "os.makedirs('../../logs', exist_ok=True)\n",
    "\n",
    "print(\"âœ… ConfiguraciÃ³n inicial completada\")\n",
    "logger.info(\"Proyecto Integrador 1 iniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a0836",
   "metadata": {},
   "source": [
    "## 2. ExtracciÃ³n de Datos\n",
    "\n",
    "### 2.1 Clase DataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de datos desde mÃºltiples fuentes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_csv(self, filepath: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde archivo CSV.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extrayendo datos de CSV: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            self.logger.info(f\"âœ… CSV extraÃ­do: {len(df)} registros\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"âŒ Archivo no encontrado: {filepath}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"âŒ Error al leer CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_api(self, url: str, params: Optional[Dict] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde API REST.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando API: {url}\")\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            self.logger.info(f\"âœ… API consultada: {len(df)} registros\")\n",
    "            return df\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"âŒ Error en peticiÃ³n API: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"âŒ Error procesando respuesta API: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sqlite(self, db_path: str, query: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde base de datos SQLite.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando SQLite: {db_path}\")\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            self.logger.info(f\"âœ… SQLite consultado: {len(df)} registros\")\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"âŒ Error en SQLite: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"âŒ Error general: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "print(\"âœ… Clase DataExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad43d63",
   "metadata": {},
   "source": [
    "### 2.2 Ejecutar Extracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# 1. Extraer ventas desde CSV\n",
    "df_ventas = extractor.extract_csv('../../datasets/raw/ventas.csv')\n",
    "\n",
    "# 2. Extraer productos desde CSV\n",
    "df_productos = extractor.extract_csv('../../datasets/raw/productos.csv')\n",
    "\n",
    "# 3. Extraer clientes desde CSV\n",
    "df_clientes = extractor.extract_csv('../../datasets/raw/clientes.csv')\n",
    "\n",
    "# 4. Extraer datos de usuarios desde API pÃºblica\n",
    "df_usuarios_api = extractor.extract_api('https://jsonplaceholder.typicode.com/users')\n",
    "\n",
    "# Verificar extracciones\n",
    "print(\"\\nğŸ“Š RESUMEN DE EXTRACCIÃ“N\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ventas: {len(df_ventas) if df_ventas is not None else 0} registros\")\n",
    "print(f\"Productos: {len(df_productos) if df_productos is not None else 0} registros\")\n",
    "print(f\"Clientes: {len(df_clientes) if df_clientes is not None else 0} registros\")\n",
    "print(f\"Usuarios API: {len(df_usuarios_api) if df_usuarios_api is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58e19a",
   "metadata": {},
   "source": [
    "## 3. TransformaciÃ³n de Datos\n",
    "\n",
    "### 3.1 Clase DataTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24908fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Transformador de datos con validaciÃ³n y limpieza.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def clean_nulls(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Limpia valores nulos.\n",
    "        \"\"\"\n",
    "        nulos_antes = df.isnull().sum().sum()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            df_clean = df.dropna()\n",
    "        elif strategy == 'fill_mean':\n",
    "            df_clean = df.copy()\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n",
    "        elif strategy == 'fill_mode':\n",
    "            df_clean = df.copy()\n",
    "            for col in df_clean.columns:\n",
    "                if df_clean[col].isnull().any():\n",
    "                    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df_clean = df.copy()\n",
    "        \n",
    "        nulos_despues = df_clean.isnull().sum().sum()\n",
    "        self.logger.info(f\"Nulos: {nulos_antes} â†’ {nulos_despues}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def normalize_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza columnas de fechas.\n",
    "        \"\"\"\n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df_norm.columns:\n",
    "                df_norm[col] = pd.to_datetime(df_norm[col], errors='coerce')\n",
    "                self.logger.info(f\"âœ… Fecha normalizada: {col}\")\n",
    "        \n",
    "        return df_norm\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Elimina duplicados.\n",
    "        \"\"\"\n",
    "        duplicados_antes = df.duplicated(subset=subset).sum()\n",
    "        df_clean = df.drop_duplicates(subset=subset)\n",
    "        duplicados_eliminados = duplicados_antes\n",
    "        \n",
    "        self.logger.info(f\"Duplicados eliminados: {duplicados_eliminados}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def enrich_data(self, df_base: pd.DataFrame, df_enrich: pd.DataFrame, \n",
    "                   on: str, how: str = 'left') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enriquece datos con merge.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_enriched = df_base.merge(df_enrich, on=on, how=how)\n",
    "            nuevas_cols = len(df_enriched.columns) - len(df_base.columns)\n",
    "            self.logger.info(f\"âœ… Datos enriquecidos: +{nuevas_cols} columnas\")\n",
    "            return df_enriched\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"âŒ Error en enriquecimiento: {str(e)}\")\n",
    "            return df_base\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula mÃ©tricas derivadas.\n",
    "        \"\"\"\n",
    "        df_metrics = df.copy()\n",
    "        \n",
    "        # Ejemplo: calcular total si hay cantidad y precio\n",
    "        if 'cantidad' in df_metrics.columns and 'precio_unitario' in df_metrics.columns:\n",
    "            if 'total' not in df_metrics.columns:\n",
    "                df_metrics['total_calculado'] = df_metrics['cantidad'] * df_metrics['precio_unitario']\n",
    "                self.logger.info(\"âœ… MÃ©trica 'total_calculado' generada\")\n",
    "        \n",
    "        return df_metrics\n",
    "\n",
    "print(\"âœ… Clase DataTransformer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50d740",
   "metadata": {},
   "source": [
    "### 3.2 Aplicar Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ef587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear transformador\n",
    "transformer = DataTransformer()\n",
    "\n",
    "print(\"ğŸ”„ INICIANDO TRANSFORMACIONES\\n\")\n",
    "\n",
    "# 1. Limpiar nulos en ventas\n",
    "df_ventas_clean = transformer.clean_nulls(df_ventas, strategy='drop')\n",
    "\n",
    "# 2. Normalizar fechas\n",
    "df_ventas_clean = transformer.normalize_dates(df_ventas_clean, ['fecha_venta'])\n",
    "df_clientes_clean = transformer.normalize_dates(df_clientes, ['fecha_registro'])\n",
    "\n",
    "# 3. Eliminar duplicados\n",
    "df_ventas_clean = transformer.remove_duplicates(df_ventas_clean, subset=['venta_id'])\n",
    "df_productos_clean = transformer.remove_duplicates(df_productos, subset=['producto'])\n",
    "\n",
    "# 4. Enriquecer ventas con productos\n",
    "df_ventas_enriched = transformer.enrich_data(\n",
    "    df_ventas_clean, \n",
    "    df_productos_clean, \n",
    "    on='producto_id'\n",
    ")\n",
    "\n",
    "# 5. Enriquecer con clientes\n",
    "df_ventas_final = transformer.enrich_data(\n",
    "    df_ventas_enriched,\n",
    "    df_clientes_clean[['cliente_id', 'segmento', 'ciudad', 'pais']],\n",
    "    on='cliente_id'\n",
    ")\n",
    "\n",
    "# 6. Calcular mÃ©tricas adicionales\n",
    "df_ventas_final['margen'] = (df_ventas_final['total'] - (df_ventas_final['precio'] * df_ventas_final['cantidad'])) / df_ventas_final['total'] * 100\n",
    "df_ventas_final['mes'] = df_ventas_final['fecha_venta'].dt.to_period('M').astype(str)\n",
    "df_ventas_final['trimestre'] = df_ventas_final['fecha_venta'].dt.to_period('Q').astype(str)\n",
    "\n",
    "print(\"\\nâœ… TRANSFORMACIONES COMPLETADAS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Registros finales: {len(df_ventas_final)}\")\n",
    "print(f\"Columnas totales: {len(df_ventas_final.columns)}\")\n",
    "print(f\"\\nPrimeras columnas:\")\n",
    "print(df_ventas_final.columns.tolist()[:10])\n",
    "\n",
    "display(df_ventas_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937942",
   "metadata": {},
   "source": [
    "## 4. ValidaciÃ³n de Calidad de Datos\n",
    "\n",
    "### 4.1 Clase DataQualityValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityValidator:\n",
    "    \"\"\"\n",
    "    Validador de calidad de datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.issues = []\n",
    "    \n",
    "    def validate_completeness(self, df: pd.DataFrame, threshold: float = 0.95) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los datos estÃ©n completos (sin nulos).\n",
    "        \"\"\"\n",
    "        total_values = df.size\n",
    "        non_null_values = df.count().sum()\n",
    "        completeness = non_null_values / total_values\n",
    "        \n",
    "        if completeness < threshold:\n",
    "            msg = f\"âŒ Completitud baja: {completeness:.2%} (umbral: {threshold:.2%})\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"âœ… Completitud OK: {completeness:.2%}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_uniqueness(self, df: pd.DataFrame, columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Valida unicidad en columnas clave.\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns).sum()\n",
    "        \n",
    "        if duplicates > 0:\n",
    "            msg = f\"âŒ Duplicados encontrados: {duplicates} en {columns}\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"âœ… Unicidad OK en {columns}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los valores estÃ©n en un rango vÃ¡lido.\n",
    "        \"\"\"\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            msg = f\"âŒ {len(out_of_range)} valores fuera de rango en '{column}' [{min_val}, {max_val}]\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"âœ… Rango OK en '{column}'\")\n",
    "            return True\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera reporte de calidad.\n",
    "        \"\"\"\n",
    "        report = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "        report += \"ğŸ“Š REPORTE DE CALIDAD DE DATOS\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\"\n",
    "        \n",
    "        if not self.issues:\n",
    "            report += \"\\nâœ… No se encontraron problemas de calidad\\n\"\n",
    "        else:\n",
    "            report += f\"\\nâš ï¸ Se encontraron {len(self.issues)} problemas:\\n\\n\"\n",
    "            for i, issue in enumerate(self.issues, 1):\n",
    "                report += f\"{i}. {issue}\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\" * 60\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"âœ… Clase DataQualityValidator definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c025fa",
   "metadata": {},
   "source": [
    "### 4.2 Ejecutar Validaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear validador\n",
    "validator = DataQualityValidator()\n",
    "\n",
    "print(\"ğŸ” VALIDANDO CALIDAD DE DATOS\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "validator.validate_completeness(df_ventas_final, threshold=0.90)\n",
    "\n",
    "# 2. Unicidad en ventas\n",
    "validator.validate_uniqueness(df_ventas_final, ['venta_id'])\n",
    "\n",
    "# 3. Rangos vÃ¡lidos\n",
    "validator.validate_ranges(df_ventas_final, 'cantidad', min_val=0, max_val=100)\n",
    "validator.validate_ranges(df_ventas_final, 'total', min_val=0, max_val=100000)\n",
    "\n",
    "# Generar reporte\n",
    "report = validator.generate_report()\n",
    "print(report)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_calidad.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "logger.info(\"Reporte de calidad generado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab51ae",
   "metadata": {},
   "source": [
    "## 5. AnÃ¡lisis y Agregaciones\n",
    "\n",
    "### 5.1 MÃ©tricas de Negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bef04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs principales\n",
    "kpis = {\n",
    "    'total_ventas': df_ventas_final['total'].sum(),\n",
    "    'num_transacciones': len(df_ventas_final),\n",
    "    'ticket_promedio': df_ventas_final['total'].mean(),\n",
    "    'ticket_mediano': df_ventas_final['total'].median(),\n",
    "    'clientes_unicos': df_ventas_final['cliente_id'].nunique(),\n",
    "    'productos_vendidos': df_ventas_final['producto'].nunique(),\n",
    "    'margen_promedio': df_ventas_final['margen'].mean()\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š KPIs PRINCIPALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ’° Total Ventas: ${kpis['total_ventas']:,.2f}\")\n",
    "print(f\"ğŸ›’ Transacciones: {kpis['num_transacciones']:,}\")\n",
    "print(f\"ğŸ“ˆ Ticket Promedio: ${kpis['ticket_promedio']:.2f}\")\n",
    "print(f\"ğŸ“Š Ticket Mediano: ${kpis['ticket_mediano']:.2f}\")\n",
    "print(f\"ğŸ‘¥ Clientes Ãšnicos: {kpis['clientes_unicos']}\")\n",
    "print(f\"ğŸ“¦ Productos Vendidos: {kpis['productos_vendidos']}\")\n",
    "print(f\"ğŸ’¹ Margen Promedio: {kpis['margen_promedio']:.2f}%\")\n",
    "\n",
    "# Guardar KPIs\n",
    "with open('../../outputs/proyecto_1/kpis.json', 'w') as f:\n",
    "    json.dump(kpis, f, indent=2)\n",
    "\n",
    "logger.info(\"KPIs calculados y guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e3bd",
   "metadata": {},
   "source": [
    "### 5.2 Agregaciones por Dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por categorÃ­a\n",
    "ventas_categoria = df_ventas_final.groupby('categoria').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count',\n",
    "    'margen': 'mean'\n",
    "}).round(2)\n",
    "ventas_categoria.columns = ['total_ventas', 'num_ventas', 'margen_promedio']\n",
    "ventas_categoria = ventas_categoria.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š VENTAS POR CATEGORÃA\")\n",
    "display(ventas_categoria)\n",
    "\n",
    "# Ventas por segmento de cliente\n",
    "ventas_segmento = df_ventas_final.groupby('segmento').agg({\n",
    "    'total': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "ventas_segmento.columns = ['total_ventas', 'ticket_promedio', 'num_transacciones']\n",
    "ventas_segmento = ventas_segmento.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ‘¥ VENTAS POR SEGMENTO DE CLIENTE\")\n",
    "display(ventas_segmento)\n",
    "\n",
    "# Ventas por mes\n",
    "ventas_mes = df_ventas_final.groupby('mes').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "ventas_mes.columns = ['total_ventas', 'num_ventas']\n",
    "\n",
    "print(\"\\nğŸ“… VENTAS POR MES\")\n",
    "display(ventas_mes)\n",
    "\n",
    "# Guardar agregaciones\n",
    "ventas_categoria.to_csv('../../outputs/proyecto_1/ventas_categoria.csv')\n",
    "ventas_segmento.to_csv('../../outputs/proyecto_1/ventas_segmento.csv')\n",
    "ventas_mes.to_csv('../../outputs/proyecto_1/ventas_mes.csv')\n",
    "\n",
    "logger.info(\"Agregaciones calculadas y guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a7c8",
   "metadata": {},
   "source": [
    "## 6. VisualizaciÃ³n de Resultados\n",
    "\n",
    "### 6.1 Dashboard de KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. KPIs principales (arriba)\n",
    "ax_kpi = fig.add_subplot(gs[0, :])\n",
    "ax_kpi.axis('off')\n",
    "\n",
    "kpi_texts = [\n",
    "    ('ğŸ’° Total Ventas', f\"${kpis['total_ventas']:,.0f}\"),\n",
    "    ('ğŸ›’ Transacciones', f\"{kpis['num_transacciones']:,}\"),\n",
    "    ('ğŸ“Š Ticket Prom.', f\"${kpis['ticket_promedio']:.2f}\"),\n",
    "    ('ğŸ‘¥ Clientes', f\"{kpis['clientes_unicos']}\"),\n",
    "    ('ğŸ’¹ Margen', f\"{kpis['margen_promedio']:.1f}%\")\n",
    "]\n",
    "\n",
    "for i, (titulo, valor) in enumerate(kpi_texts):\n",
    "    x_pos = 0.1 + i * 0.18\n",
    "    ax_kpi.text(x_pos, 0.7, titulo, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax_kpi.text(x_pos, 0.3, valor, ha='center', fontsize=16, fontweight='bold', color='#2E86AB')\n",
    "\n",
    "# 2. Ventas por categorÃ­a\n",
    "ax1 = fig.add_subplot(gs[1, :])\n",
    "ventas_categoria['total_ventas'].plot(kind='barh', ax=ax1, color='#6C5CE7')\n",
    "ax1.set_title('ğŸ’¼ Ventas por CategorÃ­a', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Total Ventas ($)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Ventas por segmento (pie)\n",
    "ax2 = fig.add_subplot(gs[2, 0])\n",
    "ventas_segmento['total_ventas'].plot(kind='pie', ax=ax2, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('ğŸ‘¥ Ventas por Segmento', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# 4. Ventas por mes\n",
    "ax3 = fig.add_subplot(gs[2, 1])\n",
    "ventas_mes['total_ventas'].plot(kind='bar', ax=ax3, color='#00B894')\n",
    "ax3.set_title('ğŸ“… Ventas por Mes', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Mes')\n",
    "ax3.set_ylabel('Total ($)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top productos\n",
    "ax4 = fig.add_subplot(gs[2, 2])\n",
    "top_productos = df_ventas_final.groupby('producto')['total'].sum().nlargest(5)\n",
    "top_productos.plot(kind='barh', ax=ax4, color='#FD79A8')\n",
    "ax4.set_title('ğŸ† Top 5 Productos', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Ventas ($)')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# TÃ­tulo principal\n",
    "fig.suptitle('ğŸ“Š DASHBOARD DE VENTAS - PROYECTO INTEGRADOR 1', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('../../outputs/proyecto_1/dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Dashboard generado y guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34f110",
   "metadata": {},
   "source": [
    "## 7. Carga de Datos (Load)\n",
    "\n",
    "### 7.1 Cargar a Base de Datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexiÃ³n a SQLite\n",
    "db_path = '../../outputs/proyecto_1/ventas_analytics.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Cargar datos transformados\n",
    "    df_ventas_final.to_sql('ventas', conn, if_exists='replace', index=False)\n",
    "    logger.info(f\"âœ… Tabla 'ventas' cargada: {len(df_ventas_final)} registros\")\n",
    "    \n",
    "    # Cargar agregaciones\n",
    "    ventas_categoria.to_sql('ventas_categoria', conn, if_exists='replace')\n",
    "    logger.info(\"âœ… Tabla 'ventas_categoria' cargada\")\n",
    "    \n",
    "    ventas_segmento.to_sql('ventas_segmento', conn, if_exists='replace')\n",
    "    logger.info(\"âœ… Tabla 'ventas_segmento' cargada\")\n",
    "    \n",
    "    print(\"\\nâœ… DATOS CARGADOS EXITOSAMENTE EN SQLite\")\n",
    "    print(f\"ğŸ“ Base de datos: {db_path}\")\n",
    "    \n",
    "    # Verificar tablas\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tablas = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Tablas creadas:\")\n",
    "    for tabla in tablas:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {tabla[0]}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  â€¢ {tabla[0]}: {count} registros\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "logger.info(\"Carga completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332031",
   "metadata": {},
   "source": [
    "### 7.2 Exportar a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a Parquet para anÃ¡lisis\n",
    "parquet_path = '../../outputs/proyecto_1/ventas_final.parquet'\n",
    "df_ventas_final.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"âœ… Datos exportados a Parquet: {parquet_path}\")\n",
    "print(f\"ğŸ“Š TamaÃ±o del archivo: {os.path.getsize(parquet_path) / 1024:.2f} KB\")\n",
    "\n",
    "logger.info(\"ExportaciÃ³n a Parquet completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58316",
   "metadata": {},
   "source": [
    "## 8. Reporte Final\n",
    "\n",
    "### 8.1 Generar Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abca488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "reporte = f\"\"\"\n",
    "{'='*80}\n",
    "ğŸ“Š PROYECTO INTEGRADOR 1: PIPELINE ETL COMPLETO\n",
    "{'='*80}\n",
    "\n",
    "Fecha de ejecuciÃ³n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "1. EXTRACCIÃ“N\n",
    "{'='*80}\n",
    "\n",
    "Fuentes de datos procesadas:\n",
    "  â€¢ Ventas (CSV): {len(df_ventas)} registros\n",
    "  â€¢ Productos (CSV): {len(df_productos)} registros\n",
    "  â€¢ Clientes (CSV): {len(df_clientes)} registros\n",
    "  â€¢ Usuarios API: {len(df_usuarios_api)} registros\n",
    "\n",
    "{'='*80}\n",
    "2. TRANSFORMACIÃ“N\n",
    "{'='*80}\n",
    "\n",
    "Transformaciones aplicadas:\n",
    "  âœ… Limpieza de valores nulos\n",
    "  âœ… NormalizaciÃ³n de fechas\n",
    "  âœ… EliminaciÃ³n de duplicados\n",
    "  âœ… Enriquecimiento con datos relacionados\n",
    "  âœ… CÃ¡lculo de mÃ©tricas derivadas (margen, agregaciones temporales)\n",
    "\n",
    "Registros finales: {len(df_ventas_final)}\n",
    "Columnas totales: {len(df_ventas_final.columns)}\n",
    "\n",
    "{'='*80}\n",
    "3. KPIs PRINCIPALES\n",
    "{'='*80}\n",
    "\n",
    "ğŸ’° Total Ventas: ${kpis['total_ventas']:,.2f}\n",
    "ğŸ›’ Transacciones: {kpis['num_transacciones']:,}\n",
    "ğŸ“ˆ Ticket Promedio: ${kpis['ticket_promedio']:.2f}\n",
    "ğŸ“Š Ticket Mediano: ${kpis['ticket_mediano']:.2f}\n",
    "ğŸ‘¥ Clientes Ãšnicos: {kpis['clientes_unicos']}\n",
    "ğŸ“¦ Productos Vendidos: {kpis['productos_vendidos']}\n",
    "ğŸ’¹ Margen Promedio: {kpis['margen_promedio']:.2f}%\n",
    "\n",
    "{'='*80}\n",
    "4. TOP INSIGHTS\n",
    "{'='*80}\n",
    "\n",
    "ğŸ† CategorÃ­a con mÃ¡s ventas: {ventas_categoria.index[0]} (${ventas_categoria.iloc[0]['total_ventas']:,.2f})\n",
    "ğŸ‘” Segmento mÃ¡s valioso: {ventas_segmento.index[0]} (${ventas_segmento.iloc[0]['total_ventas']:,.2f})\n",
    "ğŸ“… Mes con mÃ¡s ventas: {ventas_mes['total_ventas'].idxmax()} (${ventas_mes['total_ventas'].max():,.2f})\n",
    "\n",
    "{'='*80}\n",
    "5. CALIDAD DE DATOS\n",
    "{'='*80}\n",
    "\n",
    "{validator.generate_report()}\n",
    "\n",
    "{'='*80}\n",
    "6. ARCHIVOS GENERADOS\n",
    "{'='*80}\n",
    "\n",
    "ğŸ“ Base de datos: ventas_analytics.db\n",
    "ğŸ“„ Parquet: ventas_final.parquet\n",
    "ğŸ“Š Dashboard: dashboard.png\n",
    "ğŸ“ˆ CSV Agregados:\n",
    "   â€¢ ventas_categoria.csv\n",
    "   â€¢ ventas_segmento.csv\n",
    "   â€¢ ventas_mes.csv\n",
    "ğŸ“ KPIs: kpis.json\n",
    "ğŸ“‹ Logs: proyecto_integrador_1.log\n",
    "\n",
    "{'='*80}\n",
    "âœ… PIPELINE COMPLETADO EXITOSAMENTE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(reporte)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_final.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(reporte)\n",
    "\n",
    "logger.info(\"Reporte final generado\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PROYECTO INTEGRADOR 1 COMPLETADO EXITOSAMENTE\")\n",
    "logger.info(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2966",
   "metadata": {},
   "source": [
    "## ğŸ“š Conclusiones del Proyecto\n",
    "\n",
    "### âœ… Lo que Lograste:\n",
    "\n",
    "1. **ExtracciÃ³n Multi-Fuente**: CSV, API REST, SQLite\n",
    "2. **Transformaciones Robustas**: Limpieza, normalizaciÃ³n, enriquecimiento\n",
    "3. **ValidaciÃ³n de Calidad**: Checks automÃ¡ticos de completitud, unicidad, rangos\n",
    "4. **AnÃ¡lisis Avanzado**: KPIs, agregaciones por dimensiones\n",
    "5. **VisualizaciÃ³n Ejecutiva**: Dashboard completo con insights clave\n",
    "6. **Carga Optimizada**: SQLite para queries, Parquet para anÃ¡lisis\n",
    "7. **Logging Profesional**: Trazabilidad completa del proceso\n",
    "\n",
    "### ğŸ¯ Habilidades Desarrolladas:\n",
    "\n",
    "- âœ… DiseÃ±o de arquitecturas ETL\n",
    "- âœ… ProgramaciÃ³n orientada a objetos\n",
    "- âœ… Manejo de errores y logging\n",
    "- âœ… Calidad de datos\n",
    "- âœ… VisualizaciÃ³n de insights\n",
    "- âœ… DocumentaciÃ³n de pipelines\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Pasos:\n",
    "\n",
    "1. **AutomatizaciÃ³n**: Usar Apache Airflow para scheduling\n",
    "2. **Escalabilidad**: Migrar a Spark para volÃºmenes grandes\n",
    "3. **Cloud**: Desplegar en AWS/GCP/Azure\n",
    "4. **Monitoreo**: Implementar alertas y mÃ©tricas de performance\n",
    "5. **CI/CD**: Automatizar tests y despliegues\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸŠ Â¡FELICITACIONES! Has completado tu primer proyecto integrador de IngenierÃ­a de Datos.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
