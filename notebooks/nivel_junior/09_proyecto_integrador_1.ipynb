{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e10312",
   "metadata": {},
   "source": [
    "# üéØ Proyecto Integrador 1: Pipeline ETL Completo\n",
    "\n",
    "## Descripci√≥n del Proyecto\n",
    "\n",
    "En este proyecto integrador, construir√°s un **pipeline ETL completo** que integra todos los conceptos aprendidos:\n",
    "\n",
    "- ‚úÖ Extracci√≥n de datos desde m√∫ltiples fuentes (CSV, API, Base de Datos)\n",
    "- ‚úÖ Transformaci√≥n y limpieza de datos con Pandas\n",
    "- ‚úÖ Validaci√≥n de calidad de datos\n",
    "- ‚úÖ Visualizaci√≥n de resultados\n",
    "- ‚úÖ Almacenamiento en base de datos SQLite\n",
    "- ‚úÖ Generaci√≥n de reportes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Requisitos del Proyecto\n",
    "\n",
    "### Funcionalidades Requeridas:\n",
    "\n",
    "1. **Extracci√≥n** de datos desde:\n",
    "   - Archivos CSV locales\n",
    "   - API REST p√∫blica\n",
    "   - Base de datos SQLite\n",
    "\n",
    "2. **Transformaci√≥n** que incluya:\n",
    "   - Limpieza de valores nulos\n",
    "   - Normalizaci√≥n de formatos\n",
    "   - C√°lculo de m√©tricas agregadas\n",
    "   - Enriquecimiento con datos externos\n",
    "\n",
    "3. **Carga** de datos transformados:\n",
    "   - Base de datos SQLite\n",
    "   - Archivos Parquet para an√°lisis\n",
    "\n",
    "4. **Visualizaci√≥n** de insights:\n",
    "   - Dashboard con matplotlib/seaborn\n",
    "   - M√©tricas clave (KPIs)\n",
    "\n",
    "5. **Logging** y manejo de errores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53950",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../../logs/proyecto_integrador_1.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs('../../outputs/proyecto_1', exist_ok=True)\n",
    "os.makedirs('../../logs', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n inicial completada\")\n",
    "logger.info(\"Proyecto Integrador 1 iniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e7adb",
   "metadata": {},
   "source": [
    "### üîß Setup de Proyecto: Logging y Estructura\n",
    "\n",
    "**Concepto:** Proyectos profesionales requieren logging, manejo de errores y estructura de directorios organizada.\n",
    "\n",
    "**Elementos clave:**\n",
    "- **Logging:** `logging.basicConfig()` registra eventos en archivo + consola\n",
    "- **Directorios:** `os.makedirs()` crea outputs/, logs/ si no existen\n",
    "- **Logger:** seguimiento de ejecuci√≥n, errores, m√©tricas de performance\n",
    "\n",
    "**Niveles de logging:**\n",
    "- INFO: eventos normales del flujo\n",
    "- WARNING: situaciones inusuales pero manejables\n",
    "- ERROR: errores que impiden operaciones espec√≠ficas\n",
    "- CRITICAL: fallos que detienen la aplicaci√≥n\n",
    "\n",
    "**Objetivo:** Trazabilidad completa del pipeline para debugging y auditor√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a0836",
   "metadata": {},
   "source": [
    "## 2. Extracci√≥n de Datos\n",
    "\n",
    "### 2.1 Clase DataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de datos desde m√∫ltiples fuentes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_csv(self, filepath: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde archivo CSV.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extrayendo datos de CSV: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            self.logger.info(f\"‚úÖ CSV extra√≠do: {len(df)} registros\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"‚ùå Archivo no encontrado: {filepath}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error al leer CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_api(self, url: str, params: Optional[Dict] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde API REST.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando API: {url}\")\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ API consultada: {len(df)} registros\")\n",
    "            return df\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"‚ùå Error en petici√≥n API: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error procesando respuesta API: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sqlite(self, db_path: str, query: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde base de datos SQLite.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando SQLite: {db_path}\")\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ SQLite consultado: {len(df)} registros\")\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"‚ùå Error en SQLite: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error general: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "print(\"‚úÖ Clase DataExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210854f0",
   "metadata": {},
   "source": [
    "### üì• DataExtractor: Extracci√≥n Multi-Fuente\n",
    "\n",
    "**Concepto:** Clase unificada que extrae datos desde CSV, APIs y bases de datos, con manejo de errores consistente.\n",
    "\n",
    "**Patr√≥n de dise√±o:**\n",
    "- M√©todos espec√≠ficos por fuente: `extract_csv()`, `extract_api()`, `extract_db()`\n",
    "- Try/except en cada m√©todo para capturar errores espec√≠ficos\n",
    "- Logging para cada operaci√≥n (√©xito/fallo)\n",
    "- Retorno `Optional[pd.DataFrame]` permite manejar None en failures\n",
    "\n",
    "**Ventaja:** C√≥digo reutilizable, testeable, y con logging centralizado.\n",
    "\n",
    "**Uso en producci√≥n:** Base para pipelines Airflow, Luigi, Prefect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad43d63",
   "metadata": {},
   "source": [
    "### 2.2 Ejecutar Extracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# 1. Extraer ventas desde CSV\n",
    "df_ventas = extractor.extract_csv('../../datasets/raw/ventas.csv')\n",
    "\n",
    "# 2. Extraer productos desde CSV\n",
    "df_productos = extractor.extract_csv('../../datasets/raw/productos.csv')\n",
    "\n",
    "# 3. Extraer clientes desde CSV\n",
    "df_clientes = extractor.extract_csv('../../datasets/raw/clientes.csv')\n",
    "\n",
    "# 4. Extraer datos de usuarios desde API p√∫blica\n",
    "df_usuarios_api = extractor.extract_api('https://jsonplaceholder.typicode.com/users')\n",
    "\n",
    "# Verificar extracciones\n",
    "print(\"\\nüìä RESUMEN DE EXTRACCI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ventas: {len(df_ventas) if df_ventas is not None else 0} registros\")\n",
    "print(f\"Productos: {len(df_productos) if df_productos is not None else 0} registros\")\n",
    "print(f\"Clientes: {len(df_clientes) if df_clientes is not None else 0} registros\")\n",
    "print(f\"Usuarios API: {len(df_usuarios_api) if df_usuarios_api is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58e19a",
   "metadata": {},
   "source": [
    "## 3. Transformaci√≥n de Datos\n",
    "\n",
    "### 3.1 Clase DataTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24908fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Transformador de datos con validaci√≥n y limpieza.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def clean_nulls(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Limpia valores nulos.\n",
    "        \"\"\"\n",
    "        nulos_antes = df.isnull().sum().sum()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            df_clean = df.dropna()\n",
    "        elif strategy == 'fill_mean':\n",
    "            df_clean = df.copy()\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n",
    "        elif strategy == 'fill_mode':\n",
    "            df_clean = df.copy()\n",
    "            for col in df_clean.columns:\n",
    "                if df_clean[col].isnull().any():\n",
    "                    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df_clean = df.copy()\n",
    "        \n",
    "        nulos_despues = df_clean.isnull().sum().sum()\n",
    "        self.logger.info(f\"Nulos: {nulos_antes} ‚Üí {nulos_despues}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def normalize_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza columnas de fechas.\n",
    "        \"\"\"\n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df_norm.columns:\n",
    "                df_norm[col] = pd.to_datetime(df_norm[col], errors='coerce')\n",
    "                self.logger.info(f\"‚úÖ Fecha normalizada: {col}\")\n",
    "        \n",
    "        return df_norm\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Elimina duplicados.\n",
    "        \"\"\"\n",
    "        duplicados_antes = df.duplicated(subset=subset).sum()\n",
    "        df_clean = df.drop_duplicates(subset=subset)\n",
    "        duplicados_eliminados = duplicados_antes\n",
    "        \n",
    "        self.logger.info(f\"Duplicados eliminados: {duplicados_eliminados}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def enrich_data(self, df_base: pd.DataFrame, df_enrich: pd.DataFrame, \n",
    "                   on: str, how: str = 'left') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enriquece datos con merge.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_enriched = df_base.merge(df_enrich, on=on, how=how)\n",
    "            nuevas_cols = len(df_enriched.columns) - len(df_base.columns)\n",
    "            self.logger.info(f\"‚úÖ Datos enriquecidos: +{nuevas_cols} columnas\")\n",
    "            return df_enriched\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error en enriquecimiento: {str(e)}\")\n",
    "            return df_base\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula m√©tricas derivadas.\n",
    "        \"\"\"\n",
    "        df_metrics = df.copy()\n",
    "        \n",
    "        # Ejemplo: calcular total si hay cantidad y precio\n",
    "        if 'cantidad' in df_metrics.columns and 'precio_unitario' in df_metrics.columns:\n",
    "            if 'total' not in df_metrics.columns:\n",
    "                df_metrics['total_calculado'] = df_metrics['cantidad'] * df_metrics['precio_unitario']\n",
    "                self.logger.info(\"‚úÖ M√©trica 'total_calculado' generada\")\n",
    "        \n",
    "        return df_metrics\n",
    "\n",
    "print(\"‚úÖ Clase DataTransformer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68bfbb",
   "metadata": {},
   "source": [
    "### üîÑ DataTransformer: Transformaciones Estandarizadas\n",
    "\n",
    "**Concepto:** Clase con m√©todos para transformaciones comunes del pipeline ETL.\n",
    "\n",
    "**Transformaciones incluidas:**\n",
    "- **clean_nulls():** m√∫ltiples estrategias (drop, fill_mean, fill_mode)\n",
    "- **normalize_dates():** conversi√≥n robusta con `errors='coerce'`\n",
    "- **remove_duplicates():** eliminaci√≥n con logging de cantidad\n",
    "- **enrich_data():** joins/merges para enriquecimiento\n",
    "\n",
    "**Patr√≥n:**\n",
    "1. Copiar DataFrame original (`df.copy()`)\n",
    "2. Aplicar transformaci√≥n\n",
    "3. Logear m√©tricas (antes/despu√©s)\n",
    "4. Retornar DataFrame transformado\n",
    "\n",
    "**Ventaja:** Cada transformaci√≥n es una unidad testeable y auditable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50d740",
   "metadata": {},
   "source": [
    "### 3.2 Aplicar Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ef587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear transformador\n",
    "transformer = DataTransformer()\n",
    "\n",
    "print(\"üîÑ INICIANDO TRANSFORMACIONES\\n\")\n",
    "\n",
    "# 1. Limpiar nulos en ventas\n",
    "df_ventas_clean = transformer.clean_nulls(df_ventas, strategy='drop')\n",
    "\n",
    "# 2. Normalizar fechas\n",
    "df_ventas_clean = transformer.normalize_dates(df_ventas_clean, ['fecha_venta'])\n",
    "df_clientes_clean = transformer.normalize_dates(df_clientes, ['fecha_registro'])\n",
    "\n",
    "# 3. Eliminar duplicados\n",
    "df_ventas_clean = transformer.remove_duplicates(df_ventas_clean, subset=['venta_id'])\n",
    "df_productos_clean = transformer.remove_duplicates(df_productos, subset=['producto'])\n",
    "\n",
    "# 4. Enriquecer ventas con productos\n",
    "df_ventas_enriched = transformer.enrich_data(\n",
    "    df_ventas_clean, \n",
    "    df_productos_clean, \n",
    "    on='producto_id'\n",
    ")\n",
    "\n",
    "# 5. Enriquecer con clientes\n",
    "df_ventas_final = transformer.enrich_data(\n",
    "    df_ventas_enriched,\n",
    "    df_clientes_clean[['cliente_id', 'segmento', 'ciudad', 'pais']],\n",
    "    on='cliente_id'\n",
    ")\n",
    "\n",
    "# 6. Calcular m√©tricas adicionales\n",
    "df_ventas_final['margen'] = (df_ventas_final['total'] - (df_ventas_final['precio'] * df_ventas_final['cantidad'])) / df_ventas_final['total'] * 100\n",
    "df_ventas_final['mes'] = df_ventas_final['fecha_venta'].dt.to_period('M').astype(str)\n",
    "df_ventas_final['trimestre'] = df_ventas_final['fecha_venta'].dt.to_period('Q').astype(str)\n",
    "\n",
    "print(\"\\n‚úÖ TRANSFORMACIONES COMPLETADAS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Registros finales: {len(df_ventas_final)}\")\n",
    "print(f\"Columnas totales: {len(df_ventas_final.columns)}\")\n",
    "print(f\"\\nPrimeras columnas:\")\n",
    "print(df_ventas_final.columns.tolist()[:10])\n",
    "\n",
    "display(df_ventas_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937942",
   "metadata": {},
   "source": [
    "## 4. Validaci√≥n de Calidad de Datos\n",
    "\n",
    "### 4.1 Clase DataQualityValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityValidator:\n",
    "    \"\"\"\n",
    "    Validador de calidad de datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.issues = []\n",
    "    \n",
    "    def validate_completeness(self, df: pd.DataFrame, threshold: float = 0.95) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los datos est√©n completos (sin nulos).\n",
    "        \"\"\"\n",
    "        total_values = df.size\n",
    "        non_null_values = df.count().sum()\n",
    "        completeness = non_null_values / total_values\n",
    "        \n",
    "        if completeness < threshold:\n",
    "            msg = f\"‚ùå Completitud baja: {completeness:.2%} (umbral: {threshold:.2%})\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"‚úÖ Completitud OK: {completeness:.2%}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_uniqueness(self, df: pd.DataFrame, columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Valida unicidad en columnas clave.\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns).sum()\n",
    "        \n",
    "        if duplicates > 0:\n",
    "            msg = f\"‚ùå Duplicados encontrados: {duplicates} en {columns}\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"‚úÖ Unicidad OK en {columns}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los valores est√©n en un rango v√°lido.\n",
    "        \"\"\"\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            msg = f\"‚ùå {len(out_of_range)} valores fuera de rango en '{column}' [{min_val}, {max_val}]\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"‚úÖ Rango OK en '{column}'\")\n",
    "            return True\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera reporte de calidad.\n",
    "        \"\"\"\n",
    "        report = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "        report += \"üìä REPORTE DE CALIDAD DE DATOS\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\"\n",
    "        \n",
    "        if not self.issues:\n",
    "            report += \"\\n‚úÖ No se encontraron problemas de calidad\\n\"\n",
    "        else:\n",
    "            report += f\"\\n‚ö†Ô∏è Se encontraron {len(self.issues)} problemas:\\n\\n\"\n",
    "            for i, issue in enumerate(self.issues, 1):\n",
    "                report += f\"{i}. {issue}\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\" * 60\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"‚úÖ Clase DataQualityValidator definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ab48a",
   "metadata": {},
   "source": [
    "### ‚úÖ DataQualityValidator: Validaciones Automatizadas\n",
    "\n",
    "**Concepto:** Validador que verifica calidad de datos antes de cargar a destino.\n",
    "\n",
    "**Validaciones t√≠picas:**\n",
    "- **Completeness:** % valores no nulos ‚â• threshold\n",
    "- **Uniqueness:** no duplicados en claves primarias\n",
    "- **Accuracy:** valores dentro de rangos v√°lidos\n",
    "- **Consistency:** formatos y tipos de datos correctos\n",
    "\n",
    "**Patr√≥n:**\n",
    "- Acumular issues en lista\n",
    "- Retornar bool (pass/fail)\n",
    "- Logear cada validaci√≥n\n",
    "- Detener pipeline si validaciones cr√≠ticas fallan\n",
    "\n",
    "**Uso:** Gate de calidad antes de stage \"Load\", esencial en producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c025fa",
   "metadata": {},
   "source": [
    "### 4.2 Ejecutar Validaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear validador\n",
    "validator = DataQualityValidator()\n",
    "\n",
    "print(\"üîç VALIDANDO CALIDAD DE DATOS\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "validator.validate_completeness(df_ventas_final, threshold=0.90)\n",
    "\n",
    "# 2. Unicidad en ventas\n",
    "validator.validate_uniqueness(df_ventas_final, ['venta_id'])\n",
    "\n",
    "# 3. Rangos v√°lidos\n",
    "validator.validate_ranges(df_ventas_final, 'cantidad', min_val=0, max_val=100)\n",
    "validator.validate_ranges(df_ventas_final, 'total', min_val=0, max_val=100000)\n",
    "\n",
    "# Generar reporte\n",
    "report = validator.generate_report()\n",
    "print(report)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_calidad.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "logger.info(\"Reporte de calidad generado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab51ae",
   "metadata": {},
   "source": [
    "## 5. An√°lisis y Agregaciones\n",
    "\n",
    "### 5.1 M√©tricas de Negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bef04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs principales\n",
    "kpis = {\n",
    "    'total_ventas': df_ventas_final['total'].sum(),\n",
    "    'num_transacciones': len(df_ventas_final),\n",
    "    'ticket_promedio': df_ventas_final['total'].mean(),\n",
    "    'ticket_mediano': df_ventas_final['total'].median(),\n",
    "    'clientes_unicos': df_ventas_final['cliente_id'].nunique(),\n",
    "    'productos_vendidos': df_ventas_final['producto'].nunique(),\n",
    "    'margen_promedio': df_ventas_final['margen'].mean()\n",
    "}\n",
    "\n",
    "print(\"üìä KPIs PRINCIPALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üí∞ Total Ventas: ${kpis['total_ventas']:,.2f}\")\n",
    "print(f\"üõí Transacciones: {kpis['num_transacciones']:,}\")\n",
    "print(f\"üìà Ticket Promedio: ${kpis['ticket_promedio']:.2f}\")\n",
    "print(f\"üìä Ticket Mediano: ${kpis['ticket_mediano']:.2f}\")\n",
    "print(f\"üë• Clientes √önicos: {kpis['clientes_unicos']}\")\n",
    "print(f\"üì¶ Productos Vendidos: {kpis['productos_vendidos']}\")\n",
    "print(f\"üíπ Margen Promedio: {kpis['margen_promedio']:.2f}%\")\n",
    "\n",
    "# Guardar KPIs\n",
    "with open('../../outputs/proyecto_1/kpis.json', 'w') as f:\n",
    "    json.dump(kpis, f, indent=2)\n",
    "\n",
    "logger.info(\"KPIs calculados y guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e3bd",
   "metadata": {},
   "source": [
    "### 5.2 Agregaciones por Dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por categor√≠a\n",
    "ventas_categoria = df_ventas_final.groupby('categoria').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count',\n",
    "    'margen': 'mean'\n",
    "}).round(2)\n",
    "ventas_categoria.columns = ['total_ventas', 'num_ventas', 'margen_promedio']\n",
    "ventas_categoria = ventas_categoria.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\nüìä VENTAS POR CATEGOR√çA\")\n",
    "display(ventas_categoria)\n",
    "\n",
    "# Ventas por segmento de cliente\n",
    "ventas_segmento = df_ventas_final.groupby('segmento').agg({\n",
    "    'total': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "ventas_segmento.columns = ['total_ventas', 'ticket_promedio', 'num_transacciones']\n",
    "ventas_segmento = ventas_segmento.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\nüë• VENTAS POR SEGMENTO DE CLIENTE\")\n",
    "display(ventas_segmento)\n",
    "\n",
    "# Ventas por mes\n",
    "ventas_mes = df_ventas_final.groupby('mes').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "ventas_mes.columns = ['total_ventas', 'num_ventas']\n",
    "\n",
    "print(\"\\nüìÖ VENTAS POR MES\")\n",
    "display(ventas_mes)\n",
    "\n",
    "# Guardar agregaciones\n",
    "ventas_categoria.to_csv('../../outputs/proyecto_1/ventas_categoria.csv')\n",
    "ventas_segmento.to_csv('../../outputs/proyecto_1/ventas_segmento.csv')\n",
    "ventas_mes.to_csv('../../outputs/proyecto_1/ventas_mes.csv')\n",
    "\n",
    "logger.info(\"Agregaciones calculadas y guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b58527",
   "metadata": {},
   "source": [
    "### üìä An√°lisis Dimensional: Agregaciones de Negocio\n",
    "\n",
    "**Concepto:** Agregar m√©tricas por dimensiones de negocio para insights accionables.\n",
    "\n",
    "**Agregaciones t√≠picas:**\n",
    "- **groupby().agg():** m√∫ltiples funciones (sum, mean, count) por grupo\n",
    "- **Dimensiones:** categor√≠a, segmento, tiempo, geograf√≠a\n",
    "- **M√©tricas:** ventas totales, ticket promedio, cantidad transacciones\n",
    "\n",
    "**Patr√≥n:**\n",
    "```python\n",
    "df.groupby('dimension').agg({\n",
    "    'metrica1': 'sum',\n",
    "    'metrica2': 'mean'\n",
    "})\n",
    "```\n",
    "\n",
    "**Output:** Tablas agregadas base para reportes, dashboards y decisiones de negocio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a7c8",
   "metadata": {},
   "source": [
    "## 6. Visualizaci√≥n de Resultados\n",
    "\n",
    "### 6.1 Dashboard de KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. KPIs principales (arriba)\n",
    "ax_kpi = fig.add_subplot(gs[0, :])\n",
    "ax_kpi.axis('off')\n",
    "\n",
    "kpi_texts = [\n",
    "    ('üí∞ Total Ventas', f\"${kpis['total_ventas']:,.0f}\"),\n",
    "    ('üõí Transacciones', f\"{kpis['num_transacciones']:,}\"),\n",
    "    ('üìä Ticket Prom.', f\"${kpis['ticket_promedio']:.2f}\"),\n",
    "    ('üë• Clientes', f\"{kpis['clientes_unicos']}\"),\n",
    "    ('üíπ Margen', f\"{kpis['margen_promedio']:.1f}%\")\n",
    "]\n",
    "\n",
    "for i, (titulo, valor) in enumerate(kpi_texts):\n",
    "    x_pos = 0.1 + i * 0.18\n",
    "    ax_kpi.text(x_pos, 0.7, titulo, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax_kpi.text(x_pos, 0.3, valor, ha='center', fontsize=16, fontweight='bold', color='#2E86AB')\n",
    "\n",
    "# 2. Ventas por categor√≠a\n",
    "ax1 = fig.add_subplot(gs[1, :])\n",
    "ventas_categoria['total_ventas'].plot(kind='barh', ax=ax1, color='#6C5CE7')\n",
    "ax1.set_title('üíº Ventas por Categor√≠a', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Total Ventas ($)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Ventas por segmento (pie)\n",
    "ax2 = fig.add_subplot(gs[2, 0])\n",
    "ventas_segmento['total_ventas'].plot(kind='pie', ax=ax2, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('üë• Ventas por Segmento', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# 4. Ventas por mes\n",
    "ax3 = fig.add_subplot(gs[2, 1])\n",
    "ventas_mes['total_ventas'].plot(kind='bar', ax=ax3, color='#00B894')\n",
    "ax3.set_title('üìÖ Ventas por Mes', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Mes')\n",
    "ax3.set_ylabel('Total ($)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top productos\n",
    "ax4 = fig.add_subplot(gs[2, 2])\n",
    "top_productos = df_ventas_final.groupby('producto')['total'].sum().nlargest(5)\n",
    "top_productos.plot(kind='barh', ax=ax4, color='#FD79A8')\n",
    "ax4.set_title('üèÜ Top 5 Productos', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Ventas ($)')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# T√≠tulo principal\n",
    "fig.suptitle('üìä DASHBOARD DE VENTAS - PROYECTO INTEGRADOR 1', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('../../outputs/proyecto_1/dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Dashboard generado y guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860451f",
   "metadata": {},
   "source": [
    "### üìà Dashboard Ejecutivo: Visualizaci√≥n Integrada\n",
    "\n",
    "**Concepto:** Dashboard multi-panel que presenta KPIs y m√©tricas de negocio en una sola vista.\n",
    "\n",
    "**Estructura con GridSpec:**\n",
    "- **Fila 1:** KPIs principales (n√∫meros grandes)\n",
    "- **Fila 2:** Gr√°fico principal (ventas por categor√≠a)\n",
    "- **Fila 3:** M√∫ltiples vistas (segmento, temporal, productos)\n",
    "\n",
    "**Tipos de gr√°ficos:**\n",
    "- Barras horizontales: rankings y comparaciones\n",
    "- Pie chart: proporciones y distribuciones\n",
    "- Barras verticales: series temporales\n",
    "- KPI cards: m√©tricas clave destacadas\n",
    "\n",
    "**Objetivo:** Vista ejecutiva one-pager para decisiones r√°pidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34f110",
   "metadata": {},
   "source": [
    "## 7. Carga de Datos (Load)\n",
    "\n",
    "### 7.1 Cargar a Base de Datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexi√≥n a SQLite\n",
    "db_path = '../../outputs/proyecto_1/ventas_analytics.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Cargar datos transformados\n",
    "    df_ventas_final.to_sql('ventas', conn, if_exists='replace', index=False)\n",
    "    logger.info(f\"‚úÖ Tabla 'ventas' cargada: {len(df_ventas_final)} registros\")\n",
    "    \n",
    "    # Cargar agregaciones\n",
    "    ventas_categoria.to_sql('ventas_categoria', conn, if_exists='replace')\n",
    "    logger.info(\"‚úÖ Tabla 'ventas_categoria' cargada\")\n",
    "    \n",
    "    ventas_segmento.to_sql('ventas_segmento', conn, if_exists='replace')\n",
    "    logger.info(\"‚úÖ Tabla 'ventas_segmento' cargada\")\n",
    "    \n",
    "    print(\"\\n‚úÖ DATOS CARGADOS EXITOSAMENTE EN SQLite\")\n",
    "    print(f\"üìÅ Base de datos: {db_path}\")\n",
    "    \n",
    "    # Verificar tablas\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tablas = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nüìä Tablas creadas:\")\n",
    "    for tabla in tablas:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {tabla[0]}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  ‚Ä¢ {tabla[0]}: {count} registros\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "logger.info(\"Carga completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c2b0b",
   "metadata": {},
   "source": [
    "### üíæ Load Stage: Persistencia en Base de Datos\n",
    "\n",
    "**Concepto:** √öltima etapa del ETL - cargar datos transformados y validados a destino permanente.\n",
    "\n",
    "**M√©todo `to_sql()`:**\n",
    "- Crea tabla autom√°ticamente desde DataFrame\n",
    "- `if_exists='replace'`: sobrescribe tabla existente\n",
    "- `index=False`: no guardar √≠ndice como columna\n",
    "\n",
    "**M√∫ltiples tablas:**\n",
    "- **Datos detallados:** ventas completas (fact table)\n",
    "- **Agregaciones:** tablas pre-calculadas para queries r√°pidos\n",
    "\n",
    "**Verificaci√≥n:** Query de conteo para confirmar carga exitosa.\n",
    "\n",
    "**Formato alternativo:** Parquet para data lakes, mejor compresi√≥n y velocidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332031",
   "metadata": {},
   "source": [
    "### 7.2 Exportar a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a Parquet para an√°lisis\n",
    "parquet_path = '../../outputs/proyecto_1/ventas_final.parquet'\n",
    "df_ventas_final.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Datos exportados a Parquet: {parquet_path}\")\n",
    "print(f\"üìä Tama√±o del archivo: {os.path.getsize(parquet_path) / 1024:.2f} KB\")\n",
    "\n",
    "logger.info(\"Exportaci√≥n a Parquet completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58316",
   "metadata": {},
   "source": [
    "## 8. Reporte Final\n",
    "\n",
    "### 8.1 Generar Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abca488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "reporte = f\"\"\"\n",
    "{'='*80}\n",
    "üìä PROYECTO INTEGRADOR 1: PIPELINE ETL COMPLETO\n",
    "{'='*80}\n",
    "\n",
    "Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "1. EXTRACCI√ìN\n",
    "{'='*80}\n",
    "\n",
    "Fuentes de datos procesadas:\n",
    "  ‚Ä¢ Ventas (CSV): {len(df_ventas)} registros\n",
    "  ‚Ä¢ Productos (CSV): {len(df_productos)} registros\n",
    "  ‚Ä¢ Clientes (CSV): {len(df_clientes)} registros\n",
    "  ‚Ä¢ Usuarios API: {len(df_usuarios_api)} registros\n",
    "\n",
    "{'='*80}\n",
    "2. TRANSFORMACI√ìN\n",
    "{'='*80}\n",
    "\n",
    "Transformaciones aplicadas:\n",
    "  ‚úÖ Limpieza de valores nulos\n",
    "  ‚úÖ Normalizaci√≥n de fechas\n",
    "  ‚úÖ Eliminaci√≥n de duplicados\n",
    "  ‚úÖ Enriquecimiento con datos relacionados\n",
    "  ‚úÖ C√°lculo de m√©tricas derivadas (margen, agregaciones temporales)\n",
    "\n",
    "Registros finales: {len(df_ventas_final)}\n",
    "Columnas totales: {len(df_ventas_final.columns)}\n",
    "\n",
    "{'='*80}\n",
    "3. KPIs PRINCIPALES\n",
    "{'='*80}\n",
    "\n",
    "üí∞ Total Ventas: ${kpis['total_ventas']:,.2f}\n",
    "üõí Transacciones: {kpis['num_transacciones']:,}\n",
    "üìà Ticket Promedio: ${kpis['ticket_promedio']:.2f}\n",
    "üìä Ticket Mediano: ${kpis['ticket_mediano']:.2f}\n",
    "üë• Clientes √önicos: {kpis['clientes_unicos']}\n",
    "üì¶ Productos Vendidos: {kpis['productos_vendidos']}\n",
    "üíπ Margen Promedio: {kpis['margen_promedio']:.2f}%\n",
    "\n",
    "{'='*80}\n",
    "4. TOP INSIGHTS\n",
    "{'='*80}\n",
    "\n",
    "üèÜ Categor√≠a con m√°s ventas: {ventas_categoria.index[0]} (${ventas_categoria.iloc[0]['total_ventas']:,.2f})\n",
    "üëî Segmento m√°s valioso: {ventas_segmento.index[0]} (${ventas_segmento.iloc[0]['total_ventas']:,.2f})\n",
    "üìÖ Mes con m√°s ventas: {ventas_mes['total_ventas'].idxmax()} (${ventas_mes['total_ventas'].max():,.2f})\n",
    "\n",
    "{'='*80}\n",
    "5. CALIDAD DE DATOS\n",
    "{'='*80}\n",
    "\n",
    "{validator.generate_report()}\n",
    "\n",
    "{'='*80}\n",
    "6. ARCHIVOS GENERADOS\n",
    "{'='*80}\n",
    "\n",
    "üìÅ Base de datos: ventas_analytics.db\n",
    "üìÑ Parquet: ventas_final.parquet\n",
    "üìä Dashboard: dashboard.png\n",
    "üìà CSV Agregados:\n",
    "   ‚Ä¢ ventas_categoria.csv\n",
    "   ‚Ä¢ ventas_segmento.csv\n",
    "   ‚Ä¢ ventas_mes.csv\n",
    "üìù KPIs: kpis.json\n",
    "üìã Logs: proyecto_integrador_1.log\n",
    "\n",
    "{'='*80}\n",
    "‚úÖ PIPELINE COMPLETADO EXITOSAMENTE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(reporte)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_final.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(reporte)\n",
    "\n",
    "logger.info(\"Reporte final generado\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PROYECTO INTEGRADOR 1 COMPLETADO EXITOSAMENTE\")\n",
    "logger.info(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2966",
   "metadata": {},
   "source": [
    "## üìö Conclusiones del Proyecto\n",
    "\n",
    "### ‚úÖ Lo que Lograste:\n",
    "\n",
    "1. **Extracci√≥n Multi-Fuente**: CSV, API REST, SQLite\n",
    "2. **Transformaciones Robustas**: Limpieza, normalizaci√≥n, enriquecimiento\n",
    "3. **Validaci√≥n de Calidad**: Checks autom√°ticos de completitud, unicidad, rangos\n",
    "4. **An√°lisis Avanzado**: KPIs, agregaciones por dimensiones\n",
    "5. **Visualizaci√≥n Ejecutiva**: Dashboard completo con insights clave\n",
    "6. **Carga Optimizada**: SQLite para queries, Parquet para an√°lisis\n",
    "7. **Logging Profesional**: Trazabilidad completa del proceso\n",
    "\n",
    "### üéØ Habilidades Desarrolladas:\n",
    "\n",
    "- ‚úÖ Dise√±o de arquitecturas ETL\n",
    "- ‚úÖ Programaci√≥n orientada a objetos\n",
    "- ‚úÖ Manejo de errores y logging\n",
    "- ‚úÖ Calidad de datos\n",
    "- ‚úÖ Visualizaci√≥n de insights\n",
    "- ‚úÖ Documentaci√≥n de pipelines\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos:\n",
    "\n",
    "1. **Automatizaci√≥n**: Usar Apache Airflow para scheduling\n",
    "2. **Escalabilidad**: Migrar a Spark para vol√∫menes grandes\n",
    "3. **Cloud**: Desplegar en AWS/GCP/Azure\n",
    "4. **Monitoreo**: Implementar alertas y m√©tricas de performance\n",
    "5. **CI/CD**: Automatizar tests y despliegues\n",
    "\n",
    "---\n",
    "\n",
    "**üéä ¬°FELICITACIONES! Has completado tu primer proyecto integrador de Ingenier√≠a de Datos.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
