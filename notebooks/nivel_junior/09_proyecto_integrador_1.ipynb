{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e10312",
   "metadata": {},
   "source": [
    "# 🎯 Proyecto Integrador 1: Pipeline ETL Completo\n",
    "\n",
    "## Descripción del Proyecto\n",
    "\n",
    "En este proyecto integrador, construirás un **pipeline ETL completo** que integra todos los conceptos aprendidos:\n",
    "\n",
    "- ✅ Extracción de datos desde múltiples fuentes (CSV, API, Base de Datos)\n",
    "- ✅ Transformación y limpieza de datos con Pandas\n",
    "- ✅ Validación de calidad de datos\n",
    "- ✅ Visualización de resultados\n",
    "- ✅ Almacenamiento en base de datos SQLite\n",
    "- ✅ Generación de reportes\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Requisitos del Proyecto\n",
    "\n",
    "### Funcionalidades Requeridas:\n",
    "\n",
    "1. **Extracción** de datos desde:\n",
    "   - Archivos CSV locales\n",
    "   - API REST pública\n",
    "   - Base de datos SQLite\n",
    "\n",
    "2. **Transformación** que incluya:\n",
    "   - Limpieza de valores nulos\n",
    "   - Normalización de formatos\n",
    "   - Cálculo de métricas agregadas\n",
    "   - Enriquecimiento con datos externos\n",
    "\n",
    "3. **Carga** de datos transformados:\n",
    "   - Base de datos SQLite\n",
    "   - Archivos Parquet para análisis\n",
    "\n",
    "4. **Visualización** de insights:\n",
    "   - Dashboard con matplotlib/seaborn\n",
    "   - Métricas clave (KPIs)\n",
    "\n",
    "5. **Logging** y manejo de errores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53950",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../../logs/proyecto_integrador_1.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs('../../outputs/proyecto_1', exist_ok=True)\n",
    "os.makedirs('../../logs', exist_ok=True)\n",
    "\n",
    "print(\"✅ Configuración inicial completada\")\n",
    "logger.info(\"Proyecto Integrador 1 iniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a0836",
   "metadata": {},
   "source": [
    "## 2. Extracción de Datos\n",
    "\n",
    "### 2.1 Clase DataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de datos desde múltiples fuentes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_csv(self, filepath: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde archivo CSV.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extrayendo datos de CSV: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            self.logger.info(f\"✅ CSV extraído: {len(df)} registros\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"❌ Archivo no encontrado: {filepath}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error al leer CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_api(self, url: str, params: Optional[Dict] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde API REST.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando API: {url}\")\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            self.logger.info(f\"✅ API consultada: {len(df)} registros\")\n",
    "            return df\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"❌ Error en petición API: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error procesando respuesta API: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sqlite(self, db_path: str, query: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde base de datos SQLite.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando SQLite: {db_path}\")\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            self.logger.info(f\"✅ SQLite consultado: {len(df)} registros\")\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"❌ Error en SQLite: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error general: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "print(\"✅ Clase DataExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad43d63",
   "metadata": {},
   "source": [
    "### 2.2 Ejecutar Extracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# 1. Extraer ventas desde CSV\n",
    "df_ventas = extractor.extract_csv('../../datasets/raw/ventas.csv')\n",
    "\n",
    "# 2. Extraer productos desde CSV\n",
    "df_productos = extractor.extract_csv('../../datasets/raw/productos.csv')\n",
    "\n",
    "# 3. Extraer clientes desde CSV\n",
    "df_clientes = extractor.extract_csv('../../datasets/raw/clientes.csv')\n",
    "\n",
    "# 4. Extraer datos de usuarios desde API pública\n",
    "df_usuarios_api = extractor.extract_api('https://jsonplaceholder.typicode.com/users')\n",
    "\n",
    "# Verificar extracciones\n",
    "print(\"\\n📊 RESUMEN DE EXTRACCIÓN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ventas: {len(df_ventas) if df_ventas is not None else 0} registros\")\n",
    "print(f\"Productos: {len(df_productos) if df_productos is not None else 0} registros\")\n",
    "print(f\"Clientes: {len(df_clientes) if df_clientes is not None else 0} registros\")\n",
    "print(f\"Usuarios API: {len(df_usuarios_api) if df_usuarios_api is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58e19a",
   "metadata": {},
   "source": [
    "## 3. Transformación de Datos\n",
    "\n",
    "### 3.1 Clase DataTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24908fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Transformador de datos con validación y limpieza.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def clean_nulls(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Limpia valores nulos.\n",
    "        \"\"\"\n",
    "        nulos_antes = df.isnull().sum().sum()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            df_clean = df.dropna()\n",
    "        elif strategy == 'fill_mean':\n",
    "            df_clean = df.copy()\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n",
    "        elif strategy == 'fill_mode':\n",
    "            df_clean = df.copy()\n",
    "            for col in df_clean.columns:\n",
    "                if df_clean[col].isnull().any():\n",
    "                    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df_clean = df.copy()\n",
    "        \n",
    "        nulos_despues = df_clean.isnull().sum().sum()\n",
    "        self.logger.info(f\"Nulos: {nulos_antes} → {nulos_despues}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def normalize_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza columnas de fechas.\n",
    "        \"\"\"\n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df_norm.columns:\n",
    "                df_norm[col] = pd.to_datetime(df_norm[col], errors='coerce')\n",
    "                self.logger.info(f\"✅ Fecha normalizada: {col}\")\n",
    "        \n",
    "        return df_norm\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Elimina duplicados.\n",
    "        \"\"\"\n",
    "        duplicados_antes = df.duplicated(subset=subset).sum()\n",
    "        df_clean = df.drop_duplicates(subset=subset)\n",
    "        duplicados_eliminados = duplicados_antes\n",
    "        \n",
    "        self.logger.info(f\"Duplicados eliminados: {duplicados_eliminados}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def enrich_data(self, df_base: pd.DataFrame, df_enrich: pd.DataFrame, \n",
    "                   on: str, how: str = 'left') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enriquece datos con merge.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_enriched = df_base.merge(df_enrich, on=on, how=how)\n",
    "            nuevas_cols = len(df_enriched.columns) - len(df_base.columns)\n",
    "            self.logger.info(f\"✅ Datos enriquecidos: +{nuevas_cols} columnas\")\n",
    "            return df_enriched\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error en enriquecimiento: {str(e)}\")\n",
    "            return df_base\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula métricas derivadas.\n",
    "        \"\"\"\n",
    "        df_metrics = df.copy()\n",
    "        \n",
    "        # Ejemplo: calcular total si hay cantidad y precio\n",
    "        if 'cantidad' in df_metrics.columns and 'precio_unitario' in df_metrics.columns:\n",
    "            if 'total' not in df_metrics.columns:\n",
    "                df_metrics['total_calculado'] = df_metrics['cantidad'] * df_metrics['precio_unitario']\n",
    "                self.logger.info(\"✅ Métrica 'total_calculado' generada\")\n",
    "        \n",
    "        return df_metrics\n",
    "\n",
    "print(\"✅ Clase DataTransformer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50d740",
   "metadata": {},
   "source": [
    "### 3.2 Aplicar Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ef587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear transformador\n",
    "transformer = DataTransformer()\n",
    "\n",
    "print(\"🔄 INICIANDO TRANSFORMACIONES\\n\")\n",
    "\n",
    "# 1. Limpiar nulos en ventas\n",
    "df_ventas_clean = transformer.clean_nulls(df_ventas, strategy='drop')\n",
    "\n",
    "# 2. Normalizar fechas\n",
    "df_ventas_clean = transformer.normalize_dates(df_ventas_clean, ['fecha_venta'])\n",
    "df_clientes_clean = transformer.normalize_dates(df_clientes, ['fecha_registro'])\n",
    "\n",
    "# 3. Eliminar duplicados\n",
    "df_ventas_clean = transformer.remove_duplicates(df_ventas_clean, subset=['venta_id'])\n",
    "df_productos_clean = transformer.remove_duplicates(df_productos, subset=['producto'])\n",
    "\n",
    "# 4. Enriquecer ventas con productos\n",
    "df_ventas_enriched = transformer.enrich_data(\n",
    "    df_ventas_clean, \n",
    "    df_productos_clean, \n",
    "    on='producto_id'\n",
    ")\n",
    "\n",
    "# 5. Enriquecer con clientes\n",
    "df_ventas_final = transformer.enrich_data(\n",
    "    df_ventas_enriched,\n",
    "    df_clientes_clean[['cliente_id', 'segmento', 'ciudad', 'pais']],\n",
    "    on='cliente_id'\n",
    ")\n",
    "\n",
    "# 6. Calcular métricas adicionales\n",
    "df_ventas_final['margen'] = (df_ventas_final['total'] - (df_ventas_final['precio'] * df_ventas_final['cantidad'])) / df_ventas_final['total'] * 100\n",
    "df_ventas_final['mes'] = df_ventas_final['fecha_venta'].dt.to_period('M').astype(str)\n",
    "df_ventas_final['trimestre'] = df_ventas_final['fecha_venta'].dt.to_period('Q').astype(str)\n",
    "\n",
    "print(\"\\n✅ TRANSFORMACIONES COMPLETADAS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Registros finales: {len(df_ventas_final)}\")\n",
    "print(f\"Columnas totales: {len(df_ventas_final.columns)}\")\n",
    "print(f\"\\nPrimeras columnas:\")\n",
    "print(df_ventas_final.columns.tolist()[:10])\n",
    "\n",
    "display(df_ventas_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937942",
   "metadata": {},
   "source": [
    "## 4. Validación de Calidad de Datos\n",
    "\n",
    "### 4.1 Clase DataQualityValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityValidator:\n",
    "    \"\"\"\n",
    "    Validador de calidad de datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.issues = []\n",
    "    \n",
    "    def validate_completeness(self, df: pd.DataFrame, threshold: float = 0.95) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los datos estén completos (sin nulos).\n",
    "        \"\"\"\n",
    "        total_values = df.size\n",
    "        non_null_values = df.count().sum()\n",
    "        completeness = non_null_values / total_values\n",
    "        \n",
    "        if completeness < threshold:\n",
    "            msg = f\"❌ Completitud baja: {completeness:.2%} (umbral: {threshold:.2%})\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"✅ Completitud OK: {completeness:.2%}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_uniqueness(self, df: pd.DataFrame, columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Valida unicidad en columnas clave.\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns).sum()\n",
    "        \n",
    "        if duplicates > 0:\n",
    "            msg = f\"❌ Duplicados encontrados: {duplicates} en {columns}\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"✅ Unicidad OK en {columns}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los valores estén en un rango válido.\n",
    "        \"\"\"\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            msg = f\"❌ {len(out_of_range)} valores fuera de rango en '{column}' [{min_val}, {max_val}]\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"✅ Rango OK en '{column}'\")\n",
    "            return True\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera reporte de calidad.\n",
    "        \"\"\"\n",
    "        report = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "        report += \"📊 REPORTE DE CALIDAD DE DATOS\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\"\n",
    "        \n",
    "        if not self.issues:\n",
    "            report += \"\\n✅ No se encontraron problemas de calidad\\n\"\n",
    "        else:\n",
    "            report += f\"\\n⚠️ Se encontraron {len(self.issues)} problemas:\\n\\n\"\n",
    "            for i, issue in enumerate(self.issues, 1):\n",
    "                report += f\"{i}. {issue}\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\" * 60\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"✅ Clase DataQualityValidator definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c025fa",
   "metadata": {},
   "source": [
    "### 4.2 Ejecutar Validaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear validador\n",
    "validator = DataQualityValidator()\n",
    "\n",
    "print(\"🔍 VALIDANDO CALIDAD DE DATOS\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "validator.validate_completeness(df_ventas_final, threshold=0.90)\n",
    "\n",
    "# 2. Unicidad en ventas\n",
    "validator.validate_uniqueness(df_ventas_final, ['venta_id'])\n",
    "\n",
    "# 3. Rangos válidos\n",
    "validator.validate_ranges(df_ventas_final, 'cantidad', min_val=0, max_val=100)\n",
    "validator.validate_ranges(df_ventas_final, 'total', min_val=0, max_val=100000)\n",
    "\n",
    "# Generar reporte\n",
    "report = validator.generate_report()\n",
    "print(report)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_calidad.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "logger.info(\"Reporte de calidad generado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab51ae",
   "metadata": {},
   "source": [
    "## 5. Análisis y Agregaciones\n",
    "\n",
    "### 5.1 Métricas de Negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bef04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs principales\n",
    "kpis = {\n",
    "    'total_ventas': df_ventas_final['total'].sum(),\n",
    "    'num_transacciones': len(df_ventas_final),\n",
    "    'ticket_promedio': df_ventas_final['total'].mean(),\n",
    "    'ticket_mediano': df_ventas_final['total'].median(),\n",
    "    'clientes_unicos': df_ventas_final['cliente_id'].nunique(),\n",
    "    'productos_vendidos': df_ventas_final['producto'].nunique(),\n",
    "    'margen_promedio': df_ventas_final['margen'].mean()\n",
    "}\n",
    "\n",
    "print(\"📊 KPIs PRINCIPALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"💰 Total Ventas: ${kpis['total_ventas']:,.2f}\")\n",
    "print(f\"🛒 Transacciones: {kpis['num_transacciones']:,}\")\n",
    "print(f\"📈 Ticket Promedio: ${kpis['ticket_promedio']:.2f}\")\n",
    "print(f\"📊 Ticket Mediano: ${kpis['ticket_mediano']:.2f}\")\n",
    "print(f\"👥 Clientes Únicos: {kpis['clientes_unicos']}\")\n",
    "print(f\"📦 Productos Vendidos: {kpis['productos_vendidos']}\")\n",
    "print(f\"💹 Margen Promedio: {kpis['margen_promedio']:.2f}%\")\n",
    "\n",
    "# Guardar KPIs\n",
    "with open('../../outputs/proyecto_1/kpis.json', 'w') as f:\n",
    "    json.dump(kpis, f, indent=2)\n",
    "\n",
    "logger.info(\"KPIs calculados y guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e3bd",
   "metadata": {},
   "source": [
    "### 5.2 Agregaciones por Dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por categoría\n",
    "ventas_categoria = df_ventas_final.groupby('categoria').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count',\n",
    "    'margen': 'mean'\n",
    "}).round(2)\n",
    "ventas_categoria.columns = ['total_ventas', 'num_ventas', 'margen_promedio']\n",
    "ventas_categoria = ventas_categoria.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\n📊 VENTAS POR CATEGORÍA\")\n",
    "display(ventas_categoria)\n",
    "\n",
    "# Ventas por segmento de cliente\n",
    "ventas_segmento = df_ventas_final.groupby('segmento').agg({\n",
    "    'total': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "ventas_segmento.columns = ['total_ventas', 'ticket_promedio', 'num_transacciones']\n",
    "ventas_segmento = ventas_segmento.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\n👥 VENTAS POR SEGMENTO DE CLIENTE\")\n",
    "display(ventas_segmento)\n",
    "\n",
    "# Ventas por mes\n",
    "ventas_mes = df_ventas_final.groupby('mes').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "ventas_mes.columns = ['total_ventas', 'num_ventas']\n",
    "\n",
    "print(\"\\n📅 VENTAS POR MES\")\n",
    "display(ventas_mes)\n",
    "\n",
    "# Guardar agregaciones\n",
    "ventas_categoria.to_csv('../../outputs/proyecto_1/ventas_categoria.csv')\n",
    "ventas_segmento.to_csv('../../outputs/proyecto_1/ventas_segmento.csv')\n",
    "ventas_mes.to_csv('../../outputs/proyecto_1/ventas_mes.csv')\n",
    "\n",
    "logger.info(\"Agregaciones calculadas y guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a7c8",
   "metadata": {},
   "source": [
    "## 6. Visualización de Resultados\n",
    "\n",
    "### 6.1 Dashboard de KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. KPIs principales (arriba)\n",
    "ax_kpi = fig.add_subplot(gs[0, :])\n",
    "ax_kpi.axis('off')\n",
    "\n",
    "kpi_texts = [\n",
    "    ('💰 Total Ventas', f\"${kpis['total_ventas']:,.0f}\"),\n",
    "    ('🛒 Transacciones', f\"{kpis['num_transacciones']:,}\"),\n",
    "    ('📊 Ticket Prom.', f\"${kpis['ticket_promedio']:.2f}\"),\n",
    "    ('👥 Clientes', f\"{kpis['clientes_unicos']}\"),\n",
    "    ('💹 Margen', f\"{kpis['margen_promedio']:.1f}%\")\n",
    "]\n",
    "\n",
    "for i, (titulo, valor) in enumerate(kpi_texts):\n",
    "    x_pos = 0.1 + i * 0.18\n",
    "    ax_kpi.text(x_pos, 0.7, titulo, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax_kpi.text(x_pos, 0.3, valor, ha='center', fontsize=16, fontweight='bold', color='#2E86AB')\n",
    "\n",
    "# 2. Ventas por categoría\n",
    "ax1 = fig.add_subplot(gs[1, :])\n",
    "ventas_categoria['total_ventas'].plot(kind='barh', ax=ax1, color='#6C5CE7')\n",
    "ax1.set_title('💼 Ventas por Categoría', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Total Ventas ($)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Ventas por segmento (pie)\n",
    "ax2 = fig.add_subplot(gs[2, 0])\n",
    "ventas_segmento['total_ventas'].plot(kind='pie', ax=ax2, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('👥 Ventas por Segmento', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# 4. Ventas por mes\n",
    "ax3 = fig.add_subplot(gs[2, 1])\n",
    "ventas_mes['total_ventas'].plot(kind='bar', ax=ax3, color='#00B894')\n",
    "ax3.set_title('📅 Ventas por Mes', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Mes')\n",
    "ax3.set_ylabel('Total ($)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top productos\n",
    "ax4 = fig.add_subplot(gs[2, 2])\n",
    "top_productos = df_ventas_final.groupby('producto')['total'].sum().nlargest(5)\n",
    "top_productos.plot(kind='barh', ax=ax4, color='#FD79A8')\n",
    "ax4.set_title('🏆 Top 5 Productos', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Ventas ($)')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Título principal\n",
    "fig.suptitle('📊 DASHBOARD DE VENTAS - PROYECTO INTEGRADOR 1', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('../../outputs/proyecto_1/dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Dashboard generado y guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34f110",
   "metadata": {},
   "source": [
    "## 7. Carga de Datos (Load)\n",
    "\n",
    "### 7.1 Cargar a Base de Datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexión a SQLite\n",
    "db_path = '../../outputs/proyecto_1/ventas_analytics.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Cargar datos transformados\n",
    "    df_ventas_final.to_sql('ventas', conn, if_exists='replace', index=False)\n",
    "    logger.info(f\"✅ Tabla 'ventas' cargada: {len(df_ventas_final)} registros\")\n",
    "    \n",
    "    # Cargar agregaciones\n",
    "    ventas_categoria.to_sql('ventas_categoria', conn, if_exists='replace')\n",
    "    logger.info(\"✅ Tabla 'ventas_categoria' cargada\")\n",
    "    \n",
    "    ventas_segmento.to_sql('ventas_segmento', conn, if_exists='replace')\n",
    "    logger.info(\"✅ Tabla 'ventas_segmento' cargada\")\n",
    "    \n",
    "    print(\"\\n✅ DATOS CARGADOS EXITOSAMENTE EN SQLite\")\n",
    "    print(f\"📁 Base de datos: {db_path}\")\n",
    "    \n",
    "    # Verificar tablas\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tablas = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\n📊 Tablas creadas:\")\n",
    "    for tabla in tablas:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {tabla[0]}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  • {tabla[0]}: {count} registros\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "logger.info(\"Carga completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332031",
   "metadata": {},
   "source": [
    "### 7.2 Exportar a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a Parquet para análisis\n",
    "parquet_path = '../../outputs/proyecto_1/ventas_final.parquet'\n",
    "df_ventas_final.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"✅ Datos exportados a Parquet: {parquet_path}\")\n",
    "print(f\"📊 Tamaño del archivo: {os.path.getsize(parquet_path) / 1024:.2f} KB\")\n",
    "\n",
    "logger.info(\"Exportación a Parquet completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58316",
   "metadata": {},
   "source": [
    "## 8. Reporte Final\n",
    "\n",
    "### 8.1 Generar Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abca488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "reporte = f\"\"\"\n",
    "{'='*80}\n",
    "📊 PROYECTO INTEGRADOR 1: PIPELINE ETL COMPLETO\n",
    "{'='*80}\n",
    "\n",
    "Fecha de ejecución: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "1. EXTRACCIÓN\n",
    "{'='*80}\n",
    "\n",
    "Fuentes de datos procesadas:\n",
    "  • Ventas (CSV): {len(df_ventas)} registros\n",
    "  • Productos (CSV): {len(df_productos)} registros\n",
    "  • Clientes (CSV): {len(df_clientes)} registros\n",
    "  • Usuarios API: {len(df_usuarios_api)} registros\n",
    "\n",
    "{'='*80}\n",
    "2. TRANSFORMACIÓN\n",
    "{'='*80}\n",
    "\n",
    "Transformaciones aplicadas:\n",
    "  ✅ Limpieza de valores nulos\n",
    "  ✅ Normalización de fechas\n",
    "  ✅ Eliminación de duplicados\n",
    "  ✅ Enriquecimiento con datos relacionados\n",
    "  ✅ Cálculo de métricas derivadas (margen, agregaciones temporales)\n",
    "\n",
    "Registros finales: {len(df_ventas_final)}\n",
    "Columnas totales: {len(df_ventas_final.columns)}\n",
    "\n",
    "{'='*80}\n",
    "3. KPIs PRINCIPALES\n",
    "{'='*80}\n",
    "\n",
    "💰 Total Ventas: ${kpis['total_ventas']:,.2f}\n",
    "🛒 Transacciones: {kpis['num_transacciones']:,}\n",
    "📈 Ticket Promedio: ${kpis['ticket_promedio']:.2f}\n",
    "📊 Ticket Mediano: ${kpis['ticket_mediano']:.2f}\n",
    "👥 Clientes Únicos: {kpis['clientes_unicos']}\n",
    "📦 Productos Vendidos: {kpis['productos_vendidos']}\n",
    "💹 Margen Promedio: {kpis['margen_promedio']:.2f}%\n",
    "\n",
    "{'='*80}\n",
    "4. TOP INSIGHTS\n",
    "{'='*80}\n",
    "\n",
    "🏆 Categoría con más ventas: {ventas_categoria.index[0]} (${ventas_categoria.iloc[0]['total_ventas']:,.2f})\n",
    "👔 Segmento más valioso: {ventas_segmento.index[0]} (${ventas_segmento.iloc[0]['total_ventas']:,.2f})\n",
    "📅 Mes con más ventas: {ventas_mes['total_ventas'].idxmax()} (${ventas_mes['total_ventas'].max():,.2f})\n",
    "\n",
    "{'='*80}\n",
    "5. CALIDAD DE DATOS\n",
    "{'='*80}\n",
    "\n",
    "{validator.generate_report()}\n",
    "\n",
    "{'='*80}\n",
    "6. ARCHIVOS GENERADOS\n",
    "{'='*80}\n",
    "\n",
    "📁 Base de datos: ventas_analytics.db\n",
    "📄 Parquet: ventas_final.parquet\n",
    "📊 Dashboard: dashboard.png\n",
    "📈 CSV Agregados:\n",
    "   • ventas_categoria.csv\n",
    "   • ventas_segmento.csv\n",
    "   • ventas_mes.csv\n",
    "📝 KPIs: kpis.json\n",
    "📋 Logs: proyecto_integrador_1.log\n",
    "\n",
    "{'='*80}\n",
    "✅ PIPELINE COMPLETADO EXITOSAMENTE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(reporte)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_final.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(reporte)\n",
    "\n",
    "logger.info(\"Reporte final generado\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PROYECTO INTEGRADOR 1 COMPLETADO EXITOSAMENTE\")\n",
    "logger.info(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2966",
   "metadata": {},
   "source": [
    "## 📚 Conclusiones del Proyecto\n",
    "\n",
    "### ✅ Lo que Lograste:\n",
    "\n",
    "1. **Extracción Multi-Fuente**: CSV, API REST, SQLite\n",
    "2. **Transformaciones Robustas**: Limpieza, normalización, enriquecimiento\n",
    "3. **Validación de Calidad**: Checks automáticos de completitud, unicidad, rangos\n",
    "4. **Análisis Avanzado**: KPIs, agregaciones por dimensiones\n",
    "5. **Visualización Ejecutiva**: Dashboard completo con insights clave\n",
    "6. **Carga Optimizada**: SQLite para queries, Parquet para análisis\n",
    "7. **Logging Profesional**: Trazabilidad completa del proceso\n",
    "\n",
    "### 🎯 Habilidades Desarrolladas:\n",
    "\n",
    "- ✅ Diseño de arquitecturas ETL\n",
    "- ✅ Programación orientada a objetos\n",
    "- ✅ Manejo de errores y logging\n",
    "- ✅ Calidad de datos\n",
    "- ✅ Visualización de insights\n",
    "- ✅ Documentación de pipelines\n",
    "\n",
    "### 🚀 Próximos Pasos:\n",
    "\n",
    "1. **Automatización**: Usar Apache Airflow para scheduling\n",
    "2. **Escalabilidad**: Migrar a Spark para volúmenes grandes\n",
    "3. **Cloud**: Desplegar en AWS/GCP/Azure\n",
    "4. **Monitoreo**: Implementar alertas y métricas de performance\n",
    "5. **CI/CD**: Automatizar tests y despliegues\n",
    "\n",
    "---\n",
    "\n",
    "**🎊 ¡FELICITACIONES! Has completado tu primer proyecto integrador de Ingeniería de Datos.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
