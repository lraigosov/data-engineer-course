{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e10312",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Proyecto Integrador 1: Pipeline ETL Completo\n",
    "\n",
    "## Descripci\u00f3n del Proyecto\n",
    "\n",
    "En este proyecto integrador, construir\u00e1s un **pipeline ETL completo** que integra todos los conceptos aprendidos:\n",
    "\n",
    "- \u2705 Extracci\u00f3n de datos desde m\u00faltiples fuentes (CSV, API, Base de Datos)\n",
    "- \u2705 Transformaci\u00f3n y limpieza de datos con Pandas\n",
    "- \u2705 Validaci\u00f3n de calidad de datos\n",
    "- \u2705 Visualizaci\u00f3n de resultados\n",
    "- \u2705 Almacenamiento en base de datos SQLite\n",
    "- \u2705 Generaci\u00f3n de reportes\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Requisitos del Proyecto\n",
    "\n",
    "### Funcionalidades Requeridas:\n",
    "\n",
    "1. **Extracci\u00f3n** de datos desde:\n",
    "   - Archivos CSV locales\n",
    "   - API REST p\u00fablica\n",
    "   - Base de datos SQLite\n",
    "\n",
    "2. **Transformaci\u00f3n** que incluya:\n",
    "   - Limpieza de valores nulos\n",
    "   - Normalizaci\u00f3n de formatos\n",
    "   - C\u00e1lculo de m\u00e9tricas agregadas\n",
    "   - Enriquecimiento con datos externos\n",
    "\n",
    "3. **Carga** de datos transformados:\n",
    "   - Base de datos SQLite\n",
    "   - Archivos Parquet para an\u00e1lisis\n",
    "\n",
    "4. **Visualizaci\u00f3n** de insights:\n",
    "   - Dashboard con matplotlib/seaborn\n",
    "   - M\u00e9tricas clave (KPIs)\n",
    "\n",
    "5. **Logging** y manejo de errores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53950",
   "metadata": {},
   "source": [
    "## 1. Configuraci\u00f3n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../../logs/proyecto_integrador_1.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs('../../outputs/proyecto_1', exist_ok=True)\n",
    "os.makedirs('../../logs', exist_ok=True)\n",
    "\n",
    "print(\"\u2705 Configuraci\u00f3n inicial completada\")\n",
    "logger.info(\"Proyecto Integrador 1 iniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e7adb",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 Setup de Proyecto: Logging y Estructura\n",
    "\n",
    "**Concepto:** Proyectos profesionales requieren logging, manejo de errores y estructura de directorios organizada.\n",
    "\n",
    "**Elementos clave:**\n",
    "- **Logging:** `logging.basicConfig()` registra eventos en archivo + consola\n",
    "- **Directorios:** `os.makedirs()` crea outputs/, logs/ si no existen\n",
    "- **Logger:** seguimiento de ejecuci\u00f3n, errores, m\u00e9tricas de performance\n",
    "\n",
    "**Niveles de logging:**\n",
    "- INFO: eventos normales del flujo\n",
    "- WARNING: situaciones inusuales pero manejables\n",
    "- ERROR: errores que impiden operaciones espec\u00edficas\n",
    "- CRITICAL: fallos que detienen la aplicaci\u00f3n\n",
    "\n",
    "**Objetivo:** Trazabilidad completa del pipeline para debugging y auditor\u00eda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a0836",
   "metadata": {},
   "source": [
    "## 2. Extracci\u00f3n de Datos\n",
    "\n",
    "### 2.1 Clase DataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de datos desde m\u00faltiples fuentes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_csv(self, filepath: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde archivo CSV.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extrayendo datos de CSV: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            self.logger.info(f\"\u2705 CSV extra\u00eddo: {len(df)} registros\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"\u274c Archivo no encontrado: {filepath}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"\u274c Error al leer CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_api(self, url: str, params: Optional[Dict] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde API REST.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando API: {url}\")\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            self.logger.info(f\"\u2705 API consultada: {len(df)} registros\")\n",
    "            return df\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"\u274c Error en petici\u00f3n API: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"\u274c Error procesando respuesta API: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sqlite(self, db_path: str, query: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extrae datos desde base de datos SQLite.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Consultando SQLite: {db_path}\")\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            self.logger.info(f\"\u2705 SQLite consultado: {len(df)} registros\")\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"\u274c Error en SQLite: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"\u274c Error general: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "print(\"\u2705 Clase DataExtractor definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210854f0",
   "metadata": {},
   "source": [
    "### \ud83d\udce5 DataExtractor: Extracci\u00f3n Multi-Fuente\n",
    "\n",
    "**Concepto:** Clase unificada que extrae datos desde CSV, APIs y bases de datos, con manejo de errores consistente.\n",
    "\n",
    "**Patr\u00f3n de dise\u00f1o:**\n",
    "- M\u00e9todos espec\u00edficos por fuente: `extract_csv()`, `extract_api()`, `extract_db()`\n",
    "- Try/except en cada m\u00e9todo para capturar errores espec\u00edficos\n",
    "- Logging para cada operaci\u00f3n (\u00e9xito/fallo)\n",
    "- Retorno `Optional[pd.DataFrame]` permite manejar None en failures\n",
    "\n",
    "**Ventaja:** C\u00f3digo reutilizable, testeable, y con logging centralizado.\n",
    "\n",
    "**Uso en producci\u00f3n:** Base para pipelines Airflow, Luigi, Prefect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad43d63",
   "metadata": {},
   "source": [
    "### 2.2 Ejecutar Extracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear extractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# 1. Extraer ventas desde CSV\n",
    "df_ventas = extractor.extract_csv('../../datasets/raw/ventas.csv')\n",
    "\n",
    "# 2. Extraer productos desde CSV\n",
    "df_productos = extractor.extract_csv('../../datasets/raw/productos.csv')\n",
    "\n",
    "# 3. Extraer clientes desde CSV\n",
    "df_clientes = extractor.extract_csv('../../datasets/raw/clientes.csv')\n",
    "\n",
    "# 4. Extraer datos de usuarios desde API p\u00fablica\n",
    "df_usuarios_api = extractor.extract_api('https://jsonplaceholder.typicode.com/users')\n",
    "\n",
    "# Verificar extracciones\n",
    "print(\"\\n\ud83d\udcca RESUMEN DE EXTRACCI\u00d3N\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ventas: {len(df_ventas) if df_ventas is not None else 0} registros\")\n",
    "print(f\"Productos: {len(df_productos) if df_productos is not None else 0} registros\")\n",
    "print(f\"Clientes: {len(df_clientes) if df_clientes is not None else 0} registros\")\n",
    "print(f\"Usuarios API: {len(df_usuarios_api) if df_usuarios_api is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58e19a",
   "metadata": {},
   "source": [
    "## 3. Transformaci\u00f3n de Datos\n",
    "\n",
    "### 3.1 Clase DataTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24908fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Transformador de datos con validaci\u00f3n y limpieza.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def clean_nulls(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Limpia valores nulos.\n",
    "        \"\"\"\n",
    "        nulos_antes = df.isnull().sum().sum()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            df_clean = df.dropna()\n",
    "        elif strategy == 'fill_mean':\n",
    "            df_clean = df.copy()\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n",
    "        elif strategy == 'fill_mode':\n",
    "            df_clean = df.copy()\n",
    "            for col in df_clean.columns:\n",
    "                if df_clean[col].isnull().any():\n",
    "                    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df_clean = df.copy()\n",
    "        \n",
    "        nulos_despues = df_clean.isnull().sum().sum()\n",
    "        self.logger.info(f\"Nulos: {nulos_antes} \u2192 {nulos_despues}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def normalize_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normaliza columnas de fechas.\n",
    "        \"\"\"\n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df_norm.columns:\n",
    "                df_norm[col] = pd.to_datetime(df_norm[col], errors='coerce')\n",
    "                self.logger.info(f\"\u2705 Fecha normalizada: {col}\")\n",
    "        \n",
    "        return df_norm\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Elimina duplicados.\n",
    "        \"\"\"\n",
    "        duplicados_antes = df.duplicated(subset=subset).sum()\n",
    "        df_clean = df.drop_duplicates(subset=subset)\n",
    "        duplicados_eliminados = duplicados_antes\n",
    "        \n",
    "        self.logger.info(f\"Duplicados eliminados: {duplicados_eliminados}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def enrich_data(self, df_base: pd.DataFrame, df_enrich: pd.DataFrame, \n",
    "                   on: str, how: str = 'left') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enriquece datos con merge.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_enriched = df_base.merge(df_enrich, on=on, how=how)\n",
    "            nuevas_cols = len(df_enriched.columns) - len(df_base.columns)\n",
    "            self.logger.info(f\"\u2705 Datos enriquecidos: +{nuevas_cols} columnas\")\n",
    "            return df_enriched\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"\u274c Error en enriquecimiento: {str(e)}\")\n",
    "            return df_base\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula m\u00e9tricas derivadas.\n",
    "        \"\"\"\n",
    "        df_metrics = df.copy()\n",
    "        \n",
    "        # Ejemplo: calcular total si hay cantidad y precio\n",
    "        if 'cantidad' in df_metrics.columns and 'precio_unitario' in df_metrics.columns:\n",
    "            if 'total' not in df_metrics.columns:\n",
    "                df_metrics['total_calculado'] = df_metrics['cantidad'] * df_metrics['precio_unitario']\n",
    "                self.logger.info(\"\u2705 M\u00e9trica 'total_calculado' generada\")\n",
    "        \n",
    "        return df_metrics\n",
    "\n",
    "print(\"\u2705 Clase DataTransformer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68bfbb",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 DataTransformer: Transformaciones Estandarizadas\n",
    "\n",
    "**Concepto:** Clase con m\u00e9todos para transformaciones comunes del pipeline ETL.\n",
    "\n",
    "**Transformaciones incluidas:**\n",
    "- **clean_nulls():** m\u00faltiples estrategias (drop, fill_mean, fill_mode)\n",
    "- **normalize_dates():** conversi\u00f3n robusta con `errors='coerce'`\n",
    "- **remove_duplicates():** eliminaci\u00f3n con logging de cantidad\n",
    "- **enrich_data():** joins/merges para enriquecimiento\n",
    "\n",
    "**Patr\u00f3n:**\n",
    "1. Copiar DataFrame original (`df.copy()`)\n",
    "2. Aplicar transformaci\u00f3n\n",
    "3. Logear m\u00e9tricas (antes/despu\u00e9s)\n",
    "4. Retornar DataFrame transformado\n",
    "\n",
    "**Ventaja:** Cada transformaci\u00f3n es una unidad testeable y auditable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50d740",
   "metadata": {},
   "source": [
    "### 3.2 Aplicar Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ef587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear transformador\n",
    "transformer = DataTransformer()\n",
    "\n",
    "print(\"\ud83d\udd04 INICIANDO TRANSFORMACIONES\\n\")\n",
    "\n",
    "# 1. Limpiar nulos en ventas\n",
    "df_ventas_clean = transformer.clean_nulls(df_ventas, strategy='drop')\n",
    "\n",
    "# 2. Normalizar fechas\n",
    "df_ventas_clean = transformer.normalize_dates(df_ventas_clean, ['fecha_venta'])\n",
    "df_clientes_clean = transformer.normalize_dates(df_clientes, ['fecha_registro'])\n",
    "\n",
    "# 3. Eliminar duplicados\n",
    "df_ventas_clean = transformer.remove_duplicates(df_ventas_clean, subset=['venta_id'])\n",
    "df_productos_clean = transformer.remove_duplicates(df_productos, subset=['producto'])\n",
    "\n",
    "# 4. Enriquecer ventas con productos\n",
    "df_ventas_enriched = transformer.enrich_data(\n",
    "    df_ventas_clean, \n",
    "    df_productos_clean, \n",
    "    on='producto_id'\n",
    ")\n",
    "\n",
    "# 5. Enriquecer con clientes\n",
    "df_ventas_final = transformer.enrich_data(\n",
    "    df_ventas_enriched,\n",
    "    df_clientes_clean[['cliente_id', 'segmento', 'ciudad', 'pais']],\n",
    "    on='cliente_id'\n",
    ")\n",
    "\n",
    "# 6. Calcular m\u00e9tricas adicionales\n",
    "df_ventas_final['margen'] = (df_ventas_final['total'] - (df_ventas_final['precio'] * df_ventas_final['cantidad'])) / df_ventas_final['total'] * 100\n",
    "df_ventas_final['mes'] = df_ventas_final['fecha_venta'].dt.to_period('M').astype(str)\n",
    "df_ventas_final['trimestre'] = df_ventas_final['fecha_venta'].dt.to_period('Q').astype(str)\n",
    "\n",
    "print(\"\\n\u2705 TRANSFORMACIONES COMPLETADAS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Registros finales: {len(df_ventas_final)}\")\n",
    "print(f\"Columnas totales: {len(df_ventas_final.columns)}\")\n",
    "print(f\"\\nPrimeras columnas:\")\n",
    "print(df_ventas_final.columns.tolist()[:10])\n",
    "\n",
    "display(df_ventas_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937942",
   "metadata": {},
   "source": [
    "## 4. Validaci\u00f3n de Calidad de Datos\n",
    "\n",
    "### 4.1 Clase DataQualityValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityValidator:\n",
    "    \"\"\"\n",
    "    Validador de calidad de datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.issues = []\n",
    "    \n",
    "    def validate_completeness(self, df: pd.DataFrame, threshold: float = 0.95) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los datos est\u00e9n completos (sin nulos).\n",
    "        \"\"\"\n",
    "        total_values = df.size\n",
    "        non_null_values = df.count().sum()\n",
    "        completeness = non_null_values / total_values\n",
    "        \n",
    "        if completeness < threshold:\n",
    "            msg = f\"\u274c Completitud baja: {completeness:.2%} (umbral: {threshold:.2%})\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"\u2705 Completitud OK: {completeness:.2%}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_uniqueness(self, df: pd.DataFrame, columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Valida unicidad en columnas clave.\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns).sum()\n",
    "        \n",
    "        if duplicates > 0:\n",
    "            msg = f\"\u274c Duplicados encontrados: {duplicates} en {columns}\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"\u2705 Unicidad OK en {columns}\")\n",
    "            return True\n",
    "    \n",
    "    def validate_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Valida que los valores est\u00e9n en un rango v\u00e1lido.\n",
    "        \"\"\"\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            msg = f\"\u274c {len(out_of_range)} valores fuera de rango en '{column}' [{min_val}, {max_val}]\"\n",
    "            self.issues.append(msg)\n",
    "            self.logger.warning(msg)\n",
    "            return False\n",
    "        else:\n",
    "            self.logger.info(f\"\u2705 Rango OK en '{column}'\")\n",
    "            return True\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera reporte de calidad.\n",
    "        \"\"\"\n",
    "        report = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "        report += \"\ud83d\udcca REPORTE DE CALIDAD DE DATOS\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\"\n",
    "        \n",
    "        if not self.issues:\n",
    "            report += \"\\n\u2705 No se encontraron problemas de calidad\\n\"\n",
    "        else:\n",
    "            report += f\"\\n\u26a0\ufe0f Se encontraron {len(self.issues)} problemas:\\n\\n\"\n",
    "            for i, issue in enumerate(self.issues, 1):\n",
    "                report += f\"{i}. {issue}\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\" * 60\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"\u2705 Clase DataQualityValidator definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ab48a",
   "metadata": {},
   "source": [
    "### \u2705 DataQualityValidator: Validaciones Automatizadas\n",
    "\n",
    "**Concepto:** Validador que verifica calidad de datos antes de cargar a destino.\n",
    "\n",
    "**Validaciones t\u00edpicas:**\n",
    "- **Completeness:** % valores no nulos \u2265 threshold\n",
    "- **Uniqueness:** no duplicados en claves primarias\n",
    "- **Accuracy:** valores dentro de rangos v\u00e1lidos\n",
    "- **Consistency:** formatos y tipos de datos correctos\n",
    "\n",
    "**Patr\u00f3n:**\n",
    "- Acumular issues en lista\n",
    "- Retornar bool (pass/fail)\n",
    "- Logear cada validaci\u00f3n\n",
    "- Detener pipeline si validaciones cr\u00edticas fallan\n",
    "\n",
    "**Uso:** Gate de calidad antes de stage \"Load\", esencial en producci\u00f3n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c025fa",
   "metadata": {},
   "source": [
    "### 4.2 Ejecutar Validaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear validador\n",
    "validator = DataQualityValidator()\n",
    "\n",
    "print(\"\ud83d\udd0d VALIDANDO CALIDAD DE DATOS\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "validator.validate_completeness(df_ventas_final, threshold=0.90)\n",
    "\n",
    "# 2. Unicidad en ventas\n",
    "validator.validate_uniqueness(df_ventas_final, ['venta_id'])\n",
    "\n",
    "# 3. Rangos v\u00e1lidos\n",
    "validator.validate_ranges(df_ventas_final, 'cantidad', min_val=0, max_val=100)\n",
    "validator.validate_ranges(df_ventas_final, 'total', min_val=0, max_val=100000)\n",
    "\n",
    "# Generar reporte\n",
    "report = validator.generate_report()\n",
    "print(report)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_calidad.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "logger.info(\"Reporte de calidad generado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab51ae",
   "metadata": {},
   "source": [
    "## 5. An\u00e1lisis y Agregaciones\n",
    "\n",
    "### 5.1 M\u00e9tricas de Negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bef04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs principales\n",
    "kpis = {\n",
    "    'total_ventas': df_ventas_final['total'].sum(),\n",
    "    'num_transacciones': len(df_ventas_final),\n",
    "    'ticket_promedio': df_ventas_final['total'].mean(),\n",
    "    'ticket_mediano': df_ventas_final['total'].median(),\n",
    "    'clientes_unicos': df_ventas_final['cliente_id'].nunique(),\n",
    "    'productos_vendidos': df_ventas_final['producto'].nunique(),\n",
    "    'margen_promedio': df_ventas_final['margen'].mean()\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udcca KPIs PRINCIPALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\ud83d\udcb0 Total Ventas: ${kpis['total_ventas']:,.2f}\")\n",
    "print(f\"\ud83d\uded2 Transacciones: {kpis['num_transacciones']:,}\")\n",
    "print(f\"\ud83d\udcc8 Ticket Promedio: ${kpis['ticket_promedio']:.2f}\")\n",
    "print(f\"\ud83d\udcca Ticket Mediano: ${kpis['ticket_mediano']:.2f}\")\n",
    "print(f\"\ud83d\udc65 Clientes \u00danicos: {kpis['clientes_unicos']}\")\n",
    "print(f\"\ud83d\udce6 Productos Vendidos: {kpis['productos_vendidos']}\")\n",
    "print(f\"\ud83d\udcb9 Margen Promedio: {kpis['margen_promedio']:.2f}%\")\n",
    "\n",
    "# Guardar KPIs\n",
    "with open('../../outputs/proyecto_1/kpis.json', 'w') as f:\n",
    "    json.dump(kpis, f, indent=2)\n",
    "\n",
    "logger.info(\"KPIs calculados y guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e3bd",
   "metadata": {},
   "source": [
    "### 5.2 Agregaciones por Dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por categor\u00eda\n",
    "ventas_categoria = df_ventas_final.groupby('categoria').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count',\n",
    "    'margen': 'mean'\n",
    "}).round(2)\n",
    "ventas_categoria.columns = ['total_ventas', 'num_ventas', 'margen_promedio']\n",
    "ventas_categoria = ventas_categoria.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca VENTAS POR CATEGOR\u00cdA\")\n",
    "display(ventas_categoria)\n",
    "\n",
    "# Ventas por segmento de cliente\n",
    "ventas_segmento = df_ventas_final.groupby('segmento').agg({\n",
    "    'total': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "ventas_segmento.columns = ['total_ventas', 'ticket_promedio', 'num_transacciones']\n",
    "ventas_segmento = ventas_segmento.sort_values('total_ventas', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udc65 VENTAS POR SEGMENTO DE CLIENTE\")\n",
    "display(ventas_segmento)\n",
    "\n",
    "# Ventas por mes\n",
    "ventas_mes = df_ventas_final.groupby('mes').agg({\n",
    "    'total': 'sum',\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "ventas_mes.columns = ['total_ventas', 'num_ventas']\n",
    "\n",
    "print(\"\\n\ud83d\udcc5 VENTAS POR MES\")\n",
    "display(ventas_mes)\n",
    "\n",
    "# Guardar agregaciones\n",
    "ventas_categoria.to_csv('../../outputs/proyecto_1/ventas_categoria.csv')\n",
    "ventas_segmento.to_csv('../../outputs/proyecto_1/ventas_segmento.csv')\n",
    "ventas_mes.to_csv('../../outputs/proyecto_1/ventas_mes.csv')\n",
    "\n",
    "logger.info(\"Agregaciones calculadas y guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b58527",
   "metadata": {},
   "source": [
    "### \ud83d\udcca An\u00e1lisis Dimensional: Agregaciones de Negocio\n",
    "\n",
    "**Concepto:** Agregar m\u00e9tricas por dimensiones de negocio para insights accionables.\n",
    "\n",
    "**Agregaciones t\u00edpicas:**\n",
    "- **groupby().agg():** m\u00faltiples funciones (sum, mean, count) por grupo\n",
    "- **Dimensiones:** categor\u00eda, segmento, tiempo, geograf\u00eda\n",
    "- **M\u00e9tricas:** ventas totales, ticket promedio, cantidad transacciones\n",
    "\n",
    "**Patr\u00f3n:**\n",
    "```python\n",
    "df.groupby('dimension').agg({\n",
    "    'metrica1': 'sum',\n",
    "    'metrica2': 'mean'\n",
    "})\n",
    "```\n",
    "\n",
    "**Output:** Tablas agregadas base para reportes, dashboards y decisiones de negocio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a7c8",
   "metadata": {},
   "source": [
    "## 6. Visualizaci\u00f3n de Resultados\n",
    "\n",
    "### 6.1 Dashboard de KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. KPIs principales (arriba)\n",
    "ax_kpi = fig.add_subplot(gs[0, :])\n",
    "ax_kpi.axis('off')\n",
    "\n",
    "kpi_texts = [\n",
    "    ('\ud83d\udcb0 Total Ventas', f\"${kpis['total_ventas']:,.0f}\"),\n",
    "    ('\ud83d\uded2 Transacciones', f\"{kpis['num_transacciones']:,}\"),\n",
    "    ('\ud83d\udcca Ticket Prom.', f\"${kpis['ticket_promedio']:.2f}\"),\n",
    "    ('\ud83d\udc65 Clientes', f\"{kpis['clientes_unicos']}\"),\n",
    "    ('\ud83d\udcb9 Margen', f\"{kpis['margen_promedio']:.1f}%\")\n",
    "]\n",
    "\n",
    "for i, (titulo, valor) in enumerate(kpi_texts):\n",
    "    x_pos = 0.1 + i * 0.18\n",
    "    ax_kpi.text(x_pos, 0.7, titulo, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax_kpi.text(x_pos, 0.3, valor, ha='center', fontsize=16, fontweight='bold', color='#2E86AB')\n",
    "\n",
    "# 2. Ventas por categor\u00eda\n",
    "ax1 = fig.add_subplot(gs[1, :])\n",
    "ventas_categoria['total_ventas'].plot(kind='barh', ax=ax1, color='#6C5CE7')\n",
    "ax1.set_title('\ud83d\udcbc Ventas por Categor\u00eda', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Total Ventas ($)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Ventas por segmento (pie)\n",
    "ax2 = fig.add_subplot(gs[2, 0])\n",
    "ventas_segmento['total_ventas'].plot(kind='pie', ax=ax2, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('\ud83d\udc65 Ventas por Segmento', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# 4. Ventas por mes\n",
    "ax3 = fig.add_subplot(gs[2, 1])\n",
    "ventas_mes['total_ventas'].plot(kind='bar', ax=ax3, color='#00B894')\n",
    "ax3.set_title('\ud83d\udcc5 Ventas por Mes', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Mes')\n",
    "ax3.set_ylabel('Total ($)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top productos\n",
    "ax4 = fig.add_subplot(gs[2, 2])\n",
    "top_productos = df_ventas_final.groupby('producto')['total'].sum().nlargest(5)\n",
    "top_productos.plot(kind='barh', ax=ax4, color='#FD79A8')\n",
    "ax4.set_title('\ud83c\udfc6 Top 5 Productos', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Ventas ($)')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# T\u00edtulo principal\n",
    "fig.suptitle('\ud83d\udcca DASHBOARD DE VENTAS - PROYECTO INTEGRADOR 1', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('../../outputs/proyecto_1/dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Dashboard generado y guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860451f",
   "metadata": {},
   "source": [
    "### \ud83d\udcc8 Dashboard Ejecutivo: Visualizaci\u00f3n Integrada\n",
    "\n",
    "**Concepto:** Dashboard multi-panel que presenta KPIs y m\u00e9tricas de negocio en una sola vista.\n",
    "\n",
    "**Estructura con GridSpec:**\n",
    "- **Fila 1:** KPIs principales (n\u00fameros grandes)\n",
    "- **Fila 2:** Gr\u00e1fico principal (ventas por categor\u00eda)\n",
    "- **Fila 3:** M\u00faltiples vistas (segmento, temporal, productos)\n",
    "\n",
    "**Tipos de gr\u00e1ficos:**\n",
    "- Barras horizontales: rankings y comparaciones\n",
    "- Pie chart: proporciones y distribuciones\n",
    "- Barras verticales: series temporales\n",
    "- KPI cards: m\u00e9tricas clave destacadas\n",
    "\n",
    "**Objetivo:** Vista ejecutiva one-pager para decisiones r\u00e1pidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34f110",
   "metadata": {},
   "source": [
    "## 7. Carga de Datos (Load)\n",
    "\n",
    "### 7.1 Cargar a Base de Datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexi\u00f3n a SQLite\n",
    "db_path = '../../outputs/proyecto_1/ventas_analytics.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Cargar datos transformados\n",
    "    df_ventas_final.to_sql('ventas', conn, if_exists='replace', index=False)\n",
    "    logger.info(f\"\u2705 Tabla 'ventas' cargada: {len(df_ventas_final)} registros\")\n",
    "    \n",
    "    # Cargar agregaciones\n",
    "    ventas_categoria.to_sql('ventas_categoria', conn, if_exists='replace')\n",
    "    logger.info(\"\u2705 Tabla 'ventas_categoria' cargada\")\n",
    "    \n",
    "    ventas_segmento.to_sql('ventas_segmento', conn, if_exists='replace')\n",
    "    logger.info(\"\u2705 Tabla 'ventas_segmento' cargada\")\n",
    "    \n",
    "    print(\"\\n\u2705 DATOS CARGADOS EXITOSAMENTE EN SQLite\")\n",
    "    print(f\"\ud83d\udcc1 Base de datos: {db_path}\")\n",
    "    \n",
    "    # Verificar tablas\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tablas = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Tablas creadas:\")\n",
    "    for tabla in tablas:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {tabla[0]}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  \u2022 {tabla[0]}: {count} registros\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "logger.info(\"Carga completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c2b0b",
   "metadata": {},
   "source": [
    "### \ud83d\udcbe Load Stage: Persistencia en Base de Datos\n",
    "\n",
    "**Concepto:** \u00daltima etapa del ETL - cargar datos transformados y validados a destino permanente.\n",
    "\n",
    "**M\u00e9todo `to_sql()`:**\n",
    "- Crea tabla autom\u00e1ticamente desde DataFrame\n",
    "- `if_exists='replace'`: sobrescribe tabla existente\n",
    "- `index=False`: no guardar \u00edndice como columna\n",
    "\n",
    "**M\u00faltiples tablas:**\n",
    "- **Datos detallados:** ventas completas (fact table)\n",
    "- **Agregaciones:** tablas pre-calculadas para queries r\u00e1pidos\n",
    "\n",
    "**Verificaci\u00f3n:** Query de conteo para confirmar carga exitosa.\n",
    "\n",
    "**Formato alternativo:** Parquet para data lakes, mejor compresi\u00f3n y velocidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332031",
   "metadata": {},
   "source": [
    "### 7.2 Exportar a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a Parquet para an\u00e1lisis\n",
    "parquet_path = '../../outputs/proyecto_1/ventas_final.parquet'\n",
    "df_ventas_final.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"\u2705 Datos exportados a Parquet: {parquet_path}\")\n",
    "print(f\"\ud83d\udcca Tama\u00f1o del archivo: {os.path.getsize(parquet_path) / 1024:.2f} KB\")\n",
    "\n",
    "logger.info(\"Exportaci\u00f3n a Parquet completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58316",
   "metadata": {},
   "source": [
    "## 8. Reporte Final\n",
    "\n",
    "### 8.1 Generar Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abca488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "reporte = f\"\"\"\n",
    "{'='*80}\n",
    "\ud83d\udcca PROYECTO INTEGRADOR 1: PIPELINE ETL COMPLETO\n",
    "{'='*80}\n",
    "\n",
    "Fecha de ejecuci\u00f3n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "1. EXTRACCI\u00d3N\n",
    "{'='*80}\n",
    "\n",
    "Fuentes de datos procesadas:\n",
    "  \u2022 Ventas (CSV): {len(df_ventas)} registros\n",
    "  \u2022 Productos (CSV): {len(df_productos)} registros\n",
    "  \u2022 Clientes (CSV): {len(df_clientes)} registros\n",
    "  \u2022 Usuarios API: {len(df_usuarios_api)} registros\n",
    "\n",
    "{'='*80}\n",
    "2. TRANSFORMACI\u00d3N\n",
    "{'='*80}\n",
    "\n",
    "Transformaciones aplicadas:\n",
    "  \u2705 Limpieza de valores nulos\n",
    "  \u2705 Normalizaci\u00f3n de fechas\n",
    "  \u2705 Eliminaci\u00f3n de duplicados\n",
    "  \u2705 Enriquecimiento con datos relacionados\n",
    "  \u2705 C\u00e1lculo de m\u00e9tricas derivadas (margen, agregaciones temporales)\n",
    "\n",
    "Registros finales: {len(df_ventas_final)}\n",
    "Columnas totales: {len(df_ventas_final.columns)}\n",
    "\n",
    "{'='*80}\n",
    "3. KPIs PRINCIPALES\n",
    "{'='*80}\n",
    "\n",
    "\ud83d\udcb0 Total Ventas: ${kpis['total_ventas']:,.2f}\n",
    "\ud83d\uded2 Transacciones: {kpis['num_transacciones']:,}\n",
    "\ud83d\udcc8 Ticket Promedio: ${kpis['ticket_promedio']:.2f}\n",
    "\ud83d\udcca Ticket Mediano: ${kpis['ticket_mediano']:.2f}\n",
    "\ud83d\udc65 Clientes \u00danicos: {kpis['clientes_unicos']}\n",
    "\ud83d\udce6 Productos Vendidos: {kpis['productos_vendidos']}\n",
    "\ud83d\udcb9 Margen Promedio: {kpis['margen_promedio']:.2f}%\n",
    "\n",
    "{'='*80}\n",
    "4. TOP INSIGHTS\n",
    "{'='*80}\n",
    "\n",
    "\ud83c\udfc6 Categor\u00eda con m\u00e1s ventas: {ventas_categoria.index[0]} (${ventas_categoria.iloc[0]['total_ventas']:,.2f})\n",
    "\ud83d\udc54 Segmento m\u00e1s valioso: {ventas_segmento.index[0]} (${ventas_segmento.iloc[0]['total_ventas']:,.2f})\n",
    "\ud83d\udcc5 Mes con m\u00e1s ventas: {ventas_mes['total_ventas'].idxmax()} (${ventas_mes['total_ventas'].max():,.2f})\n",
    "\n",
    "{'='*80}\n",
    "5. CALIDAD DE DATOS\n",
    "{'='*80}\n",
    "\n",
    "{validator.generate_report()}\n",
    "\n",
    "{'='*80}\n",
    "6. ARCHIVOS GENERADOS\n",
    "{'='*80}\n",
    "\n",
    "\ud83d\udcc1 Base de datos: ventas_analytics.db\n",
    "\ud83d\udcc4 Parquet: ventas_final.parquet\n",
    "\ud83d\udcca Dashboard: dashboard.png\n",
    "\ud83d\udcc8 CSV Agregados:\n",
    "   \u2022 ventas_categoria.csv\n",
    "   \u2022 ventas_segmento.csv\n",
    "   \u2022 ventas_mes.csv\n",
    "\ud83d\udcdd KPIs: kpis.json\n",
    "\ud83d\udccb Logs: proyecto_integrador_1.log\n",
    "\n",
    "{'='*80}\n",
    "\u2705 PIPELINE COMPLETADO EXITOSAMENTE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(reporte)\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../../outputs/proyecto_1/reporte_final.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(reporte)\n",
    "\n",
    "logger.info(\"Reporte final generado\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PROYECTO INTEGRADOR 1 COMPLETADO EXITOSAMENTE\")\n",
    "logger.info(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2966",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Conclusiones del Proyecto\n",
    "\n",
    "### \u2705 Lo que Lograste:\n",
    "\n",
    "1. **Extracci\u00f3n Multi-Fuente**: CSV, API REST, SQLite\n",
    "2. **Transformaciones Robustas**: Limpieza, normalizaci\u00f3n, enriquecimiento\n",
    "3. **Validaci\u00f3n de Calidad**: Checks autom\u00e1ticos de completitud, unicidad, rangos\n",
    "4. **An\u00e1lisis Avanzado**: KPIs, agregaciones por dimensiones\n",
    "5. **Visualizaci\u00f3n Ejecutiva**: Dashboard completo con insights clave\n",
    "6. **Carga Optimizada**: SQLite para queries, Parquet para an\u00e1lisis\n",
    "7. **Logging Profesional**: Trazabilidad completa del proceso\n",
    "\n",
    "### \ud83c\udfaf Habilidades Desarrolladas:\n",
    "\n",
    "- \u2705 Dise\u00f1o de arquitecturas ETL\n",
    "- \u2705 Programaci\u00f3n orientada a objetos\n",
    "- \u2705 Manejo de errores y logging\n",
    "- \u2705 Calidad de datos\n",
    "- \u2705 Visualizaci\u00f3n de insights\n",
    "- \u2705 Documentaci\u00f3n de pipelines\n",
    "\n",
    "### \ud83d\ude80 Pr\u00f3ximos Pasos:\n",
    "\n",
    "1. **Automatizaci\u00f3n**: Usar Apache Airflow para scheduling\n",
    "2. **Escalabilidad**: Migrar a Spark para vol\u00famenes grandes\n",
    "3. **Cloud**: Desplegar en AWS/GCP/Azure\n",
    "4. **Monitoreo**: Implementar alertas y m\u00e9tricas de performance\n",
    "5. **CI/CD**: Automatizar tests y despliegues\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf8a \u00a1FELICITACIONES! Has completado tu primer proyecto integrador de Ingenier\u00eda de Datos.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udf10 APIs REST y Web Scraping para Ingenier\u00eda de Datos](08_apis_web_scraping.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83d\ude80 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas \u2192](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Junior:**\n",
    "- [\ud83d\udcca Junior - 01. Introducci\u00f3n a la Ingenier\u00eda de Datos](01_introduccion_ingenieria_datos.ipynb)\n",
    "- [\ud83d\udc0d Junior - 02. Python para Manipulaci\u00f3n de Datos](02_python_manipulacion_datos.ipynb)\n",
    "- [Pandas: Fundamentos para An\u00e1lisis de Datos](03_pandas_fundamentos.ipynb)\n",
    "- [SQL B\u00e1sico para Ingenier\u00eda de Datos](04_sql_basico.ipynb)\n",
    "- [Limpieza y Preparaci\u00f3n de Datos](05_limpieza_datos.ipynb)\n",
    "- [\ud83d\udcca Visualizaci\u00f3n de Datos en Ingenier\u00eda de Datos](06_visualizacion_datos.ipynb)\n",
    "- [\ud83d\udd04 Git y Control de Versiones para Ingenier\u00eda de Datos](07_git_control_versiones.ipynb)\n",
    "- [\ud83c\udf10 APIs REST y Web Scraping para Ingenier\u00eda de Datos](08_apis_web_scraping.ipynb)\n",
    "- [\ud83c\udfaf Proyecto Integrador 1: Pipeline ETL Completo](09_proyecto_integrador_1.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83d\ude80 Proyecto Integrador 2: Pipeline Near Real-Time, Scheduling y Alertas](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
