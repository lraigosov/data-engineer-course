{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bceb9d",
   "metadata": {},
   "source": [
    "# \ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa\n",
    "\n",
    "Objetivo: dise\u00f1ar e implementar una plataforma moderna de datos con governance, lakehouse, orquestaci\u00f3n, observabilidad y compliance.\n",
    "\n",
    "- Duraci\u00f3n: 180+ min (proyecto multi-d\u00eda)\n",
    "- Dificultad: Muy Alta\n",
    "- Prerrequisitos: Todos los notebooks Senior 01\u201308"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f18c5",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Dise\u00f1o de Arquitectura: Patrones y Trade-offs**\n",
    "\n",
    "**1. Arquitectura Lambda vs Kappa: Hybrid Approach**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Modern Data Platform Architecture              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                              \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502  Real-Time   \u2502        \u2502    Batch     \u2502                  \u2502\n",
    "\u2502  \u2502  (Lambda)    \u2502        \u2502   (Airflow)  \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502         \u2502                       \u2502                           \u2502\n",
    "\u2502         \u25bc                       \u25bc                           \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n",
    "\u2502  \u2502     Unified Lakehouse Layer            \u2502                \u2502\n",
    "\u2502  \u2502  (Delta Lake - ACID + Time Travel)     \u2502                \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                \u2502\n",
    "\u2502  \u2502  raw/     (bronze - sin procesar)      \u2502                \u2502\n",
    "\u2502  \u2502  curated/ (silver - validado)          \u2502                \u2502\n",
    "\u2502  \u2502  gold/    (gold - agregado)            \u2502                \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n",
    "\u2502         \u2502                                                    \u2502\n",
    "\u2502         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n",
    "\u2502         \u25bc      \u25bc         \u25bc         \u25bc           \u25bc           \u2502\n",
    "\u2502     Athena  Trino   FastAPI   Tableau   ML Models          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**\u00bfPor qu\u00e9 Hybrid?**\n",
    "\n",
    "| Aspecto | Lambda (Streaming) | Batch (Airflow) |\n",
    "|---------|-------------------|-----------------|\n",
    "| **Latencia** | <30s | 1h-24h |\n",
    "| **Complejidad** | Alta (gesti\u00f3n de estado) | Media |\n",
    "| **Costos** | Alto (always-on) | Bajo (on-demand) |\n",
    "| **Casos de Uso** | Detecci\u00f3n fraude, alertas | Reportes, ML training |\n",
    "\n",
    "**Decisi\u00f3n:** Usamos streaming para datos cr\u00edticos (transacciones, eventos de usuario) y batch para agregaciones hist\u00f3ricas y ETL complejos.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Storage Layer: S3 + Delta Lake**\n",
    "\n",
    "**\u00bfPor qu\u00e9 no Parquet solo?**\n",
    "\n",
    "```python\n",
    "# Problema con Parquet puro\n",
    "# \u274c Sin transacciones ACID\n",
    "df1.write.parquet(\"s3://bucket/data/\")  # Job 1\n",
    "df2.write.parquet(\"s3://bucket/data/\")  # Job 2 (concurrent) \u2192 CORRUPTION\n",
    "\n",
    "# \u2705 Delta Lake garantiza ACID\n",
    "df1.write.format(\"delta\").save(\"s3://bucket/data/\")\n",
    "df2.write.format(\"delta\").save(\"s3://bucket/data/\")  # Safe concurrent writes\n",
    "```\n",
    "\n",
    "**Delta Lake Features:**\n",
    "\n",
    "| Feature | Beneficio |\n",
    "|---------|-----------|\n",
    "| **ACID Transactions** | No m\u00e1s datos corruptos |\n",
    "| **Time Travel** | `SELECT * FROM delta.`table` VERSION AS OF 10` |\n",
    "| **Schema Evolution** | Add/remove columns sin reescribir |\n",
    "| **OPTIMIZE** | Compacta small files \u2192 mejor perf |\n",
    "| **VACUUM** | Elimina versiones antiguas \u2192 ahorra $$$ |\n",
    "| **MERGE** | Upserts eficientes (CDC) |\n",
    "\n",
    "**Estructura de Directorios:**\n",
    "\n",
    "```\n",
    "s3://ecommerce-data/\n",
    "\u251c\u2500\u2500 raw/                    # Bronze - Inmutable (retenci\u00f3n 7 d\u00edas)\n",
    "\u2502   \u251c\u2500\u2500 ventas/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 dt=2024-01-15/\n",
    "\u2502   \u2502       \u2514\u2500\u2500 kafka-offset-123.parquet\n",
    "\u2502   \u2514\u2500\u2500 logistica/\n",
    "\u2502       \u2514\u2500\u2500 dt=2024-01-15/\n",
    "\u2502           \u2514\u2500\u2500 sftp-file-001.csv\n",
    "\u2502\n",
    "\u251c\u2500\u2500 curated/                # Silver - Validado y limpio (retenci\u00f3n 1 a\u00f1o)\n",
    "\u2502   \u251c\u2500\u2500 ventas/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 _delta_log/    # Transaction log\n",
    "\u2502   \u2502   \u2514\u2500\u2500 region=LATAM/\n",
    "\u2502   \u2502       \u2514\u2500\u2500 dt=2024-01-15/\n",
    "\u2502   \u2502           \u2514\u2500\u2500 part-00000.parquet\n",
    "\u2502   \u2514\u2500\u2500 clientes/\n",
    "\u2502       \u251c\u2500\u2500 _delta_log/\n",
    "\u2502       \u2514\u2500\u2500 tipo=premium/\n",
    "\u2502           \u2514\u2500\u2500 dt=2024-01-15/\n",
    "\u2502\n",
    "\u2514\u2500\u2500 gold/                   # Gold - Agregado (retenci\u00f3n indefinida)\n",
    "    \u251c\u2500\u2500 ventas_diarias/\n",
    "    \u2502   \u2514\u2500\u2500 dt=2024-01-15/\n",
    "    \u2502       \u2514\u2500\u2500 region=LATAM_producto=laptop.parquet\n",
    "    \u2514\u2500\u2500 kpis/\n",
    "        \u2514\u2500\u2500 revenue_by_cohort/\n",
    "```\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "\n",
    "```python\n",
    "# \u274c Mal: Demasiadas particiones (small files)\n",
    "df.write.partitionBy(\"dt\", \"hora\", \"minuto\", \"cliente_id\").parquet(...)\n",
    "# Resultado: 1M+ archivos de 100KB \u2192 S3 list operations $$$\n",
    "\n",
    "# \u2705 Bien: Particiones equilibradas\n",
    "df.write.partitionBy(\"dt\", \"region\").format(\"delta\").save(...)\n",
    "# Resultado: ~100 archivos de 128MB (ideal para Spark)\n",
    "\n",
    "# Optimize small files\n",
    "spark.sql(\"OPTIMIZE delta.`s3://bucket/curated/ventas` ZORDER BY (cliente_id)\")\n",
    "# ZORDER: Co-locates data por cliente_id \u2192 10x faster filters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Compute Layer: Spark vs Pandas vs Polars**\n",
    "\n",
    "**Decision Matrix:**\n",
    "\n",
    "| Framework | Volume | Performance | Ecosystem |\n",
    "|-----------|--------|-------------|-----------|\n",
    "| **Pandas** | <10 GB | Baseline | Extensive |\n",
    "| **Polars** | <100 GB | 5-10x faster | Growing |\n",
    "| **Spark** | >100 GB | Scalable | Industry std |\n",
    "\n",
    "**Nuestra elecci\u00f3n:**\n",
    "- **Streaming:** Spark Structured Streaming (\u00fanica opci\u00f3n madura para Kafka)\n",
    "- **Batch simple:** Polars (reportes diarios <50 GB)\n",
    "- **Batch complejo:** Spark (joins grandes, ML pipelines)\n",
    "\n",
    "```python\n",
    "# Ejemplo: Batch con Polars (m\u00e1s r\u00e1pido que Pandas)\n",
    "import polars as pl\n",
    "\n",
    "df = pl.scan_parquet(\"s3://bucket/raw/ventas/*.parquet\")\n",
    "result = (\n",
    "    df\n",
    "    .filter(pl.col(\"monto\") > 100)\n",
    "    .groupby(\"region\")\n",
    "    .agg(pl.sum(\"monto\"))\n",
    "    .collect(streaming=True)  # Out-of-core processing\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Orchestration: Airflow vs Dagster vs Prefect**\n",
    "\n",
    "**Comparaci\u00f3n:**\n",
    "\n",
    "| Aspecto | Airflow | Dagster | Prefect |\n",
    "|---------|---------|---------|---------|\n",
    "| **Madurez** | \u2705 10+ a\u00f1os | \u26a0\ufe0f 5 a\u00f1os | \u26a0\ufe0f 5 a\u00f1os |\n",
    "| **Learning Curve** | Steep | Medium | Easy |\n",
    "| **Data Testing** | Manual (GE) | Native | Via hooks |\n",
    "| **Data Lineage** | Plugin (OpenLineage) | Native | Plugin |\n",
    "| **Deployment** | Complex | Docker-first | Cloud-native |\n",
    "| **Community** | Huge | Growing | Growing |\n",
    "\n",
    "**Decisi\u00f3n: Airflow**\n",
    "- \u2705 Ecosistema maduro (miles de operators)\n",
    "- \u2705 OpenLineage integration para linaje\n",
    "- \u2705 Skills existentes en el equipo\n",
    "- \u26a0\ufe0f Complejidad operativa (mitigada con MWAA/Astronomer)\n",
    "\n",
    "```python\n",
    "# Modern Airflow Pattern: TaskFlow API\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    schedule=\"@daily\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"ecommerce\", \"ventas\"]\n",
    ")\n",
    "def ventas_pipeline():\n",
    "    \n",
    "    @task\n",
    "    def extract():\n",
    "        return {\"path\": \"s3://bucket/raw/ventas/2024-01-15/\"}\n",
    "    \n",
    "    @task\n",
    "    def validate(data):\n",
    "        # Great Expectations\n",
    "        return {\"valid\": True, \"errors\": []}\n",
    "    \n",
    "    @task\n",
    "    def transform(data):\n",
    "        # Spark job\n",
    "        return {\"output\": \"s3://bucket/curated/ventas/\"}\n",
    "    \n",
    "    @task\n",
    "    def publish_lineage(data):\n",
    "        # OpenLineage event\n",
    "        pass\n",
    "    \n",
    "    data = extract()\n",
    "    validation = validate(data)\n",
    "    transformed = transform(validation)\n",
    "    publish_lineage(transformed)\n",
    "\n",
    "dag = ventas_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Governance: Glue Catalog vs Unity Catalog vs DataHub**\n",
    "\n",
    "**Comparaci\u00f3n:**\n",
    "\n",
    "|  | Glue Catalog | Unity Catalog | DataHub |\n",
    "|--|--------------|---------------|---------|\n",
    "| **Cloud** | AWS only | Databricks | Any |\n",
    "| **Lineage** | Limited | Column-level | Column-level |\n",
    "| **RBAC** | Lake Formation | Native | Native |\n",
    "| **API** | Boto3 | REST/SQL | GraphQL |\n",
    "| **Cost** | $1/million API calls | Included | Open-source |\n",
    "\n",
    "**Nuestra arquitectura h\u00edbrida:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         Governance Stack                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                          \u2502\n",
    "\u2502  Glue Catalog (Technical Metadata)      \u2502\n",
    "\u2502    - Table schemas                      \u2502\n",
    "\u2502    - Partitions                         \u2502\n",
    "\u2502    - Athena/Spark integration           \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502          \u2193 (sync via API)               \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  DataHub (Business Metadata)            \u2502\n",
    "\u2502    - Ownership (Data Engineer: Jane)    \u2502\n",
    "\u2502    - Tags (PII, GDPR, Confidential)     \u2502\n",
    "\u2502    - Glossary (ARR = Annual Revenue)    \u2502\n",
    "\u2502    - Lineage (upstream/downstream)      \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Sync Script:**\n",
    "\n",
    "```python\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import DatasetPropertiesClass\n",
    "import boto3\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "# Sync Glue \u2192 DataHub\n",
    "for table in glue.get_tables(DatabaseName='ecommerce')['TableList']:\n",
    "    dataset_urn = f\"urn:li:dataset:(urn:li:dataPlatform:glue,ecommerce.{table['Name']},PROD)\"\n",
    "    \n",
    "    properties = DatasetPropertiesClass(\n",
    "        customProperties={\n",
    "            \"glue_database\": \"ecommerce\",\n",
    "            \"owner\": table.get(\"Owner\", \"unknown\"),\n",
    "            \"location\": table[\"StorageDescriptor\"][\"Location\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    emitter.emit_mcp(\n",
    "        MetadataChangeProposalWrapper(\n",
    "            entityUrn=dataset_urn,\n",
    "            aspect=properties\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Trade-offs Cr\u00edticos**\n",
    "\n",
    "**Latencia vs Costo:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Streaming (Kafka + Spark)              \u2502\n",
    "\u2502  - Latencia: <30s                       \u2502\n",
    "\u2502  - Costo: $3,000/mes (EMR 24/7)         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2195 (decisi\u00f3n de negocio)\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Mini-batch (Airflow cada 5 min)        \u2502\n",
    "\u2502  - Latencia: 5-10 min                   \u2502\n",
    "\u2502  - Costo: $800/mes (spot instances)     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Pregunta clave:** \u00bfEl negocio necesita realmente <1 min de latencia?\n",
    "- Fraude detection: **S\u00ed** \u2192 Streaming\n",
    "- Reportes ejecutivos: **No** \u2192 Batch\n",
    "\n",
    "**Consistencia vs Disponibilidad (CAP Theorem):**\n",
    "\n",
    "```python\n",
    "# Opci\u00f3n 1: Fuerte consistencia (CP)\n",
    "# - Usar RDS/PostgreSQL para metadata cr\u00edtica\n",
    "# - Transacciones ACID garantizadas\n",
    "# - Downtime si network partition\n",
    "\n",
    "# Opci\u00f3n 2: Eventual consistency (AP)\n",
    "# - Usar S3 + DynamoDB\n",
    "# - Alta disponibilidad (99.99%)\n",
    "# - Posibles lecturas stale durante <1s\n",
    "\n",
    "# Decisi\u00f3n: Hybrid\n",
    "# - Transacciones financieras \u2192 RDS (CP)\n",
    "# - Eventos de usuario \u2192 S3 (AP)\n",
    "```\n",
    "\n",
    "**Open Source vs Managed Services:**\n",
    "\n",
    "| Componente | Open Source | Managed | Decisi\u00f3n |\n",
    "|------------|-------------|---------|----------|\n",
    "| **Kafka** | Self-hosted (EC2) | MSK | MSK (menos ops) |\n",
    "| **Airflow** | Docker Compose | MWAA | MWAA (scaling) |\n",
    "| **Spark** | EMR manual | EMR Serverless | EMR Serverless |\n",
    "| **DataHub** | K8s deployment | N/A | Self-hosted |\n",
    "\n",
    "**Criterio:** Si existe managed y costo <2x, usar managed (reduce toil).\n",
    "\n",
    "---\n",
    "\n",
    "**7. Disaster Recovery Strategy**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         Backup & Recovery               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                          \u2502\n",
    "\u2502  S3 (Data Lake)                         \u2502\n",
    "\u2502    - Versioning: Enabled                \u2502\n",
    "\u2502    - Replication: us-east-1 \u2192 eu-west-1 \u2502\n",
    "\u2502    - RTO: <1 hour                       \u2502\n",
    "\u2502    - RPO: <15 min                       \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  RDS (Metadata)                         \u2502\n",
    "\u2502    - Automated backups: Daily           \u2502\n",
    "\u2502    - Multi-AZ: Enabled                  \u2502\n",
    "\u2502    - RTO: <5 min (failover)             \u2502\n",
    "\u2502    - RPO: <5 min (sync replica)         \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  Glue Catalog                           \u2502\n",
    "\u2502    - Backup: CloudFormation export      \u2502\n",
    "\u2502    - Restore: boto3 script              \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Recovery Runbook:**\n",
    "\n",
    "```bash\n",
    "# Scenario: S3 bucket accidentally deleted\n",
    "# 1. Check versioning\n",
    "aws s3api list-object-versions --bucket ecommerce-data\n",
    "\n",
    "# 2. Restore from version\n",
    "aws s3api copy-object \\\n",
    "  --copy-source ecommerce-data/curated/ventas/file.parquet?versionId=abc123 \\\n",
    "  --bucket ecommerce-data-recovered \\\n",
    "  --key curated/ventas/file.parquet\n",
    "\n",
    "# 3. Failover to replica bucket\n",
    "# Update Glue catalog locations\n",
    "aws glue update-table --database ecommerce --table-input '{\n",
    "  \"Name\": \"ventas\",\n",
    "  \"StorageDescriptor\": {\n",
    "    \"Location\": \"s3://ecommerce-data-replica/curated/ventas/\"\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Scaling Strategy**\n",
    "\n",
    "**Current State (MVP):**\n",
    "- 100 GB/day ingest\n",
    "- 10 pipelines\n",
    "- 5 data engineers\n",
    "- $5K/month\n",
    "\n",
    "**12-month projection:**\n",
    "- 1 TB/day ingest (10x)\n",
    "- 50 pipelines (5x)\n",
    "- 15 engineers (3x)\n",
    "- Budget: $15K/month (3x)\n",
    "\n",
    "**Scaling plan:**\n",
    "\n",
    "```python\n",
    "# 1. Data scaling\n",
    "# - Partition strategy: dt + region (avoid skew)\n",
    "# - Compaction: OPTIMIZE every week\n",
    "# - Archival: Move raw to Glacier after 30 days\n",
    "\n",
    "# 2. Compute scaling\n",
    "# - Airflow: Celery executor (10 workers \u2192 50)\n",
    "# - Spark: EMR Serverless (auto-scale 1-100 instances)\n",
    "# - FastAPI: ECS Fargate (auto-scale on CPU)\n",
    "\n",
    "# 3. Team scaling\n",
    "# - Data Platform team: 2 SREs\n",
    "# - Domain teams: 3x Data Engineers per domain\n",
    "# - Self-service: DataHub + Airflow UI + Jupyter\n",
    "\n",
    "# 4. Cost optimization\n",
    "# - Spot instances: 70% savings on EMR\n",
    "# - S3 Intelligent Tiering: 40% savings\n",
    "# - Reserved capacity: Athena ($100/TB \u2192 $50/TB)\n",
    "```\n",
    "\n",
    "**Bottleneck analysis:**\n",
    "\n",
    "```python\n",
    "# \u00bfD\u00f3nde fallar\u00e1 primero?\n",
    "bottlenecks = {\n",
    "    \"S3 PUT rate\": \"3,500 req/s (\u2192 increase prefix diversity)\",\n",
    "    \"Glue Catalog API\": \"10 req/s (\u2192 cache with Redis)\",\n",
    "    \"Athena concurrency\": \"25 queries (\u2192 upgrade to Trino)\",\n",
    "    \"Airflow scheduler\": \"1000 tasks/s (\u2192 HA scheduler)\",\n",
    "    \"Team knowledge\": \"Onboarding (\u2192 internal wiki + mentorship)\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f4128",
   "metadata": {},
   "source": [
    "### \u26a1 **Streaming Path: Kafka \u2192 Spark \u2192 Delta Lake**\n",
    "\n",
    "**1. Kafka Setup: Event-Driven Architecture**\n",
    "\n",
    "**Docker Compose (Local Development):**\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "  \n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n",
    "      KAFKA_LOG_RETENTION_HOURS: 168  # 7 d\u00edas\n",
    "      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1 GB\n",
    "      KAFKA_COMPRESSION_TYPE: \"snappy\"\n",
    "  \n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.5.0\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092\n",
    "```\n",
    "\n",
    "**Production: AWS MSK Configuration:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "msk = boto3.client('kafka', region_name='us-east-1')\n",
    "\n",
    "# Create MSK cluster\n",
    "response = msk.create_cluster_v2(\n",
    "    ClusterName='ecommerce-kafka',\n",
    "    Serverless={\n",
    "        'VpcConfigs': [{\n",
    "            'SubnetIds': ['subnet-123', 'subnet-456'],\n",
    "            'SecurityGroupIds': ['sg-789']\n",
    "        }],\n",
    "        'ClientAuthentication': {\n",
    "            'Sasl': {\n",
    "                'Iam': {'Enabled': True}  # IAM authentication\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    Tags={\n",
    "        'Environment': 'production',\n",
    "        'CostCenter': 'data-platform'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Costo estimado: $2,500/mes (auto-scaling 2-16 brokers)\n",
    "```\n",
    "\n",
    "**Topic Design:**\n",
    "\n",
    "```python\n",
    "# topics.py\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n",
    "\n",
    "topics = [\n",
    "    NewTopic(\n",
    "        name='ecommerce.ventas.v1',\n",
    "        num_partitions=12,  # 12 partitions = 12 parallel consumers\n",
    "        replication_factor=3,  # High availability\n",
    "        topic_configs={\n",
    "            'retention.ms': '604800000',  # 7 d\u00edas\n",
    "            'cleanup.policy': 'delete',\n",
    "            'compression.type': 'snappy',\n",
    "            'max.message.bytes': '1048576',  # 1 MB\n",
    "        }\n",
    "    ),\n",
    "    NewTopic(\n",
    "        name='ecommerce.eventos_usuario.v1',\n",
    "        num_partitions=24,  # High throughput\n",
    "        replication_factor=3,\n",
    "        topic_configs={\n",
    "            'retention.ms': '86400000',  # 1 d\u00eda (eventos ef\u00edmeros)\n",
    "            'cleanup.policy': 'delete',\n",
    "            'compression.type': 'lz4',  # Mejor para logs\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "admin.create_topics(topics)\n",
    "```\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "\n",
    "```python\n",
    "# \u274c Mal: Random partitioning (no ordering)\n",
    "producer.send('ventas', value=mensaje)\n",
    "\n",
    "# \u2705 Bien: Key-based partitioning (ordering por cliente)\n",
    "producer.send(\n",
    "    'ventas',\n",
    "    key=str(cliente_id).encode('utf-8'),  # Mismo cliente \u2192 misma partition\n",
    "    value=json.dumps(mensaje).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Resultado: Eventos del cliente 123 siempre en partition 5\n",
    "# - Ordering garantizado por cliente\n",
    "# - Permite procesamiento stateful (aggregations)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Event Producer: Transacciones Sint\u00e9ticas**\n",
    "\n",
    "```python\n",
    "# producer.py\n",
    "from kafka import KafkaProducer\n",
    "from faker import Faker\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    acks='all',  # Wait for all replicas\n",
    "    retries=3,\n",
    "    max_in_flight_requests_per_connection=5,\n",
    "    compression_type='snappy'\n",
    ")\n",
    "\n",
    "def generate_transaction():\n",
    "    \"\"\"Genera transacci\u00f3n sint\u00e9tica realista\"\"\"\n",
    "    cliente_id = random.randint(1000, 9999)\n",
    "    productos = random.choices(\n",
    "        ['laptop', 'phone', 'tablet', 'headphones', 'monitor'],\n",
    "        weights=[0.1, 0.4, 0.2, 0.2, 0.1],  # Phone m\u00e1s com\u00fan\n",
    "        k=random.randint(1, 3)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'transaccion_id': fake.uuid4(),\n",
    "        'cliente_id': cliente_id,\n",
    "        'email': fake.email(),\n",
    "        'productos': productos,\n",
    "        'total': round(sum(random.uniform(50, 2000) for _ in productos), 2),\n",
    "        'metodo_pago': random.choice(['credit_card', 'debit_card', 'paypal']),\n",
    "        'tarjeta_ultimos_4': fake.credit_card_number()[-4:],\n",
    "        'region': random.choice(['LATAM', 'NA', 'EU', 'APAC']),\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        'metadata': {\n",
    "            'ip': fake.ipv4(),\n",
    "            'user_agent': fake.user_agent()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def produce_events(rate_per_second=100):\n",
    "    \"\"\"Produce eventos a rate constante\"\"\"\n",
    "    interval = 1.0 / rate_per_second\n",
    "    \n",
    "    while True:\n",
    "        start = time.time()\n",
    "        \n",
    "        transaction = generate_transaction()\n",
    "        \n",
    "        # Send con callback para manejo de errores\n",
    "        future = producer.send(\n",
    "            'ecommerce.ventas.v1',\n",
    "            key=str(transaction['cliente_id']),\n",
    "            value=transaction\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            metadata = future.get(timeout=10)\n",
    "            print(f\"\u2705 Sent to partition {metadata.partition}, offset {metadata.offset}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error: {e}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        elapsed = time.time() - start\n",
    "        time.sleep(max(0, interval - elapsed))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\ud83d\ude80 Producing events at 100/sec...\")\n",
    "    produce_events(rate_per_second=100)\n",
    "    # 100 events/sec = 8.6M events/day\n",
    "```\n",
    "\n",
    "**Schema Evolution con Avro:**\n",
    "\n",
    "```python\n",
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "# Schema Registry: versioning autom\u00e1tico\n",
    "value_schema = avro.loads('''\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Venta\",\n",
    "  \"namespace\": \"com.ecommerce\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"transaccion_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"cliente_id\", \"type\": \"int\"},\n",
    "    {\"name\": \"total\", \"type\": \"double\"},\n",
    "    {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"},\n",
    "    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}  # Nuevo campo opcional\n",
    "  ]\n",
    "}\n",
    "''')\n",
    "\n",
    "producer = AvroProducer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': 'http://localhost:8081'\n",
    "}, default_value_schema=value_schema)\n",
    "\n",
    "# Consumers autom\u00e1ticamente validan schema\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Spark Structured Streaming Consumer**\n",
    "\n",
    "```python\n",
    "# streaming_consumer.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.streaming.kafka.consumer.poll.ms\", \"512\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema del evento\n",
    "schema = StructType([\n",
    "    StructField(\"transaccion_id\", StringType()),\n",
    "    StructField(\"cliente_id\", IntegerType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"productos\", ArrayType(StringType())),\n",
    "    StructField(\"total\", DoubleType()),\n",
    "    StructField(\"metodo_pago\", StringType()),\n",
    "    StructField(\"tarjeta_ultimos_4\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"user_agent\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "raw_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"ecommerce.ventas.v1\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed_stream = raw_stream \\\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\"data.*\", \"kafka_key\", \"topic\", \"partition\", \"offset\", \"kafka_timestamp\")\n",
    "\n",
    "# Data Quality: Filter invalid records\n",
    "validated_stream = parsed_stream \\\n",
    "    .filter(col(\"transaccion_id\").isNotNull()) \\\n",
    "    .filter(col(\"cliente_id\") > 0) \\\n",
    "    .filter(col(\"total\") >= 0) \\\n",
    "    .filter(col(\"timestamp\").isNotNull())\n",
    "\n",
    "# Enrich: Parse timestamp\n",
    "enriched_stream = validated_stream \\\n",
    "    .withColumn(\"processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# PII Masking (GDPR compliance)\n",
    "masked_stream = enriched_stream \\\n",
    "    .withColumn(\"email_masked\", \n",
    "        regexp_replace(col(\"email\"), r\"(?<=.{2}).(?=[^@]*?@)\", \"*\")\n",
    "    ) \\\n",
    "    .withColumn(\"tarjeta_masked\", \n",
    "        concat(lit(\"****-\"), col(\"tarjeta_ultimos_4\"))\n",
    "    ) \\\n",
    "    .drop(\"email\", \"tarjeta_ultimos_4\")\n",
    "\n",
    "# Write to Delta Lake (curated layer)\n",
    "query = masked_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://ecommerce-data/checkpoints/ventas/\") \\\n",
    "    .option(\"path\", \"s3://ecommerce-data/curated/ventas/\") \\\n",
    "    .partitionBy(\"event_date\", \"region\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Exactly-Once Semantics:**\n",
    "\n",
    "```python\n",
    "# Checkpoint garantiza idempotencia\n",
    "# - Kafka offsets guardados en checkpoint\n",
    "# - Si job falla, restart desde \u00faltimo offset\n",
    "# - Delta Lake transactional writes previenen duplicates\n",
    "\n",
    "# Verification query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        COUNT(DISTINCT transaccion_id) as unique_transactions,\n",
    "        COUNT(*) as total_rows\n",
    "    FROM delta.`s3://ecommerce-data/curated/ventas/`\n",
    "    GROUP BY event_date\n",
    "    HAVING COUNT(*) != COUNT(DISTINCT transaccion_id)\n",
    "\"\"\").show()\n",
    "# Si hay diferencia \u2192 investigar duplicates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Stateful Aggregations: Windowing**\n",
    "\n",
    "```python\n",
    "# Real-time aggregations con watermarking\n",
    "windowed_agg = masked_stream \\\n",
    "    .withWatermark(\"processed_at\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"processed_at\", \"5 minutes\"),\n",
    "        \"region\",\n",
    "        \"metodo_pago\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transacciones\"),\n",
    "        sum(\"total\").alias(\"revenue_total\"),\n",
    "        avg(\"total\").alias(\"ticket_promedio\"),\n",
    "        approx_count_distinct(\"cliente_id\").alias(\"clientes_unicos\")\n",
    "    )\n",
    "\n",
    "# Write aggregations to gold layer\n",
    "windowed_query = windowed_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\")  # Update existing windows \\\n",
    "    .option(\"checkpointLocation\", \"s3://ecommerce-data/checkpoints/ventas_agg/\") \\\n",
    "    .option(\"path\", \"s3://ecommerce-data/gold/ventas_realtime/\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Monitoring Dashboard Query:**\n",
    "\n",
    "```sql\n",
    "-- Athena query para dashboard en tiempo real\n",
    "SELECT \n",
    "    window.start as window_start,\n",
    "    region,\n",
    "    num_transacciones,\n",
    "    revenue_total,\n",
    "    ticket_promedio\n",
    "FROM gold.ventas_realtime\n",
    "WHERE window.start >= current_timestamp - interval '1' hour\n",
    "ORDER BY window.start DESC, revenue_total DESC\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Error Handling & Dead Letter Queue**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def write_to_dlq(df, error_type):\n",
    "    \"\"\"Escribe registros problem\u00e1ticos a Dead Letter Queue\"\"\"\n",
    "    df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"error_date\") \\\n",
    "        .save(f\"s3://ecommerce-data/dlq/{error_type}/\")\n",
    "\n",
    "# Captura schema mismatch\n",
    "try:\n",
    "    parsed_stream = raw_stream.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    )\n",
    "except AnalysisException as e:\n",
    "    # Log a DLQ para debugging\n",
    "    raw_stream.writeStream \\\n",
    "        .foreachBatch(lambda df, epoch_id: write_to_dlq(df, \"schema_error\")) \\\n",
    "        .start()\n",
    "\n",
    "# Alert on DLQ growth\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        error_type,\n",
    "        COUNT(*) as error_count\n",
    "    FROM parquet.`s3://ecommerce-data/dlq/*/*`\n",
    "    WHERE error_date = current_date()\n",
    "    GROUP BY error_type\n",
    "    HAVING COUNT(*) > 100  -- Alert threshold\n",
    "\"\"\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Performance Tuning**\n",
    "\n",
    "```python\n",
    "# Optimizaciones cr\u00edticas\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Memory tuning para streaming\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "spark.conf.set(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")  # State backend eficiente\n",
    "\n",
    "# Kafka optimizations\n",
    "spark.conf.set(\"spark.streaming.kafka.consumer.cache.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.kafka.consumer.poll.ms\", \"512\")\n",
    "\n",
    "# Delta Lake optimizations\n",
    "spark.sql(\"OPTIMIZE delta.`s3://ecommerce-data/curated/ventas/` ZORDER BY (cliente_id)\")\n",
    "spark.sql(\"VACUUM delta.`s3://ecommerce-data/curated/ventas/` RETAIN 168 HOURS\")  # 7 d\u00edas\n",
    "```\n",
    "\n",
    "**Metrics Collection:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "from prometheus_client import Counter, Gauge, Histogram\n",
    "\n",
    "# Prometheus metrics\n",
    "records_processed = Counter('streaming_records_processed_total', 'Records processed')\n",
    "batch_duration = Histogram('streaming_batch_duration_seconds', 'Batch processing time')\n",
    "current_lag = Gauge('streaming_kafka_lag', 'Kafka consumer lag')\n",
    "\n",
    "def process_batch(df: DataFrame, epoch_id: int):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Write logic\n",
    "    df.write.format(\"delta\").mode(\"append\").save(\"s3://...\")\n",
    "    \n",
    "    # Update metrics\n",
    "    records_processed.inc(df.count())\n",
    "    batch_duration.observe(time.time() - start)\n",
    "    \n",
    "    # Log progress\n",
    "    print(f\"Epoch {epoch_id}: {df.count()} records in {time.time()-start:.2f}s\")\n",
    "\n",
    "query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Deployment: EMR Serverless**\n",
    "\n",
    "```python\n",
    "# deploy_streaming.py\n",
    "import boto3\n",
    "\n",
    "emr_serverless = boto3.client('emr-serverless', region_name='us-east-1')\n",
    "\n",
    "# Create application\n",
    "app_response = emr_serverless.create_application(\n",
    "    name='ecommerce-streaming',\n",
    "    releaseLabel='emr-6.12.0',\n",
    "    type='SPARK',\n",
    "    autoStartConfiguration={'enabled': True},\n",
    "    autoStopConfiguration={\n",
    "        'enabled': True,\n",
    "        'idleTimeoutMinutes': 15\n",
    "    },\n",
    "    initialCapacity={\n",
    "        'DRIVER': {\n",
    "            'workerCount': 1,\n",
    "            'workerConfiguration': {\n",
    "                'cpu': '2 vCPU',\n",
    "                'memory': '8 GB'\n",
    "            }\n",
    "        },\n",
    "        'EXECUTOR': {\n",
    "            'workerCount': 10,\n",
    "            'workerConfiguration': {\n",
    "                'cpu': '4 vCPU',\n",
    "                'memory': '16 GB',\n",
    "                'disk': '100 GB'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    maximumCapacity={\n",
    "        'cpu': '200 vCPU',\n",
    "        'memory': '800 GB'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Submit streaming job\n",
    "job_response = emr_serverless.start_job_run(\n",
    "    applicationId=app_response['applicationId'],\n",
    "    executionRoleArn='arn:aws:iam::123456:role/EMRServerlessRole',\n",
    "    jobDriver={\n",
    "        'sparkSubmit': {\n",
    "            'entryPoint': 's3://ecommerce-code/streaming_consumer.py',\n",
    "            'sparkSubmitParameters': (\n",
    "                '--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension '\n",
    "                '--conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog '\n",
    "                '--packages io.delta:delta-core_2.12:2.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0'\n",
    "            )\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Job started: {job_response['jobRunId']}\")\n",
    "# Costo: ~$0.052/vCPU-hour + $0.0065/GB-hour\n",
    "# Estimado: $1,500/mes (24/7 con auto-scaling)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f997b",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Batch Path: Airflow + Great Expectations + DataHub**\n",
    "\n",
    "**1. Airflow DAG: Orchestration Best Practices**\n",
    "\n",
    "```python\n",
    "# dags/ventas_batch_pipeline.py\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.operators.emr import EmrServerlessStartJobOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import great_expectations as gx\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-platform',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['data-team@ecommerce.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'retry_exponential_backoff': True,\n",
    "    'max_retry_delay': timedelta(minutes=30),\n",
    "    'sla': timedelta(hours=2),  # SLA: pipeline completa en 2h\n",
    "}\n",
    "\n",
    "@dag(\n",
    "    dag_id='ventas_batch_pipeline_v1',\n",
    "    default_args=default_args,\n",
    "    description='Daily batch pipeline: SFTP \u2192 validate \u2192 transform \u2192 gold',\n",
    "    schedule='0 2 * * *',  # 2 AM UTC daily\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['ecommerce', 'ventas', 'batch', 'production'],\n",
    "    doc_md=\"\"\"\n",
    "    # Ventas Batch Pipeline\n",
    "    \n",
    "    ## Purpose\n",
    "    Daily ETL for historical sales data and aggregations.\n",
    "    \n",
    "    ## SLA\n",
    "    - Completion: 2 hours\n",
    "    - Data freshness: D+1 (next day at 4 AM)\n",
    "    \n",
    "    ## Dependencies\n",
    "    - Upstream: SFTP files from SAP (uploaded at 1 AM)\n",
    "    - Downstream: BI dashboards (Tableau refresh at 6 AM)\n",
    "    \n",
    "    ## Contacts\n",
    "    - Owner: Data Platform Team\n",
    "    - On-call: data-oncall@ecommerce.com\n",
    "    \"\"\"\n",
    ")\n",
    "def ventas_batch_pipeline():\n",
    "    \n",
    "    @task(task_id='check_sftp_files')\n",
    "    def check_sftp_files(ds):\n",
    "        \"\"\"Verifica que archivos SFTP existen\"\"\"\n",
    "        from airflow.providers.sftp.hooks.sftp import SFTPHook\n",
    "        \n",
    "        sftp = SFTPHook(sftp_conn_id='sap_sftp')\n",
    "        expected_files = [\n",
    "            f'/exports/ventas_{ds}.csv',\n",
    "            f'/exports/clientes_{ds}.csv',\n",
    "            f'/exports/productos_{ds}.csv'\n",
    "        ]\n",
    "        \n",
    "        missing = []\n",
    "        for file_path in expected_files:\n",
    "            if not sftp.path_exists(file_path):\n",
    "                missing.append(file_path)\n",
    "        \n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing files: {missing}\")\n",
    "        \n",
    "        return {\n",
    "            'files': expected_files,\n",
    "            'total_size': sum(sftp.get_file_size(f) for f in expected_files)\n",
    "        }\n",
    "    \n",
    "    @task(task_id='download_to_s3')\n",
    "    def download_to_s3(file_info, ds):\n",
    "        \"\"\"Descarga archivos de SFTP a S3 raw\"\"\"\n",
    "        from airflow.providers.sftp.hooks.sftp import SFTPHook\n",
    "        \n",
    "        sftp = SFTPHook(sftp_conn_id='sap_sftp')\n",
    "        s3 = S3Hook(aws_conn_id='aws_default')\n",
    "        \n",
    "        for file_path in file_info['files']:\n",
    "            local_path = f'/tmp/{file_path.split(\"/\")[-1]}'\n",
    "            s3_key = f'raw/ventas/dt={ds}/{file_path.split(\"/\")[-1]}'\n",
    "            \n",
    "            # Download from SFTP\n",
    "            sftp.retrieve_file(file_path, local_path)\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3.load_file(\n",
    "                filename=local_path,\n",
    "                key=s3_key,\n",
    "                bucket_name='ecommerce-data',\n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            print(f\"\u2705 Uploaded {file_path} \u2192 s3://ecommerce-data/{s3_key}\")\n",
    "        \n",
    "        return f's3://ecommerce-data/raw/ventas/dt={ds}/'\n",
    "    \n",
    "    @task(task_id='run_data_quality_checks')\n",
    "    def run_data_quality_checks(s3_path, ds):\n",
    "        \"\"\"Great Expectations validations\"\"\"\n",
    "        context = gx.get_context()\n",
    "        \n",
    "        # Data source (S3)\n",
    "        datasource = context.sources.add_pandas_s3(\n",
    "            name=\"s3_datasource\",\n",
    "            bucket=\"ecommerce-data\",\n",
    "            boto3_options={\"region_name\": \"us-east-1\"}\n",
    "        )\n",
    "        \n",
    "        # Expectations\n",
    "        expectations = [\n",
    "            # Completeness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_not_be_null',\n",
    "                'kwargs': {'column': 'transaccion_id'}\n",
    "            },\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_not_be_null',\n",
    "                'kwargs': {'column': 'cliente_id'}\n",
    "            },\n",
    "            # Validity\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_between',\n",
    "                'kwargs': {'column': 'total', 'min_value': 0, 'max_value': 100000}\n",
    "            },\n",
    "            # Uniqueness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_unique',\n",
    "                'kwargs': {'column': 'transaccion_id'}\n",
    "            },\n",
    "            # Freshness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_max_to_be_between',\n",
    "                'kwargs': {\n",
    "                    'column': 'fecha_transaccion',\n",
    "                    'min_value': ds,\n",
    "                    'max_value': ds\n",
    "                }\n",
    "            },\n",
    "            # Referential integrity\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_in_set',\n",
    "                'kwargs': {\n",
    "                    'column': 'region',\n",
    "                    'value_set': ['LATAM', 'NA', 'EU', 'APAC']\n",
    "                }\n",
    "            },\n",
    "            # Volume check\n",
    "            {\n",
    "                'expectation_type': 'expect_table_row_count_to_be_between',\n",
    "                'kwargs': {\n",
    "                    'min_value': 50000,  # At least 50K transactions\n",
    "                    'max_value': 1000000\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create suite\n",
    "        suite = context.add_expectation_suite(\n",
    "            expectation_suite_name=f\"ventas_suite_{ds}\"\n",
    "        )\n",
    "        \n",
    "        for exp in expectations:\n",
    "            suite.add_expectation(**exp)\n",
    "        \n",
    "        # Validate\n",
    "        validator = context.get_validator(\n",
    "            batch_request=datasource.get_asset(f\"raw/ventas/dt={ds}/ventas_{ds}.csv\"),\n",
    "            expectation_suite_name=suite.expectation_suite_name\n",
    "        )\n",
    "        \n",
    "        results = validator.validate()\n",
    "        \n",
    "        if not results.success:\n",
    "            # Log failures\n",
    "            failures = [r for r in results.results if not r.success]\n",
    "            print(f\"\u274c {len(failures)} validation failures:\")\n",
    "            for failure in failures:\n",
    "                print(f\"  - {failure.expectation_config.expectation_type}: {failure.result}\")\n",
    "            \n",
    "            raise ValueError(f\"Data quality checks failed: {len(failures)} issues\")\n",
    "        \n",
    "        print(f\"\u2705 All {len(expectations)} validation checks passed\")\n",
    "        return results.to_json_dict()\n",
    "    \n",
    "    @task.branch(task_id='check_validation_results')\n",
    "    def check_validation_results(validation_results):\n",
    "        \"\"\"Branch based on validation results\"\"\"\n",
    "        if validation_results['success']:\n",
    "            return 'transform_to_curated'\n",
    "        else:\n",
    "            return 'send_failure_alert'\n",
    "    \n",
    "    @task(task_id='transform_to_curated')\n",
    "    def transform_to_curated(s3_path, ds):\n",
    "        \"\"\"Spark job: raw \u2192 curated (cleaned + enriched)\"\"\"\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import *\n",
    "        \n",
    "        spark = SparkSession.builder.appName(\"VentasCurated\").getOrCreate()\n",
    "        \n",
    "        # Read raw\n",
    "        raw_df = spark.read.csv(f\"{s3_path}/ventas_{ds}.csv\", header=True, inferSchema=True)\n",
    "        \n",
    "        # Transformations\n",
    "        curated_df = raw_df \\\n",
    "            .filter(col(\"total\") > 0) \\\n",
    "            .withColumn(\"fecha_procesamiento\", current_date()) \\\n",
    "            .withColumn(\"year\", year(col(\"fecha_transaccion\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"fecha_transaccion\"))) \\\n",
    "            .withColumn(\"email_masked\", \n",
    "                regexp_replace(col(\"email\"), r\"(?<=.{2}).(?=[^@]*?@)\", \"*\")\n",
    "            ) \\\n",
    "            .withColumn(\"tarjeta_masked\", \n",
    "                concat(lit(\"****-\"), substring(col(\"tarjeta_numero\"), -4, 4))\n",
    "            ) \\\n",
    "            .drop(\"email\", \"tarjeta_numero\")\n",
    "        \n",
    "        # Enrich with dimension tables\n",
    "        clientes_df = spark.read.format(\"delta\").load(\"s3://ecommerce-data/curated/clientes/\")\n",
    "        productos_df = spark.read.format(\"delta\").load(\"s3://ecommerce-data/curated/productos/\")\n",
    "        \n",
    "        enriched_df = curated_df \\\n",
    "            .join(clientes_df, \"cliente_id\", \"left\") \\\n",
    "            .join(productos_df, \"producto_id\", \"left\")\n",
    "        \n",
    "        # Write to Delta (curated)\n",
    "        enriched_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"year\", \"month\", \"region\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(f\"s3://ecommerce-data/curated/ventas/dt={ds}/\")\n",
    "        \n",
    "        # Optimize\n",
    "        spark.sql(f\"\"\"\n",
    "            OPTIMIZE delta.`s3://ecommerce-data/curated/ventas/dt={ds}/`\n",
    "            ZORDER BY (cliente_id, producto_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        return {\n",
    "            'rows_processed': enriched_df.count(),\n",
    "            'partitions_written': enriched_df.select(\"region\").distinct().count()\n",
    "        }\n",
    "    \n",
    "    @task(task_id='create_gold_aggregations')\n",
    "    def create_gold_aggregations(curated_info, ds):\n",
    "        \"\"\"Create gold layer aggregations\"\"\"\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import *\n",
    "        \n",
    "        spark = SparkSession.builder.appName(\"VentasGold\").getOrCreate()\n",
    "        \n",
    "        df = spark.read.format(\"delta\").load(f\"s3://ecommerce-data/curated/ventas/dt={ds}/\")\n",
    "        \n",
    "        # Aggregation 1: Daily sales by region\n",
    "        daily_region = df.groupBy(\"fecha_transaccion\", \"region\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"num_transacciones\"),\n",
    "                sum(\"total\").alias(\"revenue_total\"),\n",
    "                avg(\"total\").alias(\"ticket_promedio\"),\n",
    "                countDistinct(\"cliente_id\").alias(\"clientes_unicos\")\n",
    "            )\n",
    "        \n",
    "        daily_region.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"fecha_transaccion\") \\\n",
    "            .save(\"s3://ecommerce-data/gold/ventas_diarias_region/\")\n",
    "        \n",
    "        # Aggregation 2: Product performance\n",
    "        product_perf = df.groupBy(\"producto_id\", \"producto_nombre\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"unidades_vendidas\"),\n",
    "                sum(\"total\").alias(\"revenue_total\"),\n",
    "                avg(\"total\").alias(\"precio_promedio\")\n",
    "            ) \\\n",
    "            .orderBy(desc(\"revenue_total\"))\n",
    "        \n",
    "        product_perf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"s3://ecommerce-data/gold/productos_performance/dt={ds}/\")\n",
    "        \n",
    "        # Aggregation 3: Customer RFM (Recency, Frequency, Monetary)\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        customer_rfm = df.groupBy(\"cliente_id\") \\\n",
    "            .agg(\n",
    "                max(\"fecha_transaccion\").alias(\"ultima_compra\"),\n",
    "                count(\"*\").alias(\"frecuencia\"),\n",
    "                sum(\"total\").alias(\"valor_monetario\")\n",
    "            ) \\\n",
    "            .withColumn(\"recencia_dias\", \n",
    "                datediff(lit(ds), col(\"ultima_compra\"))\n",
    "            ) \\\n",
    "            .withColumn(\"rfm_score\",\n",
    "                col(\"frecuencia\") * 0.3 + col(\"valor_monetario\") * 0.5 - col(\"recencia_dias\") * 0.2\n",
    "            )\n",
    "        \n",
    "        customer_rfm.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"s3://ecommerce-data/gold/customer_rfm/dt={ds}/\")\n",
    "        \n",
    "        return {\n",
    "            'aggregations_created': 3,\n",
    "            'gold_path': f\"s3://ecommerce-data/gold/\"\n",
    "        }\n",
    "    \n",
    "    @task(task_id='update_glue_catalog')\n",
    "    def update_glue_catalog(gold_info, ds):\n",
    "        \"\"\"Update Glue Data Catalog partitions\"\"\"\n",
    "        import boto3\n",
    "        \n",
    "        glue = boto3.client('glue', region_name='us-east-1')\n",
    "        \n",
    "        tables = [\n",
    "            ('ecommerce', 'ventas_diarias_region'),\n",
    "            ('ecommerce', 'productos_performance'),\n",
    "            ('ecommerce', 'customer_rfm')\n",
    "        ]\n",
    "        \n",
    "        for database, table in tables:\n",
    "            try:\n",
    "                # Add partition\n",
    "                glue.create_partition(\n",
    "                    DatabaseName=database,\n",
    "                    TableName=table,\n",
    "                    PartitionInput={\n",
    "                        'Values': [ds],\n",
    "                        'StorageDescriptor': {\n",
    "                            'Location': f\"s3://ecommerce-data/gold/{table}/dt={ds}/\",\n",
    "                            'InputFormat': 'org.apache.hadoop.mapred.SequenceFileInputFormat',\n",
    "                            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat',\n",
    "                            'SerdeInfo': {\n",
    "                                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "                print(f\"\u2705 Partition added: {database}.{table} dt={ds}\")\n",
    "            except glue.exceptions.AlreadyExistsException:\n",
    "                print(f\"\u26a0\ufe0f Partition already exists: {database}.{table} dt={ds}\")\n",
    "    \n",
    "    @task(task_id='emit_lineage_to_datahub')\n",
    "    def emit_lineage_to_datahub(gold_info, ds):\n",
    "        \"\"\"Emit data lineage to DataHub\"\"\"\n",
    "        from datahub.emitter.mce_builder import make_dataset_urn\n",
    "        from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "        from datahub.metadata.schema_classes import DatasetLineageTypeClass, UpstreamClass, UpstreamLineageClass\n",
    "        \n",
    "        emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "        \n",
    "        # Define lineage: raw \u2192 curated \u2192 gold\n",
    "        lineage_map = {\n",
    "            f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.curated.ventas.dt={ds},PROD)\": [\n",
    "                f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.raw.ventas.dt={ds},PROD)\"\n",
    "            ],\n",
    "            f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.gold.ventas_diarias_region,PROD)\": [\n",
    "                f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.curated.ventas.dt={ds},PROD)\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for downstream_urn, upstream_urns in lineage_map.items():\n",
    "            upstreams = [\n",
    "                UpstreamClass(\n",
    "                    dataset=urn,\n",
    "                    type=DatasetLineageTypeClass.TRANSFORMED\n",
    "                )\n",
    "                for urn in upstream_urns\n",
    "            ]\n",
    "            \n",
    "            lineage = UpstreamLineageClass(upstreams=upstreams)\n",
    "            \n",
    "            emitter.emit_mcp(\n",
    "                MetadataChangeProposalWrapper(\n",
    "                    entityUrn=downstream_urn,\n",
    "                    aspect=lineage\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"\u2705 Lineage emitted: {downstream_urn}\")\n",
    "    \n",
    "    @task(task_id='export_metrics_to_prometheus')\n",
    "    def export_metrics_to_prometheus(curated_info, gold_info, ds):\n",
    "        \"\"\"Push pipeline metrics to Prometheus Pushgateway\"\"\"\n",
    "        from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "        \n",
    "        registry = CollectorRegistry()\n",
    "        \n",
    "        # Define metrics\n",
    "        rows_processed = Gauge('airflow_ventas_rows_processed', 'Rows processed', registry=registry)\n",
    "        pipeline_duration = Gauge('airflow_ventas_pipeline_duration_seconds', 'Pipeline duration', registry=registry)\n",
    "        \n",
    "        rows_processed.set(curated_info['rows_processed'])\n",
    "        # Duration calculado por Airflow\n",
    "        \n",
    "        push_to_gateway('prometheus-pushgateway:9091', job='airflow_ventas_batch', registry=registry)\n",
    "    \n",
    "    @task(task_id='send_success_notification')\n",
    "    def send_success_notification(ds, **context):\n",
    "        \"\"\"Send Slack notification on success\"\"\"\n",
    "        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n",
    "        \n",
    "        slack = SlackWebhookHook(slack_webhook_conn_id='slack_data_notifications')\n",
    "        \n",
    "        execution_time = context['ti'].duration\n",
    "        \n",
    "        message = f\"\"\"\n",
    "        \u2705 *Ventas Batch Pipeline Succeeded*\n",
    "        \n",
    "        \u2022 Date: {ds}\n",
    "        \u2022 Duration: {execution_time:.0f}s\n",
    "        \u2022 Rows: {context['ti'].xcom_pull(task_ids='transform_to_curated')['rows_processed']:,}\n",
    "        \u2022 Dashboard: <https://grafana.ecommerce.com/d/ventas|View Metrics>\n",
    "        \"\"\"\n",
    "        \n",
    "        slack.send_text(message)\n",
    "    \n",
    "    @task(task_id='send_failure_alert', trigger_rule='one_failed')\n",
    "    def send_failure_alert(ds, **context):\n",
    "        \"\"\"Send alert on failure\"\"\"\n",
    "        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n",
    "        \n",
    "        slack = SlackWebhookHook(slack_webhook_conn_id='slack_data_alerts')\n",
    "        \n",
    "        failed_task = context['ti'].task_id\n",
    "        \n",
    "        message = f\"\"\"\n",
    "        \ud83d\udea8 *ALERT: Ventas Batch Pipeline Failed*\n",
    "        \n",
    "        \u2022 Date: {ds}\n",
    "        \u2022 Failed Task: {failed_task}\n",
    "        \u2022 Logs: <https://airflow.ecommerce.com/log?dag_id=ventas_batch_pipeline_v1&task_id={failed_task}|View Logs>\n",
    "        \u2022 Runbook: <https://wiki.ecommerce.com/runbooks/ventas-pipeline|Troubleshooting Guide>\n",
    "        \n",
    "        @data-oncall please investigate\n",
    "        \"\"\"\n",
    "        \n",
    "        slack.send_text(message)\n",
    "    \n",
    "    # Define task dependencies\n",
    "    file_info = check_sftp_files()\n",
    "    s3_path = download_to_s3(file_info)\n",
    "    validation_results = run_data_quality_checks(s3_path)\n",
    "    branch = check_validation_results(validation_results)\n",
    "    \n",
    "    curated_info = transform_to_curated(s3_path)\n",
    "    gold_info = create_gold_aggregations(curated_info)\n",
    "    \n",
    "    update_glue_catalog(gold_info)\n",
    "    emit_lineage_to_datahub(gold_info)\n",
    "    export_metrics_to_prometheus(curated_info, gold_info)\n",
    "    \n",
    "    success = send_success_notification()\n",
    "    failure = send_failure_alert()\n",
    "    \n",
    "    # Dependencies\n",
    "    branch >> [curated_info, failure]\n",
    "    curated_info >> gold_info >> [update_glue_catalog, emit_lineage_to_datahub, export_metrics_to_prometheus] >> success\n",
    "\n",
    "dag = ventas_batch_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Airflow Production Deployment: MWAA**\n",
    "\n",
    "```python\n",
    "# deploy_airflow.py\n",
    "import boto3\n",
    "\n",
    "mwaa = boto3.client('mwaa', region_name='us-east-1')\n",
    "\n",
    "# Create MWAA environment\n",
    "response = mwaa.create_environment(\n",
    "    Name='ecommerce-airflow-prod',\n",
    "    ExecutionRoleArn='arn:aws:iam::123456:role/MWAAExecutionRole',\n",
    "    SourceBucketArn='arn:aws:s3:::ecommerce-airflow-bucket',\n",
    "    DagS3Path='dags/',\n",
    "    PluginsS3Path='plugins.zip',\n",
    "    RequirementsS3Path='requirements.txt',\n",
    "    NetworkConfiguration={\n",
    "        'SubnetIds': ['subnet-123', 'subnet-456'],\n",
    "        'SecurityGroupIds': ['sg-789']\n",
    "    },\n",
    "    LoggingConfiguration={\n",
    "        'DagProcessingLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'SchedulerLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'TaskLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'WorkerLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'WebserverLogs': {'LogLevel': 'INFO', 'Enabled': True}\n",
    "    },\n",
    "    AirflowVersion='2.7.2',\n",
    "    EnvironmentClass='mw1.medium',  # 2 workers\n",
    "    MaxWorkers=10,\n",
    "    MinWorkers=2,\n",
    "    Schedulers=2,  # HA\n",
    "    AirflowConfigurationOptions={\n",
    "        'core.parallelism': '32',\n",
    "        'core.max_active_runs_per_dag': '3',\n",
    "        'scheduler.catchup_by_default': 'False',\n",
    "        'webserver.expose_config': 'True'\n",
    "    },\n",
    "    Tags={\n",
    "        'Environment': 'production',\n",
    "        'CostCenter': 'data-platform'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Costo: ~$1,000/mes (mw1.medium con 2-10 workers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d61235",
   "metadata": {},
   "source": [
    "### \ud83d\ude80 **Operacionalizaci\u00f3n y SRE: Production Readiness**\n",
    "\n",
    "**1. Infrastructure as Code: Terraform**\n",
    "\n",
    "```hcl\n",
    "# infrastructure/main.tf\n",
    "terraform {\n",
    "  required_version = \">= 1.5\"\n",
    "  \n",
    "  backend \"s3\" {\n",
    "    bucket = \"ecommerce-terraform-state\"\n",
    "    key    = \"data-platform/terraform.tfstate\"\n",
    "    region = \"us-east-1\"\n",
    "    encrypt = true\n",
    "    dynamodb_table = \"terraform-state-lock\"\n",
    "  }\n",
    "  \n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = var.region\n",
    "  \n",
    "  default_tags {\n",
    "    tags = {\n",
    "      Project     = \"data-platform\"\n",
    "      ManagedBy   = \"terraform\"\n",
    "      Environment = var.environment\n",
    "      CostCenter  = \"data-engineering\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# S3 Buckets with lifecycle policies\n",
    "module \"data_lake\" {\n",
    "  source = \"./modules/s3-datalake\"\n",
    "  \n",
    "  bucket_name = \"ecommerce-data-${var.environment}\"\n",
    "  \n",
    "  lifecycle_rules = [\n",
    "    {\n",
    "      id     = \"raw-retention\"\n",
    "      prefix = \"raw/\"\n",
    "      transitions = [\n",
    "        { days = 7, storage_class = \"INTELLIGENT_TIERING\" },\n",
    "        { days = 30, storage_class = \"GLACIER\" }\n",
    "      ]\n",
    "      expiration_days = 90\n",
    "    },\n",
    "    {\n",
    "      id     = \"curated-retention\"\n",
    "      prefix = \"curated/\"\n",
    "      transitions = [\n",
    "        { days = 90, storage_class = \"INTELLIGENT_TIERING\" }\n",
    "      ]\n",
    "      expiration_days = 365\n",
    "    }\n",
    "  ]\n",
    "  \n",
    "  versioning_enabled = true\n",
    "  replication_config = {\n",
    "    enabled = true\n",
    "    destination_bucket = \"ecommerce-data-replica-eu-west-1\"\n",
    "  }\n",
    "  \n",
    "  kms_key_id = module.kms.key_id\n",
    "}\n",
    "\n",
    "# MSK (Managed Kafka)\n",
    "module \"msk\" {\n",
    "  source = \"./modules/msk\"\n",
    "  \n",
    "  cluster_name    = \"ecommerce-kafka-${var.environment}\"\n",
    "  kafka_version   = \"3.5.1\"\n",
    "  broker_instance = \"kafka.m5.large\"\n",
    "  broker_count    = 3\n",
    "  \n",
    "  ebs_volume_size = 1000  # GB per broker\n",
    "  \n",
    "  encryption_in_transit = true\n",
    "  encryption_at_rest    = true\n",
    "  \n",
    "  monitoring_level = \"PER_TOPIC_PER_PARTITION\"\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.msk_sg_id]\n",
    "}\n",
    "\n",
    "# EMR Serverless\n",
    "module \"emr_serverless\" {\n",
    "  source = \"./modules/emr-serverless\"\n",
    "  \n",
    "  application_name = \"ecommerce-spark-${var.environment}\"\n",
    "  \n",
    "  initial_capacity = {\n",
    "    driver = {\n",
    "      count = 1\n",
    "      cpu   = \"2 vCPU\"\n",
    "      memory = \"8 GB\"\n",
    "    }\n",
    "    executor = {\n",
    "      count = 5\n",
    "      cpu   = \"4 vCPU\"\n",
    "      memory = \"16 GB\"\n",
    "      disk   = \"100 GB\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  maximum_capacity = {\n",
    "    cpu    = \"200 vCPU\"\n",
    "    memory = \"800 GB\"\n",
    "    disk   = \"2000 GB\"\n",
    "  }\n",
    "  \n",
    "  auto_stop_idle_timeout = 15  # minutes\n",
    "  \n",
    "  execution_role_arn = module.iam.emr_execution_role_arn\n",
    "}\n",
    "\n",
    "# MWAA (Managed Airflow)\n",
    "module \"mwaa\" {\n",
    "  source = \"./modules/mwaa\"\n",
    "  \n",
    "  environment_name = \"ecommerce-airflow-${var.environment}\"\n",
    "  \n",
    "  airflow_version    = \"2.7.2\"\n",
    "  environment_class  = \"mw1.medium\"\n",
    "  min_workers        = 2\n",
    "  max_workers        = 10\n",
    "  schedulers         = 2  # HA\n",
    "  \n",
    "  dag_s3_path          = \"dags/\"\n",
    "  plugins_s3_path      = \"plugins.zip\"\n",
    "  requirements_s3_path = \"requirements.txt\"\n",
    "  \n",
    "  logging_configuration = {\n",
    "    dag_processing_logs = { enabled = true, log_level = \"INFO\" }\n",
    "    scheduler_logs      = { enabled = true, log_level = \"INFO\" }\n",
    "    task_logs           = { enabled = true, log_level = \"INFO\" }\n",
    "    worker_logs         = { enabled = true, log_level = \"INFO\" }\n",
    "    webserver_logs      = { enabled = true, log_level = \"INFO\" }\n",
    "  }\n",
    "  \n",
    "  airflow_configuration_options = {\n",
    "    \"core.parallelism\"                 = \"32\"\n",
    "    \"core.max_active_runs_per_dag\"     = \"3\"\n",
    "    \"scheduler.catchup_by_default\"     = \"False\"\n",
    "    \"secrets.backend\"                  = \"airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\"\n",
    "  }\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.mwaa_sg_id]\n",
    "  \n",
    "  execution_role_arn = module.iam.mwaa_execution_role_arn\n",
    "}\n",
    "\n",
    "# RDS for DataHub metadata\n",
    "module \"datahub_rds\" {\n",
    "  source = \"./modules/rds\"\n",
    "  \n",
    "  identifier = \"datahub-metadata-${var.environment}\"\n",
    "  \n",
    "  engine         = \"postgres\"\n",
    "  engine_version = \"15.3\"\n",
    "  instance_class = \"db.r6g.xlarge\"\n",
    "  \n",
    "  allocated_storage     = 100\n",
    "  max_allocated_storage = 1000\n",
    "  storage_encrypted     = true\n",
    "  kms_key_id            = module.kms.key_id\n",
    "  \n",
    "  multi_az = true  # HA\n",
    "  \n",
    "  backup_retention_period = 7\n",
    "  backup_window          = \"03:00-04:00\"\n",
    "  maintenance_window     = \"mon:04:00-mon:05:00\"\n",
    "  \n",
    "  performance_insights_enabled = true\n",
    "  \n",
    "  subnets            = module.vpc.database_subnet_ids\n",
    "  security_group_ids = [module.security_groups.rds_sg_id]\n",
    "}\n",
    "\n",
    "# ECS for DataHub services\n",
    "module \"datahub_ecs\" {\n",
    "  source = \"./modules/ecs-fargate\"\n",
    "  \n",
    "  cluster_name = \"datahub-${var.environment}\"\n",
    "  \n",
    "  services = {\n",
    "    datahub-gms = {\n",
    "      image            = \"acryldata/datahub-gms:v0.12.0\"\n",
    "      cpu              = 2048\n",
    "      memory           = 4096\n",
    "      desired_count    = 2\n",
    "      health_check_path = \"/health\"\n",
    "      environment_variables = {\n",
    "        DATAHUB_ANALYTICS_ENABLED = \"true\"\n",
    "        ELASTICSEARCH_HOST        = module.elasticsearch.endpoint\n",
    "      }\n",
    "    }\n",
    "    datahub-frontend = {\n",
    "      image            = \"acryldata/datahub-frontend-react:v0.12.0\"\n",
    "      cpu              = 1024\n",
    "      memory           = 2048\n",
    "      desired_count    = 2\n",
    "      health_check_path = \"/admin\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.ecs_sg_id]\n",
    "  \n",
    "  load_balancer = {\n",
    "    enabled         = true\n",
    "    certificate_arn = module.acm.certificate_arn\n",
    "    domain_name     = \"datahub.ecommerce.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Prometheus & Grafana (ECS)\n",
    "module \"observability\" {\n",
    "  source = \"./modules/observability\"\n",
    "  \n",
    "  cluster_name = \"observability-${var.environment}\"\n",
    "  \n",
    "  prometheus = {\n",
    "    image         = \"prom/prometheus:v2.47.0\"\n",
    "    cpu           = 2048\n",
    "    memory        = 4096\n",
    "    desired_count = 1\n",
    "    storage_size  = 100  # GB (EBS)\n",
    "    retention     = \"30d\"\n",
    "  }\n",
    "  \n",
    "  grafana = {\n",
    "    image         = \"grafana/grafana:10.1.0\"\n",
    "    cpu           = 1024\n",
    "    memory        = 2048\n",
    "    desired_count = 2\n",
    "    admin_password = data.aws_secretsmanager_secret_version.grafana_password.secret_string\n",
    "  }\n",
    "  \n",
    "  alertmanager = {\n",
    "    image         = \"prom/alertmanager:v0.26.0\"\n",
    "    cpu           = 512\n",
    "    memory        = 1024\n",
    "    desired_count = 2\n",
    "    slack_webhook = data.aws_secretsmanager_secret_version.slack_webhook.secret_string\n",
    "  }\n",
    "}\n",
    "\n",
    "# Budgets & Cost Alerts\n",
    "module \"cost_management\" {\n",
    "  source = \"./modules/cost-management\"\n",
    "  \n",
    "  budgets = [\n",
    "    {\n",
    "      name   = \"data-platform-monthly\"\n",
    "      amount = 5000\n",
    "      threshold_percentages = [50, 80, 100, 120]\n",
    "      notification_emails = [\"data-leads@ecommerce.com\"]\n",
    "    },\n",
    "    {\n",
    "      name   = \"emr-serverless-daily\"\n",
    "      amount = 150\n",
    "      time_unit = \"DAILY\"\n",
    "      threshold_percentages = [100]\n",
    "    }\n",
    "  ]\n",
    "  \n",
    "  cost_anomaly_detection = {\n",
    "    enabled           = true\n",
    "    monitor_name      = \"data-platform-anomalies\"\n",
    "    threshold_dollars = 100\n",
    "  }\n",
    "}\n",
    "\n",
    "# Outputs\n",
    "output \"data_lake_bucket\" {\n",
    "  value = module.data_lake.bucket_name\n",
    "}\n",
    "\n",
    "output \"msk_bootstrap_brokers\" {\n",
    "  value = module.msk.bootstrap_brokers\n",
    "}\n",
    "\n",
    "output \"mwaa_webserver_url\" {\n",
    "  value = module.mwaa.webserver_url\n",
    "}\n",
    "\n",
    "output \"datahub_url\" {\n",
    "  value = \"https://${module.datahub_ecs.load_balancer_dns}\"\n",
    "}\n",
    "\n",
    "output \"grafana_url\" {\n",
    "  value = \"https://${module.observability.grafana_url}\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Deploy Commands:**\n",
    "\n",
    "```bash\n",
    "# Initialize\n",
    "terraform init\n",
    "\n",
    "# Plan\n",
    "terraform plan -var-file=\"environments/prod.tfvars\" -out=tfplan\n",
    "\n",
    "# Review changes\n",
    "terraform show tfplan\n",
    "\n",
    "# Apply\n",
    "terraform apply tfplan\n",
    "\n",
    "# Estimated monthly cost: $4,850\n",
    "# - S3: $200\n",
    "# - MSK: $2,500\n",
    "# - EMR Serverless: $1,200\n",
    "# - MWAA: $800\n",
    "# - RDS: $300\n",
    "# - ECS (DataHub): $400\n",
    "# - Observability: $250\n",
    "# - Data transfer: $200\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. CI/CD Pipeline: GitHub Actions**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/data-platform-ci.yml\n",
    "name: Data Platform CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, develop]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "env:\n",
    "  AWS_REGION: us-east-1\n",
    "  TERRAFORM_VERSION: 1.5.7\n",
    "\n",
    "jobs:\n",
    "  code-quality:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python 3.11\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements-dev.txt\n",
    "      \n",
    "      - name: Lint with Ruff\n",
    "        run: |\n",
    "          ruff check src/ tests/\n",
    "      \n",
    "      - name: Type check with mypy\n",
    "        run: |\n",
    "          mypy src/\n",
    "      \n",
    "      - name: Format check with Black\n",
    "        run: |\n",
    "          black --check src/ tests/\n",
    "      \n",
    "      - name: Security scan with Bandit\n",
    "        run: |\n",
    "          bandit -r src/ -ll\n",
    "  \n",
    "  unit-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: code-quality\n",
    "    \n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:15\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: test\n",
    "        options: >-\n",
    "          --health-cmd pg_isready\n",
    "          --health-interval 10s\n",
    "          --health-timeout 5s\n",
    "          --health-retries 5\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov\n",
    "      \n",
    "      - name: Run tests with coverage\n",
    "        run: |\n",
    "          pytest tests/ \\\n",
    "            --cov=src \\\n",
    "            --cov-report=xml \\\n",
    "            --cov-report=html \\\n",
    "            --junitxml=test-results.xml\n",
    "      \n",
    "      - name: Upload coverage to Codecov\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          files: ./coverage.xml\n",
    "          fail_ci_if_error: true\n",
    "      \n",
    "      - name: Check coverage threshold\n",
    "        run: |\n",
    "          coverage report --fail-under=80\n",
    "  \n",
    "  integration-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: unit-tests\n",
    "    \n",
    "    services:\n",
    "      localstack:\n",
    "        image: localstack/localstack:latest\n",
    "        env:\n",
    "          SERVICES: s3,glue,emr,mwaa\n",
    "        ports:\n",
    "          - 4566:4566\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Run integration tests\n",
    "        run: |\n",
    "          pytest tests/integration/ \\\n",
    "            --localstack-endpoint=http://localhost:4566\n",
    "  \n",
    "  airflow-dags-validation:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: code-quality\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Validate Airflow DAGs\n",
    "        run: |\n",
    "          pip install apache-airflow==2.7.2\n",
    "          \n",
    "          # Import all DAGs (catch syntax errors)\n",
    "          python -c \"from airflow.models import DagBag; \\\n",
    "                     db = DagBag('dags/'); \\\n",
    "                     assert len(db.import_errors) == 0, db.import_errors\"\n",
    "      \n",
    "      - name: Test DAG integrity\n",
    "        run: |\n",
    "          pytest tests/dags/ -v\n",
    "  \n",
    "  terraform-plan:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [unit-tests, integration-tests]\n",
    "    if: github.event_name == 'pull_request'\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Setup Terraform\n",
    "        uses: hashicorp/setup-terraform@v2\n",
    "        with:\n",
    "          terraform_version: ${{ env.TERRAFORM_VERSION }}\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v4\n",
    "        with:\n",
    "          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n",
    "          aws-region: ${{ env.AWS_REGION }}\n",
    "      \n",
    "      - name: Terraform Init\n",
    "        run: terraform init\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Terraform Validate\n",
    "        run: terraform validate\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Terraform Plan\n",
    "        run: terraform plan -var-file=\"environments/prod.tfvars\" -no-color\n",
    "        working-directory: infrastructure/\n",
    "        continue-on-error: true\n",
    "        id: plan\n",
    "      \n",
    "      - name: Comment Plan on PR\n",
    "        uses: actions/github-script@v6\n",
    "        with:\n",
    "          script: |\n",
    "            github.rest.issues.createComment({\n",
    "              issue_number: context.issue.number,\n",
    "              owner: context.repo.owner,\n",
    "              repo: context.repo.repo,\n",
    "              body: `#### Terraform Plan \ud83d\udcd6\\n\\`\\`\\`\\n${{ steps.plan.outputs.stdout }}\\n\\`\\`\\``\n",
    "            })\n",
    "  \n",
    "  deploy-staging:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [unit-tests, integration-tests, airflow-dags-validation]\n",
    "    if: github.ref == 'refs/heads/develop'\n",
    "    environment: staging\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Deploy Airflow DAGs to S3\n",
    "        run: |\n",
    "          aws s3 sync dags/ s3://ecommerce-airflow-staging/dags/ \\\n",
    "            --exclude \"*.pyc\" \\\n",
    "            --exclude \"__pycache__/*\" \\\n",
    "            --delete\n",
    "      \n",
    "      - name: Trigger MWAA DAG refresh\n",
    "        run: |\n",
    "          aws mwaa create-cli-token \\\n",
    "            --name ecommerce-airflow-staging \\\n",
    "            --query CliToken \\\n",
    "            --output text\n",
    "      \n",
    "      - name: Deploy Spark jobs to S3\n",
    "        run: |\n",
    "          aws s3 sync src/spark/ s3://ecommerce-code-staging/spark/ \\\n",
    "            --delete\n",
    "  \n",
    "  deploy-production:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [deploy-staging]\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    environment: production\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v4\n",
    "        with:\n",
    "          role-to-assume: ${{ secrets.AWS_PROD_ROLE_ARN }}\n",
    "          aws-region: ${{ env.AWS_REGION }}\n",
    "      \n",
    "      - name: Deploy Terraform changes\n",
    "        run: |\n",
    "          terraform init\n",
    "          terraform apply -var-file=\"environments/prod.tfvars\" -auto-approve\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Deploy Airflow DAGs\n",
    "        run: |\n",
    "          aws s3 sync dags/ s3://ecommerce-airflow-prod/dags/ --delete\n",
    "      \n",
    "      - name: Deploy Spark jobs\n",
    "        run: |\n",
    "          aws s3 sync src/spark/ s3://ecommerce-code-prod/spark/ --delete\n",
    "      \n",
    "      - name: Create GitHub Release\n",
    "        uses: actions/create-release@v1\n",
    "        env:\n",
    "          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "        with:\n",
    "          tag_name: v${{ github.run_number }}\n",
    "          release_name: Release v${{ github.run_number }}\n",
    "          body: |\n",
    "            ## Changes\n",
    "            ${{ github.event.head_commit.message }}\n",
    "            \n",
    "            ## Deployment\n",
    "            - Terraform applied\n",
    "            - Airflow DAGs updated\n",
    "            - Spark jobs deployed\n",
    "          draft: false\n",
    "          prerelease: false\n",
    "      \n",
    "      - name: Notify Slack\n",
    "        uses: 8398a7/action-slack@v3\n",
    "        with:\n",
    "          status: ${{ job.status }}\n",
    "          text: '\ud83d\ude80 Production deployment completed'\n",
    "          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "        if: always()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. SLO & SLI Definitions**\n",
    "\n",
    "```yaml\n",
    "# slos.yml\n",
    "service_level_objectives:\n",
    "  \n",
    "  # Pipeline Availability\n",
    "  - name: ventas_pipeline_availability\n",
    "    description: \"Percentage of time ventas pipeline completes successfully\"\n",
    "    objective: 99.5%\n",
    "    window: 30d\n",
    "    \n",
    "    sli_query: |\n",
    "      sum(rate(airflow_dag_run_success{dag_id=\"ventas_batch_pipeline_v1\"}[30d]))\n",
    "      /\n",
    "      sum(rate(airflow_dag_run_total{dag_id=\"ventas_batch_pipeline_v1\"}[30d]))\n",
    "    \n",
    "    error_budget: 0.5%  # 3.6 hours/month\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 99.0%\n",
    "        window: 7d\n",
    "      - severity: critical\n",
    "        threshold: 98.0%\n",
    "        window: 24h\n",
    "  \n",
    "  # Pipeline Latency\n",
    "  - name: ventas_pipeline_latency_p99\n",
    "    description: \"99th percentile pipeline duration\"\n",
    "    objective: \"<2 hours\"\n",
    "    window: 7d\n",
    "    \n",
    "    sli_query: |\n",
    "      histogram_quantile(0.99,\n",
    "        rate(airflow_dag_duration_seconds_bucket{dag_id=\"ventas_batch_pipeline_v1\"}[7d])\n",
    "      )\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 7200  # 2 hours\n",
    "      - severity: critical\n",
    "        threshold: 10800  # 3 hours\n",
    "  \n",
    "  # Data Freshness\n",
    "  - name: data_freshness\n",
    "    description: \"Time since last successful data update\"\n",
    "    objective: \"<90 minutes\"\n",
    "    \n",
    "    sli_query: |\n",
    "      time() - max(delta_table_last_updated{table=\"curated.ventas\"})\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 5400  # 90 min\n",
    "      - severity: critical\n",
    "        threshold: 7200  # 2 hours\n",
    "  \n",
    "  # Data Quality\n",
    "  - name: data_quality_pass_rate\n",
    "    description: \"Percentage of records passing validation\"\n",
    "    objective: 99.9%\n",
    "    window: 7d\n",
    "    \n",
    "    sli_query: |\n",
    "      sum(rate(great_expectations_validation_success[7d]))\n",
    "      /\n",
    "      sum(rate(great_expectations_validation_total[7d]))\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 99.5%\n",
    "      - severity: critical\n",
    "        threshold: 99.0%\n",
    "  \n",
    "  # API Performance\n",
    "  - name: api_latency_p95\n",
    "    description: \"95th percentile API response time\"\n",
    "    objective: \"<500ms\"\n",
    "    window: 24h\n",
    "    \n",
    "    sli_query: |\n",
    "      histogram_quantile(0.95,\n",
    "        rate(fastapi_request_duration_seconds_bucket{endpoint=\"/query\"}[24h])\n",
    "      )\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 0.5  # 500ms\n",
    "      - severity: critical\n",
    "        threshold: 1.0  # 1s\n",
    "```\n",
    "\n",
    "**Prometheus Alert Rules:**\n",
    "\n",
    "```yaml\n",
    "# alerts/data-platform.yml\n",
    "groups:\n",
    "  - name: data_platform_alerts\n",
    "    interval: 1m\n",
    "    \n",
    "    rules:\n",
    "      - alert: DataPipelineFailure\n",
    "        expr: |\n",
    "          rate(airflow_dag_run_failure{dag_id=~\".*_pipeline.*\"}[5m]) > 0\n",
    "        for: 1m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Airflow DAG {{ $labels.dag_id }} failing\"\n",
    "          description: \"{{ $value | humanizePercentage }} failure rate in last 5 min\"\n",
    "          runbook: \"https://wiki.ecommerce.com/runbooks/airflow-failures\"\n",
    "      \n",
    "      - alert: DataFreshnessViolation\n",
    "        expr: |\n",
    "          (time() - delta_table_last_updated{table=\"curated.ventas\"}) > 7200\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Stale data in {{ $labels.table }}\"\n",
    "          description: \"Last update {{ $value | humanizeDuration }} ago\"\n",
    "      \n",
    "      - alert: KafkaConsumerLag\n",
    "        expr: |\n",
    "          kafka_consumer_lag{topic=\"ecommerce.ventas.v1\"} > 100000\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"High Kafka consumer lag on {{ $labels.topic }}\"\n",
    "          description: \"Lag: {{ $value }} messages\"\n",
    "      \n",
    "      - alert: S3CostAnomaly\n",
    "        expr: |\n",
    "          (\n",
    "            increase(aws_s3_storage_bytes{bucket=\"ecommerce-data\"}[1d])\n",
    "            /\n",
    "            increase(aws_s3_storage_bytes{bucket=\"ecommerce-data\"} offset 7d[1d])\n",
    "          ) > 1.5\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: finops\n",
    "        annotations:\n",
    "          summary: \"S3 storage growth anomaly detected\"\n",
    "          description: \"50% increase vs last week\"\n",
    "      \n",
    "      - alert: DataQualityDegraded\n",
    "        expr: |\n",
    "          (\n",
    "            sum(rate(great_expectations_validation_failure[1h]))\n",
    "            /\n",
    "            sum(rate(great_expectations_validation_total[1h]))\n",
    "          ) > 0.01\n",
    "        for: 30m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Data quality degraded\"\n",
    "          description: \"{{ $value | humanizePercentage }} validation failure rate\"\n",
    "      \n",
    "      - alert: EMRServerlessCostRunaway\n",
    "        expr: |\n",
    "          sum(increase(aws_emr_serverless_vcore_hours[1h])) > 1000\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: finops\n",
    "        annotations:\n",
    "          summary: \"EMR Serverless usage spike\"\n",
    "          description: \"{{ $value }} vCore-hours in last hour (normal: <500)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Incident Response Runbook**\n",
    "\n",
    "```markdown\n",
    "# Runbook: Ventas Pipeline Failure\n",
    "\n",
    "## Severity: P1 (Critical)\n",
    "\n",
    "### Symptoms\n",
    "- Airflow DAG `ventas_batch_pipeline_v1` status: FAILED\n",
    "- Alert: \"DataPipelineFailure\" in PagerDuty\n",
    "- Dashboard: Red status in Grafana\n",
    "\n",
    "### Impact\n",
    "- BI dashboards stale (>2 hours)\n",
    "- Executive reports unavailable\n",
    "- Customer RFM scores not updated\n",
    "\n",
    "### Diagnosis Steps\n",
    "\n",
    "1. **Check Airflow UI**\n",
    "   ```bash\n",
    "   # Open Airflow\n",
    "   open https://airflow.ecommerce.com/dags/ventas_batch_pipeline_v1\n",
    "   \n",
    "   # Identify failed task\n",
    "   # Common failures:\n",
    "   # - check_sftp_files: Missing upstream data\n",
    "   # - run_data_quality_checks: Validation failure\n",
    "   # - transform_to_curated: Spark job OOM\n",
    "   ```\n",
    "\n",
    "2. **Check Logs**\n",
    "   ```bash\n",
    "   # CloudWatch Logs\n",
    "   aws logs tail /aws/mwaa/ecommerce-airflow-prod/Task \\\n",
    "     --follow \\\n",
    "     --filter-pattern \"ERROR\"\n",
    "   \n",
    "   # S3 Spark logs\n",
    "   aws s3 ls s3://ecommerce-logs/spark/ventas/\n",
    "   ```\n",
    "\n",
    "3. **Check Dependencies**\n",
    "   ```bash\n",
    "   # SFTP files present?\n",
    "   sftp sap_sftp_user@sftp.sap.com\n",
    "   > ls /exports/ventas_2024-01-15.csv\n",
    "   \n",
    "   # S3 accessible?\n",
    "   aws s3 ls s3://ecommerce-data/raw/ventas/\n",
    "   \n",
    "   # EMR Serverless healthy?\n",
    "   aws emr-serverless list-applications\n",
    "   ```\n",
    "\n",
    "### Resolution Procedures\n",
    "\n",
    "#### Scenario A: Missing SFTP Files\n",
    "```bash\n",
    "# 1. Contact SAP team (slack: #sap-integration)\n",
    "# 2. Verify export schedule\n",
    "# 3. Manual trigger if needed\n",
    "# 4. Re-run DAG once files available\n",
    "\n",
    "airflow dags trigger ventas_batch_pipeline_v1 \\\n",
    "  --conf '{\"execution_date\": \"2024-01-15\"}'\n",
    "```\n",
    "\n",
    "#### Scenario B: Data Quality Failure\n",
    "```python\n",
    "# 1. Review Great Expectations report\n",
    "import great_expectations as gx\n",
    "context = gx.get_context()\n",
    "\n",
    "# 2. Check failed validations\n",
    "results = context.get_validation_result(\"ventas_suite_2024-01-15\")\n",
    "failures = [r for r in results.results if not r.success]\n",
    "\n",
    "# 3. Determine if acceptable\n",
    "# - If data issue: Contact source system owner\n",
    "# - If expectation too strict: Update suite\n",
    "\n",
    "# 4. Override validation (emergency only)\n",
    "airflow tasks run ventas_batch_pipeline_v1 \\\n",
    "  transform_to_curated \\\n",
    "  2024-01-15 \\\n",
    "  --ignore-dependencies\n",
    "```\n",
    "\n",
    "#### Scenario C: Spark OOM\n",
    "```bash\n",
    "# 1. Check memory usage\n",
    "aws emr-serverless get-job-run \\\n",
    "  --application-id app-123 \\\n",
    "  --job-run-id jr-456 \\\n",
    "  | jq '.jobRun.totalExecutionDurationSeconds, .jobRun.totalResourceUtilization'\n",
    "\n",
    "# 2. Increase memory allocation\n",
    "# Edit DAG: increase executor memory 16GB \u2192 32GB\n",
    "\n",
    "# 3. Optimize query\n",
    "# - Add .repartition(100) before expensive operations\n",
    "# - Use broadcast joins for small tables\n",
    "# - Enable AQE (Adaptive Query Execution)\n",
    "\n",
    "# 4. Re-run\n",
    "airflow dags trigger ventas_batch_pipeline_v1\n",
    "```\n",
    "\n",
    "### Escalation Path\n",
    "- **L1 (On-call DE):** 0-15 min\n",
    "- **L2 (Senior DE):** 15-30 min\n",
    "- **L3 (Staff DE + Manager):** 30-60 min\n",
    "- **Incident Commander:** >60 min or customer-facing\n",
    "\n",
    "### Post-Incident\n",
    "1. **Root Cause Analysis** (48h after resolution)\n",
    "2. **Postmortem doc** (Confluence template)\n",
    "3. **Action items** (Jira tickets)\n",
    "4. **Knowledge base update**\n",
    "\n",
    "### Related Links\n",
    "- [Airflow UI](https://airflow.ecommerce.com)\n",
    "- [Grafana Dashboard](https://grafana.ecommerce.com/d/ventas)\n",
    "- [CloudWatch Logs](https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups)\n",
    "- [PagerDuty Escalation](https://ecommerce.pagerduty.com)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Cost Monitoring Dashboard**\n",
    "\n",
    "```python\n",
    "# monitoring/cost_dashboard.py\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Metrics\n",
    "s3_storage_bytes = Gauge('aws_s3_storage_bytes', 'S3 storage size', ['bucket', 'storage_class'])\n",
    "s3_request_count = Gauge('aws_s3_requests_total', 'S3 requests', ['bucket', 'operation'])\n",
    "emr_vcore_hours = Gauge('aws_emr_serverless_vcore_hours', 'EMR vCore hours', ['application'])\n",
    "msk_throughput_mb = Gauge('aws_msk_throughput_mb', 'MSK throughput', ['cluster'])\n",
    "rds_cpu_utilization = Gauge('aws_rds_cpu_percent', 'RDS CPU', ['instance'])\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')\n",
    "ce = boto3.client('ce', region_name='us-east-1')  # Cost Explorer\n",
    "\n",
    "def collect_s3_metrics():\n",
    "    \"\"\"Collect S3 metrics from CloudWatch\"\"\"\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/S3',\n",
    "        MetricName='BucketSizeBytes',\n",
    "        Dimensions=[\n",
    "            {'Name': 'BucketName', 'Value': 'ecommerce-data'},\n",
    "            {'Name': 'StorageType', 'Value': 'StandardStorage'}\n",
    "        ],\n",
    "        StartTime=datetime.utcnow() - timedelta(hours=24),\n",
    "        EndTime=datetime.utcnow(),\n",
    "        Period=86400,\n",
    "        Statistics=['Average']\n",
    "    )\n",
    "    \n",
    "    if response['Datapoints']:\n",
    "        bytes_stored = response['Datapoints'][0]['Average']\n",
    "        s3_storage_bytes.labels(bucket='ecommerce-data', storage_class='standard').set(bytes_stored)\n",
    "\n",
    "def collect_cost_metrics():\n",
    "    \"\"\"Collect cost data from Cost Explorer\"\"\"\n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "            'End': datetime.utcnow().strftime('%Y-%m-%d')\n",
    "        },\n",
    "        Granularity='DAILY',\n",
    "        Metrics=['UnblendedCost'],\n",
    "        GroupBy=[\n",
    "            {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n",
    "        ],\n",
    "        Filter={\n",
    "            'Tags': {\n",
    "                'Key': 'Project',\n",
    "                'Values': ['data-platform']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for result in response['ResultsByTime']:\n",
    "        for group in result['Groups']:\n",
    "            service = group['Keys'][0]\n",
    "            cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "            \n",
    "            # Export as Prometheus metric\n",
    "            # (define gauge for each service)\n",
    "            print(f\"{service}: ${cost:.2f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start Prometheus HTTP server\n",
    "    start_http_server(8000)\n",
    "    \n",
    "    while True:\n",
    "        collect_s3_metrics()\n",
    "        collect_cost_metrics()\n",
    "        time.sleep(300)  # Every 5 min\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e867067",
   "metadata": {},
   "source": [
    "## 1. Contexto y requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cb5ee",
   "metadata": {},
   "source": [
    "**Empresa**: E-commerce global con 3 dominios de datos (Ventas, Log\u00edstica, Anal\u00edtica).\n",
    "\n",
    "**Requerimientos funcionales**:\n",
    "- Ingestar transacciones en tiempo real (Kafka) y batch nocturno (archivos SFTP).\n",
    "- Almacenar en data lakehouse (Parquet + Delta Lake) particionado por fecha y regi\u00f3n.\n",
    "- Cat\u00e1logo central con metadatos, linaje y pol\u00edticas de acceso (Glue/Unity/DataHub).\n",
    "- Orquestaci\u00f3n diaria con Airflow: validaciones de calidad, transformaciones, reportes.\n",
    "- APIs de servicio (FastAPI) para consultas ad-hoc por BI y cient\u00edficos de datos.\n",
    "\n",
    "**Requerimientos no funcionales**:\n",
    "- Compliance GDPR: enmascaramiento de PII, derecho al olvido.\n",
    "- SLO: latencia p99 < 30 min, disponibilidad > 99.5%.\n",
    "- Costos: < $5000/mes, optimizaci\u00f3n continua (FinOps).\n",
    "- Observabilidad: logs estructurados, m\u00e9tricas en Prometheus, linaje en DataHub.\n",
    "- Seguridad: IAM con m\u00ednimo privilegio, cifrado at-rest y in-transit, auditor\u00eda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdb0bc",
   "metadata": {},
   "source": [
    "## 2. Arquitectura propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquitectura_diagrama = '''\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Transac-   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502    Kafka     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Spark        \u2502\n",
    "\u2502  ciones RT  \u2502       \u2502  (streaming) \u2502       \u2502  Streaming    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                                      \u2502\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502  Archivos   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Airflow    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  SFTP Batch \u2502       \u2502  (orquesta)  \u2502              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "                                                     \u25bc\n",
    "                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                      \u2502  Data Lakehouse (S3 + Delta)     \u2502\n",
    "                      \u2502  - raw/                          \u2502\n",
    "                      \u2502  - curated/                      \u2502\n",
    "                      \u2502  - gold/ (agregados)             \u2502\n",
    "                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                \u2502\n",
    "                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                \u25bc               \u25bc               \u25bc\n",
    "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502  Athena  \u2502   \u2502 FastAPI  \u2502   \u2502   BI     \u2502\n",
    "         \u2502  (SQL)   \u2502   \u2502 (APIs)   \u2502   \u2502 (Tableau)\u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Observabilidad: Prometheus + Grafana + DataHub (linaje)\n",
    "Seguridad: IAM, KMS, CloudTrail, enmascaramiento PII\n",
    "'''\n",
    "print(arquitectura_diagrama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f7e2e",
   "metadata": {},
   "source": [
    "## 3. Componentes a implementar (checklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17901a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = '''\n",
    "\u2610 1. Kafka cluster (Docker Compose local o MSK en AWS)\n",
    "\u2610 2. Productor de eventos simulados (transacciones)\n",
    "\u2610 3. Consumidor Spark Streaming \u2192 Delta Lake (S3)\n",
    "\u2610 4. Airflow DAG batch: SFTP \u2192 raw \u2192 validaci\u00f3n \u2192 curated \u2192 gold\n",
    "\u2610 5. Validaciones de calidad con Great Expectations\n",
    "\u2610 6. Enmascaramiento de PII (email, tarjeta)\n",
    "\u2610 7. Cat\u00e1logo con Glue Data Catalog o DataHub\n",
    "\u2610 8. Linaje con OpenLineage (plugin Airflow)\n",
    "\u2610 9. FastAPI endpoint para consultas SQL (proxy a Athena/Trino)\n",
    "\u2610 10. M\u00e9tricas Prometheus exportadas por pipelines\n",
    "\u2610 11. Dashboard Grafana con SLOs y alertas\n",
    "\u2610 12. Pol\u00edticas IAM con m\u00ednimo privilegio\n",
    "\u2610 13. Cifrado KMS para S3 y RDS\n",
    "\u2610 14. Auditor\u00eda CloudTrail habilitada\n",
    "\u2610 15. Presupuestos y alertas de costos (AWS Budgets)\n",
    "\u2610 16. Documentaci\u00f3n t\u00e9cnica y runbooks\n",
    "\u2610 17. Tests de integraci\u00f3n (Pytest)\n",
    "\u2610 18. CI/CD con GitHub Actions (lint, test, deploy)\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb8e10",
   "metadata": {},
   "source": [
    "## 4. Implementaci\u00f3n paso a paso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a21a9",
   "metadata": {},
   "source": [
    "### 4.1 Setup inicial\n",
    "- Crear bucket S3 con estructura `raw/`, `curated/`, `gold/`.\n",
    "- Configurar Glue Data Catalog con base de datos `ecommerce`.\n",
    "- Levantar Kafka local con Docker Compose (zookeeper + broker).\n",
    "\n",
    "### 4.2 Streaming path\n",
    "- Productor Python: genera eventos JSON (transacci\u00f3n_id, cliente_id, monto, timestamp).\n",
    "- Spark Structured Streaming: consume de Kafka, valida schema, escribe a Delta en `curated/ventas/`.\n",
    "- Checkpointing idempotente.\n",
    "\n",
    "### 4.3 Batch path\n",
    "- Airflow DAG: sensor SFTP \u2192 download \u2192 validate (GE) \u2192 transform (Pandas/Spark) \u2192 write Delta \u2192 optimize.\n",
    "- Agregaciones gold: ventas por d\u00eda/regi\u00f3n/producto.\n",
    "\n",
    "### 4.4 Governance y seguridad\n",
    "- Enmascarar email y tarjeta antes de escribir en curated.\n",
    "- Registrar linaje en DataHub v\u00eda OpenLineage.\n",
    "- Configurar IAM roles para Spark, Airflow, APIs.\n",
    "\n",
    "### 4.5 Observabilidad\n",
    "- Exportar m\u00e9tricas de conteo, latencia, errores.\n",
    "- Dashboard Grafana con paneles por pipeline.\n",
    "- Alertas en Slack si SLO violado.\n",
    "\n",
    "### 4.6 Servicio de consultas\n",
    "- FastAPI endpoint `/query` que ejecuta SQL en Athena y retorna JSON.\n",
    "- Cach\u00e9 Redis para queries repetitivas.\n",
    "- Rate limiting por API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728559c",
   "metadata": {},
   "source": [
    "## 5. Entregables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d2f42",
   "metadata": {},
   "source": [
    "- Repositorio Git con c\u00f3digo (pipelines, DAGs, APIs, tests).\n",
    "- Diagrama de arquitectura actualizado.\n",
    "- Documento de dise\u00f1o (decisiones t\u00e9cnicas, trade-offs).\n",
    "- Dashboard Grafana exportado (JSON).\n",
    "- Runbook de operaciones (troubleshooting, rollback).\n",
    "- Video/demo ejecutando pipeline end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc021c5",
   "metadata": {},
   "source": [
    "## 6. Evaluaci\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259a0dd",
   "metadata": {},
   "source": [
    "- Funcionalidad: \u00bfpipelines ejecutan correctamente?\n",
    "- Calidad: \u00bfvalidaciones y tests implementados?\n",
    "- Observabilidad: \u00bfm\u00e9tricas y linaje visibles?\n",
    "- Seguridad: \u00bfIAM, cifrado, PII enmascarado?\n",
    "- Costos: \u00bfpresupuesto respetado?\n",
    "- Documentaci\u00f3n: \u00bfclara y completa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store \u2192](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
