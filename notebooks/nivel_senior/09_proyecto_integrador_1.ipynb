{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bceb9d",
   "metadata": {},
   "source": [
    "# 🏆 Proyecto Integrador Senior 1: Plataforma de Datos Completa\n",
    "\n",
    "Objetivo: diseñar e implementar una plataforma moderna de datos con governance, lakehouse, orquestación, observabilidad y compliance.\n",
    "\n",
    "- Duración: 180+ min (proyecto multi-día)\n",
    "- Dificultad: Muy Alta\n",
    "- Prerrequisitos: Todos los notebooks Senior 01–08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f18c5",
   "metadata": {},
   "source": [
    "### 🏗️ **Diseño de Arquitectura: Patrones y Trade-offs**\n",
    "\n",
    "**1. Arquitectura Lambda vs Kappa: Hybrid Approach**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Modern Data Platform Architecture              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  ┌──────────────┐        ┌──────────────┐                  │\n",
    "│  │  Real-Time   │        │    Batch     │                  │\n",
    "│  │  (Lambda)    │        │   (Airflow)  │                  │\n",
    "│  └──────┬───────┘        └──────┬───────┘                  │\n",
    "│         │                       │                           │\n",
    "│         ▼                       ▼                           │\n",
    "│  ┌────────────────────────────────────────┐                │\n",
    "│  │     Unified Lakehouse Layer            │                │\n",
    "│  │  (Delta Lake - ACID + Time Travel)     │                │\n",
    "│  ├────────────────────────────────────────┤                │\n",
    "│  │  raw/     (bronze - sin procesar)      │                │\n",
    "│  │  curated/ (silver - validado)          │                │\n",
    "│  │  gold/    (gold - agregado)            │                │\n",
    "│  └────────────────────────────────────────┘                │\n",
    "│         │                                                    │\n",
    "│         ├──────┬─────────┬─────────┬───────────┐           │\n",
    "│         ▼      ▼         ▼         ▼           ▼           │\n",
    "│     Athena  Trino   FastAPI   Tableau   ML Models          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**¿Por qué Hybrid?**\n",
    "\n",
    "| Aspecto | Lambda (Streaming) | Batch (Airflow) |\n",
    "|---------|-------------------|-----------------|\n",
    "| **Latencia** | <30s | 1h-24h |\n",
    "| **Complejidad** | Alta (gestión de estado) | Media |\n",
    "| **Costos** | Alto (always-on) | Bajo (on-demand) |\n",
    "| **Casos de Uso** | Detección fraude, alertas | Reportes, ML training |\n",
    "\n",
    "**Decisión:** Usamos streaming para datos críticos (transacciones, eventos de usuario) y batch para agregaciones históricas y ETL complejos.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Storage Layer: S3 + Delta Lake**\n",
    "\n",
    "**¿Por qué no Parquet solo?**\n",
    "\n",
    "```python\n",
    "# Problema con Parquet puro\n",
    "# ❌ Sin transacciones ACID\n",
    "df1.write.parquet(\"s3://bucket/data/\")  # Job 1\n",
    "df2.write.parquet(\"s3://bucket/data/\")  # Job 2 (concurrent) → CORRUPTION\n",
    "\n",
    "# ✅ Delta Lake garantiza ACID\n",
    "df1.write.format(\"delta\").save(\"s3://bucket/data/\")\n",
    "df2.write.format(\"delta\").save(\"s3://bucket/data/\")  # Safe concurrent writes\n",
    "```\n",
    "\n",
    "**Delta Lake Features:**\n",
    "\n",
    "| Feature | Beneficio |\n",
    "|---------|-----------|\n",
    "| **ACID Transactions** | No más datos corruptos |\n",
    "| **Time Travel** | `SELECT * FROM delta.`table` VERSION AS OF 10` |\n",
    "| **Schema Evolution** | Add/remove columns sin reescribir |\n",
    "| **OPTIMIZE** | Compacta small files → mejor perf |\n",
    "| **VACUUM** | Elimina versiones antiguas → ahorra $$$ |\n",
    "| **MERGE** | Upserts eficientes (CDC) |\n",
    "\n",
    "**Estructura de Directorios:**\n",
    "\n",
    "```\n",
    "s3://ecommerce-data/\n",
    "├── raw/                    # Bronze - Inmutable (retención 7 días)\n",
    "│   ├── ventas/\n",
    "│   │   └── dt=2024-01-15/\n",
    "│   │       └── kafka-offset-123.parquet\n",
    "│   └── logistica/\n",
    "│       └── dt=2024-01-15/\n",
    "│           └── sftp-file-001.csv\n",
    "│\n",
    "├── curated/                # Silver - Validado y limpio (retención 1 año)\n",
    "│   ├── ventas/\n",
    "│   │   └── _delta_log/    # Transaction log\n",
    "│   │   └── region=LATAM/\n",
    "│   │       └── dt=2024-01-15/\n",
    "│   │           └── part-00000.parquet\n",
    "│   └── clientes/\n",
    "│       ├── _delta_log/\n",
    "│       └── tipo=premium/\n",
    "│           └── dt=2024-01-15/\n",
    "│\n",
    "└── gold/                   # Gold - Agregado (retención indefinida)\n",
    "    ├── ventas_diarias/\n",
    "    │   └── dt=2024-01-15/\n",
    "    │       └── region=LATAM_producto=laptop.parquet\n",
    "    └── kpis/\n",
    "        └── revenue_by_cohort/\n",
    "```\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "\n",
    "```python\n",
    "# ❌ Mal: Demasiadas particiones (small files)\n",
    "df.write.partitionBy(\"dt\", \"hora\", \"minuto\", \"cliente_id\").parquet(...)\n",
    "# Resultado: 1M+ archivos de 100KB → S3 list operations $$$\n",
    "\n",
    "# ✅ Bien: Particiones equilibradas\n",
    "df.write.partitionBy(\"dt\", \"region\").format(\"delta\").save(...)\n",
    "# Resultado: ~100 archivos de 128MB (ideal para Spark)\n",
    "\n",
    "# Optimize small files\n",
    "spark.sql(\"OPTIMIZE delta.`s3://bucket/curated/ventas` ZORDER BY (cliente_id)\")\n",
    "# ZORDER: Co-locates data por cliente_id → 10x faster filters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Compute Layer: Spark vs Pandas vs Polars**\n",
    "\n",
    "**Decision Matrix:**\n",
    "\n",
    "| Framework | Volume | Performance | Ecosystem |\n",
    "|-----------|--------|-------------|-----------|\n",
    "| **Pandas** | <10 GB | Baseline | Extensive |\n",
    "| **Polars** | <100 GB | 5-10x faster | Growing |\n",
    "| **Spark** | >100 GB | Scalable | Industry std |\n",
    "\n",
    "**Nuestra elección:**\n",
    "- **Streaming:** Spark Structured Streaming (única opción madura para Kafka)\n",
    "- **Batch simple:** Polars (reportes diarios <50 GB)\n",
    "- **Batch complejo:** Spark (joins grandes, ML pipelines)\n",
    "\n",
    "```python\n",
    "# Ejemplo: Batch con Polars (más rápido que Pandas)\n",
    "import polars as pl\n",
    "\n",
    "df = pl.scan_parquet(\"s3://bucket/raw/ventas/*.parquet\")\n",
    "result = (\n",
    "    df\n",
    "    .filter(pl.col(\"monto\") > 100)\n",
    "    .groupby(\"region\")\n",
    "    .agg(pl.sum(\"monto\"))\n",
    "    .collect(streaming=True)  # Out-of-core processing\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Orchestration: Airflow vs Dagster vs Prefect**\n",
    "\n",
    "**Comparación:**\n",
    "\n",
    "| Aspecto | Airflow | Dagster | Prefect |\n",
    "|---------|---------|---------|---------|\n",
    "| **Madurez** | ✅ 10+ años | ⚠️ 5 años | ⚠️ 5 años |\n",
    "| **Learning Curve** | Steep | Medium | Easy |\n",
    "| **Data Testing** | Manual (GE) | Native | Via hooks |\n",
    "| **Data Lineage** | Plugin (OpenLineage) | Native | Plugin |\n",
    "| **Deployment** | Complex | Docker-first | Cloud-native |\n",
    "| **Community** | Huge | Growing | Growing |\n",
    "\n",
    "**Decisión: Airflow**\n",
    "- ✅ Ecosistema maduro (miles de operators)\n",
    "- ✅ OpenLineage integration para linaje\n",
    "- ✅ Skills existentes en el equipo\n",
    "- ⚠️ Complejidad operativa (mitigada con MWAA/Astronomer)\n",
    "\n",
    "```python\n",
    "# Modern Airflow Pattern: TaskFlow API\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    schedule=\"@daily\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"ecommerce\", \"ventas\"]\n",
    ")\n",
    "def ventas_pipeline():\n",
    "    \n",
    "    @task\n",
    "    def extract():\n",
    "        return {\"path\": \"s3://bucket/raw/ventas/2024-01-15/\"}\n",
    "    \n",
    "    @task\n",
    "    def validate(data):\n",
    "        # Great Expectations\n",
    "        return {\"valid\": True, \"errors\": []}\n",
    "    \n",
    "    @task\n",
    "    def transform(data):\n",
    "        # Spark job\n",
    "        return {\"output\": \"s3://bucket/curated/ventas/\"}\n",
    "    \n",
    "    @task\n",
    "    def publish_lineage(data):\n",
    "        # OpenLineage event\n",
    "        pass\n",
    "    \n",
    "    data = extract()\n",
    "    validation = validate(data)\n",
    "    transformed = transform(validation)\n",
    "    publish_lineage(transformed)\n",
    "\n",
    "dag = ventas_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Governance: Glue Catalog vs Unity Catalog vs DataHub**\n",
    "\n",
    "**Comparación:**\n",
    "\n",
    "|  | Glue Catalog | Unity Catalog | DataHub |\n",
    "|--|--------------|---------------|---------|\n",
    "| **Cloud** | AWS only | Databricks | Any |\n",
    "| **Lineage** | Limited | Column-level | Column-level |\n",
    "| **RBAC** | Lake Formation | Native | Native |\n",
    "| **API** | Boto3 | REST/SQL | GraphQL |\n",
    "| **Cost** | $1/million API calls | Included | Open-source |\n",
    "\n",
    "**Nuestra arquitectura híbrida:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Governance Stack                │\n",
    "├─────────────────────────────────────────┤\n",
    "│                                          │\n",
    "│  Glue Catalog (Technical Metadata)      │\n",
    "│    - Table schemas                      │\n",
    "│    - Partitions                         │\n",
    "│    - Athena/Spark integration           │\n",
    "│                                          │\n",
    "│          ↓ (sync via API)               │\n",
    "│                                          │\n",
    "│  DataHub (Business Metadata)            │\n",
    "│    - Ownership (Data Engineer: Jane)    │\n",
    "│    - Tags (PII, GDPR, Confidential)     │\n",
    "│    - Glossary (ARR = Annual Revenue)    │\n",
    "│    - Lineage (upstream/downstream)      │\n",
    "│                                          │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Sync Script:**\n",
    "\n",
    "```python\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import DatasetPropertiesClass\n",
    "import boto3\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "# Sync Glue → DataHub\n",
    "for table in glue.get_tables(DatabaseName='ecommerce')['TableList']:\n",
    "    dataset_urn = f\"urn:li:dataset:(urn:li:dataPlatform:glue,ecommerce.{table['Name']},PROD)\"\n",
    "    \n",
    "    properties = DatasetPropertiesClass(\n",
    "        customProperties={\n",
    "            \"glue_database\": \"ecommerce\",\n",
    "            \"owner\": table.get(\"Owner\", \"unknown\"),\n",
    "            \"location\": table[\"StorageDescriptor\"][\"Location\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    emitter.emit_mcp(\n",
    "        MetadataChangeProposalWrapper(\n",
    "            entityUrn=dataset_urn,\n",
    "            aspect=properties\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Trade-offs Críticos**\n",
    "\n",
    "**Latencia vs Costo:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Streaming (Kafka + Spark)              │\n",
    "│  - Latencia: <30s                       │\n",
    "│  - Costo: $3,000/mes (EMR 24/7)         │\n",
    "└─────────────────────────────────────────┘\n",
    "           ↕ (decisión de negocio)\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Mini-batch (Airflow cada 5 min)        │\n",
    "│  - Latencia: 5-10 min                   │\n",
    "│  - Costo: $800/mes (spot instances)     │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Pregunta clave:** ¿El negocio necesita realmente <1 min de latencia?\n",
    "- Fraude detection: **Sí** → Streaming\n",
    "- Reportes ejecutivos: **No** → Batch\n",
    "\n",
    "**Consistencia vs Disponibilidad (CAP Theorem):**\n",
    "\n",
    "```python\n",
    "# Opción 1: Fuerte consistencia (CP)\n",
    "# - Usar RDS/PostgreSQL para metadata crítica\n",
    "# - Transacciones ACID garantizadas\n",
    "# - Downtime si network partition\n",
    "\n",
    "# Opción 2: Eventual consistency (AP)\n",
    "# - Usar S3 + DynamoDB\n",
    "# - Alta disponibilidad (99.99%)\n",
    "# - Posibles lecturas stale durante <1s\n",
    "\n",
    "# Decisión: Hybrid\n",
    "# - Transacciones financieras → RDS (CP)\n",
    "# - Eventos de usuario → S3 (AP)\n",
    "```\n",
    "\n",
    "**Open Source vs Managed Services:**\n",
    "\n",
    "| Componente | Open Source | Managed | Decisión |\n",
    "|------------|-------------|---------|----------|\n",
    "| **Kafka** | Self-hosted (EC2) | MSK | MSK (menos ops) |\n",
    "| **Airflow** | Docker Compose | MWAA | MWAA (scaling) |\n",
    "| **Spark** | EMR manual | EMR Serverless | EMR Serverless |\n",
    "| **DataHub** | K8s deployment | N/A | Self-hosted |\n",
    "\n",
    "**Criterio:** Si existe managed y costo <2x, usar managed (reduce toil).\n",
    "\n",
    "---\n",
    "\n",
    "**7. Disaster Recovery Strategy**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Backup & Recovery               │\n",
    "├─────────────────────────────────────────┤\n",
    "│                                          │\n",
    "│  S3 (Data Lake)                         │\n",
    "│    - Versioning: Enabled                │\n",
    "│    - Replication: us-east-1 → eu-west-1 │\n",
    "│    - RTO: <1 hour                       │\n",
    "│    - RPO: <15 min                       │\n",
    "│                                          │\n",
    "│  RDS (Metadata)                         │\n",
    "│    - Automated backups: Daily           │\n",
    "│    - Multi-AZ: Enabled                  │\n",
    "│    - RTO: <5 min (failover)             │\n",
    "│    - RPO: <5 min (sync replica)         │\n",
    "│                                          │\n",
    "│  Glue Catalog                           │\n",
    "│    - Backup: CloudFormation export      │\n",
    "│    - Restore: boto3 script              │\n",
    "│                                          │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Recovery Runbook:**\n",
    "\n",
    "```bash\n",
    "# Scenario: S3 bucket accidentally deleted\n",
    "# 1. Check versioning\n",
    "aws s3api list-object-versions --bucket ecommerce-data\n",
    "\n",
    "# 2. Restore from version\n",
    "aws s3api copy-object \\\n",
    "  --copy-source ecommerce-data/curated/ventas/file.parquet?versionId=abc123 \\\n",
    "  --bucket ecommerce-data-recovered \\\n",
    "  --key curated/ventas/file.parquet\n",
    "\n",
    "# 3. Failover to replica bucket\n",
    "# Update Glue catalog locations\n",
    "aws glue update-table --database ecommerce --table-input '{\n",
    "  \"Name\": \"ventas\",\n",
    "  \"StorageDescriptor\": {\n",
    "    \"Location\": \"s3://ecommerce-data-replica/curated/ventas/\"\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Scaling Strategy**\n",
    "\n",
    "**Current State (MVP):**\n",
    "- 100 GB/day ingest\n",
    "- 10 pipelines\n",
    "- 5 data engineers\n",
    "- $5K/month\n",
    "\n",
    "**12-month projection:**\n",
    "- 1 TB/day ingest (10x)\n",
    "- 50 pipelines (5x)\n",
    "- 15 engineers (3x)\n",
    "- Budget: $15K/month (3x)\n",
    "\n",
    "**Scaling plan:**\n",
    "\n",
    "```python\n",
    "# 1. Data scaling\n",
    "# - Partition strategy: dt + region (avoid skew)\n",
    "# - Compaction: OPTIMIZE every week\n",
    "# - Archival: Move raw to Glacier after 30 days\n",
    "\n",
    "# 2. Compute scaling\n",
    "# - Airflow: Celery executor (10 workers → 50)\n",
    "# - Spark: EMR Serverless (auto-scale 1-100 instances)\n",
    "# - FastAPI: ECS Fargate (auto-scale on CPU)\n",
    "\n",
    "# 3. Team scaling\n",
    "# - Data Platform team: 2 SREs\n",
    "# - Domain teams: 3x Data Engineers per domain\n",
    "# - Self-service: DataHub + Airflow UI + Jupyter\n",
    "\n",
    "# 4. Cost optimization\n",
    "# - Spot instances: 70% savings on EMR\n",
    "# - S3 Intelligent Tiering: 40% savings\n",
    "# - Reserved capacity: Athena ($100/TB → $50/TB)\n",
    "```\n",
    "\n",
    "**Bottleneck analysis:**\n",
    "\n",
    "```python\n",
    "# ¿Dónde fallará primero?\n",
    "bottlenecks = {\n",
    "    \"S3 PUT rate\": \"3,500 req/s (→ increase prefix diversity)\",\n",
    "    \"Glue Catalog API\": \"10 req/s (→ cache with Redis)\",\n",
    "    \"Athena concurrency\": \"25 queries (→ upgrade to Trino)\",\n",
    "    \"Airflow scheduler\": \"1000 tasks/s (→ HA scheduler)\",\n",
    "    \"Team knowledge\": \"Onboarding (→ internal wiki + mentorship)\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f4128",
   "metadata": {},
   "source": [
    "### ⚡ **Streaming Path: Kafka → Spark → Delta Lake**\n",
    "\n",
    "**1. Kafka Setup: Event-Driven Architecture**\n",
    "\n",
    "**Docker Compose (Local Development):**\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "  \n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n",
    "      KAFKA_LOG_RETENTION_HOURS: 168  # 7 días\n",
    "      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1 GB\n",
    "      KAFKA_COMPRESSION_TYPE: \"snappy\"\n",
    "  \n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.5.0\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092\n",
    "```\n",
    "\n",
    "**Production: AWS MSK Configuration:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "msk = boto3.client('kafka', region_name='us-east-1')\n",
    "\n",
    "# Create MSK cluster\n",
    "response = msk.create_cluster_v2(\n",
    "    ClusterName='ecommerce-kafka',\n",
    "    Serverless={\n",
    "        'VpcConfigs': [{\n",
    "            'SubnetIds': ['subnet-123', 'subnet-456'],\n",
    "            'SecurityGroupIds': ['sg-789']\n",
    "        }],\n",
    "        'ClientAuthentication': {\n",
    "            'Sasl': {\n",
    "                'Iam': {'Enabled': True}  # IAM authentication\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    Tags={\n",
    "        'Environment': 'production',\n",
    "        'CostCenter': 'data-platform'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Costo estimado: $2,500/mes (auto-scaling 2-16 brokers)\n",
    "```\n",
    "\n",
    "**Topic Design:**\n",
    "\n",
    "```python\n",
    "# topics.py\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n",
    "\n",
    "topics = [\n",
    "    NewTopic(\n",
    "        name='ecommerce.ventas.v1',\n",
    "        num_partitions=12,  # 12 partitions = 12 parallel consumers\n",
    "        replication_factor=3,  # High availability\n",
    "        topic_configs={\n",
    "            'retention.ms': '604800000',  # 7 días\n",
    "            'cleanup.policy': 'delete',\n",
    "            'compression.type': 'snappy',\n",
    "            'max.message.bytes': '1048576',  # 1 MB\n",
    "        }\n",
    "    ),\n",
    "    NewTopic(\n",
    "        name='ecommerce.eventos_usuario.v1',\n",
    "        num_partitions=24,  # High throughput\n",
    "        replication_factor=3,\n",
    "        topic_configs={\n",
    "            'retention.ms': '86400000',  # 1 día (eventos efímeros)\n",
    "            'cleanup.policy': 'delete',\n",
    "            'compression.type': 'lz4',  # Mejor para logs\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "admin.create_topics(topics)\n",
    "```\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "\n",
    "```python\n",
    "# ❌ Mal: Random partitioning (no ordering)\n",
    "producer.send('ventas', value=mensaje)\n",
    "\n",
    "# ✅ Bien: Key-based partitioning (ordering por cliente)\n",
    "producer.send(\n",
    "    'ventas',\n",
    "    key=str(cliente_id).encode('utf-8'),  # Mismo cliente → misma partition\n",
    "    value=json.dumps(mensaje).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Resultado: Eventos del cliente 123 siempre en partition 5\n",
    "# - Ordering garantizado por cliente\n",
    "# - Permite procesamiento stateful (aggregations)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Event Producer: Transacciones Sintéticas**\n",
    "\n",
    "```python\n",
    "# producer.py\n",
    "from kafka import KafkaProducer\n",
    "from faker import Faker\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    acks='all',  # Wait for all replicas\n",
    "    retries=3,\n",
    "    max_in_flight_requests_per_connection=5,\n",
    "    compression_type='snappy'\n",
    ")\n",
    "\n",
    "def generate_transaction():\n",
    "    \"\"\"Genera transacción sintética realista\"\"\"\n",
    "    cliente_id = random.randint(1000, 9999)\n",
    "    productos = random.choices(\n",
    "        ['laptop', 'phone', 'tablet', 'headphones', 'monitor'],\n",
    "        weights=[0.1, 0.4, 0.2, 0.2, 0.1],  # Phone más común\n",
    "        k=random.randint(1, 3)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'transaccion_id': fake.uuid4(),\n",
    "        'cliente_id': cliente_id,\n",
    "        'email': fake.email(),\n",
    "        'productos': productos,\n",
    "        'total': round(sum(random.uniform(50, 2000) for _ in productos), 2),\n",
    "        'metodo_pago': random.choice(['credit_card', 'debit_card', 'paypal']),\n",
    "        'tarjeta_ultimos_4': fake.credit_card_number()[-4:],\n",
    "        'region': random.choice(['LATAM', 'NA', 'EU', 'APAC']),\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        'metadata': {\n",
    "            'ip': fake.ipv4(),\n",
    "            'user_agent': fake.user_agent()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def produce_events(rate_per_second=100):\n",
    "    \"\"\"Produce eventos a rate constante\"\"\"\n",
    "    interval = 1.0 / rate_per_second\n",
    "    \n",
    "    while True:\n",
    "        start = time.time()\n",
    "        \n",
    "        transaction = generate_transaction()\n",
    "        \n",
    "        # Send con callback para manejo de errores\n",
    "        future = producer.send(\n",
    "            'ecommerce.ventas.v1',\n",
    "            key=str(transaction['cliente_id']),\n",
    "            value=transaction\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            metadata = future.get(timeout=10)\n",
    "            print(f\"✅ Sent to partition {metadata.partition}, offset {metadata.offset}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        elapsed = time.time() - start\n",
    "        time.sleep(max(0, interval - elapsed))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"🚀 Producing events at 100/sec...\")\n",
    "    produce_events(rate_per_second=100)\n",
    "    # 100 events/sec = 8.6M events/day\n",
    "```\n",
    "\n",
    "**Schema Evolution con Avro:**\n",
    "\n",
    "```python\n",
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "# Schema Registry: versioning automático\n",
    "value_schema = avro.loads('''\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Venta\",\n",
    "  \"namespace\": \"com.ecommerce\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"transaccion_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"cliente_id\", \"type\": \"int\"},\n",
    "    {\"name\": \"total\", \"type\": \"double\"},\n",
    "    {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"},\n",
    "    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}  # Nuevo campo opcional\n",
    "  ]\n",
    "}\n",
    "''')\n",
    "\n",
    "producer = AvroProducer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': 'http://localhost:8081'\n",
    "}, default_value_schema=value_schema)\n",
    "\n",
    "# Consumers automáticamente validan schema\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Spark Structured Streaming Consumer**\n",
    "\n",
    "```python\n",
    "# streaming_consumer.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.streaming.kafka.consumer.poll.ms\", \"512\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema del evento\n",
    "schema = StructType([\n",
    "    StructField(\"transaccion_id\", StringType()),\n",
    "    StructField(\"cliente_id\", IntegerType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"productos\", ArrayType(StringType())),\n",
    "    StructField(\"total\", DoubleType()),\n",
    "    StructField(\"metodo_pago\", StringType()),\n",
    "    StructField(\"tarjeta_ultimos_4\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"user_agent\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "raw_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"ecommerce.ventas.v1\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed_stream = raw_stream \\\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\"data.*\", \"kafka_key\", \"topic\", \"partition\", \"offset\", \"kafka_timestamp\")\n",
    "\n",
    "# Data Quality: Filter invalid records\n",
    "validated_stream = parsed_stream \\\n",
    "    .filter(col(\"transaccion_id\").isNotNull()) \\\n",
    "    .filter(col(\"cliente_id\") > 0) \\\n",
    "    .filter(col(\"total\") >= 0) \\\n",
    "    .filter(col(\"timestamp\").isNotNull())\n",
    "\n",
    "# Enrich: Parse timestamp\n",
    "enriched_stream = validated_stream \\\n",
    "    .withColumn(\"processed_at\", current_timestamp()) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# PII Masking (GDPR compliance)\n",
    "masked_stream = enriched_stream \\\n",
    "    .withColumn(\"email_masked\", \n",
    "        regexp_replace(col(\"email\"), r\"(?<=.{2}).(?=[^@]*?@)\", \"*\")\n",
    "    ) \\\n",
    "    .withColumn(\"tarjeta_masked\", \n",
    "        concat(lit(\"****-\"), col(\"tarjeta_ultimos_4\"))\n",
    "    ) \\\n",
    "    .drop(\"email\", \"tarjeta_ultimos_4\")\n",
    "\n",
    "# Write to Delta Lake (curated layer)\n",
    "query = masked_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://ecommerce-data/checkpoints/ventas/\") \\\n",
    "    .option(\"path\", \"s3://ecommerce-data/curated/ventas/\") \\\n",
    "    .partitionBy(\"event_date\", \"region\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Exactly-Once Semantics:**\n",
    "\n",
    "```python\n",
    "# Checkpoint garantiza idempotencia\n",
    "# - Kafka offsets guardados en checkpoint\n",
    "# - Si job falla, restart desde último offset\n",
    "# - Delta Lake transactional writes previenen duplicates\n",
    "\n",
    "# Verification query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        COUNT(DISTINCT transaccion_id) as unique_transactions,\n",
    "        COUNT(*) as total_rows\n",
    "    FROM delta.`s3://ecommerce-data/curated/ventas/`\n",
    "    GROUP BY event_date\n",
    "    HAVING COUNT(*) != COUNT(DISTINCT transaccion_id)\n",
    "\"\"\").show()\n",
    "# Si hay diferencia → investigar duplicates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Stateful Aggregations: Windowing**\n",
    "\n",
    "```python\n",
    "# Real-time aggregations con watermarking\n",
    "windowed_agg = masked_stream \\\n",
    "    .withWatermark(\"processed_at\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"processed_at\", \"5 minutes\"),\n",
    "        \"region\",\n",
    "        \"metodo_pago\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transacciones\"),\n",
    "        sum(\"total\").alias(\"revenue_total\"),\n",
    "        avg(\"total\").alias(\"ticket_promedio\"),\n",
    "        approx_count_distinct(\"cliente_id\").alias(\"clientes_unicos\")\n",
    "    )\n",
    "\n",
    "# Write aggregations to gold layer\n",
    "windowed_query = windowed_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\")  # Update existing windows \\\n",
    "    .option(\"checkpointLocation\", \"s3://ecommerce-data/checkpoints/ventas_agg/\") \\\n",
    "    .option(\"path\", \"s3://ecommerce-data/gold/ventas_realtime/\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Monitoring Dashboard Query:**\n",
    "\n",
    "```sql\n",
    "-- Athena query para dashboard en tiempo real\n",
    "SELECT \n",
    "    window.start as window_start,\n",
    "    region,\n",
    "    num_transacciones,\n",
    "    revenue_total,\n",
    "    ticket_promedio\n",
    "FROM gold.ventas_realtime\n",
    "WHERE window.start >= current_timestamp - interval '1' hour\n",
    "ORDER BY window.start DESC, revenue_total DESC\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Error Handling & Dead Letter Queue**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def write_to_dlq(df, error_type):\n",
    "    \"\"\"Escribe registros problemáticos a Dead Letter Queue\"\"\"\n",
    "    df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"error_date\") \\\n",
    "        .save(f\"s3://ecommerce-data/dlq/{error_type}/\")\n",
    "\n",
    "# Captura schema mismatch\n",
    "try:\n",
    "    parsed_stream = raw_stream.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    )\n",
    "except AnalysisException as e:\n",
    "    # Log a DLQ para debugging\n",
    "    raw_stream.writeStream \\\n",
    "        .foreachBatch(lambda df, epoch_id: write_to_dlq(df, \"schema_error\")) \\\n",
    "        .start()\n",
    "\n",
    "# Alert on DLQ growth\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        error_type,\n",
    "        COUNT(*) as error_count\n",
    "    FROM parquet.`s3://ecommerce-data/dlq/*/*`\n",
    "    WHERE error_date = current_date()\n",
    "    GROUP BY error_type\n",
    "    HAVING COUNT(*) > 100  -- Alert threshold\n",
    "\"\"\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Performance Tuning**\n",
    "\n",
    "```python\n",
    "# Optimizaciones críticas\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Memory tuning para streaming\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "spark.conf.set(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")  # State backend eficiente\n",
    "\n",
    "# Kafka optimizations\n",
    "spark.conf.set(\"spark.streaming.kafka.consumer.cache.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.kafka.consumer.poll.ms\", \"512\")\n",
    "\n",
    "# Delta Lake optimizations\n",
    "spark.sql(\"OPTIMIZE delta.`s3://ecommerce-data/curated/ventas/` ZORDER BY (cliente_id)\")\n",
    "spark.sql(\"VACUUM delta.`s3://ecommerce-data/curated/ventas/` RETAIN 168 HOURS\")  # 7 días\n",
    "```\n",
    "\n",
    "**Metrics Collection:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "from prometheus_client import Counter, Gauge, Histogram\n",
    "\n",
    "# Prometheus metrics\n",
    "records_processed = Counter('streaming_records_processed_total', 'Records processed')\n",
    "batch_duration = Histogram('streaming_batch_duration_seconds', 'Batch processing time')\n",
    "current_lag = Gauge('streaming_kafka_lag', 'Kafka consumer lag')\n",
    "\n",
    "def process_batch(df: DataFrame, epoch_id: int):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Write logic\n",
    "    df.write.format(\"delta\").mode(\"append\").save(\"s3://...\")\n",
    "    \n",
    "    # Update metrics\n",
    "    records_processed.inc(df.count())\n",
    "    batch_duration.observe(time.time() - start)\n",
    "    \n",
    "    # Log progress\n",
    "    print(f\"Epoch {epoch_id}: {df.count()} records in {time.time()-start:.2f}s\")\n",
    "\n",
    "query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Deployment: EMR Serverless**\n",
    "\n",
    "```python\n",
    "# deploy_streaming.py\n",
    "import boto3\n",
    "\n",
    "emr_serverless = boto3.client('emr-serverless', region_name='us-east-1')\n",
    "\n",
    "# Create application\n",
    "app_response = emr_serverless.create_application(\n",
    "    name='ecommerce-streaming',\n",
    "    releaseLabel='emr-6.12.0',\n",
    "    type='SPARK',\n",
    "    autoStartConfiguration={'enabled': True},\n",
    "    autoStopConfiguration={\n",
    "        'enabled': True,\n",
    "        'idleTimeoutMinutes': 15\n",
    "    },\n",
    "    initialCapacity={\n",
    "        'DRIVER': {\n",
    "            'workerCount': 1,\n",
    "            'workerConfiguration': {\n",
    "                'cpu': '2 vCPU',\n",
    "                'memory': '8 GB'\n",
    "            }\n",
    "        },\n",
    "        'EXECUTOR': {\n",
    "            'workerCount': 10,\n",
    "            'workerConfiguration': {\n",
    "                'cpu': '4 vCPU',\n",
    "                'memory': '16 GB',\n",
    "                'disk': '100 GB'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    maximumCapacity={\n",
    "        'cpu': '200 vCPU',\n",
    "        'memory': '800 GB'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Submit streaming job\n",
    "job_response = emr_serverless.start_job_run(\n",
    "    applicationId=app_response['applicationId'],\n",
    "    executionRoleArn='arn:aws:iam::123456:role/EMRServerlessRole',\n",
    "    jobDriver={\n",
    "        'sparkSubmit': {\n",
    "            'entryPoint': 's3://ecommerce-code/streaming_consumer.py',\n",
    "            'sparkSubmitParameters': (\n",
    "                '--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension '\n",
    "                '--conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog '\n",
    "                '--packages io.delta:delta-core_2.12:2.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0'\n",
    "            )\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Job started: {job_response['jobRunId']}\")\n",
    "# Costo: ~$0.052/vCPU-hour + $0.0065/GB-hour\n",
    "# Estimado: $1,500/mes (24/7 con auto-scaling)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f997b",
   "metadata": {},
   "source": [
    "### 🔄 **Batch Path: Airflow + Great Expectations + DataHub**\n",
    "\n",
    "**1. Airflow DAG: Orchestration Best Practices**\n",
    "\n",
    "```python\n",
    "# dags/ventas_batch_pipeline.py\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.operators.emr import EmrServerlessStartJobOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import great_expectations as gx\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-platform',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['data-team@ecommerce.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'retry_exponential_backoff': True,\n",
    "    'max_retry_delay': timedelta(minutes=30),\n",
    "    'sla': timedelta(hours=2),  # SLA: pipeline completa en 2h\n",
    "}\n",
    "\n",
    "@dag(\n",
    "    dag_id='ventas_batch_pipeline_v1',\n",
    "    default_args=default_args,\n",
    "    description='Daily batch pipeline: SFTP → validate → transform → gold',\n",
    "    schedule='0 2 * * *',  # 2 AM UTC daily\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['ecommerce', 'ventas', 'batch', 'production'],\n",
    "    doc_md=\"\"\"\n",
    "    # Ventas Batch Pipeline\n",
    "    \n",
    "    ## Purpose\n",
    "    Daily ETL for historical sales data and aggregations.\n",
    "    \n",
    "    ## SLA\n",
    "    - Completion: 2 hours\n",
    "    - Data freshness: D+1 (next day at 4 AM)\n",
    "    \n",
    "    ## Dependencies\n",
    "    - Upstream: SFTP files from SAP (uploaded at 1 AM)\n",
    "    - Downstream: BI dashboards (Tableau refresh at 6 AM)\n",
    "    \n",
    "    ## Contacts\n",
    "    - Owner: Data Platform Team\n",
    "    - On-call: data-oncall@ecommerce.com\n",
    "    \"\"\"\n",
    ")\n",
    "def ventas_batch_pipeline():\n",
    "    \n",
    "    @task(task_id='check_sftp_files')\n",
    "    def check_sftp_files(ds):\n",
    "        \"\"\"Verifica que archivos SFTP existen\"\"\"\n",
    "        from airflow.providers.sftp.hooks.sftp import SFTPHook\n",
    "        \n",
    "        sftp = SFTPHook(sftp_conn_id='sap_sftp')\n",
    "        expected_files = [\n",
    "            f'/exports/ventas_{ds}.csv',\n",
    "            f'/exports/clientes_{ds}.csv',\n",
    "            f'/exports/productos_{ds}.csv'\n",
    "        ]\n",
    "        \n",
    "        missing = []\n",
    "        for file_path in expected_files:\n",
    "            if not sftp.path_exists(file_path):\n",
    "                missing.append(file_path)\n",
    "        \n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing files: {missing}\")\n",
    "        \n",
    "        return {\n",
    "            'files': expected_files,\n",
    "            'total_size': sum(sftp.get_file_size(f) for f in expected_files)\n",
    "        }\n",
    "    \n",
    "    @task(task_id='download_to_s3')\n",
    "    def download_to_s3(file_info, ds):\n",
    "        \"\"\"Descarga archivos de SFTP a S3 raw\"\"\"\n",
    "        from airflow.providers.sftp.hooks.sftp import SFTPHook\n",
    "        \n",
    "        sftp = SFTPHook(sftp_conn_id='sap_sftp')\n",
    "        s3 = S3Hook(aws_conn_id='aws_default')\n",
    "        \n",
    "        for file_path in file_info['files']:\n",
    "            local_path = f'/tmp/{file_path.split(\"/\")[-1]}'\n",
    "            s3_key = f'raw/ventas/dt={ds}/{file_path.split(\"/\")[-1]}'\n",
    "            \n",
    "            # Download from SFTP\n",
    "            sftp.retrieve_file(file_path, local_path)\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3.load_file(\n",
    "                filename=local_path,\n",
    "                key=s3_key,\n",
    "                bucket_name='ecommerce-data',\n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Uploaded {file_path} → s3://ecommerce-data/{s3_key}\")\n",
    "        \n",
    "        return f's3://ecommerce-data/raw/ventas/dt={ds}/'\n",
    "    \n",
    "    @task(task_id='run_data_quality_checks')\n",
    "    def run_data_quality_checks(s3_path, ds):\n",
    "        \"\"\"Great Expectations validations\"\"\"\n",
    "        context = gx.get_context()\n",
    "        \n",
    "        # Data source (S3)\n",
    "        datasource = context.sources.add_pandas_s3(\n",
    "            name=\"s3_datasource\",\n",
    "            bucket=\"ecommerce-data\",\n",
    "            boto3_options={\"region_name\": \"us-east-1\"}\n",
    "        )\n",
    "        \n",
    "        # Expectations\n",
    "        expectations = [\n",
    "            # Completeness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_not_be_null',\n",
    "                'kwargs': {'column': 'transaccion_id'}\n",
    "            },\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_not_be_null',\n",
    "                'kwargs': {'column': 'cliente_id'}\n",
    "            },\n",
    "            # Validity\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_between',\n",
    "                'kwargs': {'column': 'total', 'min_value': 0, 'max_value': 100000}\n",
    "            },\n",
    "            # Uniqueness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_unique',\n",
    "                'kwargs': {'column': 'transaccion_id'}\n",
    "            },\n",
    "            # Freshness\n",
    "            {\n",
    "                'expectation_type': 'expect_column_max_to_be_between',\n",
    "                'kwargs': {\n",
    "                    'column': 'fecha_transaccion',\n",
    "                    'min_value': ds,\n",
    "                    'max_value': ds\n",
    "                }\n",
    "            },\n",
    "            # Referential integrity\n",
    "            {\n",
    "                'expectation_type': 'expect_column_values_to_be_in_set',\n",
    "                'kwargs': {\n",
    "                    'column': 'region',\n",
    "                    'value_set': ['LATAM', 'NA', 'EU', 'APAC']\n",
    "                }\n",
    "            },\n",
    "            # Volume check\n",
    "            {\n",
    "                'expectation_type': 'expect_table_row_count_to_be_between',\n",
    "                'kwargs': {\n",
    "                    'min_value': 50000,  # At least 50K transactions\n",
    "                    'max_value': 1000000\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create suite\n",
    "        suite = context.add_expectation_suite(\n",
    "            expectation_suite_name=f\"ventas_suite_{ds}\"\n",
    "        )\n",
    "        \n",
    "        for exp in expectations:\n",
    "            suite.add_expectation(**exp)\n",
    "        \n",
    "        # Validate\n",
    "        validator = context.get_validator(\n",
    "            batch_request=datasource.get_asset(f\"raw/ventas/dt={ds}/ventas_{ds}.csv\"),\n",
    "            expectation_suite_name=suite.expectation_suite_name\n",
    "        )\n",
    "        \n",
    "        results = validator.validate()\n",
    "        \n",
    "        if not results.success:\n",
    "            # Log failures\n",
    "            failures = [r for r in results.results if not r.success]\n",
    "            print(f\"❌ {len(failures)} validation failures:\")\n",
    "            for failure in failures:\n",
    "                print(f\"  - {failure.expectation_config.expectation_type}: {failure.result}\")\n",
    "            \n",
    "            raise ValueError(f\"Data quality checks failed: {len(failures)} issues\")\n",
    "        \n",
    "        print(f\"✅ All {len(expectations)} validation checks passed\")\n",
    "        return results.to_json_dict()\n",
    "    \n",
    "    @task.branch(task_id='check_validation_results')\n",
    "    def check_validation_results(validation_results):\n",
    "        \"\"\"Branch based on validation results\"\"\"\n",
    "        if validation_results['success']:\n",
    "            return 'transform_to_curated'\n",
    "        else:\n",
    "            return 'send_failure_alert'\n",
    "    \n",
    "    @task(task_id='transform_to_curated')\n",
    "    def transform_to_curated(s3_path, ds):\n",
    "        \"\"\"Spark job: raw → curated (cleaned + enriched)\"\"\"\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import *\n",
    "        \n",
    "        spark = SparkSession.builder.appName(\"VentasCurated\").getOrCreate()\n",
    "        \n",
    "        # Read raw\n",
    "        raw_df = spark.read.csv(f\"{s3_path}/ventas_{ds}.csv\", header=True, inferSchema=True)\n",
    "        \n",
    "        # Transformations\n",
    "        curated_df = raw_df \\\n",
    "            .filter(col(\"total\") > 0) \\\n",
    "            .withColumn(\"fecha_procesamiento\", current_date()) \\\n",
    "            .withColumn(\"year\", year(col(\"fecha_transaccion\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"fecha_transaccion\"))) \\\n",
    "            .withColumn(\"email_masked\", \n",
    "                regexp_replace(col(\"email\"), r\"(?<=.{2}).(?=[^@]*?@)\", \"*\")\n",
    "            ) \\\n",
    "            .withColumn(\"tarjeta_masked\", \n",
    "                concat(lit(\"****-\"), substring(col(\"tarjeta_numero\"), -4, 4))\n",
    "            ) \\\n",
    "            .drop(\"email\", \"tarjeta_numero\")\n",
    "        \n",
    "        # Enrich with dimension tables\n",
    "        clientes_df = spark.read.format(\"delta\").load(\"s3://ecommerce-data/curated/clientes/\")\n",
    "        productos_df = spark.read.format(\"delta\").load(\"s3://ecommerce-data/curated/productos/\")\n",
    "        \n",
    "        enriched_df = curated_df \\\n",
    "            .join(clientes_df, \"cliente_id\", \"left\") \\\n",
    "            .join(productos_df, \"producto_id\", \"left\")\n",
    "        \n",
    "        # Write to Delta (curated)\n",
    "        enriched_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"year\", \"month\", \"region\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(f\"s3://ecommerce-data/curated/ventas/dt={ds}/\")\n",
    "        \n",
    "        # Optimize\n",
    "        spark.sql(f\"\"\"\n",
    "            OPTIMIZE delta.`s3://ecommerce-data/curated/ventas/dt={ds}/`\n",
    "            ZORDER BY (cliente_id, producto_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        return {\n",
    "            'rows_processed': enriched_df.count(),\n",
    "            'partitions_written': enriched_df.select(\"region\").distinct().count()\n",
    "        }\n",
    "    \n",
    "    @task(task_id='create_gold_aggregations')\n",
    "    def create_gold_aggregations(curated_info, ds):\n",
    "        \"\"\"Create gold layer aggregations\"\"\"\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import *\n",
    "        \n",
    "        spark = SparkSession.builder.appName(\"VentasGold\").getOrCreate()\n",
    "        \n",
    "        df = spark.read.format(\"delta\").load(f\"s3://ecommerce-data/curated/ventas/dt={ds}/\")\n",
    "        \n",
    "        # Aggregation 1: Daily sales by region\n",
    "        daily_region = df.groupBy(\"fecha_transaccion\", \"region\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"num_transacciones\"),\n",
    "                sum(\"total\").alias(\"revenue_total\"),\n",
    "                avg(\"total\").alias(\"ticket_promedio\"),\n",
    "                countDistinct(\"cliente_id\").alias(\"clientes_unicos\")\n",
    "            )\n",
    "        \n",
    "        daily_region.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"fecha_transaccion\") \\\n",
    "            .save(\"s3://ecommerce-data/gold/ventas_diarias_region/\")\n",
    "        \n",
    "        # Aggregation 2: Product performance\n",
    "        product_perf = df.groupBy(\"producto_id\", \"producto_nombre\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"unidades_vendidas\"),\n",
    "                sum(\"total\").alias(\"revenue_total\"),\n",
    "                avg(\"total\").alias(\"precio_promedio\")\n",
    "            ) \\\n",
    "            .orderBy(desc(\"revenue_total\"))\n",
    "        \n",
    "        product_perf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"s3://ecommerce-data/gold/productos_performance/dt={ds}/\")\n",
    "        \n",
    "        # Aggregation 3: Customer RFM (Recency, Frequency, Monetary)\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        customer_rfm = df.groupBy(\"cliente_id\") \\\n",
    "            .agg(\n",
    "                max(\"fecha_transaccion\").alias(\"ultima_compra\"),\n",
    "                count(\"*\").alias(\"frecuencia\"),\n",
    "                sum(\"total\").alias(\"valor_monetario\")\n",
    "            ) \\\n",
    "            .withColumn(\"recencia_dias\", \n",
    "                datediff(lit(ds), col(\"ultima_compra\"))\n",
    "            ) \\\n",
    "            .withColumn(\"rfm_score\",\n",
    "                col(\"frecuencia\") * 0.3 + col(\"valor_monetario\") * 0.5 - col(\"recencia_dias\") * 0.2\n",
    "            )\n",
    "        \n",
    "        customer_rfm.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"s3://ecommerce-data/gold/customer_rfm/dt={ds}/\")\n",
    "        \n",
    "        return {\n",
    "            'aggregations_created': 3,\n",
    "            'gold_path': f\"s3://ecommerce-data/gold/\"\n",
    "        }\n",
    "    \n",
    "    @task(task_id='update_glue_catalog')\n",
    "    def update_glue_catalog(gold_info, ds):\n",
    "        \"\"\"Update Glue Data Catalog partitions\"\"\"\n",
    "        import boto3\n",
    "        \n",
    "        glue = boto3.client('glue', region_name='us-east-1')\n",
    "        \n",
    "        tables = [\n",
    "            ('ecommerce', 'ventas_diarias_region'),\n",
    "            ('ecommerce', 'productos_performance'),\n",
    "            ('ecommerce', 'customer_rfm')\n",
    "        ]\n",
    "        \n",
    "        for database, table in tables:\n",
    "            try:\n",
    "                # Add partition\n",
    "                glue.create_partition(\n",
    "                    DatabaseName=database,\n",
    "                    TableName=table,\n",
    "                    PartitionInput={\n",
    "                        'Values': [ds],\n",
    "                        'StorageDescriptor': {\n",
    "                            'Location': f\"s3://ecommerce-data/gold/{table}/dt={ds}/\",\n",
    "                            'InputFormat': 'org.apache.hadoop.mapred.SequenceFileInputFormat',\n",
    "                            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat',\n",
    "                            'SerdeInfo': {\n",
    "                                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✅ Partition added: {database}.{table} dt={ds}\")\n",
    "            except glue.exceptions.AlreadyExistsException:\n",
    "                print(f\"⚠️ Partition already exists: {database}.{table} dt={ds}\")\n",
    "    \n",
    "    @task(task_id='emit_lineage_to_datahub')\n",
    "    def emit_lineage_to_datahub(gold_info, ds):\n",
    "        \"\"\"Emit data lineage to DataHub\"\"\"\n",
    "        from datahub.emitter.mce_builder import make_dataset_urn\n",
    "        from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "        from datahub.metadata.schema_classes import DatasetLineageTypeClass, UpstreamClass, UpstreamLineageClass\n",
    "        \n",
    "        emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "        \n",
    "        # Define lineage: raw → curated → gold\n",
    "        lineage_map = {\n",
    "            f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.curated.ventas.dt={ds},PROD)\": [\n",
    "                f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.raw.ventas.dt={ds},PROD)\"\n",
    "            ],\n",
    "            f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.gold.ventas_diarias_region,PROD)\": [\n",
    "                f\"urn:li:dataset:(urn:li:dataPlatform:s3,ecommerce-data.curated.ventas.dt={ds},PROD)\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for downstream_urn, upstream_urns in lineage_map.items():\n",
    "            upstreams = [\n",
    "                UpstreamClass(\n",
    "                    dataset=urn,\n",
    "                    type=DatasetLineageTypeClass.TRANSFORMED\n",
    "                )\n",
    "                for urn in upstream_urns\n",
    "            ]\n",
    "            \n",
    "            lineage = UpstreamLineageClass(upstreams=upstreams)\n",
    "            \n",
    "            emitter.emit_mcp(\n",
    "                MetadataChangeProposalWrapper(\n",
    "                    entityUrn=downstream_urn,\n",
    "                    aspect=lineage\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Lineage emitted: {downstream_urn}\")\n",
    "    \n",
    "    @task(task_id='export_metrics_to_prometheus')\n",
    "    def export_metrics_to_prometheus(curated_info, gold_info, ds):\n",
    "        \"\"\"Push pipeline metrics to Prometheus Pushgateway\"\"\"\n",
    "        from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "        \n",
    "        registry = CollectorRegistry()\n",
    "        \n",
    "        # Define metrics\n",
    "        rows_processed = Gauge('airflow_ventas_rows_processed', 'Rows processed', registry=registry)\n",
    "        pipeline_duration = Gauge('airflow_ventas_pipeline_duration_seconds', 'Pipeline duration', registry=registry)\n",
    "        \n",
    "        rows_processed.set(curated_info['rows_processed'])\n",
    "        # Duration calculado por Airflow\n",
    "        \n",
    "        push_to_gateway('prometheus-pushgateway:9091', job='airflow_ventas_batch', registry=registry)\n",
    "    \n",
    "    @task(task_id='send_success_notification')\n",
    "    def send_success_notification(ds, **context):\n",
    "        \"\"\"Send Slack notification on success\"\"\"\n",
    "        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n",
    "        \n",
    "        slack = SlackWebhookHook(slack_webhook_conn_id='slack_data_notifications')\n",
    "        \n",
    "        execution_time = context['ti'].duration\n",
    "        \n",
    "        message = f\"\"\"\n",
    "        ✅ *Ventas Batch Pipeline Succeeded*\n",
    "        \n",
    "        • Date: {ds}\n",
    "        • Duration: {execution_time:.0f}s\n",
    "        • Rows: {context['ti'].xcom_pull(task_ids='transform_to_curated')['rows_processed']:,}\n",
    "        • Dashboard: <https://grafana.ecommerce.com/d/ventas|View Metrics>\n",
    "        \"\"\"\n",
    "        \n",
    "        slack.send_text(message)\n",
    "    \n",
    "    @task(task_id='send_failure_alert', trigger_rule='one_failed')\n",
    "    def send_failure_alert(ds, **context):\n",
    "        \"\"\"Send alert on failure\"\"\"\n",
    "        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n",
    "        \n",
    "        slack = SlackWebhookHook(slack_webhook_conn_id='slack_data_alerts')\n",
    "        \n",
    "        failed_task = context['ti'].task_id\n",
    "        \n",
    "        message = f\"\"\"\n",
    "        🚨 *ALERT: Ventas Batch Pipeline Failed*\n",
    "        \n",
    "        • Date: {ds}\n",
    "        • Failed Task: {failed_task}\n",
    "        • Logs: <https://airflow.ecommerce.com/log?dag_id=ventas_batch_pipeline_v1&task_id={failed_task}|View Logs>\n",
    "        • Runbook: <https://wiki.ecommerce.com/runbooks/ventas-pipeline|Troubleshooting Guide>\n",
    "        \n",
    "        @data-oncall please investigate\n",
    "        \"\"\"\n",
    "        \n",
    "        slack.send_text(message)\n",
    "    \n",
    "    # Define task dependencies\n",
    "    file_info = check_sftp_files()\n",
    "    s3_path = download_to_s3(file_info)\n",
    "    validation_results = run_data_quality_checks(s3_path)\n",
    "    branch = check_validation_results(validation_results)\n",
    "    \n",
    "    curated_info = transform_to_curated(s3_path)\n",
    "    gold_info = create_gold_aggregations(curated_info)\n",
    "    \n",
    "    update_glue_catalog(gold_info)\n",
    "    emit_lineage_to_datahub(gold_info)\n",
    "    export_metrics_to_prometheus(curated_info, gold_info)\n",
    "    \n",
    "    success = send_success_notification()\n",
    "    failure = send_failure_alert()\n",
    "    \n",
    "    # Dependencies\n",
    "    branch >> [curated_info, failure]\n",
    "    curated_info >> gold_info >> [update_glue_catalog, emit_lineage_to_datahub, export_metrics_to_prometheus] >> success\n",
    "\n",
    "dag = ventas_batch_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Airflow Production Deployment: MWAA**\n",
    "\n",
    "```python\n",
    "# deploy_airflow.py\n",
    "import boto3\n",
    "\n",
    "mwaa = boto3.client('mwaa', region_name='us-east-1')\n",
    "\n",
    "# Create MWAA environment\n",
    "response = mwaa.create_environment(\n",
    "    Name='ecommerce-airflow-prod',\n",
    "    ExecutionRoleArn='arn:aws:iam::123456:role/MWAAExecutionRole',\n",
    "    SourceBucketArn='arn:aws:s3:::ecommerce-airflow-bucket',\n",
    "    DagS3Path='dags/',\n",
    "    PluginsS3Path='plugins.zip',\n",
    "    RequirementsS3Path='requirements.txt',\n",
    "    NetworkConfiguration={\n",
    "        'SubnetIds': ['subnet-123', 'subnet-456'],\n",
    "        'SecurityGroupIds': ['sg-789']\n",
    "    },\n",
    "    LoggingConfiguration={\n",
    "        'DagProcessingLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'SchedulerLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'TaskLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'WorkerLogs': {'LogLevel': 'INFO', 'Enabled': True},\n",
    "        'WebserverLogs': {'LogLevel': 'INFO', 'Enabled': True}\n",
    "    },\n",
    "    AirflowVersion='2.7.2',\n",
    "    EnvironmentClass='mw1.medium',  # 2 workers\n",
    "    MaxWorkers=10,\n",
    "    MinWorkers=2,\n",
    "    Schedulers=2,  # HA\n",
    "    AirflowConfigurationOptions={\n",
    "        'core.parallelism': '32',\n",
    "        'core.max_active_runs_per_dag': '3',\n",
    "        'scheduler.catchup_by_default': 'False',\n",
    "        'webserver.expose_config': 'True'\n",
    "    },\n",
    "    Tags={\n",
    "        'Environment': 'production',\n",
    "        'CostCenter': 'data-platform'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Costo: ~$1,000/mes (mw1.medium con 2-10 workers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d61235",
   "metadata": {},
   "source": [
    "### 🚀 **Operacionalización y SRE: Production Readiness**\n",
    "\n",
    "**1. Infrastructure as Code: Terraform**\n",
    "\n",
    "```hcl\n",
    "# infrastructure/main.tf\n",
    "terraform {\n",
    "  required_version = \">= 1.5\"\n",
    "  \n",
    "  backend \"s3\" {\n",
    "    bucket = \"ecommerce-terraform-state\"\n",
    "    key    = \"data-platform/terraform.tfstate\"\n",
    "    region = \"us-east-1\"\n",
    "    encrypt = true\n",
    "    dynamodb_table = \"terraform-state-lock\"\n",
    "  }\n",
    "  \n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = var.region\n",
    "  \n",
    "  default_tags {\n",
    "    tags = {\n",
    "      Project     = \"data-platform\"\n",
    "      ManagedBy   = \"terraform\"\n",
    "      Environment = var.environment\n",
    "      CostCenter  = \"data-engineering\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# S3 Buckets with lifecycle policies\n",
    "module \"data_lake\" {\n",
    "  source = \"./modules/s3-datalake\"\n",
    "  \n",
    "  bucket_name = \"ecommerce-data-${var.environment}\"\n",
    "  \n",
    "  lifecycle_rules = [\n",
    "    {\n",
    "      id     = \"raw-retention\"\n",
    "      prefix = \"raw/\"\n",
    "      transitions = [\n",
    "        { days = 7, storage_class = \"INTELLIGENT_TIERING\" },\n",
    "        { days = 30, storage_class = \"GLACIER\" }\n",
    "      ]\n",
    "      expiration_days = 90\n",
    "    },\n",
    "    {\n",
    "      id     = \"curated-retention\"\n",
    "      prefix = \"curated/\"\n",
    "      transitions = [\n",
    "        { days = 90, storage_class = \"INTELLIGENT_TIERING\" }\n",
    "      ]\n",
    "      expiration_days = 365\n",
    "    }\n",
    "  ]\n",
    "  \n",
    "  versioning_enabled = true\n",
    "  replication_config = {\n",
    "    enabled = true\n",
    "    destination_bucket = \"ecommerce-data-replica-eu-west-1\"\n",
    "  }\n",
    "  \n",
    "  kms_key_id = module.kms.key_id\n",
    "}\n",
    "\n",
    "# MSK (Managed Kafka)\n",
    "module \"msk\" {\n",
    "  source = \"./modules/msk\"\n",
    "  \n",
    "  cluster_name    = \"ecommerce-kafka-${var.environment}\"\n",
    "  kafka_version   = \"3.5.1\"\n",
    "  broker_instance = \"kafka.m5.large\"\n",
    "  broker_count    = 3\n",
    "  \n",
    "  ebs_volume_size = 1000  # GB per broker\n",
    "  \n",
    "  encryption_in_transit = true\n",
    "  encryption_at_rest    = true\n",
    "  \n",
    "  monitoring_level = \"PER_TOPIC_PER_PARTITION\"\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.msk_sg_id]\n",
    "}\n",
    "\n",
    "# EMR Serverless\n",
    "module \"emr_serverless\" {\n",
    "  source = \"./modules/emr-serverless\"\n",
    "  \n",
    "  application_name = \"ecommerce-spark-${var.environment}\"\n",
    "  \n",
    "  initial_capacity = {\n",
    "    driver = {\n",
    "      count = 1\n",
    "      cpu   = \"2 vCPU\"\n",
    "      memory = \"8 GB\"\n",
    "    }\n",
    "    executor = {\n",
    "      count = 5\n",
    "      cpu   = \"4 vCPU\"\n",
    "      memory = \"16 GB\"\n",
    "      disk   = \"100 GB\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  maximum_capacity = {\n",
    "    cpu    = \"200 vCPU\"\n",
    "    memory = \"800 GB\"\n",
    "    disk   = \"2000 GB\"\n",
    "  }\n",
    "  \n",
    "  auto_stop_idle_timeout = 15  # minutes\n",
    "  \n",
    "  execution_role_arn = module.iam.emr_execution_role_arn\n",
    "}\n",
    "\n",
    "# MWAA (Managed Airflow)\n",
    "module \"mwaa\" {\n",
    "  source = \"./modules/mwaa\"\n",
    "  \n",
    "  environment_name = \"ecommerce-airflow-${var.environment}\"\n",
    "  \n",
    "  airflow_version    = \"2.7.2\"\n",
    "  environment_class  = \"mw1.medium\"\n",
    "  min_workers        = 2\n",
    "  max_workers        = 10\n",
    "  schedulers         = 2  # HA\n",
    "  \n",
    "  dag_s3_path          = \"dags/\"\n",
    "  plugins_s3_path      = \"plugins.zip\"\n",
    "  requirements_s3_path = \"requirements.txt\"\n",
    "  \n",
    "  logging_configuration = {\n",
    "    dag_processing_logs = { enabled = true, log_level = \"INFO\" }\n",
    "    scheduler_logs      = { enabled = true, log_level = \"INFO\" }\n",
    "    task_logs           = { enabled = true, log_level = \"INFO\" }\n",
    "    worker_logs         = { enabled = true, log_level = \"INFO\" }\n",
    "    webserver_logs      = { enabled = true, log_level = \"INFO\" }\n",
    "  }\n",
    "  \n",
    "  airflow_configuration_options = {\n",
    "    \"core.parallelism\"                 = \"32\"\n",
    "    \"core.max_active_runs_per_dag\"     = \"3\"\n",
    "    \"scheduler.catchup_by_default\"     = \"False\"\n",
    "    \"secrets.backend\"                  = \"airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend\"\n",
    "  }\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.mwaa_sg_id]\n",
    "  \n",
    "  execution_role_arn = module.iam.mwaa_execution_role_arn\n",
    "}\n",
    "\n",
    "# RDS for DataHub metadata\n",
    "module \"datahub_rds\" {\n",
    "  source = \"./modules/rds\"\n",
    "  \n",
    "  identifier = \"datahub-metadata-${var.environment}\"\n",
    "  \n",
    "  engine         = \"postgres\"\n",
    "  engine_version = \"15.3\"\n",
    "  instance_class = \"db.r6g.xlarge\"\n",
    "  \n",
    "  allocated_storage     = 100\n",
    "  max_allocated_storage = 1000\n",
    "  storage_encrypted     = true\n",
    "  kms_key_id            = module.kms.key_id\n",
    "  \n",
    "  multi_az = true  # HA\n",
    "  \n",
    "  backup_retention_period = 7\n",
    "  backup_window          = \"03:00-04:00\"\n",
    "  maintenance_window     = \"mon:04:00-mon:05:00\"\n",
    "  \n",
    "  performance_insights_enabled = true\n",
    "  \n",
    "  subnets            = module.vpc.database_subnet_ids\n",
    "  security_group_ids = [module.security_groups.rds_sg_id]\n",
    "}\n",
    "\n",
    "# ECS for DataHub services\n",
    "module \"datahub_ecs\" {\n",
    "  source = \"./modules/ecs-fargate\"\n",
    "  \n",
    "  cluster_name = \"datahub-${var.environment}\"\n",
    "  \n",
    "  services = {\n",
    "    datahub-gms = {\n",
    "      image            = \"acryldata/datahub-gms:v0.12.0\"\n",
    "      cpu              = 2048\n",
    "      memory           = 4096\n",
    "      desired_count    = 2\n",
    "      health_check_path = \"/health\"\n",
    "      environment_variables = {\n",
    "        DATAHUB_ANALYTICS_ENABLED = \"true\"\n",
    "        ELASTICSEARCH_HOST        = module.elasticsearch.endpoint\n",
    "      }\n",
    "    }\n",
    "    datahub-frontend = {\n",
    "      image            = \"acryldata/datahub-frontend-react:v0.12.0\"\n",
    "      cpu              = 1024\n",
    "      memory           = 2048\n",
    "      desired_count    = 2\n",
    "      health_check_path = \"/admin\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  subnets            = module.vpc.private_subnet_ids\n",
    "  security_group_ids = [module.security_groups.ecs_sg_id]\n",
    "  \n",
    "  load_balancer = {\n",
    "    enabled         = true\n",
    "    certificate_arn = module.acm.certificate_arn\n",
    "    domain_name     = \"datahub.ecommerce.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Prometheus & Grafana (ECS)\n",
    "module \"observability\" {\n",
    "  source = \"./modules/observability\"\n",
    "  \n",
    "  cluster_name = \"observability-${var.environment}\"\n",
    "  \n",
    "  prometheus = {\n",
    "    image         = \"prom/prometheus:v2.47.0\"\n",
    "    cpu           = 2048\n",
    "    memory        = 4096\n",
    "    desired_count = 1\n",
    "    storage_size  = 100  # GB (EBS)\n",
    "    retention     = \"30d\"\n",
    "  }\n",
    "  \n",
    "  grafana = {\n",
    "    image         = \"grafana/grafana:10.1.0\"\n",
    "    cpu           = 1024\n",
    "    memory        = 2048\n",
    "    desired_count = 2\n",
    "    admin_password = data.aws_secretsmanager_secret_version.grafana_password.secret_string\n",
    "  }\n",
    "  \n",
    "  alertmanager = {\n",
    "    image         = \"prom/alertmanager:v0.26.0\"\n",
    "    cpu           = 512\n",
    "    memory        = 1024\n",
    "    desired_count = 2\n",
    "    slack_webhook = data.aws_secretsmanager_secret_version.slack_webhook.secret_string\n",
    "  }\n",
    "}\n",
    "\n",
    "# Budgets & Cost Alerts\n",
    "module \"cost_management\" {\n",
    "  source = \"./modules/cost-management\"\n",
    "  \n",
    "  budgets = [\n",
    "    {\n",
    "      name   = \"data-platform-monthly\"\n",
    "      amount = 5000\n",
    "      threshold_percentages = [50, 80, 100, 120]\n",
    "      notification_emails = [\"data-leads@ecommerce.com\"]\n",
    "    },\n",
    "    {\n",
    "      name   = \"emr-serverless-daily\"\n",
    "      amount = 150\n",
    "      time_unit = \"DAILY\"\n",
    "      threshold_percentages = [100]\n",
    "    }\n",
    "  ]\n",
    "  \n",
    "  cost_anomaly_detection = {\n",
    "    enabled           = true\n",
    "    monitor_name      = \"data-platform-anomalies\"\n",
    "    threshold_dollars = 100\n",
    "  }\n",
    "}\n",
    "\n",
    "# Outputs\n",
    "output \"data_lake_bucket\" {\n",
    "  value = module.data_lake.bucket_name\n",
    "}\n",
    "\n",
    "output \"msk_bootstrap_brokers\" {\n",
    "  value = module.msk.bootstrap_brokers\n",
    "}\n",
    "\n",
    "output \"mwaa_webserver_url\" {\n",
    "  value = module.mwaa.webserver_url\n",
    "}\n",
    "\n",
    "output \"datahub_url\" {\n",
    "  value = \"https://${module.datahub_ecs.load_balancer_dns}\"\n",
    "}\n",
    "\n",
    "output \"grafana_url\" {\n",
    "  value = \"https://${module.observability.grafana_url}\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Deploy Commands:**\n",
    "\n",
    "```bash\n",
    "# Initialize\n",
    "terraform init\n",
    "\n",
    "# Plan\n",
    "terraform plan -var-file=\"environments/prod.tfvars\" -out=tfplan\n",
    "\n",
    "# Review changes\n",
    "terraform show tfplan\n",
    "\n",
    "# Apply\n",
    "terraform apply tfplan\n",
    "\n",
    "# Estimated monthly cost: $4,850\n",
    "# - S3: $200\n",
    "# - MSK: $2,500\n",
    "# - EMR Serverless: $1,200\n",
    "# - MWAA: $800\n",
    "# - RDS: $300\n",
    "# - ECS (DataHub): $400\n",
    "# - Observability: $250\n",
    "# - Data transfer: $200\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. CI/CD Pipeline: GitHub Actions**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/data-platform-ci.yml\n",
    "name: Data Platform CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, develop]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "env:\n",
    "  AWS_REGION: us-east-1\n",
    "  TERRAFORM_VERSION: 1.5.7\n",
    "\n",
    "jobs:\n",
    "  code-quality:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python 3.11\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements-dev.txt\n",
    "      \n",
    "      - name: Lint with Ruff\n",
    "        run: |\n",
    "          ruff check src/ tests/\n",
    "      \n",
    "      - name: Type check with mypy\n",
    "        run: |\n",
    "          mypy src/\n",
    "      \n",
    "      - name: Format check with Black\n",
    "        run: |\n",
    "          black --check src/ tests/\n",
    "      \n",
    "      - name: Security scan with Bandit\n",
    "        run: |\n",
    "          bandit -r src/ -ll\n",
    "  \n",
    "  unit-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: code-quality\n",
    "    \n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:15\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: test\n",
    "        options: >-\n",
    "          --health-cmd pg_isready\n",
    "          --health-interval 10s\n",
    "          --health-timeout 5s\n",
    "          --health-retries 5\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov\n",
    "      \n",
    "      - name: Run tests with coverage\n",
    "        run: |\n",
    "          pytest tests/ \\\n",
    "            --cov=src \\\n",
    "            --cov-report=xml \\\n",
    "            --cov-report=html \\\n",
    "            --junitxml=test-results.xml\n",
    "      \n",
    "      - name: Upload coverage to Codecov\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          files: ./coverage.xml\n",
    "          fail_ci_if_error: true\n",
    "      \n",
    "      - name: Check coverage threshold\n",
    "        run: |\n",
    "          coverage report --fail-under=80\n",
    "  \n",
    "  integration-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: unit-tests\n",
    "    \n",
    "    services:\n",
    "      localstack:\n",
    "        image: localstack/localstack:latest\n",
    "        env:\n",
    "          SERVICES: s3,glue,emr,mwaa\n",
    "        ports:\n",
    "          - 4566:4566\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Run integration tests\n",
    "        run: |\n",
    "          pytest tests/integration/ \\\n",
    "            --localstack-endpoint=http://localhost:4566\n",
    "  \n",
    "  airflow-dags-validation:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: code-quality\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Validate Airflow DAGs\n",
    "        run: |\n",
    "          pip install apache-airflow==2.7.2\n",
    "          \n",
    "          # Import all DAGs (catch syntax errors)\n",
    "          python -c \"from airflow.models import DagBag; \\\n",
    "                     db = DagBag('dags/'); \\\n",
    "                     assert len(db.import_errors) == 0, db.import_errors\"\n",
    "      \n",
    "      - name: Test DAG integrity\n",
    "        run: |\n",
    "          pytest tests/dags/ -v\n",
    "  \n",
    "  terraform-plan:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [unit-tests, integration-tests]\n",
    "    if: github.event_name == 'pull_request'\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Setup Terraform\n",
    "        uses: hashicorp/setup-terraform@v2\n",
    "        with:\n",
    "          terraform_version: ${{ env.TERRAFORM_VERSION }}\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v4\n",
    "        with:\n",
    "          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n",
    "          aws-region: ${{ env.AWS_REGION }}\n",
    "      \n",
    "      - name: Terraform Init\n",
    "        run: terraform init\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Terraform Validate\n",
    "        run: terraform validate\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Terraform Plan\n",
    "        run: terraform plan -var-file=\"environments/prod.tfvars\" -no-color\n",
    "        working-directory: infrastructure/\n",
    "        continue-on-error: true\n",
    "        id: plan\n",
    "      \n",
    "      - name: Comment Plan on PR\n",
    "        uses: actions/github-script@v6\n",
    "        with:\n",
    "          script: |\n",
    "            github.rest.issues.createComment({\n",
    "              issue_number: context.issue.number,\n",
    "              owner: context.repo.owner,\n",
    "              repo: context.repo.repo,\n",
    "              body: `#### Terraform Plan 📖\\n\\`\\`\\`\\n${{ steps.plan.outputs.stdout }}\\n\\`\\`\\``\n",
    "            })\n",
    "  \n",
    "  deploy-staging:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [unit-tests, integration-tests, airflow-dags-validation]\n",
    "    if: github.ref == 'refs/heads/develop'\n",
    "    environment: staging\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Deploy Airflow DAGs to S3\n",
    "        run: |\n",
    "          aws s3 sync dags/ s3://ecommerce-airflow-staging/dags/ \\\n",
    "            --exclude \"*.pyc\" \\\n",
    "            --exclude \"__pycache__/*\" \\\n",
    "            --delete\n",
    "      \n",
    "      - name: Trigger MWAA DAG refresh\n",
    "        run: |\n",
    "          aws mwaa create-cli-token \\\n",
    "            --name ecommerce-airflow-staging \\\n",
    "            --query CliToken \\\n",
    "            --output text\n",
    "      \n",
    "      - name: Deploy Spark jobs to S3\n",
    "        run: |\n",
    "          aws s3 sync src/spark/ s3://ecommerce-code-staging/spark/ \\\n",
    "            --delete\n",
    "  \n",
    "  deploy-production:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [deploy-staging]\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    environment: production\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v4\n",
    "        with:\n",
    "          role-to-assume: ${{ secrets.AWS_PROD_ROLE_ARN }}\n",
    "          aws-region: ${{ env.AWS_REGION }}\n",
    "      \n",
    "      - name: Deploy Terraform changes\n",
    "        run: |\n",
    "          terraform init\n",
    "          terraform apply -var-file=\"environments/prod.tfvars\" -auto-approve\n",
    "        working-directory: infrastructure/\n",
    "      \n",
    "      - name: Deploy Airflow DAGs\n",
    "        run: |\n",
    "          aws s3 sync dags/ s3://ecommerce-airflow-prod/dags/ --delete\n",
    "      \n",
    "      - name: Deploy Spark jobs\n",
    "        run: |\n",
    "          aws s3 sync src/spark/ s3://ecommerce-code-prod/spark/ --delete\n",
    "      \n",
    "      - name: Create GitHub Release\n",
    "        uses: actions/create-release@v1\n",
    "        env:\n",
    "          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "        with:\n",
    "          tag_name: v${{ github.run_number }}\n",
    "          release_name: Release v${{ github.run_number }}\n",
    "          body: |\n",
    "            ## Changes\n",
    "            ${{ github.event.head_commit.message }}\n",
    "            \n",
    "            ## Deployment\n",
    "            - Terraform applied\n",
    "            - Airflow DAGs updated\n",
    "            - Spark jobs deployed\n",
    "          draft: false\n",
    "          prerelease: false\n",
    "      \n",
    "      - name: Notify Slack\n",
    "        uses: 8398a7/action-slack@v3\n",
    "        with:\n",
    "          status: ${{ job.status }}\n",
    "          text: '🚀 Production deployment completed'\n",
    "          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "        if: always()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. SLO & SLI Definitions**\n",
    "\n",
    "```yaml\n",
    "# slos.yml\n",
    "service_level_objectives:\n",
    "  \n",
    "  # Pipeline Availability\n",
    "  - name: ventas_pipeline_availability\n",
    "    description: \"Percentage of time ventas pipeline completes successfully\"\n",
    "    objective: 99.5%\n",
    "    window: 30d\n",
    "    \n",
    "    sli_query: |\n",
    "      sum(rate(airflow_dag_run_success{dag_id=\"ventas_batch_pipeline_v1\"}[30d]))\n",
    "      /\n",
    "      sum(rate(airflow_dag_run_total{dag_id=\"ventas_batch_pipeline_v1\"}[30d]))\n",
    "    \n",
    "    error_budget: 0.5%  # 3.6 hours/month\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 99.0%\n",
    "        window: 7d\n",
    "      - severity: critical\n",
    "        threshold: 98.0%\n",
    "        window: 24h\n",
    "  \n",
    "  # Pipeline Latency\n",
    "  - name: ventas_pipeline_latency_p99\n",
    "    description: \"99th percentile pipeline duration\"\n",
    "    objective: \"<2 hours\"\n",
    "    window: 7d\n",
    "    \n",
    "    sli_query: |\n",
    "      histogram_quantile(0.99,\n",
    "        rate(airflow_dag_duration_seconds_bucket{dag_id=\"ventas_batch_pipeline_v1\"}[7d])\n",
    "      )\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 7200  # 2 hours\n",
    "      - severity: critical\n",
    "        threshold: 10800  # 3 hours\n",
    "  \n",
    "  # Data Freshness\n",
    "  - name: data_freshness\n",
    "    description: \"Time since last successful data update\"\n",
    "    objective: \"<90 minutes\"\n",
    "    \n",
    "    sli_query: |\n",
    "      time() - max(delta_table_last_updated{table=\"curated.ventas\"})\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 5400  # 90 min\n",
    "      - severity: critical\n",
    "        threshold: 7200  # 2 hours\n",
    "  \n",
    "  # Data Quality\n",
    "  - name: data_quality_pass_rate\n",
    "    description: \"Percentage of records passing validation\"\n",
    "    objective: 99.9%\n",
    "    window: 7d\n",
    "    \n",
    "    sli_query: |\n",
    "      sum(rate(great_expectations_validation_success[7d]))\n",
    "      /\n",
    "      sum(rate(great_expectations_validation_total[7d]))\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 99.5%\n",
    "      - severity: critical\n",
    "        threshold: 99.0%\n",
    "  \n",
    "  # API Performance\n",
    "  - name: api_latency_p95\n",
    "    description: \"95th percentile API response time\"\n",
    "    objective: \"<500ms\"\n",
    "    window: 24h\n",
    "    \n",
    "    sli_query: |\n",
    "      histogram_quantile(0.95,\n",
    "        rate(fastapi_request_duration_seconds_bucket{endpoint=\"/query\"}[24h])\n",
    "      )\n",
    "    \n",
    "    alerting:\n",
    "      - severity: warning\n",
    "        threshold: 0.5  # 500ms\n",
    "      - severity: critical\n",
    "        threshold: 1.0  # 1s\n",
    "```\n",
    "\n",
    "**Prometheus Alert Rules:**\n",
    "\n",
    "```yaml\n",
    "# alerts/data-platform.yml\n",
    "groups:\n",
    "  - name: data_platform_alerts\n",
    "    interval: 1m\n",
    "    \n",
    "    rules:\n",
    "      - alert: DataPipelineFailure\n",
    "        expr: |\n",
    "          rate(airflow_dag_run_failure{dag_id=~\".*_pipeline.*\"}[5m]) > 0\n",
    "        for: 1m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Airflow DAG {{ $labels.dag_id }} failing\"\n",
    "          description: \"{{ $value | humanizePercentage }} failure rate in last 5 min\"\n",
    "          runbook: \"https://wiki.ecommerce.com/runbooks/airflow-failures\"\n",
    "      \n",
    "      - alert: DataFreshnessViolation\n",
    "        expr: |\n",
    "          (time() - delta_table_last_updated{table=\"curated.ventas\"}) > 7200\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Stale data in {{ $labels.table }}\"\n",
    "          description: \"Last update {{ $value | humanizeDuration }} ago\"\n",
    "      \n",
    "      - alert: KafkaConsumerLag\n",
    "        expr: |\n",
    "          kafka_consumer_lag{topic=\"ecommerce.ventas.v1\"} > 100000\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"High Kafka consumer lag on {{ $labels.topic }}\"\n",
    "          description: \"Lag: {{ $value }} messages\"\n",
    "      \n",
    "      - alert: S3CostAnomaly\n",
    "        expr: |\n",
    "          (\n",
    "            increase(aws_s3_storage_bytes{bucket=\"ecommerce-data\"}[1d])\n",
    "            /\n",
    "            increase(aws_s3_storage_bytes{bucket=\"ecommerce-data\"} offset 7d[1d])\n",
    "          ) > 1.5\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: finops\n",
    "        annotations:\n",
    "          summary: \"S3 storage growth anomaly detected\"\n",
    "          description: \"50% increase vs last week\"\n",
    "      \n",
    "      - alert: DataQualityDegraded\n",
    "        expr: |\n",
    "          (\n",
    "            sum(rate(great_expectations_validation_failure[1h]))\n",
    "            /\n",
    "            sum(rate(great_expectations_validation_total[1h]))\n",
    "          ) > 0.01\n",
    "        for: 30m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: data-platform\n",
    "        annotations:\n",
    "          summary: \"Data quality degraded\"\n",
    "          description: \"{{ $value | humanizePercentage }} validation failure rate\"\n",
    "      \n",
    "      - alert: EMRServerlessCostRunaway\n",
    "        expr: |\n",
    "          sum(increase(aws_emr_serverless_vcore_hours[1h])) > 1000\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: finops\n",
    "        annotations:\n",
    "          summary: \"EMR Serverless usage spike\"\n",
    "          description: \"{{ $value }} vCore-hours in last hour (normal: <500)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Incident Response Runbook**\n",
    "\n",
    "```markdown\n",
    "# Runbook: Ventas Pipeline Failure\n",
    "\n",
    "## Severity: P1 (Critical)\n",
    "\n",
    "### Symptoms\n",
    "- Airflow DAG `ventas_batch_pipeline_v1` status: FAILED\n",
    "- Alert: \"DataPipelineFailure\" in PagerDuty\n",
    "- Dashboard: Red status in Grafana\n",
    "\n",
    "### Impact\n",
    "- BI dashboards stale (>2 hours)\n",
    "- Executive reports unavailable\n",
    "- Customer RFM scores not updated\n",
    "\n",
    "### Diagnosis Steps\n",
    "\n",
    "1. **Check Airflow UI**\n",
    "   ```bash\n",
    "   # Open Airflow\n",
    "   open https://airflow.ecommerce.com/dags/ventas_batch_pipeline_v1\n",
    "   \n",
    "   # Identify failed task\n",
    "   # Common failures:\n",
    "   # - check_sftp_files: Missing upstream data\n",
    "   # - run_data_quality_checks: Validation failure\n",
    "   # - transform_to_curated: Spark job OOM\n",
    "   ```\n",
    "\n",
    "2. **Check Logs**\n",
    "   ```bash\n",
    "   # CloudWatch Logs\n",
    "   aws logs tail /aws/mwaa/ecommerce-airflow-prod/Task \\\n",
    "     --follow \\\n",
    "     --filter-pattern \"ERROR\"\n",
    "   \n",
    "   # S3 Spark logs\n",
    "   aws s3 ls s3://ecommerce-logs/spark/ventas/\n",
    "   ```\n",
    "\n",
    "3. **Check Dependencies**\n",
    "   ```bash\n",
    "   # SFTP files present?\n",
    "   sftp sap_sftp_user@sftp.sap.com\n",
    "   > ls /exports/ventas_2024-01-15.csv\n",
    "   \n",
    "   # S3 accessible?\n",
    "   aws s3 ls s3://ecommerce-data/raw/ventas/\n",
    "   \n",
    "   # EMR Serverless healthy?\n",
    "   aws emr-serverless list-applications\n",
    "   ```\n",
    "\n",
    "### Resolution Procedures\n",
    "\n",
    "#### Scenario A: Missing SFTP Files\n",
    "```bash\n",
    "# 1. Contact SAP team (slack: #sap-integration)\n",
    "# 2. Verify export schedule\n",
    "# 3. Manual trigger if needed\n",
    "# 4. Re-run DAG once files available\n",
    "\n",
    "airflow dags trigger ventas_batch_pipeline_v1 \\\n",
    "  --conf '{\"execution_date\": \"2024-01-15\"}'\n",
    "```\n",
    "\n",
    "#### Scenario B: Data Quality Failure\n",
    "```python\n",
    "# 1. Review Great Expectations report\n",
    "import great_expectations as gx\n",
    "context = gx.get_context()\n",
    "\n",
    "# 2. Check failed validations\n",
    "results = context.get_validation_result(\"ventas_suite_2024-01-15\")\n",
    "failures = [r for r in results.results if not r.success]\n",
    "\n",
    "# 3. Determine if acceptable\n",
    "# - If data issue: Contact source system owner\n",
    "# - If expectation too strict: Update suite\n",
    "\n",
    "# 4. Override validation (emergency only)\n",
    "airflow tasks run ventas_batch_pipeline_v1 \\\n",
    "  transform_to_curated \\\n",
    "  2024-01-15 \\\n",
    "  --ignore-dependencies\n",
    "```\n",
    "\n",
    "#### Scenario C: Spark OOM\n",
    "```bash\n",
    "# 1. Check memory usage\n",
    "aws emr-serverless get-job-run \\\n",
    "  --application-id app-123 \\\n",
    "  --job-run-id jr-456 \\\n",
    "  | jq '.jobRun.totalExecutionDurationSeconds, .jobRun.totalResourceUtilization'\n",
    "\n",
    "# 2. Increase memory allocation\n",
    "# Edit DAG: increase executor memory 16GB → 32GB\n",
    "\n",
    "# 3. Optimize query\n",
    "# - Add .repartition(100) before expensive operations\n",
    "# - Use broadcast joins for small tables\n",
    "# - Enable AQE (Adaptive Query Execution)\n",
    "\n",
    "# 4. Re-run\n",
    "airflow dags trigger ventas_batch_pipeline_v1\n",
    "```\n",
    "\n",
    "### Escalation Path\n",
    "- **L1 (On-call DE):** 0-15 min\n",
    "- **L2 (Senior DE):** 15-30 min\n",
    "- **L3 (Staff DE + Manager):** 30-60 min\n",
    "- **Incident Commander:** >60 min or customer-facing\n",
    "\n",
    "### Post-Incident\n",
    "1. **Root Cause Analysis** (48h after resolution)\n",
    "2. **Postmortem doc** (Confluence template)\n",
    "3. **Action items** (Jira tickets)\n",
    "4. **Knowledge base update**\n",
    "\n",
    "### Related Links\n",
    "- [Airflow UI](https://airflow.ecommerce.com)\n",
    "- [Grafana Dashboard](https://grafana.ecommerce.com/d/ventas)\n",
    "- [CloudWatch Logs](https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups)\n",
    "- [PagerDuty Escalation](https://ecommerce.pagerduty.com)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Cost Monitoring Dashboard**\n",
    "\n",
    "```python\n",
    "# monitoring/cost_dashboard.py\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Metrics\n",
    "s3_storage_bytes = Gauge('aws_s3_storage_bytes', 'S3 storage size', ['bucket', 'storage_class'])\n",
    "s3_request_count = Gauge('aws_s3_requests_total', 'S3 requests', ['bucket', 'operation'])\n",
    "emr_vcore_hours = Gauge('aws_emr_serverless_vcore_hours', 'EMR vCore hours', ['application'])\n",
    "msk_throughput_mb = Gauge('aws_msk_throughput_mb', 'MSK throughput', ['cluster'])\n",
    "rds_cpu_utilization = Gauge('aws_rds_cpu_percent', 'RDS CPU', ['instance'])\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')\n",
    "ce = boto3.client('ce', region_name='us-east-1')  # Cost Explorer\n",
    "\n",
    "def collect_s3_metrics():\n",
    "    \"\"\"Collect S3 metrics from CloudWatch\"\"\"\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/S3',\n",
    "        MetricName='BucketSizeBytes',\n",
    "        Dimensions=[\n",
    "            {'Name': 'BucketName', 'Value': 'ecommerce-data'},\n",
    "            {'Name': 'StorageType', 'Value': 'StandardStorage'}\n",
    "        ],\n",
    "        StartTime=datetime.utcnow() - timedelta(hours=24),\n",
    "        EndTime=datetime.utcnow(),\n",
    "        Period=86400,\n",
    "        Statistics=['Average']\n",
    "    )\n",
    "    \n",
    "    if response['Datapoints']:\n",
    "        bytes_stored = response['Datapoints'][0]['Average']\n",
    "        s3_storage_bytes.labels(bucket='ecommerce-data', storage_class='standard').set(bytes_stored)\n",
    "\n",
    "def collect_cost_metrics():\n",
    "    \"\"\"Collect cost data from Cost Explorer\"\"\"\n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "            'End': datetime.utcnow().strftime('%Y-%m-%d')\n",
    "        },\n",
    "        Granularity='DAILY',\n",
    "        Metrics=['UnblendedCost'],\n",
    "        GroupBy=[\n",
    "            {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n",
    "        ],\n",
    "        Filter={\n",
    "            'Tags': {\n",
    "                'Key': 'Project',\n",
    "                'Values': ['data-platform']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for result in response['ResultsByTime']:\n",
    "        for group in result['Groups']:\n",
    "            service = group['Keys'][0]\n",
    "            cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "            \n",
    "            # Export as Prometheus metric\n",
    "            # (define gauge for each service)\n",
    "            print(f\"{service}: ${cost:.2f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start Prometheus HTTP server\n",
    "    start_http_server(8000)\n",
    "    \n",
    "    while True:\n",
    "        collect_s3_metrics()\n",
    "        collect_cost_metrics()\n",
    "        time.sleep(300)  # Every 5 min\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e867067",
   "metadata": {},
   "source": [
    "## 1. Contexto y requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cb5ee",
   "metadata": {},
   "source": [
    "**Empresa**: E-commerce global con 3 dominios de datos (Ventas, Logística, Analítica).\n",
    "\n",
    "**Requerimientos funcionales**:\n",
    "- Ingestar transacciones en tiempo real (Kafka) y batch nocturno (archivos SFTP).\n",
    "- Almacenar en data lakehouse (Parquet + Delta Lake) particionado por fecha y región.\n",
    "- Catálogo central con metadatos, linaje y políticas de acceso (Glue/Unity/DataHub).\n",
    "- Orquestación diaria con Airflow: validaciones de calidad, transformaciones, reportes.\n",
    "- APIs de servicio (FastAPI) para consultas ad-hoc por BI y científicos de datos.\n",
    "\n",
    "**Requerimientos no funcionales**:\n",
    "- Compliance GDPR: enmascaramiento de PII, derecho al olvido.\n",
    "- SLO: latencia p99 < 30 min, disponibilidad > 99.5%.\n",
    "- Costos: < $5000/mes, optimización continua (FinOps).\n",
    "- Observabilidad: logs estructurados, métricas en Prometheus, linaje en DataHub.\n",
    "- Seguridad: IAM con mínimo privilegio, cifrado at-rest y in-transit, auditoría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdb0bc",
   "metadata": {},
   "source": [
    "## 2. Arquitectura propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquitectura_diagrama = '''\n",
    "┌─────────────┐       ┌──────────────┐       ┌───────────────┐\n",
    "│  Transac-   │──────▶│    Kafka     │──────▶│  Spark        │\n",
    "│  ciones RT  │       │  (streaming) │       │  Streaming    │\n",
    "└─────────────┘       └──────────────┘       └───────┬───────┘\n",
    "                                                      │\n",
    "┌─────────────┐       ┌──────────────┐              │\n",
    "│  Archivos   │──────▶│   Airflow    │──────────────┤\n",
    "│  SFTP Batch │       │  (orquesta)  │              │\n",
    "└─────────────┘       └──────────────┘              │\n",
    "                                                     ▼\n",
    "                      ┌──────────────────────────────────┐\n",
    "                      │  Data Lakehouse (S3 + Delta)     │\n",
    "                      │  - raw/                          │\n",
    "                      │  - curated/                      │\n",
    "                      │  - gold/ (agregados)             │\n",
    "                      └─────────┬────────────────────────┘\n",
    "                                │\n",
    "                ┌───────────────┼───────────────┐\n",
    "                ▼               ▼               ▼\n",
    "         ┌──────────┐   ┌──────────┐   ┌──────────┐\n",
    "         │  Athena  │   │ FastAPI  │   │   BI     │\n",
    "         │  (SQL)   │   │ (APIs)   │   │ (Tableau)│\n",
    "         └──────────┘   └──────────┘   └──────────┘\n",
    "\n",
    "Observabilidad: Prometheus + Grafana + DataHub (linaje)\n",
    "Seguridad: IAM, KMS, CloudTrail, enmascaramiento PII\n",
    "'''\n",
    "print(arquitectura_diagrama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f7e2e",
   "metadata": {},
   "source": [
    "## 3. Componentes a implementar (checklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17901a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = '''\n",
    "☐ 1. Kafka cluster (Docker Compose local o MSK en AWS)\n",
    "☐ 2. Productor de eventos simulados (transacciones)\n",
    "☐ 3. Consumidor Spark Streaming → Delta Lake (S3)\n",
    "☐ 4. Airflow DAG batch: SFTP → raw → validación → curated → gold\n",
    "☐ 5. Validaciones de calidad con Great Expectations\n",
    "☐ 6. Enmascaramiento de PII (email, tarjeta)\n",
    "☐ 7. Catálogo con Glue Data Catalog o DataHub\n",
    "☐ 8. Linaje con OpenLineage (plugin Airflow)\n",
    "☐ 9. FastAPI endpoint para consultas SQL (proxy a Athena/Trino)\n",
    "☐ 10. Métricas Prometheus exportadas por pipelines\n",
    "☐ 11. Dashboard Grafana con SLOs y alertas\n",
    "☐ 12. Políticas IAM con mínimo privilegio\n",
    "☐ 13. Cifrado KMS para S3 y RDS\n",
    "☐ 14. Auditoría CloudTrail habilitada\n",
    "☐ 15. Presupuestos y alertas de costos (AWS Budgets)\n",
    "☐ 16. Documentación técnica y runbooks\n",
    "☐ 17. Tests de integración (Pytest)\n",
    "☐ 18. CI/CD con GitHub Actions (lint, test, deploy)\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb8e10",
   "metadata": {},
   "source": [
    "## 4. Implementación paso a paso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a21a9",
   "metadata": {},
   "source": [
    "### 4.1 Setup inicial\n",
    "- Crear bucket S3 con estructura `raw/`, `curated/`, `gold/`.\n",
    "- Configurar Glue Data Catalog con base de datos `ecommerce`.\n",
    "- Levantar Kafka local con Docker Compose (zookeeper + broker).\n",
    "\n",
    "### 4.2 Streaming path\n",
    "- Productor Python: genera eventos JSON (transacción_id, cliente_id, monto, timestamp).\n",
    "- Spark Structured Streaming: consume de Kafka, valida schema, escribe a Delta en `curated/ventas/`.\n",
    "- Checkpointing idempotente.\n",
    "\n",
    "### 4.3 Batch path\n",
    "- Airflow DAG: sensor SFTP → download → validate (GE) → transform (Pandas/Spark) → write Delta → optimize.\n",
    "- Agregaciones gold: ventas por día/región/producto.\n",
    "\n",
    "### 4.4 Governance y seguridad\n",
    "- Enmascarar email y tarjeta antes de escribir en curated.\n",
    "- Registrar linaje en DataHub vía OpenLineage.\n",
    "- Configurar IAM roles para Spark, Airflow, APIs.\n",
    "\n",
    "### 4.5 Observabilidad\n",
    "- Exportar métricas de conteo, latencia, errores.\n",
    "- Dashboard Grafana con paneles por pipeline.\n",
    "- Alertas en Slack si SLO violado.\n",
    "\n",
    "### 4.6 Servicio de consultas\n",
    "- FastAPI endpoint `/query` que ejecuta SQL en Athena y retorna JSON.\n",
    "- Caché Redis para queries repetitivas.\n",
    "- Rate limiting por API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728559c",
   "metadata": {},
   "source": [
    "## 5. Entregables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d2f42",
   "metadata": {},
   "source": [
    "- Repositorio Git con código (pipelines, DAGs, APIs, tests).\n",
    "- Diagrama de arquitectura actualizado.\n",
    "- Documento de diseño (decisiones técnicas, trade-offs).\n",
    "- Dashboard Grafana exportado (JSON).\n",
    "- Runbook de operaciones (troubleshooting, rollback).\n",
    "- Video/demo ejecutando pipeline end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc021c5",
   "metadata": {},
   "source": [
    "## 6. Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259a0dd",
   "metadata": {},
   "source": [
    "- Funcionalidad: ¿pipelines ejecutan correctamente?\n",
    "- Calidad: ¿validaciones y tests implementados?\n",
    "- Observabilidad: ¿métricas y linaje visibles?\n",
    "- Seguridad: ¿IAM, cifrado, PII enmascarado?\n",
    "- Costos: ¿presupuesto respetado?\n",
    "- Documentación: ¿clara y completa?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
