{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3e3e0f",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 ML Pipelines y Feature Stores\n",
    "\n",
    "Objetivo: dise\u00f1ar pipelines reproducibles de datos para ML, integrar feature stores (Feast/Tecton) y aplicar MLOps b\u00e1sico (versionado, reentrenamiento, monitoreo).\n",
    "\n",
    "- Duraci\u00f3n: 120\u2013150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Python ML b\u00e1sico, pipelines batch/streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99744d",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **ML Pipeline Architecture: Training vs Inference**\n",
    "\n",
    "**Desaf\u00edo Cl\u00e1sico: Research vs Production Gap**\n",
    "\n",
    "```\n",
    "Data Science Notebook (Research):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Jupyter Notebook               \u2502\n",
    "\u2502 \u2022 Pandas local (100K rows)     \u2502\n",
    "\u2502 \u2022 Manual feature engineering   \u2502\n",
    "\u2502 \u2022 sklearn model training       \u2502\n",
    "\u2502 \u2022 Pickle model \u2192 disk          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "   \u2193 \"Works on my machine!\"\n",
    "   \u2193 Deploy to production...\n",
    "   \u274c Features different\n",
    "   \u274c Data scale breaks\n",
    "   \u274c Not reproducible\n",
    "   \u274c Drift undetected\n",
    "\n",
    "Production Reality:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 \u2022 Spark (billions of rows)     \u2502\n",
    "\u2502 \u2022 Streaming features           \u2502\n",
    "\u2502 \u2022 Model serving infrastructure\u2502\n",
    "\u2502 \u2022 Monitoring & alerts          \u2502\n",
    "\u2502 \u2022 A/B testing                  \u2502\n",
    "\u2502 \u2022 Retraining automation        \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**ML Pipeline Complete Architecture:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    DATA SOURCES                              \u2502\n",
    "\u2502  \u2022 Transactional DB (PostgreSQL)                            \u2502\n",
    "\u2502  \u2022 Event Stream (Kafka)                                     \u2502\n",
    "\u2502  \u2022 External APIs (Weather, Demographics)                    \u2502\n",
    "\u2502  \u2022 Historical Data Lake (S3/Delta)                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "      \u2502                             \u2502\n",
    "      \u25bc                             \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   OFFLINE    \u2502            \u2502   ONLINE     \u2502\n",
    "\u2502   FEATURES   \u2502            \u2502  FEATURES    \u2502\n",
    "\u2502              \u2502            \u2502              \u2502\n",
    "\u2502 \u2022 Batch ETL  \u2502            \u2502\u2022 Streaming   \u2502\n",
    "\u2502   (Spark)    \u2502            \u2502  (Flink)     \u2502\n",
    "\u2502 \u2022 Daily agg  \u2502            \u2502\u2022 Real-time   \u2502\n",
    "\u2502 \u2022 Historical \u2502            \u2502  agg         \u2502\n",
    "\u2502   30d, 90d   \u2502            \u2502\u2022 Last 24h    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502                           \u2502\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                  \u2502\n",
    "                  \u25bc\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u2502 FEATURE STORE   \u2502\n",
    "       \u2502                 \u2502\n",
    "       \u2502 \u2022 Feast/Tecton  \u2502\n",
    "       \u2502 \u2022 Offline Store \u2502\n",
    "       \u2502   (Delta Lake)  \u2502\n",
    "       \u2502 \u2022 Online Store  \u2502\n",
    "       \u2502   (Redis/DDB)   \u2502\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2502\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                \u2502\n",
    "        \u25bc                \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   TRAINING   \u2502  \u2502   SERVING    \u2502\n",
    "\u2502   PIPELINE   \u2502  \u2502   PIPELINE   \u2502\n",
    "\u2502              \u2502  \u2502              \u2502\n",
    "\u2502 \u2022 Feature    \u2502  \u2502 \u2022 Feature    \u2502\n",
    "\u2502   retrieval  \u2502  \u2502   retrieval  \u2502\n",
    "\u2502 \u2022 Model      \u2502  \u2502   (online)   \u2502\n",
    "\u2502   training   \u2502  \u2502 \u2022 Model      \u2502\n",
    "\u2502 \u2022 Validation \u2502  \u2502   inference  \u2502\n",
    "\u2502 \u2022 Registry   \u2502  \u2502 \u2022 Monitoring \u2502\n",
    "\u2502   (MLflow)   \u2502  \u2502              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502                 \u2502\n",
    "       \u25bc                 \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 MODEL        \u2502  \u2502  PREDICTIONS \u2502\n",
    "\u2502 REGISTRY     \u2502  \u2502              \u2502\n",
    "\u2502              \u2502  \u2502 \u2022 Online API \u2502\n",
    "\u2502 \u2022 Versions   \u2502  \u2502   (<100ms)   \u2502\n",
    "\u2502 \u2022 Metadata   \u2502  \u2502 \u2022 Batch job  \u2502\n",
    "\u2502 \u2022 Stage      \u2502  \u2502   (daily)    \u2502\n",
    "\u2502   (staging/  \u2502  \u2502 \u2022 A/B test   \u2502\n",
    "\u2502    prod)     \u2502  \u2502              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                         \u2502\n",
    "                         \u25bc\n",
    "                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                  \u2502  MONITORING  \u2502\n",
    "                  \u2502              \u2502\n",
    "                  \u2502 \u2022 Data drift \u2502\n",
    "                  \u2502 \u2022 Model perf \u2502\n",
    "                  \u2502 \u2022 Latency    \u2502\n",
    "                  \u2502 \u2022 Alerts     \u2502\n",
    "                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Training Pipeline (Offline):**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, datediff, sum as spark_sum\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Step 1: Feature Engineering (Spark Batch)\n",
    "spark = SparkSession.builder.appName(\"ML_Training\").getOrCreate()\n",
    "\n",
    "# Historical transactions\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# Aggregate features per customer (last 30 days)\n",
    "training_date = datetime(2025, 10, 30)\n",
    "lookback_start = training_date - timedelta(days=30)\n",
    "\n",
    "customer_features = transactions \\\n",
    "    .filter(\n",
    "        (col(\"transaction_date\") >= lookback_start) & \n",
    "        (col(\"transaction_date\") < training_date)\n",
    "    ) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"txn_count_30d\"),\n",
    "        spark_sum(\"amount\").alias(\"total_spent_30d\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_30d\"),\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(lit(training_date), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# Join with labels (did customer churn in next 7 days?)\n",
    "labels = spark.read.format(\"delta\").load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(col(\"label_date\") == training_date)\n",
    "\n",
    "training_data = customer_features.join(labels, \"customer_id\")\n",
    "\n",
    "# Convert to Pandas for sklearn (or use Spark MLlib)\n",
    "df_train = training_data.toPandas()\n",
    "\n",
    "# Step 2: Train Model\n",
    "feature_cols = [\"txn_count_30d\", \"total_spent_30d\", \"avg_transaction_30d\", \"days_since_last_txn\"]\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"churned\"]\n",
    "\n",
    "mlflow.set_experiment(\"customer_churn\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rf_train_{training_date.strftime('%Y%m%d')}\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"training_date\", training_date.isoformat())\n",
    "    mlflow.log_param(\"lookback_days\", 30)\n",
    "    \n",
    "    # Train\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_auc\", auc)\n",
    "    mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=\"customer_churn_model\"\n",
    "    )\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), \"feature_importance.json\")\n",
    "    \n",
    "    print(f\"Model trained with AUC: {auc:.4f}\")\n",
    "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "```\n",
    "\n",
    "**Inference Pipeline (Online):**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model from MLflow Registry (production stage)\n",
    "model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "\n",
    "# Redis for feature caching\n",
    "redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: str\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str  # low, medium, high\n",
    "    features_used: Dict[str, float]\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict_churn(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Real-time churn prediction\n",
    "    Latency target: <100ms\n",
    "    \"\"\"\n",
    "    customer_id = request.customer_id\n",
    "    \n",
    "    # 1. Get features from Redis (online feature store)\n",
    "    cache_key = f\"features:customer:{customer_id}\"\n",
    "    cached_features = redis_client.get(cache_key)\n",
    "    \n",
    "    if not cached_features:\n",
    "        raise HTTPException(404, f\"Features not found for customer {customer_id}\")\n",
    "    \n",
    "    features = json.loads(cached_features)\n",
    "    \n",
    "    # 2. Prepare input for model\n",
    "    feature_vector = [[\n",
    "        features[\"txn_count_30d\"],\n",
    "        features[\"total_spent_30d\"],\n",
    "        features[\"avg_transaction_30d\"],\n",
    "        features[\"days_since_last_txn\"]\n",
    "    ]]\n",
    "    \n",
    "    # 3. Predict\n",
    "    churn_proba = model.predict(feature_vector)[0]\n",
    "    \n",
    "    # 4. Classify risk\n",
    "    if churn_proba < 0.3:\n",
    "        risk = \"low\"\n",
    "    elif churn_proba < 0.7:\n",
    "        risk = \"medium\"\n",
    "    else:\n",
    "        risk = \"high\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=customer_id,\n",
    "        churn_probability=float(churn_proba),\n",
    "        churn_risk=risk,\n",
    "        features_used=features\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model_version\": model.metadata.get_model_info().version}\n",
    "```\n",
    "\n",
    "**Batch Inference Pipeline:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "# Load model in Spark context\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers\n",
    "active_customers = spark.read.format(\"delta\").load(\"s3://datalake/gold/active_customers\")\n",
    "\n",
    "# Get their features\n",
    "customer_features = spark.read.format(\"delta\").load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers.join(customer_features, \"customer_id\")\n",
    "\n",
    "# Predict in batch (distributed)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    lit(current_date()).alias(\"prediction_date\")\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers for marketing campaign\n",
    "high_risk = predictions.filter(col(\"churn_probability\") > 0.7)\n",
    "\n",
    "high_risk.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm-db:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"Scored {predictions.count():,} customers\")\n",
    "print(f\"High risk: {high_risk.count():,} customers\")\n",
    "```\n",
    "\n",
    "**Key Differences: Training vs Serving**\n",
    "\n",
    "```python\n",
    "differences = {\n",
    "    \"Data Volume\": {\n",
    "        \"Training\": \"Full historical data (TB-scale, years)\",\n",
    "        \"Serving Online\": \"Single customer features (KB-scale)\",\n",
    "        \"Serving Batch\": \"All active customers (GB-scale, daily)\"\n",
    "    },\n",
    "    \n",
    "    \"Latency\": {\n",
    "        \"Training\": \"Hours/days OK (run weekly/monthly)\",\n",
    "        \"Serving Online\": \"<100ms required (SLA critical)\",\n",
    "        \"Serving Batch\": \"Minutes/hours OK (overnight jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Features\": {\n",
    "        \"Training\": \"Pre-computed offline features (historical accuracy)\",\n",
    "        \"Serving Online\": \"Real-time features from online store (Redis)\",\n",
    "        \"Serving Batch\": \"Daily snapshot features\"\n",
    "    },\n",
    "    \n",
    "    \"Compute\": {\n",
    "        \"Training\": \"Large Spark cluster (100+ cores)\",\n",
    "        \"Serving Online\": \"Small API server (4-8 cores, low memory)\",\n",
    "        \"Serving Batch\": \"Medium Spark cluster (20-50 cores)\"\n",
    "    },\n",
    "    \n",
    "    \"Tools\": {\n",
    "        \"Training\": \"Spark, Pandas, sklearn/XGBoost, MLflow\",\n",
    "        \"Serving Online\": \"FastAPI, Redis, model in memory\",\n",
    "        \"Serving Batch\": \"Spark with mlflow.pyfunc.spark_udf\"\n",
    "    },\n",
    "    \n",
    "    \"Challenges\": {\n",
    "        \"Training\": \"\"\"\n",
    "            \u2022 Feature-label leakage (use point-in-time correct features)\n",
    "            \u2022 Class imbalance (use SMOTE, class weights)\n",
    "            \u2022 Reproducibility (seed, versions, data snapshots)\n",
    "        \"\"\",\n",
    "        \"Serving Online\": \"\"\"\n",
    "            \u2022 Feature freshness (stale features in cache)\n",
    "            \u2022 Latency spikes (model load, garbage collection)\n",
    "            \u2022 Feature skew (training vs serving mismatch)\n",
    "        \"\"\",\n",
    "        \"Serving Batch\": \"\"\"\n",
    "            \u2022 Scale (billions of predictions)\n",
    "            \u2022 Resource contention (don't starve other jobs)\n",
    "            \u2022 Failure recovery (checkpoint, retry logic)\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Automated Retraining with Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_model_performance(**context):\n",
    "    \"\"\"\n",
    "    Compare current model vs new model\n",
    "    Promote to production if better\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    \n",
    "    # Get current production model metrics\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    prod_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Production\"])\n",
    "    prod_run_id = prod_versions[0].run_id\n",
    "    prod_auc = mlflow.get_run(prod_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    # Get latest staging model\n",
    "    staging_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Staging\"])\n",
    "    staging_run_id = staging_versions[0].run_id\n",
    "    staging_auc = mlflow.get_run(staging_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    print(f\"Production AUC: {prod_auc:.4f}\")\n",
    "    print(f\"Staging AUC: {staging_auc:.4f}\")\n",
    "    \n",
    "    # Promote if better by at least 1%\n",
    "    if staging_auc > prod_auc * 1.01:\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"customer_churn_model\",\n",
    "            version=staging_versions[0].version,\n",
    "            stage=\"Production\"\n",
    "        )\n",
    "        print(\"\u2705 New model promoted to Production!\")\n",
    "        return \"promote\"\n",
    "    else:\n",
    "        print(\"\u274c New model not better, keeping current Production\")\n",
    "        return \"keep_current\"\n",
    "\n",
    "with DAG(\n",
    "    \"ml_training_pipeline\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@weekly\",  # Every Monday\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    \n",
    "    # Step 1: Feature engineering (Spark on EMR)\n",
    "    feature_engineering = EmrAddStepsOperator(\n",
    "        task_id=\"feature_engineering\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Feature Engineering',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/feature_engineering.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Model training\n",
    "    model_training = EmrAddStepsOperator(\n",
    "        task_id=\"model_training\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Model Training',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/train_model.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Model evaluation & promotion\n",
    "    evaluate_and_promote = PythonOperator(\n",
    "        task_id=\"evaluate_and_promote\",\n",
    "        python_callable=check_model_performance\n",
    "    )\n",
    "    \n",
    "    feature_engineering >> model_training >> evaluate_and_promote\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed380e1c",
   "metadata": {},
   "source": [
    "### \ud83c\udfea **Feature Stores: Feast, Tecton y Feature Engineering at Scale**\n",
    "\n",
    "**Problema: Training-Serving Skew**\n",
    "\n",
    "```\n",
    "Scenario sin Feature Store:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 TRAINING (Data Scientist)            \u2502\n",
    "\u2502                                      \u2502\n",
    "\u2502 # feature_engineering_training.py    \u2502\n",
    "\u2502 df['avg_purchase_30d'] =             \u2502\n",
    "\u2502   df.groupby('customer_id')          \u2502\n",
    "\u2502     ['amount']                       \u2502\n",
    "\u2502     .rolling(30)                     \u2502\n",
    "\u2502     .mean()                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2193 Model trained con esta feature\n",
    "       \u2193 Deploy to production...\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 SERVING (Engineer implementa en API) \u2502\n",
    "\u2502                                      \u2502\n",
    "\u2502 # feature_api.py                     \u2502\n",
    "\u2502 avg_purchase = db.query(             \u2502\n",
    "\u2502   f\"SELECT AVG(amount)               \u2502\n",
    "\u2502    FROM purchases                    \u2502\n",
    "\u2502    WHERE customer_id={id}            \u2502\n",
    "\u2502    AND date >= NOW() - 30\"           \u2502\n",
    "\u2502 )                                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u274c Subtle difference in logic!\n",
    "       \u274c Training: rolling mean (include current)\n",
    "       \u274c Serving: SQL AVG (different timestamp handling)\n",
    "       \u274c Model performance degrades silently\n",
    "```\n",
    "\n",
    "**Feature Store Solution:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              FEATURE STORE                             \u2502\n",
    "\u2502                                                        \u2502\n",
    "\u2502  Single Source of Truth for Features                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n",
    "\u2502  \u2502   OFFLINE    \u2502           \u2502   ONLINE     \u2502         \u2502\n",
    "\u2502  \u2502   STORE      \u2502           \u2502   STORE      \u2502         \u2502\n",
    "\u2502  \u2502              \u2502           \u2502              \u2502         \u2502\n",
    "\u2502  \u2502 \u2022 Delta Lake \u2502           \u2502 \u2022 Redis      \u2502         \u2502\n",
    "\u2502  \u2502 \u2022 Historical \u2502           \u2502 \u2022 DynamoDB   \u2502         \u2502\n",
    "\u2502  \u2502   features   \u2502           \u2502 \u2022 Cassandra  \u2502         \u2502\n",
    "\u2502  \u2502 \u2022 Training   \u2502           \u2502              \u2502         \u2502\n",
    "\u2502  \u2502   retrieval  \u2502           \u2502 \u2022 Serving    \u2502         \u2502\n",
    "\u2502  \u2502              \u2502           \u2502   retrieval  \u2502         \u2502\n",
    "\u2502  \u2502 \u2022 Point-in-  \u2502           \u2502 \u2022 <10ms      \u2502         \u2502\n",
    "\u2502  \u2502   time       \u2502           \u2502   latency    \u2502         \u2502\n",
    "\u2502  \u2502   correct    \u2502           \u2502              \u2502         \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n",
    "\u2502         \u2502                          \u2502                 \u2502\n",
    "\u2502         \u2502  SAME DEFINITION         \u2502                 \u2502\n",
    "\u2502         \u2502  GUARANTEED CONSISTENCY  \u2502                 \u2502\n",
    "\u2502         \u2502                          \u2502                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502                          \u2502\n",
    "          \u25bc                          \u25bc\n",
    "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  TRAINING   \u2502           \u2502   SERVING   \u2502\n",
    "   \u2502             \u2502           \u2502             \u2502\n",
    "   \u2502 get_        \u2502           \u2502 get_        \u2502\n",
    "   \u2502 historical_ \u2502           \u2502 online_     \u2502\n",
    "   \u2502 features()  \u2502           \u2502 features()  \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Feast: Open-Source Feature Store**\n",
    "\n",
    "```python\n",
    "# feast_repo/feature_store.yaml\n",
    "project: customer_churn\n",
    "registry: s3://feast-registry/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "    type: redis\n",
    "    connection_string: \"redis:6379\"\n",
    "offline_store:\n",
    "    type: file  # Or \"spark\", \"snowflake\", \"bigquery\"\n",
    "    \n",
    "# feast_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, Feature\n",
    "from feast.types import Float32, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define entity (primary key)\n",
    "customer = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    description=\"Customer entity\"\n",
    ")\n",
    "\n",
    "# Offline data source (historical features)\n",
    "customer_transactions_source = FileSource(\n",
    "    path=\"s3://datalake/gold/customer_features_daily/\",\n",
    "    timestamp_field=\"feature_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "# Feature view (collection of features)\n",
    "customer_transaction_features = FeatureView(\n",
    "    name=\"customer_transaction_features\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=30),  # Features valid for 30 days\n",
    "    schema=[\n",
    "        Field(name=\"txn_count_30d\", dtype=Int64),\n",
    "        Field(name=\"total_spent_30d\", dtype=Float32),\n",
    "        Field(name=\"avg_transaction_30d\", dtype=Float32),\n",
    "        Field(name=\"days_since_last_txn\", dtype=Int64),\n",
    "        Field(name=\"favorite_category\", dtype=String),\n",
    "    ],\n",
    "    online=True,  # Enable online serving\n",
    "    source=customer_transactions_source,\n",
    "    tags={\"team\": \"data-science\", \"pii\": \"false\"}\n",
    ")\n",
    "\n",
    "# On-demand features (computed at request time)\n",
    "from feast import OnDemandFeatureView, RequestSource\n",
    "\n",
    "request_source = RequestSource(\n",
    "    name=\"request_data\",\n",
    "    schema=[\n",
    "        Field(name=\"current_cart_value\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "@OnDemandFeatureView(\n",
    "    sources=[customer_transaction_features, request_source],\n",
    "    schema=[\n",
    "        Field(name=\"cart_to_avg_ratio\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "def cart_features(inputs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ratio of current cart to historical average\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df[\"cart_to_avg_ratio\"] = (\n",
    "        inputs[\"current_cart_value\"] / inputs[\"avg_transaction_30d\"]\n",
    "    )\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Deploy Feature Definitions:**\n",
    "\n",
    "```bash\n",
    "# Apply feature definitions to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# Registered entity customer\n",
    "# Registered feature view customer_transaction_features\n",
    "# Deploying infrastructure for online store...\n",
    "# \u2705 Deployment successful!\n",
    "```\n",
    "\n",
    "**Training: Get Historical Features**\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feast_repo/\")\n",
    "\n",
    "# Entity dataframe (customers and timestamps for training)\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, 1004],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Get point-in-time correct features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "\n",
    "# Output:\n",
    "#   customer_id event_timestamp  txn_count_30d  total_spent_30d  avg_transaction_30d  days_since_last_txn\n",
    "# 0        1001      2025-10-01             15           1250.0                 83.3                    2\n",
    "# 1        1002      2025-10-01              8            430.0                 53.8                    5\n",
    "# 2        1003      2025-10-01             25           3200.0                128.0                    1\n",
    "# 3        1004      2025-10-01              3            120.0                 40.0                   14\n",
    "\n",
    "# Point-in-time correct:\n",
    "# For customer 1001 on 2025-10-01:\n",
    "# - Only uses data from 2025-09-01 to 2025-09-30 (before event_timestamp)\n",
    "# - No data leakage from future\n",
    "```\n",
    "\n",
    "**Materialize Features to Online Store:**\n",
    "\n",
    "```python\n",
    "# Materialize features to Redis for online serving\n",
    "from datetime import datetime\n",
    "\n",
    "store.materialize(\n",
    "    start_date=datetime(2025, 10, 1),\n",
    "    end_date=datetime(2025, 10, 30)\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 4 features for customer_transaction_features\n",
    "# Progress: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 customers\n",
    "# \u2705 Materialization complete!\n",
    "# Online store: Redis updated with latest features\n",
    "```\n",
    "\n",
    "**Serving: Get Online Features**\n",
    "\n",
    "```python\n",
    "# Real-time feature retrieval (< 10ms)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\"customer_id\": 1001},\n",
    "        {\"customer_id\": 1002}\n",
    "    ]\n",
    ").to_dict()\n",
    "\n",
    "print(features)\n",
    "\n",
    "# Output:\n",
    "# {\n",
    "#   'customer_id': [1001, 1002],\n",
    "#   'txn_count_30d': [15, 8],\n",
    "#   'total_spent_30d': [1250.0, 430.0],\n",
    "#   'avg_transaction_30d': [83.3, 53.8],\n",
    "#   'days_since_last_txn': [2, 5]\n",
    "# }\n",
    "\n",
    "# Latency: 5-10ms (Redis lookup)\n",
    "```\n",
    "\n",
    "**Feature Engineering Patterns:**\n",
    "\n",
    "```python\n",
    "# 1. AGGREGATION FEATURES (most common)\n",
    "\"\"\"\n",
    "Aggregate historical transactions over windows\n",
    "\"\"\"\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, avg, count, datediff, current_date\n",
    "\n",
    "# Daily feature computation\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# 7-day, 30-day, 90-day windows\n",
    "for days in [7, 30, 90]:\n",
    "    window_spec = Window \\\n",
    "        .partitionBy(\"customer_id\") \\\n",
    "        .orderBy(\"transaction_date\") \\\n",
    "        .rangeBetween(-days * 86400, 0)  # seconds\n",
    "    \n",
    "    transactions = transactions \\\n",
    "        .withColumn(f\"txn_count_{days}d\", count(\"*\").over(window_spec)) \\\n",
    "        .withColumn(f\"total_spent_{days}d\", sum(\"amount\").over(window_spec)) \\\n",
    "        .withColumn(f\"avg_transaction_{days}d\", avg(\"amount\").over(window_spec))\n",
    "\n",
    "# 2. RECENCY FEATURES\n",
    "\"\"\"\n",
    "How recent was last activity\n",
    "\"\"\"\n",
    "customer_features = transactions \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(current_date(), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# 3. FREQUENCY FEATURES\n",
    "\"\"\"\n",
    "How often customer transacts\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"avg_days_between_txn\",\n",
    "        col(\"days_since_first_txn\") / col(\"total_txn_count\")\n",
    "    )\n",
    "\n",
    "# 4. RATIO FEATURES\n",
    "\"\"\"\n",
    "Relative comparisons\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"spend_acceleration\",\n",
    "        col(\"total_spent_30d\") / (col(\"total_spent_90d\") + 1)  # +1 avoid div/0\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"high_value_txn_ratio\",\n",
    "        col(\"high_value_txn_count\") / (col(\"total_txn_count\") + 1)\n",
    "    )\n",
    "\n",
    "# 5. CATEGORICAL EMBEDDINGS\n",
    "\"\"\"\n",
    "Convert categories to vectors (for neural networks)\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"favorite_category\", outputCol=\"category_idx\")\n",
    "encoder = OneHotEncoder(inputCol=\"category_idx\", outputCol=\"category_vec\")\n",
    "\n",
    "# 6. TEXT FEATURES\n",
    "\"\"\"\n",
    "NLP on customer reviews\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "```\n",
    "\n",
    "**Tecton: Enterprise Feature Store**\n",
    "\n",
    "```python\n",
    "# Tecton advantages over Feast:\n",
    "tecton_features = {\n",
    "    \"1. Real-time Stream Features\": \"\"\"\n",
    "        Feast: Batch only (daily materialization)\n",
    "        Tecton: Native Flink/Spark Streaming\n",
    "        \n",
    "        @stream_feature_view(\n",
    "            source=KafkaStreamSource(topic=\"transactions\"),\n",
    "            aggregation_interval=timedelta(minutes=1)\n",
    "        )\n",
    "        def user_transaction_counts(transactions):\n",
    "            return f.count(transactions)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Monitoring Built-in\": \"\"\"\n",
    "        Tecton tracks:\n",
    "        - Data quality (nulls, outliers)\n",
    "        - Data drift (distribution changes)\n",
    "        - Feature freshness (lag time)\n",
    "        - Feature usage (which models use which features)\n",
    "        \n",
    "        Auto-alerts cuando features degradan\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Multi-tenant & Enterprise\": \"\"\"\n",
    "        - RBAC (role-based access control)\n",
    "        - Cost tracking per team\n",
    "        - SLA enforcement\n",
    "        - Audit logs\n",
    "        \n",
    "        Feast: Open-source, self-managed\n",
    "        Tecton: Managed SaaS ($$$)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Transformation Pushdown\": \"\"\"\n",
    "        Tecton optimiza transformaciones:\n",
    "        - Spark transformations compiled\n",
    "        - Predicate pushdown\n",
    "        - Partition pruning\n",
    "        \n",
    "        10x faster que Feast para large scale\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Tecton example (similar API)\n",
    "from tecton import Entity, BatchSource, FeatureView\n",
    "from tecton.types import Field, String, Float64, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "customer = Entity(name=\"customer\", join_keys=[\"customer_id\"])\n",
    "\n",
    "transactions_batch = BatchSource(\n",
    "    name=\"transactions\",\n",
    "    batch_config=SnowflakeBatchConfig(\n",
    "        database=\"ANALYTICS\",\n",
    "        schema=\"SILVER\",\n",
    "        table=\"TRANSACTIONS\",\n",
    "        timestamp_field=\"transaction_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "@batch_feature_view(\n",
    "    sources=[transactions_batch],\n",
    "    entities=[customer],\n",
    "    mode=\"spark_sql\",\n",
    "    online=True,\n",
    "    offline=True,\n",
    "    feature_start_time=datetime(2020, 1, 1),\n",
    "    batch_schedule=timedelta(days=1),\n",
    "    ttl=timedelta(days=30)\n",
    ")\n",
    "def customer_transaction_metrics(transactions):\n",
    "    return f\"\"\"\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            transaction_date as timestamp,\n",
    "            COUNT(*) as txn_count_30d,\n",
    "            SUM(amount) as total_spent_30d,\n",
    "            AVG(amount) as avg_transaction_30d\n",
    "        FROM {transactions}\n",
    "        WHERE transaction_date >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "        GROUP BY customer_id, transaction_date\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Feature Naming Convention\": \"\"\"\n",
    "        Pattern: {entity}_{aggregation}_{window}\n",
    "        \n",
    "        \u2705 Good:\n",
    "        - customer_txn_count_30d\n",
    "        - product_view_count_7d\n",
    "        - session_avg_duration_1h\n",
    "        \n",
    "        \u274c Bad:\n",
    "        - feature1\n",
    "        - count\n",
    "        - customer_feature\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Documentation\": \"\"\"\n",
    "        Every feature needs:\n",
    "        - Description: What it measures\n",
    "        - Business logic: How it's computed\n",
    "        - Owner: Who to contact\n",
    "        - Dependencies: Source tables\n",
    "        - SLA: Freshness, availability\n",
    "        \n",
    "        Store in catalog (DataHub, Amundsen)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Feature Versioning\": \"\"\"\n",
    "        Version features like code:\n",
    "        - customer_txn_count_30d_v1\n",
    "        - customer_txn_count_30d_v2 (changed logic)\n",
    "        \n",
    "        Old models use v1, new models use v2\n",
    "        Gradual migration, no breaking changes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Feature Reusability\": \"\"\"\n",
    "        Don't duplicate features:\n",
    "        - Search catalog before creating\n",
    "        - Reuse across models/teams\n",
    "        - Contribute back to central store\n",
    "        \n",
    "        Example: \"customer_txn_count_30d\" usado por:\n",
    "        - Churn model\n",
    "        - Recommendation model  \n",
    "        - Fraud detection model\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Point-in-Time Correctness\": \"\"\"\n",
    "        CRITICAL: No data leakage\n",
    "        \n",
    "        \u274c Wrong:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "        -- Uses ALL data including future!\n",
    "        \n",
    "        \u2705 Correct:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "          AND transaction_date < :prediction_date\n",
    "        -- Only past data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"6. Monitoring & Alerting\": \"\"\"\n",
    "        Track metrics:\n",
    "        - Feature freshness (lag time)\n",
    "        - Null rates (data quality)\n",
    "        - Distribution shifts (drift)\n",
    "        - Query latency (performance)\n",
    "        \n",
    "        Alert if:\n",
    "        - Freshness > 2 hours (SLA violation)\n",
    "        - Null rate > 10% (data issue)\n",
    "        - P95 distribution shift > 2 std devs\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "\n",
    "```python\n",
    "# Online store costs can be HIGH\n",
    "cost_example = {\n",
    "    \"Scenario\": \"1M customers, 20 features, 8 bytes/feature\",\n",
    "    \n",
    "    \"Redis\": {\n",
    "        \"Storage\": \"1M \u00d7 20 \u00d7 8 bytes = 160 MB\",\n",
    "        \"Cost\": \"$0.023/GB/hour \u00d7 0.16 GB \u00d7 730 hours = $2.70/month\",\n",
    "        \"Latency\": \"1-5ms\",\n",
    "        \"Best for\": \"High QPS (>1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"DynamoDB\": {\n",
    "        \"Storage\": \"1M \u00d7 20 \u00d7 8 bytes = 160 MB = $0.04/month\",\n",
    "        \"Reads\": \"100K reads/day \u00d7 $0.25/million = $0.75/month\",\n",
    "        \"Cost\": \"$0.79/month\",\n",
    "        \"Latency\": \"5-20ms\",\n",
    "        \"Best for\": \"Medium QPS (100-1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"Optimization\": \"\"\"\n",
    "        1. TTL: Expire stale features (save storage)\n",
    "        2. Lazy loading: Only materialize requested features\n",
    "        3. Batch reads: Get features for multiple entities\n",
    "        4. Caching: Application-level cache (reduce lookups)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab5f5a",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 **MLOps: Versionado, Experimentos y Model Registry**\n",
    "\n",
    "**ML Lifecycle Challenges:**\n",
    "\n",
    "```\n",
    "Traditional Software:\n",
    "Code \u2192 Build \u2192 Test \u2192 Deploy\n",
    "\u2705 Deterministic (same input = same output)\n",
    "\u2705 Tests catch regressions\n",
    "\u2705 Rollback = redeploy old code\n",
    "\n",
    "Machine Learning:\n",
    "Code + Data + Hyperparameters \u2192 Train \u2192 Validate \u2192 Deploy\n",
    "\u274c Non-deterministic (randomness, data drift)\n",
    "\u274c Tests don't catch model degradation\n",
    "\u274c Rollback = redeploy old model + old features + old data pipeline\n",
    "```\n",
    "\n",
    "**MLOps Stack:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  ML LIFECYCLE                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                          \u2502\n",
    "\u2502  1. EXPERIMENTATION                                     \u2502\n",
    "\u2502     \u2022 Jupyter / VS Code                                 \u2502\n",
    "\u2502     \u2022 MLflow Tracking                                   \u2502\n",
    "\u2502     \u2022 Weights & Biases                                  \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  2. VERSIONING                                          \u2502\n",
    "\u2502     \u2022 Data: DVC, Delta Lake versions                    \u2502\n",
    "\u2502     \u2022 Code: Git                                         \u2502\n",
    "\u2502     \u2022 Models: MLflow Model Registry                     \u2502\n",
    "\u2502     \u2022 Features: Feature Store                           \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  3. TRAINING PIPELINE                                   \u2502\n",
    "\u2502     \u2022 Orchestration: Airflow, Kubeflow, Metaflow       \u2502\n",
    "\u2502     \u2022 Compute: Spark, Kubernetes, SageMaker            \u2502\n",
    "\u2502     \u2022 Hyperparameter tuning: Optuna, Ray Tune          \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  4. VALIDATION & TESTING                                \u2502\n",
    "\u2502     \u2022 Model validation: Great Expectations              \u2502\n",
    "\u2502     \u2022 A/B testing framework                             \u2502\n",
    "\u2502     \u2022 Shadow mode deployment                            \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  5. MODEL REGISTRY                                      \u2502\n",
    "\u2502     \u2022 Staging \u2192 Production promotion                    \u2502\n",
    "\u2502     \u2022 Metadata: metrics, lineage, dependencies         \u2502\n",
    "\u2502     \u2022 RBAC: who can promote                            \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  6. DEPLOYMENT                                          \u2502\n",
    "\u2502     \u2022 Online: REST API (FastAPI, TorchServe)           \u2502\n",
    "\u2502     \u2022 Batch: Spark jobs                                \u2502\n",
    "\u2502     \u2022 Edge: TensorFlow Lite, ONNX                      \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  7. MONITORING                                          \u2502\n",
    "\u2502     \u2022 Data drift: Evidently, WhyLabs                   \u2502\n",
    "\u2502     \u2022 Model performance: Custom dashboards             \u2502\n",
    "\u2502     \u2022 Infrastructure: Prometheus, Datadog              \u2502\n",
    "\u2502     \u2193                                                   \u2502\n",
    "\u2502  8. RETRAINING (back to step 3)                        \u2502\n",
    "\u2502     \u2022 Scheduled: weekly/monthly                        \u2502\n",
    "\u2502     \u2022 Triggered: drift detected                        \u2502\n",
    "\u2502                                                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**MLflow: Experiment Tracking**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Set tracking server (or use local)\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "mlflow.set_experiment(\"customer_churn_v2\")\n",
    "\n",
    "# Hyperparameter grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "best_auc = 0\n",
    "best_run_id = None\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for depth in param_grid['max_depth']:\n",
    "        for min_split in param_grid['min_samples_split']:\n",
    "            \n",
    "            # Start MLflow run\n",
    "            with mlflow.start_run(run_name=f\"rf_ne{n_est}_md{depth}_ms{min_split}\"):\n",
    "                \n",
    "                # Log parameters\n",
    "                mlflow.log_param(\"n_estimators\", n_est)\n",
    "                mlflow.log_param(\"max_depth\", depth)\n",
    "                mlflow.log_param(\"min_samples_split\", min_split)\n",
    "                mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "                mlflow.log_param(\"feature_version\", \"v2.1\")\n",
    "                mlflow.log_param(\"dataset_version\", \"2025-10-30\")\n",
    "                \n",
    "                # Train model\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_est,\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_split,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "                mlflow.log_metric(\"roc_auc\", auc)\n",
    "                mlflow.log_metric(\"test_size\", len(X_test))\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    \"model\",\n",
    "                    signature=mlflow.models.infer_signature(X_train, y_train)\n",
    "                )\n",
    "                \n",
    "                # Log artifacts\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "                mlflow.log_artifact(\"feature_importance.csv\")\n",
    "                \n",
    "                # Track best model\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    best_run_id = mlflow.active_run().info.run_id\n",
    "                    mlflow.set_tag(\"best_model\", \"true\")\n",
    "                \n",
    "                print(f\"Run: ne={n_est}, md={depth}, ms={min_split} \u2192 AUC={auc:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best model: {best_run_id} with AUC={best_auc:.4f}\")\n",
    "```\n",
    "\n",
    "**MLflow Model Registry: Staging \u2192 Production**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Register best model\n",
    "model_name = \"customer_churn_model\"\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "model_version = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model registered: {model_name} version {model_version.version}\")\n",
    "\n",
    "# Transition to Staging for validation\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "# Add description and tags\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    description=f\"\"\"\n",
    "    Churn prediction model trained on 2025-10-30\n",
    "    Features: customer_transaction_features v2.1\n",
    "    Performance: AUC=0.89, F1=0.82\n",
    "    Training data: 100K customers, 30 days lookback\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    key=\"validation_status\",\n",
    "    value=\"pending\"\n",
    ")\n",
    "\n",
    "# Validation tests\n",
    "print(\"\\n\ud83e\uddea Running validation tests...\")\n",
    "\n",
    "# Test 1: Model can load\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/Staging\")\n",
    "print(\"\u2705 Model loads successfully\")\n",
    "\n",
    "# Test 2: Predictions on validation set\n",
    "val_predictions = loaded_model.predict(X_val)\n",
    "val_auc = roc_auc_score(y_val, loaded_model.predict_proba(X_val)[:, 1])\n",
    "print(f\"\u2705 Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Test 3: No feature drift\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for feature in feature_names:\n",
    "    train_dist = X_train[feature]\n",
    "    val_dist = X_val[feature]\n",
    "    ks_stat, p_value = ks_2samp(train_dist, val_dist)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"\u26a0\ufe0f Feature drift detected in {feature} (p={p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"\u2705 {feature}: no drift\")\n",
    "\n",
    "# Test 4: Latency test\n",
    "import time\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = loaded_model.predict(X_val[:1])\n",
    "latency = (time.time() - start) / 1000 * 1000  # ms\n",
    "print(f\"\u2705 Average latency: {latency:.2f}ms\")\n",
    "\n",
    "# If all tests pass, promote to Production\n",
    "if val_auc > 0.85 and latency < 100:\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive old Production\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        key=\"validation_status\",\n",
    "        value=\"passed\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83d\ude80 Model promoted to Production!\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Model failed validation, staying in Staging\")\n",
    "```\n",
    "\n",
    "**Data Versioning with DVC:**\n",
    "\n",
    "```python\n",
    "# dvc.yaml - Define data pipeline\n",
    "\"\"\"\n",
    "stages:\n",
    "  extract:\n",
    "    cmd: python src/extract_data.py\n",
    "    deps:\n",
    "      - src/extract_data.py\n",
    "    outs:\n",
    "      - data/raw/transactions.parquet\n",
    "  \n",
    "  features:\n",
    "    cmd: python src/generate_features.py\n",
    "    deps:\n",
    "      - src/generate_features.py\n",
    "      - data/raw/transactions.parquet\n",
    "    params:\n",
    "      - features.lookback_days\n",
    "      - features.aggregation_windows\n",
    "    outs:\n",
    "      - data/features/customer_features.parquet\n",
    "  \n",
    "  train:\n",
    "    cmd: python src/train_model.py\n",
    "    deps:\n",
    "      - src/train_model.py\n",
    "      - data/features/customer_features.parquet\n",
    "    params:\n",
    "      - model.n_estimators\n",
    "      - model.max_depth\n",
    "    metrics:\n",
    "      - metrics.json:\n",
    "          cache: false\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "\"\"\"\n",
    "\n",
    "# params.yaml - Hyperparameters\n",
    "\"\"\"\n",
    "features:\n",
    "  lookback_days: 30\n",
    "  aggregation_windows: [7, 30, 90]\n",
    "\n",
    "model:\n",
    "  n_estimators: 100\n",
    "  max_depth: 10\n",
    "  min_samples_split: 5\n",
    "\"\"\"\n",
    "\n",
    "# Track data version\n",
    "\"\"\"\n",
    "$ dvc add data/raw/transactions.parquet\n",
    "$ git add data/raw/transactions.parquet.dvc .gitignore\n",
    "$ git commit -m \"Add transactions data v1\"\n",
    "$ git tag -a \"data-v1\" -m \"Initial dataset\"\n",
    "\n",
    "$ dvc push  # Upload to S3/GCS\n",
    "\"\"\"\n",
    "\n",
    "# Reproduce experiment\n",
    "\"\"\"\n",
    "$ git checkout data-v1\n",
    "$ dvc checkout\n",
    "$ dvc repro  # Runs entire pipeline\n",
    "\n",
    "Output:\n",
    "  Running stage 'extract'\n",
    "  Running stage 'features'  \n",
    "  Running stage 'train'\n",
    "  \u2705 Reproduced experiment from data-v1\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Continuous Training with Kubeflow:**\n",
    "\n",
    "```python\n",
    "# kubeflow_pipeline.py\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "@func_to_container_op\n",
    "def extract_data(output_path: str):\n",
    "    \"\"\"Extract training data\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    df = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "    df.write.parquet(output_path)\n",
    "\n",
    "@func_to_container_op\n",
    "def train_model(data_path: str, model_path: str) -> float:\n",
    "    \"\"\"Train model and return AUC\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import joblib\n",
    "    \n",
    "    df = pd.read_parquet(data_path)\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    return auc\n",
    "\n",
    "@func_to_container_op\n",
    "def deploy_model(model_path: str, auc: float):\n",
    "    \"\"\"Deploy if AUC > threshold\"\"\"\n",
    "    if auc > 0.85:\n",
    "        # Deploy to production\n",
    "        import mlflow\n",
    "        mlflow.register_model(f\"file://{model_path}\", \"churn_model\")\n",
    "        print(f\"\u2705 Model deployed with AUC={auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"\u274c Model not deployed, AUC={auc:.4f} < 0.85\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Customer Churn Training Pipeline',\n",
    "    description='Extract, train, deploy churn model'\n",
    ")\n",
    "def training_pipeline():\n",
    "    # Extract\n",
    "    extract_op = extract_data(output_path=\"/data/training.parquet\")\n",
    "    \n",
    "    # Train\n",
    "    train_op = train_model(\n",
    "        data_path=extract_op.output,\n",
    "        model_path=\"/models/churn_model.pkl\"\n",
    "    )\n",
    "    \n",
    "    # Deploy\n",
    "    deploy_op = deploy_model(\n",
    "        model_path=train_op.outputs['model_path'],\n",
    "        auc=train_op.outputs['Output']\n",
    "    )\n",
    "\n",
    "# Compile and run\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(training_pipeline, 'pipeline.yaml')\n",
    "    \n",
    "    # Submit to Kubeflow\n",
    "    client = kfp.Client(host='http://kubeflow.example.com')\n",
    "    client.create_run_from_pipeline_func(\n",
    "        training_pipeline,\n",
    "        arguments={},\n",
    "        run_name='churn_training_2025_10_30'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Model Comparison Dashboard:**\n",
    "\n",
    "```python\n",
    "# Compare multiple model versions\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get all versions of model\n",
    "versions = client.search_model_versions(f\"name='customer_churn_model'\")\n",
    "\n",
    "comparison_data = []\n",
    "for version in versions:\n",
    "    run = mlflow.get_run(version.run_id)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'version': version.version,\n",
    "        'stage': version.current_stage,\n",
    "        'created': version.creation_timestamp,\n",
    "        'auc': run.data.metrics.get('roc_auc', 0),\n",
    "        'f1': run.data.metrics.get('f1_score', 0),\n",
    "        'accuracy': run.data.metrics.get('accuracy', 0),\n",
    "        'n_estimators': run.data.params.get('n_estimators', 0),\n",
    "        'max_depth': run.data.params.get('max_depth', 0),\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Plot comparison\n",
    "fig = px.line(\n",
    "    df_comparison,\n",
    "    x='version',\n",
    "    y=['auc', 'f1', 'accuracy'],\n",
    "    title='Model Performance Over Versions',\n",
    "    labels={'value': 'Score', 'variable': 'Metric'}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Production model details\n",
    "prod_version = df_comparison[df_comparison['stage'] == 'Production'].iloc[0]\n",
    "print(f\"\\n\ud83d\udcca Production Model (v{prod_version['version']}):\")\n",
    "print(f\"  AUC: {prod_version['auc']:.4f}\")\n",
    "print(f\"  F1: {prod_version['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {prod_version['accuracy']:.4f}\")\n",
    "print(f\"  Hyperparams: n_estimators={prod_version['n_estimators']}, max_depth={prod_version['max_depth']}\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "mlops_best_practices = {\n",
    "    \"1. Reproducibility\": \"\"\"\n",
    "        Track EVERYTHING:\n",
    "        - Code version (git commit hash)\n",
    "        - Data version (DVC, Delta version)\n",
    "        - Environment (Docker image, requirements.txt)\n",
    "        - Random seeds\n",
    "        - Hyperparameters\n",
    "        \n",
    "        Goal: Reproduce exact model 6 months later\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Automated Testing\": \"\"\"\n",
    "        Unit tests:\n",
    "        - Feature engineering logic\n",
    "        - Model preprocessing\n",
    "        - Prediction postprocessing\n",
    "        \n",
    "        Integration tests:\n",
    "        - End-to-end pipeline\n",
    "        - Model serving API\n",
    "        - Feature store integration\n",
    "        \n",
    "        Model tests:\n",
    "        - Minimum accuracy threshold\n",
    "        - Inference latency < 100ms\n",
    "        - No feature drift\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Gradual Rollout\": \"\"\"\n",
    "        Don't deploy to 100% traffic immediately\n",
    "        \n",
    "        Stage 1: Shadow mode (30 days)\n",
    "        - New model runs parallel to old\n",
    "        - Predictions logged but not used\n",
    "        - Compare performance\n",
    "        \n",
    "        Stage 2: A/B test (7 days)\n",
    "        - 10% traffic to new model\n",
    "        - Monitor metrics closely\n",
    "        - Rollback if issues\n",
    "        \n",
    "        Stage 3: Ramp up (14 days)\n",
    "        - 25% \u2192 50% \u2192 75% \u2192 100%\n",
    "        - Gradual increase\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Model Governance\": \"\"\"\n",
    "        RBAC for model promotion:\n",
    "        - Data Scientists: can register to Staging\n",
    "        - ML Engineers: can promote to Production\n",
    "        - Approvals: require 2+ reviewers\n",
    "        \n",
    "        Audit trail:\n",
    "        - Who deployed what when\n",
    "        - Why model was promoted\n",
    "        - What tests were run\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Rollback Plan\": \"\"\"\n",
    "        Always have rollback ready:\n",
    "        - Keep 2-3 previous versions in Production\n",
    "        - Canary deployment (route % traffic)\n",
    "        - Circuit breaker (auto-rollback if errors spike)\n",
    "        \n",
    "        Example:\n",
    "        v10 deployed \u2192 errors 5% \u2192 auto-rollback to v9\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564959f0",
   "metadata": {},
   "source": [
    "### \ud83d\ude80 **Production Serving: Online, Batch y Model Monitoring**\n",
    "\n",
    "**Serving Strategies:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502            MODEL SERVING PATTERNS                          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                            \u2502\n",
    "\u2502  1. ONLINE SERVING (Real-time)                            \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                       \u2502\n",
    "\u2502     \u2502   Client    \u2502                                       \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2502\n",
    "\u2502            \u2502 HTTP Request                                 \u2502\n",
    "\u2502            \u25bc                                               \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502     \u2502  Load        \u2502        \u2502   Feature    \u2502            \u2502\n",
    "\u2502     \u2502  Balancer    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Store      \u2502            \u2502\n",
    "\u2502     \u2502  (Nginx)     \u2502        \u2502   (Redis)    \u2502            \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2502            \u2502                        \u2502                     \u2502\n",
    "\u2502            \u25bc                        \u2502                     \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502                     \u2502\n",
    "\u2502     \u2502  API Server  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n",
    "\u2502     \u2502  (FastAPI)   \u2502                                      \u2502\n",
    "\u2502     \u2502              \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 Model in   \u2502                                      \u2502\n",
    "\u2502     \u2502   memory     \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 Features   \u2502                                      \u2502\n",
    "\u2502     \u2502   from Redis \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 <100ms     \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502                                                            \u2502\n",
    "\u2502  2. BATCH SERVING (Bulk predictions)                      \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n",
    "\u2502     \u2502   Scheduler  \u2502                                      \u2502\n",
    "\u2502     \u2502   (Airflow)  \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502            \u2502 Daily @2AM                                   \u2502\n",
    "\u2502            \u25bc                                               \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502     \u2502  Spark Job   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Feature     \u2502            \u2502\n",
    "\u2502     \u2502              \u2502        \u2502  Store       \u2502            \u2502\n",
    "\u2502     \u2502 \u2022 Load model \u2502        \u2502  (Delta)     \u2502            \u2502\n",
    "\u2502     \u2502 \u2022 Get        \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2502     \u2502   features   \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 Predict    \u2502                                      \u2502\n",
    "\u2502     \u2502   millions   \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502            \u2502                                               \u2502\n",
    "\u2502            \u25bc                                               \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n",
    "\u2502     \u2502  Predictions \u2502                                      \u2502\n",
    "\u2502     \u2502  Delta Table \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502                                                            \u2502\n",
    "\u2502  3. STREAMING SERVING (Micro-batch)                       \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n",
    "\u2502     \u2502    Kafka     \u2502                                      \u2502\n",
    "\u2502     \u2502   (Events)   \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502            \u2502                                               \u2502\n",
    "\u2502            \u25bc                                               \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502     \u2502 Flink/Spark  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Feature     \u2502            \u2502\n",
    "\u2502     \u2502 Streaming    \u2502        \u2502  Store       \u2502            \u2502\n",
    "\u2502     \u2502              \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2502     \u2502 \u2022 Enrich     \u2502                                      \u2502\n",
    "\u2502     \u2502   with       \u2502                                      \u2502\n",
    "\u2502     \u2502   features   \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 Predict    \u2502                                      \u2502\n",
    "\u2502     \u2502 \u2022 <1s        \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502            \u2502                                               \u2502\n",
    "\u2502            \u25bc                                               \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n",
    "\u2502     \u2502 Output Kafka \u2502                                      \u2502\n",
    "\u2502     \u2502   Topic      \u2502                                      \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n",
    "\u2502                                                            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Online Serving: FastAPI Production Setup**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import List\n",
    "import time\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Metrics\n",
    "PREDICTION_COUNT = Counter('prediction_requests_total', 'Total prediction requests')\n",
    "PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "PREDICTION_ERRORS = Counter('prediction_errors_total', 'Total prediction errors')\n",
    "\n",
    "# Global model variable\n",
    "model = None\n",
    "redis_client = None\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Load model once at startup (not per request)\n",
    "    \"\"\"\n",
    "    global model, redis_client\n",
    "    \n",
    "    # Load model from MLflow Registry\n",
    "    print(\"Loading model from MLflow...\")\n",
    "    model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "    print(\"\u2705 Model loaded\")\n",
    "    \n",
    "    # Connect to Redis\n",
    "    redis_client = redis.Redis(\n",
    "        host='redis',\n",
    "        port=6379,\n",
    "        decode_responses=True,\n",
    "        max_connections=50\n",
    "    )\n",
    "    print(\"\u2705 Redis connected\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup on shutdown\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Churn Prediction API\",\n",
    "    description=\"Real-time customer churn prediction\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_ids: List[str] = Field(..., max_items=100)  # Batch up to 100\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str\n",
    "    features: dict\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "    latency_ms: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Batch prediction endpoint\n",
    "    Latency: <100ms for 100 customers\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        PREDICTION_COUNT.inc(len(request.customer_ids))\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get features from Redis (batch)\n",
    "        pipeline = redis_client.pipeline()\n",
    "        for customer_id in request.customer_ids:\n",
    "            pipeline.get(f\"features:customer:{customer_id}\")\n",
    "        \n",
    "        features_raw = pipeline.execute()\n",
    "        \n",
    "        # Prepare batch input\n",
    "        feature_vectors = []\n",
    "        valid_customers = []\n",
    "        \n",
    "        for customer_id, features_str in zip(request.customer_ids, features_raw):\n",
    "            if not features_str:\n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=0.5,  # Default\n",
    "                    churn_risk=\"unknown\",\n",
    "                    features={}\n",
    "                ))\n",
    "                continue\n",
    "            \n",
    "            features = json.loads(features_str)\n",
    "            feature_vectors.append([\n",
    "                features[\"txn_count_30d\"],\n",
    "                features[\"total_spent_30d\"],\n",
    "                features[\"avg_transaction_30d\"],\n",
    "                features[\"days_since_last_txn\"]\n",
    "            ])\n",
    "            valid_customers.append((customer_id, features))\n",
    "        \n",
    "        # Batch prediction (efficient!)\n",
    "        if feature_vectors:\n",
    "            churn_probas = model.predict(feature_vectors)\n",
    "            \n",
    "            for (customer_id, features), churn_proba in zip(valid_customers, churn_probas):\n",
    "                risk = \"low\" if churn_proba < 0.3 else \"medium\" if churn_proba < 0.7 else \"high\"\n",
    "                \n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=float(churn_proba),\n",
    "                    churn_risk=risk,\n",
    "                    features=features\n",
    "                ))\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        PREDICTION_LATENCY.observe(latency_ms / 1000)\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            predictions=predictions,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        PREDICTION_ERRORS.inc()\n",
    "        raise HTTPException(500, f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check\"\"\"\n",
    "    try:\n",
    "        redis_client.ping()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"model_version\": model.metadata.get_model_info().version,\n",
    "            \"redis\": \"connected\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Run with: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4\n",
    "```\n",
    "\n",
    "**Kubernetes Deployment:**\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: churn-prediction-api\n",
    "spec:\n",
    "  replicas: 4  # Horizontal scaling\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: churn-prediction\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: churn-prediction\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: api\n",
    "        image: myregistry/churn-prediction:v1.2.3\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "        env:\n",
    "        - name: MLFLOW_TRACKING_URI\n",
    "          value: \"http://mlflow:5000\"\n",
    "        - name: REDIS_HOST\n",
    "          value: \"redis\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: churn-prediction-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: churn-prediction\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: churn-prediction-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: churn-prediction-api\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "```\n",
    "\n",
    "**Batch Serving with Spark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct, col, current_date\n",
    "import mlflow.pyfunc\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPredictionBatch\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load model as Spark UDF\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers (10M customers)\n",
    "active_customers = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/active_customers\") \\\n",
    "    .filter(col(\"is_active\") == True)\n",
    "\n",
    "print(f\"Scoring {active_customers.count():,} customers...\")\n",
    "\n",
    "# Get features\n",
    "customer_features = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers \\\n",
    "    .join(customer_features, \"customer_id\", \"inner\")\n",
    "\n",
    "# Batch predict (distributed across cluster)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ").withColumn(\n",
    "    \"churn_risk\",\n",
    "    when(col(\"churn_probability\") < 0.3, \"low\")\n",
    "    .when(col(\"churn_probability\") < 0.7, \"medium\")\n",
    "    .otherwise(\"high\")\n",
    ").withColumn(\n",
    "    \"prediction_date\",\n",
    "    current_date()\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"churn_risk\",\n",
    "    \"prediction_date\"\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers \u2192 CRM\n",
    "high_risk = predictions.filter(col(\"churn_risk\") == \"high\")\n",
    "\n",
    "high_risk.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"total_spent_30d\",\n",
    "    \"days_since_last_txn\"\n",
    ").write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .option(\"user\", \"crm_user\") \\\n",
    "    .option(\"password\", \"***\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"\u2705 Scored {predictions.count():,} customers\")\n",
    "print(f\"\u26a0\ufe0f High risk: {high_risk.count():,} customers\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Model Monitoring: Data Drift Detection**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "import mlflow\n",
    "\n",
    "# Reference data (training data distribution)\n",
    "reference_df = pd.read_parquet(\"s3://datalake/training/reference_data.parquet\")\n",
    "\n",
    "# Current data (production predictions last 7 days)\n",
    "current_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/prediction_inputs\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 7\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# Evidently report\n",
    "report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "    DataQualityPreset()\n",
    "])\n",
    "\n",
    "report.run(\n",
    "    reference_data=reference_df,\n",
    "    current_data=current_df,\n",
    "    column_mapping=None\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report.save_html(\"drift_report.html\")\n",
    "\n",
    "# Extract drift metrics\n",
    "drift_metrics = report.as_dict()\n",
    "\n",
    "# Check for drift\n",
    "features_with_drift = []\n",
    "for feature, metrics in drift_metrics['metrics'][0]['result']['drift_by_columns'].items():\n",
    "    if metrics['drift_detected']:\n",
    "        features_with_drift.append({\n",
    "            'feature': feature,\n",
    "            'drift_score': metrics['drift_score'],\n",
    "            'p_value': metrics.get('stattest_threshold', 0)\n",
    "        })\n",
    "\n",
    "if features_with_drift:\n",
    "    print(\"\u26a0\ufe0f DRIFT DETECTED in features:\")\n",
    "    for feature_drift in features_with_drift:\n",
    "        print(f\"  \u2022 {feature_drift['feature']}: score={feature_drift['drift_score']:.4f}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=\"drift_detection\"):\n",
    "        mlflow.log_metric(\"features_with_drift\", len(features_with_drift))\n",
    "        mlflow.log_artifact(\"drift_report.html\")\n",
    "    \n",
    "    # Send alert\n",
    "    send_alert(\n",
    "        title=\"\u26a0\ufe0f Data Drift Detected\",\n",
    "        message=f\"{len(features_with_drift)} features drifted\",\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\u2705 No drift detected\")\n",
    "```\n",
    "\n",
    "**Model Performance Monitoring:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get predictions and actuals\n",
    "predictions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_predictions\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 30\")\n",
    "\n",
    "# Join with actuals (churned or not in next 7 days)\n",
    "actuals = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(\"label_date >= current_date() - 30\")\n",
    "\n",
    "performance = predictions \\\n",
    "    .join(actuals, [\"customer_id\", \"prediction_date\"]) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Calculate metrics over time\n",
    "daily_metrics = []\n",
    "\n",
    "for date in pd.date_range(\n",
    "    start=datetime.now() - timedelta(days=30),\n",
    "    end=datetime.now(),\n",
    "    freq='D'\n",
    "):\n",
    "    day_data = performance[performance['prediction_date'] == date.date()]\n",
    "    \n",
    "    if len(day_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "    \n",
    "    auc = roc_auc_score(day_data['churned'], day_data['churn_probability'])\n",
    "    \n",
    "    # Classify at 0.7 threshold\n",
    "    predictions_binary = (day_data['churn_probability'] > 0.7).astype(int)\n",
    "    precision = precision_score(day_data['churned'], predictions_binary)\n",
    "    recall = recall_score(day_data['churned'], predictions_binary)\n",
    "    \n",
    "    daily_metrics.append({\n",
    "        'date': date,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'num_predictions': len(day_data)\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(daily_metrics)\n",
    "\n",
    "# Plot performance over time\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['auc'], name='AUC'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['precision'], name='Precision'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['recall'], name='Recall'))\n",
    "\n",
    "# Add threshold line\n",
    "fig.add_hline(y=0.80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Minimum AUC\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Over Last 30 Days',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Score',\n",
    "    yaxis_range=[0, 1]\n",
    ")\n",
    "\n",
    "fig.write_html(\"performance_dashboard.html\")\n",
    "\n",
    "# Check if below threshold\n",
    "if df_metrics['auc'].iloc[-1] < 0.80:\n",
    "    print(\"\u26a0\ufe0f MODEL PERFORMANCE DEGRADED\")\n",
    "    print(f\"  Current AUC: {df_metrics['auc'].iloc[-1]:.4f}\")\n",
    "    print(f\"  Threshold: 0.80\")\n",
    "    \n",
    "    # Trigger retraining\n",
    "    from airflow.api.client.local_client import Client\n",
    "    client = Client(None, None)\n",
    "    client.trigger_dag(\n",
    "        dag_id='ml_training_pipeline',\n",
    "        conf={'reason': 'performance_degradation'}\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Retraining triggered\")\n",
    "else:\n",
    "    print(\"\u2705 Model performance healthy\")\n",
    "```\n",
    "\n",
    "**A/B Testing Framework:**\n",
    "\n",
    "```python\n",
    "# Route traffic between model versions\n",
    "from fastapi import FastAPI, Request\n",
    "import random\n",
    "import mlflow\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load multiple model versions\n",
    "model_v1 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/1\")\n",
    "model_v2 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/2\")\n",
    "\n",
    "# A/B test config\n",
    "AB_TEST_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"model_v1_traffic\": 0.9,  # 90% traffic\n",
    "    \"model_v2_traffic\": 0.1   # 10% traffic (new model)\n",
    "}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: Request, customer_id: str):\n",
    "    # Assign to variant\n",
    "    if AB_TEST_CONFIG[\"enabled\"]:\n",
    "        rand = random.random()\n",
    "        if rand < AB_TEST_CONFIG[\"model_v2_traffic\"]:\n",
    "            variant = \"v2\"\n",
    "            model = model_v2\n",
    "        else:\n",
    "            variant = \"v1\"\n",
    "            model = model_v1\n",
    "    else:\n",
    "        variant = \"v1\"\n",
    "        model = model_v1\n",
    "    \n",
    "    # Get features and predict\n",
    "    features = get_features(customer_id)\n",
    "    prediction = model.predict([features])[0]\n",
    "    \n",
    "    # Log for analysis\n",
    "    log_ab_test(\n",
    "        customer_id=customer_id,\n",
    "        variant=variant,\n",
    "        prediction=prediction,\n",
    "        timestamp=datetime.now()\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"churn_probability\": float(prediction),\n",
    "        \"model_variant\": variant  # For debugging\n",
    "    }\n",
    "\n",
    "# Analyze A/B test results\n",
    "def analyze_ab_test():\n",
    "    \"\"\"\n",
    "    Compare model v1 vs v2 performance\n",
    "    \"\"\"\n",
    "    results = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"s3://datalake/gold/ab_test_logs\") \\\n",
    "        .filter(\"timestamp >= current_timestamp() - interval 7 days\")\n",
    "    \n",
    "    # Group by variant\n",
    "    v1_results = results.filter(\"variant == 'v1'\")\n",
    "    v2_results = results.filter(\"variant == 'v2'\")\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    # Compare AUC\n",
    "    v1_auc = calculate_auc(v1_results)\n",
    "    v2_auc = calculate_auc(v2_results)\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(v1_results['auc'], v2_results['auc'])\n",
    "    \n",
    "    print(f\"Model v1 AUC: {v1_auc:.4f}\")\n",
    "    print(f\"Model v2 AUC: {v2_auc:.4f}\")\n",
    "    print(f\"Improvement: {(v2_auc - v1_auc) / v1_auc * 100:.2f}%\")\n",
    "    print(f\"Statistical significance: p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05 and v2_auc > v1_auc:\n",
    "        print(\"\u2705 Model v2 is significantly better!\")\n",
    "        print(\"Recommendation: Ramp up v2 to 100% traffic\")\n",
    "    else:\n",
    "        print(\"\u274c No significant improvement, keep v1\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b644b1",
   "metadata": {},
   "source": [
    "## 1. Pipeline de datos para ML: ETL \u2192 Feature Engineering \u2192 Training \u2192 Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02075bc",
   "metadata": {},
   "source": [
    "- Extract: fuentes raw (transacciones, logs, eventos).\n",
    "- Transform: limpieza, agregaciones por ventana, joins.\n",
    "- Feature Engineering: crear features (ratios, lags, embeddings).\n",
    "- Training: dataset versionado, experimentos (MLflow/Weights&Biases).\n",
    "- Serving: online (API real-time) y offline (batch predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed38e",
   "metadata": {},
   "source": [
    "## 2. Feature Store: concepto y pr\u00e1ctica con Feast (demo local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: instala feast si quieres ejecutar (no en requirements por defecto)\n",
    "feast_demo = r'''\n",
    "# feature_repo/feature_definitions.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "source = FileSource(\n",
    "    path='data/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "cliente_fv = FeatureView(\n",
    "    name='cliente_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras', dtype=Float32),\n",
    "        Field(name='num_transacciones', dtype=Int64),\n",
    "    ],\n",
    "    source=source\n",
    ")\n",
    "# feast apply\n",
    "# feast materialize-incremental\n",
    "# store.get_online_features(...)\n",
    "'''\n",
    "print(feast_demo.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61f6f9",
   "metadata": {},
   "source": [
    "## 3. Versionado de datasets y experimentos con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf33f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_snippet = r'''\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('ventas_prediccion')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('model', 'xgboost')\n",
    "    mlflow.log_param('n_estimators', 100)\n",
    "    mlflow.log_metric('rmse', 0.85)\n",
    "    mlflow.log_artifact('model.pkl')\n",
    "'''\n",
    "print(mlflow_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e01bb",
   "metadata": {},
   "source": [
    "## 4. Reentrenamiento automatizado (Airflow + MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7556724",
   "metadata": {},
   "source": [
    "- DAG semanal: extraer nuevos datos, generar features, entrenar, evaluar.\n",
    "- Si m\u00e9trica mejora sobre baseline \u2192 registrar modelo, promover a producci\u00f3n.\n",
    "- Si drift detectado \u2192 alerta y reentrenamiento fuera de calendario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d1c42",
   "metadata": {},
   "source": [
    "## 5. Serving online y batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85229297",
   "metadata": {},
   "source": [
    "- Online: endpoint REST (FastAPI + modelo en memoria o via MLflow Model Registry).\n",
    "- Batch: Spark job semanal para scoring de todo el cat\u00e1logo/clientes.\n",
    "- Cach\u00e9 de features online (Redis) para latencia < 50ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59fa8b",
   "metadata": {},
   "source": [
    "## 6. Monitoreo de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa955c",
   "metadata": {},
   "source": [
    "- Data drift: distribuci\u00f3n de features cambia (KS test, PSI).\n",
    "- Concept drift: relaci\u00f3n X\u2192Y cambia (performance degrada).\n",
    "- M\u00e9tricas de negocio: precisi\u00f3n, recall, ROC-AUC, ingresos.\n",
    "- Alertas autom\u00e1ticas y rollback si threshold cruzado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83d\udcb0 Cost Optimization y FinOps en la Nube \u2192](06_cost_optimization_finops.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
