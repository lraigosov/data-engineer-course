{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3e3e0f",
   "metadata": {},
   "source": [
    "# 🤖 ML Pipelines y Feature Stores\n",
    "\n",
    "Objetivo: diseñar pipelines reproducibles de datos para ML, integrar feature stores (Feast/Tecton) y aplicar MLOps básico (versionado, reentrenamiento, monitoreo).\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Python ML básico, pipelines batch/streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99744d",
   "metadata": {},
   "source": [
    "### 🔄 **ML Pipeline Architecture: Training vs Inference**\n",
    "\n",
    "**Desafío Clásico: Research vs Production Gap**\n",
    "\n",
    "```\n",
    "Data Science Notebook (Research):\n",
    "┌────────────────────────────────┐\n",
    "│ Jupyter Notebook               │\n",
    "│ • Pandas local (100K rows)     │\n",
    "│ • Manual feature engineering   │\n",
    "│ • sklearn model training       │\n",
    "│ • Pickle model → disk          │\n",
    "└────────────────────────────────┘\n",
    "   ↓ \"Works on my machine!\"\n",
    "   ↓ Deploy to production...\n",
    "   ❌ Features different\n",
    "   ❌ Data scale breaks\n",
    "   ❌ Not reproducible\n",
    "   ❌ Drift undetected\n",
    "\n",
    "Production Reality:\n",
    "┌────────────────────────────────┐\n",
    "│ • Spark (billions of rows)     │\n",
    "│ • Streaming features           │\n",
    "│ • Model serving infrastructure│\n",
    "│ • Monitoring & alerts          │\n",
    "│ • A/B testing                  │\n",
    "│ • Retraining automation        │\n",
    "└────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**ML Pipeline Complete Architecture:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    DATA SOURCES                              │\n",
    "│  • Transactional DB (PostgreSQL)                            │\n",
    "│  • Event Stream (Kafka)                                     │\n",
    "│  • External APIs (Weather, Demographics)                    │\n",
    "│  • Historical Data Lake (S3/Delta)                          │\n",
    "└────────────────────┬────────────────────────────────────────┘\n",
    "                     │\n",
    "      ┌──────────────┴──────────────┐\n",
    "      │                             │\n",
    "      ▼                             ▼\n",
    "┌──────────────┐            ┌──────────────┐\n",
    "│   OFFLINE    │            │   ONLINE     │\n",
    "│   FEATURES   │            │  FEATURES    │\n",
    "│              │            │              │\n",
    "│ • Batch ETL  │            │• Streaming   │\n",
    "│   (Spark)    │            │  (Flink)     │\n",
    "│ • Daily agg  │            │• Real-time   │\n",
    "│ • Historical │            │  agg         │\n",
    "│   30d, 90d   │            │• Last 24h    │\n",
    "└──────┬───────┘            └──────┬───────┘\n",
    "       │                           │\n",
    "       └──────────┬────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "       ┌─────────────────┐\n",
    "       │ FEATURE STORE   │\n",
    "       │                 │\n",
    "       │ • Feast/Tecton  │\n",
    "       │ • Offline Store │\n",
    "       │   (Delta Lake)  │\n",
    "       │ • Online Store  │\n",
    "       │   (Redis/DDB)   │\n",
    "       └────────┬────────┘\n",
    "                │\n",
    "        ┌───────┴────────┐\n",
    "        │                │\n",
    "        ▼                ▼\n",
    "┌──────────────┐  ┌──────────────┐\n",
    "│   TRAINING   │  │   SERVING    │\n",
    "│   PIPELINE   │  │   PIPELINE   │\n",
    "│              │  │              │\n",
    "│ • Feature    │  │ • Feature    │\n",
    "│   retrieval  │  │   retrieval  │\n",
    "│ • Model      │  │   (online)   │\n",
    "│   training   │  │ • Model      │\n",
    "│ • Validation │  │   inference  │\n",
    "│ • Registry   │  │ • Monitoring │\n",
    "│   (MLflow)   │  │              │\n",
    "└──────┬───────┘  └──────┬───────┘\n",
    "       │                 │\n",
    "       ▼                 ▼\n",
    "┌──────────────┐  ┌──────────────┐\n",
    "│ MODEL        │  │  PREDICTIONS │\n",
    "│ REGISTRY     │  │              │\n",
    "│              │  │ • Online API │\n",
    "│ • Versions   │  │   (<100ms)   │\n",
    "│ • Metadata   │  │ • Batch job  │\n",
    "│ • Stage      │  │   (daily)    │\n",
    "│   (staging/  │  │ • A/B test   │\n",
    "│    prod)     │  │              │\n",
    "└──────────────┘  └──────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                  ┌──────────────┐\n",
    "                  │  MONITORING  │\n",
    "                  │              │\n",
    "                  │ • Data drift │\n",
    "                  │ • Model perf │\n",
    "                  │ • Latency    │\n",
    "                  │ • Alerts     │\n",
    "                  └──────────────┘\n",
    "```\n",
    "\n",
    "**Training Pipeline (Offline):**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, datediff, sum as spark_sum\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Step 1: Feature Engineering (Spark Batch)\n",
    "spark = SparkSession.builder.appName(\"ML_Training\").getOrCreate()\n",
    "\n",
    "# Historical transactions\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# Aggregate features per customer (last 30 days)\n",
    "training_date = datetime(2025, 10, 30)\n",
    "lookback_start = training_date - timedelta(days=30)\n",
    "\n",
    "customer_features = transactions \\\n",
    "    .filter(\n",
    "        (col(\"transaction_date\") >= lookback_start) & \n",
    "        (col(\"transaction_date\") < training_date)\n",
    "    ) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"txn_count_30d\"),\n",
    "        spark_sum(\"amount\").alias(\"total_spent_30d\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_30d\"),\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(lit(training_date), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# Join with labels (did customer churn in next 7 days?)\n",
    "labels = spark.read.format(\"delta\").load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(col(\"label_date\") == training_date)\n",
    "\n",
    "training_data = customer_features.join(labels, \"customer_id\")\n",
    "\n",
    "# Convert to Pandas for sklearn (or use Spark MLlib)\n",
    "df_train = training_data.toPandas()\n",
    "\n",
    "# Step 2: Train Model\n",
    "feature_cols = [\"txn_count_30d\", \"total_spent_30d\", \"avg_transaction_30d\", \"days_since_last_txn\"]\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"churned\"]\n",
    "\n",
    "mlflow.set_experiment(\"customer_churn\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rf_train_{training_date.strftime('%Y%m%d')}\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"training_date\", training_date.isoformat())\n",
    "    mlflow.log_param(\"lookback_days\", 30)\n",
    "    \n",
    "    # Train\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_auc\", auc)\n",
    "    mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=\"customer_churn_model\"\n",
    "    )\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), \"feature_importance.json\")\n",
    "    \n",
    "    print(f\"Model trained with AUC: {auc:.4f}\")\n",
    "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "```\n",
    "\n",
    "**Inference Pipeline (Online):**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model from MLflow Registry (production stage)\n",
    "model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "\n",
    "# Redis for feature caching\n",
    "redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: str\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str  # low, medium, high\n",
    "    features_used: Dict[str, float]\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict_churn(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Real-time churn prediction\n",
    "    Latency target: <100ms\n",
    "    \"\"\"\n",
    "    customer_id = request.customer_id\n",
    "    \n",
    "    # 1. Get features from Redis (online feature store)\n",
    "    cache_key = f\"features:customer:{customer_id}\"\n",
    "    cached_features = redis_client.get(cache_key)\n",
    "    \n",
    "    if not cached_features:\n",
    "        raise HTTPException(404, f\"Features not found for customer {customer_id}\")\n",
    "    \n",
    "    features = json.loads(cached_features)\n",
    "    \n",
    "    # 2. Prepare input for model\n",
    "    feature_vector = [[\n",
    "        features[\"txn_count_30d\"],\n",
    "        features[\"total_spent_30d\"],\n",
    "        features[\"avg_transaction_30d\"],\n",
    "        features[\"days_since_last_txn\"]\n",
    "    ]]\n",
    "    \n",
    "    # 3. Predict\n",
    "    churn_proba = model.predict(feature_vector)[0]\n",
    "    \n",
    "    # 4. Classify risk\n",
    "    if churn_proba < 0.3:\n",
    "        risk = \"low\"\n",
    "    elif churn_proba < 0.7:\n",
    "        risk = \"medium\"\n",
    "    else:\n",
    "        risk = \"high\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=customer_id,\n",
    "        churn_probability=float(churn_proba),\n",
    "        churn_risk=risk,\n",
    "        features_used=features\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model_version\": model.metadata.get_model_info().version}\n",
    "```\n",
    "\n",
    "**Batch Inference Pipeline:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "# Load model in Spark context\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers\n",
    "active_customers = spark.read.format(\"delta\").load(\"s3://datalake/gold/active_customers\")\n",
    "\n",
    "# Get their features\n",
    "customer_features = spark.read.format(\"delta\").load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers.join(customer_features, \"customer_id\")\n",
    "\n",
    "# Predict in batch (distributed)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    lit(current_date()).alias(\"prediction_date\")\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers for marketing campaign\n",
    "high_risk = predictions.filter(col(\"churn_probability\") > 0.7)\n",
    "\n",
    "high_risk.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm-db:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"Scored {predictions.count():,} customers\")\n",
    "print(f\"High risk: {high_risk.count():,} customers\")\n",
    "```\n",
    "\n",
    "**Key Differences: Training vs Serving**\n",
    "\n",
    "```python\n",
    "differences = {\n",
    "    \"Data Volume\": {\n",
    "        \"Training\": \"Full historical data (TB-scale, years)\",\n",
    "        \"Serving Online\": \"Single customer features (KB-scale)\",\n",
    "        \"Serving Batch\": \"All active customers (GB-scale, daily)\"\n",
    "    },\n",
    "    \n",
    "    \"Latency\": {\n",
    "        \"Training\": \"Hours/days OK (run weekly/monthly)\",\n",
    "        \"Serving Online\": \"<100ms required (SLA critical)\",\n",
    "        \"Serving Batch\": \"Minutes/hours OK (overnight jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Features\": {\n",
    "        \"Training\": \"Pre-computed offline features (historical accuracy)\",\n",
    "        \"Serving Online\": \"Real-time features from online store (Redis)\",\n",
    "        \"Serving Batch\": \"Daily snapshot features\"\n",
    "    },\n",
    "    \n",
    "    \"Compute\": {\n",
    "        \"Training\": \"Large Spark cluster (100+ cores)\",\n",
    "        \"Serving Online\": \"Small API server (4-8 cores, low memory)\",\n",
    "        \"Serving Batch\": \"Medium Spark cluster (20-50 cores)\"\n",
    "    },\n",
    "    \n",
    "    \"Tools\": {\n",
    "        \"Training\": \"Spark, Pandas, sklearn/XGBoost, MLflow\",\n",
    "        \"Serving Online\": \"FastAPI, Redis, model in memory\",\n",
    "        \"Serving Batch\": \"Spark with mlflow.pyfunc.spark_udf\"\n",
    "    },\n",
    "    \n",
    "    \"Challenges\": {\n",
    "        \"Training\": \"\"\"\n",
    "            • Feature-label leakage (use point-in-time correct features)\n",
    "            • Class imbalance (use SMOTE, class weights)\n",
    "            • Reproducibility (seed, versions, data snapshots)\n",
    "        \"\"\",\n",
    "        \"Serving Online\": \"\"\"\n",
    "            • Feature freshness (stale features in cache)\n",
    "            • Latency spikes (model load, garbage collection)\n",
    "            • Feature skew (training vs serving mismatch)\n",
    "        \"\"\",\n",
    "        \"Serving Batch\": \"\"\"\n",
    "            • Scale (billions of predictions)\n",
    "            • Resource contention (don't starve other jobs)\n",
    "            • Failure recovery (checkpoint, retry logic)\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Automated Retraining with Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_model_performance(**context):\n",
    "    \"\"\"\n",
    "    Compare current model vs new model\n",
    "    Promote to production if better\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    \n",
    "    # Get current production model metrics\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    prod_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Production\"])\n",
    "    prod_run_id = prod_versions[0].run_id\n",
    "    prod_auc = mlflow.get_run(prod_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    # Get latest staging model\n",
    "    staging_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Staging\"])\n",
    "    staging_run_id = staging_versions[0].run_id\n",
    "    staging_auc = mlflow.get_run(staging_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    print(f\"Production AUC: {prod_auc:.4f}\")\n",
    "    print(f\"Staging AUC: {staging_auc:.4f}\")\n",
    "    \n",
    "    # Promote if better by at least 1%\n",
    "    if staging_auc > prod_auc * 1.01:\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"customer_churn_model\",\n",
    "            version=staging_versions[0].version,\n",
    "            stage=\"Production\"\n",
    "        )\n",
    "        print(\"✅ New model promoted to Production!\")\n",
    "        return \"promote\"\n",
    "    else:\n",
    "        print(\"❌ New model not better, keeping current Production\")\n",
    "        return \"keep_current\"\n",
    "\n",
    "with DAG(\n",
    "    \"ml_training_pipeline\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@weekly\",  # Every Monday\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    \n",
    "    # Step 1: Feature engineering (Spark on EMR)\n",
    "    feature_engineering = EmrAddStepsOperator(\n",
    "        task_id=\"feature_engineering\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Feature Engineering',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/feature_engineering.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Model training\n",
    "    model_training = EmrAddStepsOperator(\n",
    "        task_id=\"model_training\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Model Training',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/train_model.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Model evaluation & promotion\n",
    "    evaluate_and_promote = PythonOperator(\n",
    "        task_id=\"evaluate_and_promote\",\n",
    "        python_callable=check_model_performance\n",
    "    )\n",
    "    \n",
    "    feature_engineering >> model_training >> evaluate_and_promote\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed380e1c",
   "metadata": {},
   "source": [
    "### 🏪 **Feature Stores: Feast, Tecton y Feature Engineering at Scale**\n",
    "\n",
    "**Problema: Training-Serving Skew**\n",
    "\n",
    "```\n",
    "Scenario sin Feature Store:\n",
    "┌──────────────────────────────────────┐\n",
    "│ TRAINING (Data Scientist)            │\n",
    "│                                      │\n",
    "│ # feature_engineering_training.py    │\n",
    "│ df['avg_purchase_30d'] =             │\n",
    "│   df.groupby('customer_id')          │\n",
    "│     ['amount']                       │\n",
    "│     .rolling(30)                     │\n",
    "│     .mean()                          │\n",
    "└──────────────────────────────────────┘\n",
    "       ↓ Model trained con esta feature\n",
    "       ↓ Deploy to production...\n",
    "┌──────────────────────────────────────┐\n",
    "│ SERVING (Engineer implementa en API) │\n",
    "│                                      │\n",
    "│ # feature_api.py                     │\n",
    "│ avg_purchase = db.query(             │\n",
    "│   f\"SELECT AVG(amount)               │\n",
    "│    FROM purchases                    │\n",
    "│    WHERE customer_id={id}            │\n",
    "│    AND date >= NOW() - 30\"           │\n",
    "│ )                                    │\n",
    "└──────────────────────────────────────┘\n",
    "       ❌ Subtle difference in logic!\n",
    "       ❌ Training: rolling mean (include current)\n",
    "       ❌ Serving: SQL AVG (different timestamp handling)\n",
    "       ❌ Model performance degrades silently\n",
    "```\n",
    "\n",
    "**Feature Store Solution:**\n",
    "\n",
    "```python\n",
    "┌───────────────────────────────────────────────────────┐\n",
    "│              FEATURE STORE                             │\n",
    "│                                                        │\n",
    "│  Single Source of Truth for Features                  │\n",
    "│  ┌──────────────┐           ┌──────────────┐         │\n",
    "│  │   OFFLINE    │           │   ONLINE     │         │\n",
    "│  │   STORE      │           │   STORE      │         │\n",
    "│  │              │           │              │         │\n",
    "│  │ • Delta Lake │           │ • Redis      │         │\n",
    "│  │ • Historical │           │ • DynamoDB   │         │\n",
    "│  │   features   │           │ • Cassandra  │         │\n",
    "│  │ • Training   │           │              │         │\n",
    "│  │   retrieval  │           │ • Serving    │         │\n",
    "│  │              │           │   retrieval  │         │\n",
    "│  │ • Point-in-  │           │ • <10ms      │         │\n",
    "│  │   time       │           │   latency    │         │\n",
    "│  │   correct    │           │              │         │\n",
    "│  └──────┬───────┘           └──────┬───────┘         │\n",
    "│         │                          │                 │\n",
    "│         │  SAME DEFINITION         │                 │\n",
    "│         │  GUARANTEED CONSISTENCY  │                 │\n",
    "│         │                          │                 │\n",
    "└─────────┴──────────────────────────┴─────────────────┘\n",
    "          │                          │\n",
    "          ▼                          ▼\n",
    "   ┌─────────────┐           ┌─────────────┐\n",
    "   │  TRAINING   │           │   SERVING   │\n",
    "   │             │           │             │\n",
    "   │ get_        │           │ get_        │\n",
    "   │ historical_ │           │ online_     │\n",
    "   │ features()  │           │ features()  │\n",
    "   └─────────────┘           └─────────────┘\n",
    "```\n",
    "\n",
    "**Feast: Open-Source Feature Store**\n",
    "\n",
    "```python\n",
    "# feast_repo/feature_store.yaml\n",
    "project: customer_churn\n",
    "registry: s3://feast-registry/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "    type: redis\n",
    "    connection_string: \"redis:6379\"\n",
    "offline_store:\n",
    "    type: file  # Or \"spark\", \"snowflake\", \"bigquery\"\n",
    "    \n",
    "# feast_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, Feature\n",
    "from feast.types import Float32, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define entity (primary key)\n",
    "customer = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    description=\"Customer entity\"\n",
    ")\n",
    "\n",
    "# Offline data source (historical features)\n",
    "customer_transactions_source = FileSource(\n",
    "    path=\"s3://datalake/gold/customer_features_daily/\",\n",
    "    timestamp_field=\"feature_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "# Feature view (collection of features)\n",
    "customer_transaction_features = FeatureView(\n",
    "    name=\"customer_transaction_features\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=30),  # Features valid for 30 days\n",
    "    schema=[\n",
    "        Field(name=\"txn_count_30d\", dtype=Int64),\n",
    "        Field(name=\"total_spent_30d\", dtype=Float32),\n",
    "        Field(name=\"avg_transaction_30d\", dtype=Float32),\n",
    "        Field(name=\"days_since_last_txn\", dtype=Int64),\n",
    "        Field(name=\"favorite_category\", dtype=String),\n",
    "    ],\n",
    "    online=True,  # Enable online serving\n",
    "    source=customer_transactions_source,\n",
    "    tags={\"team\": \"data-science\", \"pii\": \"false\"}\n",
    ")\n",
    "\n",
    "# On-demand features (computed at request time)\n",
    "from feast import OnDemandFeatureView, RequestSource\n",
    "\n",
    "request_source = RequestSource(\n",
    "    name=\"request_data\",\n",
    "    schema=[\n",
    "        Field(name=\"current_cart_value\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "@OnDemandFeatureView(\n",
    "    sources=[customer_transaction_features, request_source],\n",
    "    schema=[\n",
    "        Field(name=\"cart_to_avg_ratio\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "def cart_features(inputs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ratio of current cart to historical average\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df[\"cart_to_avg_ratio\"] = (\n",
    "        inputs[\"current_cart_value\"] / inputs[\"avg_transaction_30d\"]\n",
    "    )\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Deploy Feature Definitions:**\n",
    "\n",
    "```bash\n",
    "# Apply feature definitions to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# Registered entity customer\n",
    "# Registered feature view customer_transaction_features\n",
    "# Deploying infrastructure for online store...\n",
    "# ✅ Deployment successful!\n",
    "```\n",
    "\n",
    "**Training: Get Historical Features**\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feast_repo/\")\n",
    "\n",
    "# Entity dataframe (customers and timestamps for training)\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, 1004],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Get point-in-time correct features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "\n",
    "# Output:\n",
    "#   customer_id event_timestamp  txn_count_30d  total_spent_30d  avg_transaction_30d  days_since_last_txn\n",
    "# 0        1001      2025-10-01             15           1250.0                 83.3                    2\n",
    "# 1        1002      2025-10-01              8            430.0                 53.8                    5\n",
    "# 2        1003      2025-10-01             25           3200.0                128.0                    1\n",
    "# 3        1004      2025-10-01              3            120.0                 40.0                   14\n",
    "\n",
    "# Point-in-time correct:\n",
    "# For customer 1001 on 2025-10-01:\n",
    "# - Only uses data from 2025-09-01 to 2025-09-30 (before event_timestamp)\n",
    "# - No data leakage from future\n",
    "```\n",
    "\n",
    "**Materialize Features to Online Store:**\n",
    "\n",
    "```python\n",
    "# Materialize features to Redis for online serving\n",
    "from datetime import datetime\n",
    "\n",
    "store.materialize(\n",
    "    start_date=datetime(2025, 10, 1),\n",
    "    end_date=datetime(2025, 10, 30)\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 4 features for customer_transaction_features\n",
    "# Progress: 100% |████████████████████| 10000/10000 customers\n",
    "# ✅ Materialization complete!\n",
    "# Online store: Redis updated with latest features\n",
    "```\n",
    "\n",
    "**Serving: Get Online Features**\n",
    "\n",
    "```python\n",
    "# Real-time feature retrieval (< 10ms)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\"customer_id\": 1001},\n",
    "        {\"customer_id\": 1002}\n",
    "    ]\n",
    ").to_dict()\n",
    "\n",
    "print(features)\n",
    "\n",
    "# Output:\n",
    "# {\n",
    "#   'customer_id': [1001, 1002],\n",
    "#   'txn_count_30d': [15, 8],\n",
    "#   'total_spent_30d': [1250.0, 430.0],\n",
    "#   'avg_transaction_30d': [83.3, 53.8],\n",
    "#   'days_since_last_txn': [2, 5]\n",
    "# }\n",
    "\n",
    "# Latency: 5-10ms (Redis lookup)\n",
    "```\n",
    "\n",
    "**Feature Engineering Patterns:**\n",
    "\n",
    "```python\n",
    "# 1. AGGREGATION FEATURES (most common)\n",
    "\"\"\"\n",
    "Aggregate historical transactions over windows\n",
    "\"\"\"\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, avg, count, datediff, current_date\n",
    "\n",
    "# Daily feature computation\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# 7-day, 30-day, 90-day windows\n",
    "for days in [7, 30, 90]:\n",
    "    window_spec = Window \\\n",
    "        .partitionBy(\"customer_id\") \\\n",
    "        .orderBy(\"transaction_date\") \\\n",
    "        .rangeBetween(-days * 86400, 0)  # seconds\n",
    "    \n",
    "    transactions = transactions \\\n",
    "        .withColumn(f\"txn_count_{days}d\", count(\"*\").over(window_spec)) \\\n",
    "        .withColumn(f\"total_spent_{days}d\", sum(\"amount\").over(window_spec)) \\\n",
    "        .withColumn(f\"avg_transaction_{days}d\", avg(\"amount\").over(window_spec))\n",
    "\n",
    "# 2. RECENCY FEATURES\n",
    "\"\"\"\n",
    "How recent was last activity\n",
    "\"\"\"\n",
    "customer_features = transactions \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(current_date(), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# 3. FREQUENCY FEATURES\n",
    "\"\"\"\n",
    "How often customer transacts\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"avg_days_between_txn\",\n",
    "        col(\"days_since_first_txn\") / col(\"total_txn_count\")\n",
    "    )\n",
    "\n",
    "# 4. RATIO FEATURES\n",
    "\"\"\"\n",
    "Relative comparisons\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"spend_acceleration\",\n",
    "        col(\"total_spent_30d\") / (col(\"total_spent_90d\") + 1)  # +1 avoid div/0\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"high_value_txn_ratio\",\n",
    "        col(\"high_value_txn_count\") / (col(\"total_txn_count\") + 1)\n",
    "    )\n",
    "\n",
    "# 5. CATEGORICAL EMBEDDINGS\n",
    "\"\"\"\n",
    "Convert categories to vectors (for neural networks)\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"favorite_category\", outputCol=\"category_idx\")\n",
    "encoder = OneHotEncoder(inputCol=\"category_idx\", outputCol=\"category_vec\")\n",
    "\n",
    "# 6. TEXT FEATURES\n",
    "\"\"\"\n",
    "NLP on customer reviews\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "```\n",
    "\n",
    "**Tecton: Enterprise Feature Store**\n",
    "\n",
    "```python\n",
    "# Tecton advantages over Feast:\n",
    "tecton_features = {\n",
    "    \"1. Real-time Stream Features\": \"\"\"\n",
    "        Feast: Batch only (daily materialization)\n",
    "        Tecton: Native Flink/Spark Streaming\n",
    "        \n",
    "        @stream_feature_view(\n",
    "            source=KafkaStreamSource(topic=\"transactions\"),\n",
    "            aggregation_interval=timedelta(minutes=1)\n",
    "        )\n",
    "        def user_transaction_counts(transactions):\n",
    "            return f.count(transactions)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Monitoring Built-in\": \"\"\"\n",
    "        Tecton tracks:\n",
    "        - Data quality (nulls, outliers)\n",
    "        - Data drift (distribution changes)\n",
    "        - Feature freshness (lag time)\n",
    "        - Feature usage (which models use which features)\n",
    "        \n",
    "        Auto-alerts cuando features degradan\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Multi-tenant & Enterprise\": \"\"\"\n",
    "        - RBAC (role-based access control)\n",
    "        - Cost tracking per team\n",
    "        - SLA enforcement\n",
    "        - Audit logs\n",
    "        \n",
    "        Feast: Open-source, self-managed\n",
    "        Tecton: Managed SaaS ($$$)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Transformation Pushdown\": \"\"\"\n",
    "        Tecton optimiza transformaciones:\n",
    "        - Spark transformations compiled\n",
    "        - Predicate pushdown\n",
    "        - Partition pruning\n",
    "        \n",
    "        10x faster que Feast para large scale\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Tecton example (similar API)\n",
    "from tecton import Entity, BatchSource, FeatureView\n",
    "from tecton.types import Field, String, Float64, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "customer = Entity(name=\"customer\", join_keys=[\"customer_id\"])\n",
    "\n",
    "transactions_batch = BatchSource(\n",
    "    name=\"transactions\",\n",
    "    batch_config=SnowflakeBatchConfig(\n",
    "        database=\"ANALYTICS\",\n",
    "        schema=\"SILVER\",\n",
    "        table=\"TRANSACTIONS\",\n",
    "        timestamp_field=\"transaction_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "@batch_feature_view(\n",
    "    sources=[transactions_batch],\n",
    "    entities=[customer],\n",
    "    mode=\"spark_sql\",\n",
    "    online=True,\n",
    "    offline=True,\n",
    "    feature_start_time=datetime(2020, 1, 1),\n",
    "    batch_schedule=timedelta(days=1),\n",
    "    ttl=timedelta(days=30)\n",
    ")\n",
    "def customer_transaction_metrics(transactions):\n",
    "    return f\"\"\"\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            transaction_date as timestamp,\n",
    "            COUNT(*) as txn_count_30d,\n",
    "            SUM(amount) as total_spent_30d,\n",
    "            AVG(amount) as avg_transaction_30d\n",
    "        FROM {transactions}\n",
    "        WHERE transaction_date >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "        GROUP BY customer_id, transaction_date\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Feature Naming Convention\": \"\"\"\n",
    "        Pattern: {entity}_{aggregation}_{window}\n",
    "        \n",
    "        ✅ Good:\n",
    "        - customer_txn_count_30d\n",
    "        - product_view_count_7d\n",
    "        - session_avg_duration_1h\n",
    "        \n",
    "        ❌ Bad:\n",
    "        - feature1\n",
    "        - count\n",
    "        - customer_feature\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Documentation\": \"\"\"\n",
    "        Every feature needs:\n",
    "        - Description: What it measures\n",
    "        - Business logic: How it's computed\n",
    "        - Owner: Who to contact\n",
    "        - Dependencies: Source tables\n",
    "        - SLA: Freshness, availability\n",
    "        \n",
    "        Store in catalog (DataHub, Amundsen)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Feature Versioning\": \"\"\"\n",
    "        Version features like code:\n",
    "        - customer_txn_count_30d_v1\n",
    "        - customer_txn_count_30d_v2 (changed logic)\n",
    "        \n",
    "        Old models use v1, new models use v2\n",
    "        Gradual migration, no breaking changes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Feature Reusability\": \"\"\"\n",
    "        Don't duplicate features:\n",
    "        - Search catalog before creating\n",
    "        - Reuse across models/teams\n",
    "        - Contribute back to central store\n",
    "        \n",
    "        Example: \"customer_txn_count_30d\" usado por:\n",
    "        - Churn model\n",
    "        - Recommendation model  \n",
    "        - Fraud detection model\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Point-in-Time Correctness\": \"\"\"\n",
    "        CRITICAL: No data leakage\n",
    "        \n",
    "        ❌ Wrong:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "        -- Uses ALL data including future!\n",
    "        \n",
    "        ✅ Correct:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "          AND transaction_date < :prediction_date\n",
    "        -- Only past data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"6. Monitoring & Alerting\": \"\"\"\n",
    "        Track metrics:\n",
    "        - Feature freshness (lag time)\n",
    "        - Null rates (data quality)\n",
    "        - Distribution shifts (drift)\n",
    "        - Query latency (performance)\n",
    "        \n",
    "        Alert if:\n",
    "        - Freshness > 2 hours (SLA violation)\n",
    "        - Null rate > 10% (data issue)\n",
    "        - P95 distribution shift > 2 std devs\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "\n",
    "```python\n",
    "# Online store costs can be HIGH\n",
    "cost_example = {\n",
    "    \"Scenario\": \"1M customers, 20 features, 8 bytes/feature\",\n",
    "    \n",
    "    \"Redis\": {\n",
    "        \"Storage\": \"1M × 20 × 8 bytes = 160 MB\",\n",
    "        \"Cost\": \"$0.023/GB/hour × 0.16 GB × 730 hours = $2.70/month\",\n",
    "        \"Latency\": \"1-5ms\",\n",
    "        \"Best for\": \"High QPS (>1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"DynamoDB\": {\n",
    "        \"Storage\": \"1M × 20 × 8 bytes = 160 MB = $0.04/month\",\n",
    "        \"Reads\": \"100K reads/day × $0.25/million = $0.75/month\",\n",
    "        \"Cost\": \"$0.79/month\",\n",
    "        \"Latency\": \"5-20ms\",\n",
    "        \"Best for\": \"Medium QPS (100-1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"Optimization\": \"\"\"\n",
    "        1. TTL: Expire stale features (save storage)\n",
    "        2. Lazy loading: Only materialize requested features\n",
    "        3. Batch reads: Get features for multiple entities\n",
    "        4. Caching: Application-level cache (reduce lookups)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab5f5a",
   "metadata": {},
   "source": [
    "### 🔧 **MLOps: Versionado, Experimentos y Model Registry**\n",
    "\n",
    "**ML Lifecycle Challenges:**\n",
    "\n",
    "```\n",
    "Traditional Software:\n",
    "Code → Build → Test → Deploy\n",
    "✅ Deterministic (same input = same output)\n",
    "✅ Tests catch regressions\n",
    "✅ Rollback = redeploy old code\n",
    "\n",
    "Machine Learning:\n",
    "Code + Data + Hyperparameters → Train → Validate → Deploy\n",
    "❌ Non-deterministic (randomness, data drift)\n",
    "❌ Tests don't catch model degradation\n",
    "❌ Rollback = redeploy old model + old features + old data pipeline\n",
    "```\n",
    "\n",
    "**MLOps Stack:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                  ML LIFECYCLE                            │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                          │\n",
    "│  1. EXPERIMENTATION                                     │\n",
    "│     • Jupyter / VS Code                                 │\n",
    "│     • MLflow Tracking                                   │\n",
    "│     • Weights & Biases                                  │\n",
    "│     ↓                                                   │\n",
    "│  2. VERSIONING                                          │\n",
    "│     • Data: DVC, Delta Lake versions                    │\n",
    "│     • Code: Git                                         │\n",
    "│     • Models: MLflow Model Registry                     │\n",
    "│     • Features: Feature Store                           │\n",
    "│     ↓                                                   │\n",
    "│  3. TRAINING PIPELINE                                   │\n",
    "│     • Orchestration: Airflow, Kubeflow, Metaflow       │\n",
    "│     • Compute: Spark, Kubernetes, SageMaker            │\n",
    "│     • Hyperparameter tuning: Optuna, Ray Tune          │\n",
    "│     ↓                                                   │\n",
    "│  4. VALIDATION & TESTING                                │\n",
    "│     • Model validation: Great Expectations              │\n",
    "│     • A/B testing framework                             │\n",
    "│     • Shadow mode deployment                            │\n",
    "│     ↓                                                   │\n",
    "│  5. MODEL REGISTRY                                      │\n",
    "│     • Staging → Production promotion                    │\n",
    "│     • Metadata: metrics, lineage, dependencies         │\n",
    "│     • RBAC: who can promote                            │\n",
    "│     ↓                                                   │\n",
    "│  6. DEPLOYMENT                                          │\n",
    "│     • Online: REST API (FastAPI, TorchServe)           │\n",
    "│     • Batch: Spark jobs                                │\n",
    "│     • Edge: TensorFlow Lite, ONNX                      │\n",
    "│     ↓                                                   │\n",
    "│  7. MONITORING                                          │\n",
    "│     • Data drift: Evidently, WhyLabs                   │\n",
    "│     • Model performance: Custom dashboards             │\n",
    "│     • Infrastructure: Prometheus, Datadog              │\n",
    "│     ↓                                                   │\n",
    "│  8. RETRAINING (back to step 3)                        │\n",
    "│     • Scheduled: weekly/monthly                        │\n",
    "│     • Triggered: drift detected                        │\n",
    "│                                                          │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**MLflow: Experiment Tracking**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Set tracking server (or use local)\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "mlflow.set_experiment(\"customer_churn_v2\")\n",
    "\n",
    "# Hyperparameter grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "best_auc = 0\n",
    "best_run_id = None\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for depth in param_grid['max_depth']:\n",
    "        for min_split in param_grid['min_samples_split']:\n",
    "            \n",
    "            # Start MLflow run\n",
    "            with mlflow.start_run(run_name=f\"rf_ne{n_est}_md{depth}_ms{min_split}\"):\n",
    "                \n",
    "                # Log parameters\n",
    "                mlflow.log_param(\"n_estimators\", n_est)\n",
    "                mlflow.log_param(\"max_depth\", depth)\n",
    "                mlflow.log_param(\"min_samples_split\", min_split)\n",
    "                mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "                mlflow.log_param(\"feature_version\", \"v2.1\")\n",
    "                mlflow.log_param(\"dataset_version\", \"2025-10-30\")\n",
    "                \n",
    "                # Train model\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_est,\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_split,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "                mlflow.log_metric(\"roc_auc\", auc)\n",
    "                mlflow.log_metric(\"test_size\", len(X_test))\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    \"model\",\n",
    "                    signature=mlflow.models.infer_signature(X_train, y_train)\n",
    "                )\n",
    "                \n",
    "                # Log artifacts\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "                mlflow.log_artifact(\"feature_importance.csv\")\n",
    "                \n",
    "                # Track best model\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    best_run_id = mlflow.active_run().info.run_id\n",
    "                    mlflow.set_tag(\"best_model\", \"true\")\n",
    "                \n",
    "                print(f\"Run: ne={n_est}, md={depth}, ms={min_split} → AUC={auc:.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 Best model: {best_run_id} with AUC={best_auc:.4f}\")\n",
    "```\n",
    "\n",
    "**MLflow Model Registry: Staging → Production**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Register best model\n",
    "model_name = \"customer_churn_model\"\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "model_version = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model registered: {model_name} version {model_version.version}\")\n",
    "\n",
    "# Transition to Staging for validation\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "# Add description and tags\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    description=f\"\"\"\n",
    "    Churn prediction model trained on 2025-10-30\n",
    "    Features: customer_transaction_features v2.1\n",
    "    Performance: AUC=0.89, F1=0.82\n",
    "    Training data: 100K customers, 30 days lookback\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    key=\"validation_status\",\n",
    "    value=\"pending\"\n",
    ")\n",
    "\n",
    "# Validation tests\n",
    "print(\"\\n🧪 Running validation tests...\")\n",
    "\n",
    "# Test 1: Model can load\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/Staging\")\n",
    "print(\"✅ Model loads successfully\")\n",
    "\n",
    "# Test 2: Predictions on validation set\n",
    "val_predictions = loaded_model.predict(X_val)\n",
    "val_auc = roc_auc_score(y_val, loaded_model.predict_proba(X_val)[:, 1])\n",
    "print(f\"✅ Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Test 3: No feature drift\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for feature in feature_names:\n",
    "    train_dist = X_train[feature]\n",
    "    val_dist = X_val[feature]\n",
    "    ks_stat, p_value = ks_2samp(train_dist, val_dist)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"⚠️ Feature drift detected in {feature} (p={p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"✅ {feature}: no drift\")\n",
    "\n",
    "# Test 4: Latency test\n",
    "import time\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = loaded_model.predict(X_val[:1])\n",
    "latency = (time.time() - start) / 1000 * 1000  # ms\n",
    "print(f\"✅ Average latency: {latency:.2f}ms\")\n",
    "\n",
    "# If all tests pass, promote to Production\n",
    "if val_auc > 0.85 and latency < 100:\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive old Production\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        key=\"validation_status\",\n",
    "        value=\"passed\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🚀 Model promoted to Production!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Model failed validation, staying in Staging\")\n",
    "```\n",
    "\n",
    "**Data Versioning with DVC:**\n",
    "\n",
    "```python\n",
    "# dvc.yaml - Define data pipeline\n",
    "\"\"\"\n",
    "stages:\n",
    "  extract:\n",
    "    cmd: python src/extract_data.py\n",
    "    deps:\n",
    "      - src/extract_data.py\n",
    "    outs:\n",
    "      - data/raw/transactions.parquet\n",
    "  \n",
    "  features:\n",
    "    cmd: python src/generate_features.py\n",
    "    deps:\n",
    "      - src/generate_features.py\n",
    "      - data/raw/transactions.parquet\n",
    "    params:\n",
    "      - features.lookback_days\n",
    "      - features.aggregation_windows\n",
    "    outs:\n",
    "      - data/features/customer_features.parquet\n",
    "  \n",
    "  train:\n",
    "    cmd: python src/train_model.py\n",
    "    deps:\n",
    "      - src/train_model.py\n",
    "      - data/features/customer_features.parquet\n",
    "    params:\n",
    "      - model.n_estimators\n",
    "      - model.max_depth\n",
    "    metrics:\n",
    "      - metrics.json:\n",
    "          cache: false\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "\"\"\"\n",
    "\n",
    "# params.yaml - Hyperparameters\n",
    "\"\"\"\n",
    "features:\n",
    "  lookback_days: 30\n",
    "  aggregation_windows: [7, 30, 90]\n",
    "\n",
    "model:\n",
    "  n_estimators: 100\n",
    "  max_depth: 10\n",
    "  min_samples_split: 5\n",
    "\"\"\"\n",
    "\n",
    "# Track data version\n",
    "\"\"\"\n",
    "$ dvc add data/raw/transactions.parquet\n",
    "$ git add data/raw/transactions.parquet.dvc .gitignore\n",
    "$ git commit -m \"Add transactions data v1\"\n",
    "$ git tag -a \"data-v1\" -m \"Initial dataset\"\n",
    "\n",
    "$ dvc push  # Upload to S3/GCS\n",
    "\"\"\"\n",
    "\n",
    "# Reproduce experiment\n",
    "\"\"\"\n",
    "$ git checkout data-v1\n",
    "$ dvc checkout\n",
    "$ dvc repro  # Runs entire pipeline\n",
    "\n",
    "Output:\n",
    "  Running stage 'extract'\n",
    "  Running stage 'features'  \n",
    "  Running stage 'train'\n",
    "  ✅ Reproduced experiment from data-v1\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Continuous Training with Kubeflow:**\n",
    "\n",
    "```python\n",
    "# kubeflow_pipeline.py\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "@func_to_container_op\n",
    "def extract_data(output_path: str):\n",
    "    \"\"\"Extract training data\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    df = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "    df.write.parquet(output_path)\n",
    "\n",
    "@func_to_container_op\n",
    "def train_model(data_path: str, model_path: str) -> float:\n",
    "    \"\"\"Train model and return AUC\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import joblib\n",
    "    \n",
    "    df = pd.read_parquet(data_path)\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    return auc\n",
    "\n",
    "@func_to_container_op\n",
    "def deploy_model(model_path: str, auc: float):\n",
    "    \"\"\"Deploy if AUC > threshold\"\"\"\n",
    "    if auc > 0.85:\n",
    "        # Deploy to production\n",
    "        import mlflow\n",
    "        mlflow.register_model(f\"file://{model_path}\", \"churn_model\")\n",
    "        print(f\"✅ Model deployed with AUC={auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"❌ Model not deployed, AUC={auc:.4f} < 0.85\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Customer Churn Training Pipeline',\n",
    "    description='Extract, train, deploy churn model'\n",
    ")\n",
    "def training_pipeline():\n",
    "    # Extract\n",
    "    extract_op = extract_data(output_path=\"/data/training.parquet\")\n",
    "    \n",
    "    # Train\n",
    "    train_op = train_model(\n",
    "        data_path=extract_op.output,\n",
    "        model_path=\"/models/churn_model.pkl\"\n",
    "    )\n",
    "    \n",
    "    # Deploy\n",
    "    deploy_op = deploy_model(\n",
    "        model_path=train_op.outputs['model_path'],\n",
    "        auc=train_op.outputs['Output']\n",
    "    )\n",
    "\n",
    "# Compile and run\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(training_pipeline, 'pipeline.yaml')\n",
    "    \n",
    "    # Submit to Kubeflow\n",
    "    client = kfp.Client(host='http://kubeflow.example.com')\n",
    "    client.create_run_from_pipeline_func(\n",
    "        training_pipeline,\n",
    "        arguments={},\n",
    "        run_name='churn_training_2025_10_30'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Model Comparison Dashboard:**\n",
    "\n",
    "```python\n",
    "# Compare multiple model versions\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get all versions of model\n",
    "versions = client.search_model_versions(f\"name='customer_churn_model'\")\n",
    "\n",
    "comparison_data = []\n",
    "for version in versions:\n",
    "    run = mlflow.get_run(version.run_id)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'version': version.version,\n",
    "        'stage': version.current_stage,\n",
    "        'created': version.creation_timestamp,\n",
    "        'auc': run.data.metrics.get('roc_auc', 0),\n",
    "        'f1': run.data.metrics.get('f1_score', 0),\n",
    "        'accuracy': run.data.metrics.get('accuracy', 0),\n",
    "        'n_estimators': run.data.params.get('n_estimators', 0),\n",
    "        'max_depth': run.data.params.get('max_depth', 0),\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Plot comparison\n",
    "fig = px.line(\n",
    "    df_comparison,\n",
    "    x='version',\n",
    "    y=['auc', 'f1', 'accuracy'],\n",
    "    title='Model Performance Over Versions',\n",
    "    labels={'value': 'Score', 'variable': 'Metric'}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Production model details\n",
    "prod_version = df_comparison[df_comparison['stage'] == 'Production'].iloc[0]\n",
    "print(f\"\\n📊 Production Model (v{prod_version['version']}):\")\n",
    "print(f\"  AUC: {prod_version['auc']:.4f}\")\n",
    "print(f\"  F1: {prod_version['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {prod_version['accuracy']:.4f}\")\n",
    "print(f\"  Hyperparams: n_estimators={prod_version['n_estimators']}, max_depth={prod_version['max_depth']}\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "mlops_best_practices = {\n",
    "    \"1. Reproducibility\": \"\"\"\n",
    "        Track EVERYTHING:\n",
    "        - Code version (git commit hash)\n",
    "        - Data version (DVC, Delta version)\n",
    "        - Environment (Docker image, requirements.txt)\n",
    "        - Random seeds\n",
    "        - Hyperparameters\n",
    "        \n",
    "        Goal: Reproduce exact model 6 months later\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Automated Testing\": \"\"\"\n",
    "        Unit tests:\n",
    "        - Feature engineering logic\n",
    "        - Model preprocessing\n",
    "        - Prediction postprocessing\n",
    "        \n",
    "        Integration tests:\n",
    "        - End-to-end pipeline\n",
    "        - Model serving API\n",
    "        - Feature store integration\n",
    "        \n",
    "        Model tests:\n",
    "        - Minimum accuracy threshold\n",
    "        - Inference latency < 100ms\n",
    "        - No feature drift\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Gradual Rollout\": \"\"\"\n",
    "        Don't deploy to 100% traffic immediately\n",
    "        \n",
    "        Stage 1: Shadow mode (30 days)\n",
    "        - New model runs parallel to old\n",
    "        - Predictions logged but not used\n",
    "        - Compare performance\n",
    "        \n",
    "        Stage 2: A/B test (7 days)\n",
    "        - 10% traffic to new model\n",
    "        - Monitor metrics closely\n",
    "        - Rollback if issues\n",
    "        \n",
    "        Stage 3: Ramp up (14 days)\n",
    "        - 25% → 50% → 75% → 100%\n",
    "        - Gradual increase\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Model Governance\": \"\"\"\n",
    "        RBAC for model promotion:\n",
    "        - Data Scientists: can register to Staging\n",
    "        - ML Engineers: can promote to Production\n",
    "        - Approvals: require 2+ reviewers\n",
    "        \n",
    "        Audit trail:\n",
    "        - Who deployed what when\n",
    "        - Why model was promoted\n",
    "        - What tests were run\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Rollback Plan\": \"\"\"\n",
    "        Always have rollback ready:\n",
    "        - Keep 2-3 previous versions in Production\n",
    "        - Canary deployment (route % traffic)\n",
    "        - Circuit breaker (auto-rollback if errors spike)\n",
    "        \n",
    "        Example:\n",
    "        v10 deployed → errors 5% → auto-rollback to v9\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564959f0",
   "metadata": {},
   "source": [
    "### 🚀 **Production Serving: Online, Batch y Model Monitoring**\n",
    "\n",
    "**Serving Strategies:**\n",
    "\n",
    "```python\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│            MODEL SERVING PATTERNS                          │\n",
    "├────────────────────────────────────────────────────────────┤\n",
    "│                                                            │\n",
    "│  1. ONLINE SERVING (Real-time)                            │\n",
    "│     ┌─────────────┐                                       │\n",
    "│     │   Client    │                                       │\n",
    "│     └──────┬──────┘                                       │\n",
    "│            │ HTTP Request                                 │\n",
    "│            ▼                                               │\n",
    "│     ┌──────────────┐        ┌──────────────┐            │\n",
    "│     │  Load        │        │   Feature    │            │\n",
    "│     │  Balancer    │───────▶│   Store      │            │\n",
    "│     │  (Nginx)     │        │   (Redis)    │            │\n",
    "│     └──────┬───────┘        └──────────────┘            │\n",
    "│            │                        │                     │\n",
    "│            ▼                        │                     │\n",
    "│     ┌──────────────┐                │                     │\n",
    "│     │  API Server  │◀───────────────┘                     │\n",
    "│     │  (FastAPI)   │                                      │\n",
    "│     │              │                                      │\n",
    "│     │ • Model in   │                                      │\n",
    "│     │   memory     │                                      │\n",
    "│     │ • Features   │                                      │\n",
    "│     │   from Redis │                                      │\n",
    "│     │ • <100ms     │                                      │\n",
    "│     └──────────────┘                                      │\n",
    "│                                                            │\n",
    "│  2. BATCH SERVING (Bulk predictions)                      │\n",
    "│     ┌──────────────┐                                      │\n",
    "│     │   Scheduler  │                                      │\n",
    "│     │   (Airflow)  │                                      │\n",
    "│     └──────┬───────┘                                      │\n",
    "│            │ Daily @2AM                                   │\n",
    "│            ▼                                               │\n",
    "│     ┌──────────────┐        ┌──────────────┐            │\n",
    "│     │  Spark Job   │───────▶│  Feature     │            │\n",
    "│     │              │        │  Store       │            │\n",
    "│     │ • Load model │        │  (Delta)     │            │\n",
    "│     │ • Get        │        └──────────────┘            │\n",
    "│     │   features   │                                      │\n",
    "│     │ • Predict    │                                      │\n",
    "│     │   millions   │                                      │\n",
    "│     └──────┬───────┘                                      │\n",
    "│            │                                               │\n",
    "│            ▼                                               │\n",
    "│     ┌──────────────┐                                      │\n",
    "│     │  Predictions │                                      │\n",
    "│     │  Delta Table │                                      │\n",
    "│     └──────────────┘                                      │\n",
    "│                                                            │\n",
    "│  3. STREAMING SERVING (Micro-batch)                       │\n",
    "│     ┌──────────────┐                                      │\n",
    "│     │    Kafka     │                                      │\n",
    "│     │   (Events)   │                                      │\n",
    "│     └──────┬───────┘                                      │\n",
    "│            │                                               │\n",
    "│            ▼                                               │\n",
    "│     ┌──────────────┐        ┌──────────────┐            │\n",
    "│     │ Flink/Spark  │───────▶│  Feature     │            │\n",
    "│     │ Streaming    │        │  Store       │            │\n",
    "│     │              │        └──────────────┘            │\n",
    "│     │ • Enrich     │                                      │\n",
    "│     │   with       │                                      │\n",
    "│     │   features   │                                      │\n",
    "│     │ • Predict    │                                      │\n",
    "│     │ • <1s        │                                      │\n",
    "│     └──────┬───────┘                                      │\n",
    "│            │                                               │\n",
    "│            ▼                                               │\n",
    "│     ┌──────────────┐                                      │\n",
    "│     │ Output Kafka │                                      │\n",
    "│     │   Topic      │                                      │\n",
    "│     └──────────────┘                                      │\n",
    "│                                                            │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Online Serving: FastAPI Production Setup**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import List\n",
    "import time\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Metrics\n",
    "PREDICTION_COUNT = Counter('prediction_requests_total', 'Total prediction requests')\n",
    "PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "PREDICTION_ERRORS = Counter('prediction_errors_total', 'Total prediction errors')\n",
    "\n",
    "# Global model variable\n",
    "model = None\n",
    "redis_client = None\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Load model once at startup (not per request)\n",
    "    \"\"\"\n",
    "    global model, redis_client\n",
    "    \n",
    "    # Load model from MLflow Registry\n",
    "    print(\"Loading model from MLflow...\")\n",
    "    model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "    print(\"✅ Model loaded\")\n",
    "    \n",
    "    # Connect to Redis\n",
    "    redis_client = redis.Redis(\n",
    "        host='redis',\n",
    "        port=6379,\n",
    "        decode_responses=True,\n",
    "        max_connections=50\n",
    "    )\n",
    "    print(\"✅ Redis connected\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup on shutdown\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Churn Prediction API\",\n",
    "    description=\"Real-time customer churn prediction\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_ids: List[str] = Field(..., max_items=100)  # Batch up to 100\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str\n",
    "    features: dict\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "    latency_ms: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Batch prediction endpoint\n",
    "    Latency: <100ms for 100 customers\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        PREDICTION_COUNT.inc(len(request.customer_ids))\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get features from Redis (batch)\n",
    "        pipeline = redis_client.pipeline()\n",
    "        for customer_id in request.customer_ids:\n",
    "            pipeline.get(f\"features:customer:{customer_id}\")\n",
    "        \n",
    "        features_raw = pipeline.execute()\n",
    "        \n",
    "        # Prepare batch input\n",
    "        feature_vectors = []\n",
    "        valid_customers = []\n",
    "        \n",
    "        for customer_id, features_str in zip(request.customer_ids, features_raw):\n",
    "            if not features_str:\n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=0.5,  # Default\n",
    "                    churn_risk=\"unknown\",\n",
    "                    features={}\n",
    "                ))\n",
    "                continue\n",
    "            \n",
    "            features = json.loads(features_str)\n",
    "            feature_vectors.append([\n",
    "                features[\"txn_count_30d\"],\n",
    "                features[\"total_spent_30d\"],\n",
    "                features[\"avg_transaction_30d\"],\n",
    "                features[\"days_since_last_txn\"]\n",
    "            ])\n",
    "            valid_customers.append((customer_id, features))\n",
    "        \n",
    "        # Batch prediction (efficient!)\n",
    "        if feature_vectors:\n",
    "            churn_probas = model.predict(feature_vectors)\n",
    "            \n",
    "            for (customer_id, features), churn_proba in zip(valid_customers, churn_probas):\n",
    "                risk = \"low\" if churn_proba < 0.3 else \"medium\" if churn_proba < 0.7 else \"high\"\n",
    "                \n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=float(churn_proba),\n",
    "                    churn_risk=risk,\n",
    "                    features=features\n",
    "                ))\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        PREDICTION_LATENCY.observe(latency_ms / 1000)\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            predictions=predictions,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        PREDICTION_ERRORS.inc()\n",
    "        raise HTTPException(500, f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check\"\"\"\n",
    "    try:\n",
    "        redis_client.ping()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"model_version\": model.metadata.get_model_info().version,\n",
    "            \"redis\": \"connected\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Run with: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4\n",
    "```\n",
    "\n",
    "**Kubernetes Deployment:**\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: churn-prediction-api\n",
    "spec:\n",
    "  replicas: 4  # Horizontal scaling\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: churn-prediction\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: churn-prediction\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: api\n",
    "        image: myregistry/churn-prediction:v1.2.3\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "        env:\n",
    "        - name: MLFLOW_TRACKING_URI\n",
    "          value: \"http://mlflow:5000\"\n",
    "        - name: REDIS_HOST\n",
    "          value: \"redis\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: churn-prediction-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: churn-prediction\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: churn-prediction-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: churn-prediction-api\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "```\n",
    "\n",
    "**Batch Serving with Spark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct, col, current_date\n",
    "import mlflow.pyfunc\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPredictionBatch\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load model as Spark UDF\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers (10M customers)\n",
    "active_customers = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/active_customers\") \\\n",
    "    .filter(col(\"is_active\") == True)\n",
    "\n",
    "print(f\"Scoring {active_customers.count():,} customers...\")\n",
    "\n",
    "# Get features\n",
    "customer_features = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers \\\n",
    "    .join(customer_features, \"customer_id\", \"inner\")\n",
    "\n",
    "# Batch predict (distributed across cluster)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ").withColumn(\n",
    "    \"churn_risk\",\n",
    "    when(col(\"churn_probability\") < 0.3, \"low\")\n",
    "    .when(col(\"churn_probability\") < 0.7, \"medium\")\n",
    "    .otherwise(\"high\")\n",
    ").withColumn(\n",
    "    \"prediction_date\",\n",
    "    current_date()\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"churn_risk\",\n",
    "    \"prediction_date\"\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers → CRM\n",
    "high_risk = predictions.filter(col(\"churn_risk\") == \"high\")\n",
    "\n",
    "high_risk.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"total_spent_30d\",\n",
    "    \"days_since_last_txn\"\n",
    ").write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .option(\"user\", \"crm_user\") \\\n",
    "    .option(\"password\", \"***\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"✅ Scored {predictions.count():,} customers\")\n",
    "print(f\"⚠️ High risk: {high_risk.count():,} customers\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Model Monitoring: Data Drift Detection**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "import mlflow\n",
    "\n",
    "# Reference data (training data distribution)\n",
    "reference_df = pd.read_parquet(\"s3://datalake/training/reference_data.parquet\")\n",
    "\n",
    "# Current data (production predictions last 7 days)\n",
    "current_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/prediction_inputs\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 7\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# Evidently report\n",
    "report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "    DataQualityPreset()\n",
    "])\n",
    "\n",
    "report.run(\n",
    "    reference_data=reference_df,\n",
    "    current_data=current_df,\n",
    "    column_mapping=None\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report.save_html(\"drift_report.html\")\n",
    "\n",
    "# Extract drift metrics\n",
    "drift_metrics = report.as_dict()\n",
    "\n",
    "# Check for drift\n",
    "features_with_drift = []\n",
    "for feature, metrics in drift_metrics['metrics'][0]['result']['drift_by_columns'].items():\n",
    "    if metrics['drift_detected']:\n",
    "        features_with_drift.append({\n",
    "            'feature': feature,\n",
    "            'drift_score': metrics['drift_score'],\n",
    "            'p_value': metrics.get('stattest_threshold', 0)\n",
    "        })\n",
    "\n",
    "if features_with_drift:\n",
    "    print(\"⚠️ DRIFT DETECTED in features:\")\n",
    "    for feature_drift in features_with_drift:\n",
    "        print(f\"  • {feature_drift['feature']}: score={feature_drift['drift_score']:.4f}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=\"drift_detection\"):\n",
    "        mlflow.log_metric(\"features_with_drift\", len(features_with_drift))\n",
    "        mlflow.log_artifact(\"drift_report.html\")\n",
    "    \n",
    "    # Send alert\n",
    "    send_alert(\n",
    "        title=\"⚠️ Data Drift Detected\",\n",
    "        message=f\"{len(features_with_drift)} features drifted\",\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "else:\n",
    "    print(\"✅ No drift detected\")\n",
    "```\n",
    "\n",
    "**Model Performance Monitoring:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get predictions and actuals\n",
    "predictions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_predictions\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 30\")\n",
    "\n",
    "# Join with actuals (churned or not in next 7 days)\n",
    "actuals = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(\"label_date >= current_date() - 30\")\n",
    "\n",
    "performance = predictions \\\n",
    "    .join(actuals, [\"customer_id\", \"prediction_date\"]) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Calculate metrics over time\n",
    "daily_metrics = []\n",
    "\n",
    "for date in pd.date_range(\n",
    "    start=datetime.now() - timedelta(days=30),\n",
    "    end=datetime.now(),\n",
    "    freq='D'\n",
    "):\n",
    "    day_data = performance[performance['prediction_date'] == date.date()]\n",
    "    \n",
    "    if len(day_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "    \n",
    "    auc = roc_auc_score(day_data['churned'], day_data['churn_probability'])\n",
    "    \n",
    "    # Classify at 0.7 threshold\n",
    "    predictions_binary = (day_data['churn_probability'] > 0.7).astype(int)\n",
    "    precision = precision_score(day_data['churned'], predictions_binary)\n",
    "    recall = recall_score(day_data['churned'], predictions_binary)\n",
    "    \n",
    "    daily_metrics.append({\n",
    "        'date': date,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'num_predictions': len(day_data)\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(daily_metrics)\n",
    "\n",
    "# Plot performance over time\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['auc'], name='AUC'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['precision'], name='Precision'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['recall'], name='Recall'))\n",
    "\n",
    "# Add threshold line\n",
    "fig.add_hline(y=0.80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Minimum AUC\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Over Last 30 Days',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Score',\n",
    "    yaxis_range=[0, 1]\n",
    ")\n",
    "\n",
    "fig.write_html(\"performance_dashboard.html\")\n",
    "\n",
    "# Check if below threshold\n",
    "if df_metrics['auc'].iloc[-1] < 0.80:\n",
    "    print(\"⚠️ MODEL PERFORMANCE DEGRADED\")\n",
    "    print(f\"  Current AUC: {df_metrics['auc'].iloc[-1]:.4f}\")\n",
    "    print(f\"  Threshold: 0.80\")\n",
    "    \n",
    "    # Trigger retraining\n",
    "    from airflow.api.client.local_client import Client\n",
    "    client = Client(None, None)\n",
    "    client.trigger_dag(\n",
    "        dag_id='ml_training_pipeline',\n",
    "        conf={'reason': 'performance_degradation'}\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Retraining triggered\")\n",
    "else:\n",
    "    print(\"✅ Model performance healthy\")\n",
    "```\n",
    "\n",
    "**A/B Testing Framework:**\n",
    "\n",
    "```python\n",
    "# Route traffic between model versions\n",
    "from fastapi import FastAPI, Request\n",
    "import random\n",
    "import mlflow\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load multiple model versions\n",
    "model_v1 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/1\")\n",
    "model_v2 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/2\")\n",
    "\n",
    "# A/B test config\n",
    "AB_TEST_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"model_v1_traffic\": 0.9,  # 90% traffic\n",
    "    \"model_v2_traffic\": 0.1   # 10% traffic (new model)\n",
    "}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: Request, customer_id: str):\n",
    "    # Assign to variant\n",
    "    if AB_TEST_CONFIG[\"enabled\"]:\n",
    "        rand = random.random()\n",
    "        if rand < AB_TEST_CONFIG[\"model_v2_traffic\"]:\n",
    "            variant = \"v2\"\n",
    "            model = model_v2\n",
    "        else:\n",
    "            variant = \"v1\"\n",
    "            model = model_v1\n",
    "    else:\n",
    "        variant = \"v1\"\n",
    "        model = model_v1\n",
    "    \n",
    "    # Get features and predict\n",
    "    features = get_features(customer_id)\n",
    "    prediction = model.predict([features])[0]\n",
    "    \n",
    "    # Log for analysis\n",
    "    log_ab_test(\n",
    "        customer_id=customer_id,\n",
    "        variant=variant,\n",
    "        prediction=prediction,\n",
    "        timestamp=datetime.now()\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"churn_probability\": float(prediction),\n",
    "        \"model_variant\": variant  # For debugging\n",
    "    }\n",
    "\n",
    "# Analyze A/B test results\n",
    "def analyze_ab_test():\n",
    "    \"\"\"\n",
    "    Compare model v1 vs v2 performance\n",
    "    \"\"\"\n",
    "    results = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"s3://datalake/gold/ab_test_logs\") \\\n",
    "        .filter(\"timestamp >= current_timestamp() - interval 7 days\")\n",
    "    \n",
    "    # Group by variant\n",
    "    v1_results = results.filter(\"variant == 'v1'\")\n",
    "    v2_results = results.filter(\"variant == 'v2'\")\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    # Compare AUC\n",
    "    v1_auc = calculate_auc(v1_results)\n",
    "    v2_auc = calculate_auc(v2_results)\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(v1_results['auc'], v2_results['auc'])\n",
    "    \n",
    "    print(f\"Model v1 AUC: {v1_auc:.4f}\")\n",
    "    print(f\"Model v2 AUC: {v2_auc:.4f}\")\n",
    "    print(f\"Improvement: {(v2_auc - v1_auc) / v1_auc * 100:.2f}%\")\n",
    "    print(f\"Statistical significance: p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05 and v2_auc > v1_auc:\n",
    "        print(\"✅ Model v2 is significantly better!\")\n",
    "        print(\"Recommendation: Ramp up v2 to 100% traffic\")\n",
    "    else:\n",
    "        print(\"❌ No significant improvement, keep v1\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b644b1",
   "metadata": {},
   "source": [
    "## 1. Pipeline de datos para ML: ETL → Feature Engineering → Training → Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02075bc",
   "metadata": {},
   "source": [
    "- Extract: fuentes raw (transacciones, logs, eventos).\n",
    "- Transform: limpieza, agregaciones por ventana, joins.\n",
    "- Feature Engineering: crear features (ratios, lags, embeddings).\n",
    "- Training: dataset versionado, experimentos (MLflow/Weights&Biases).\n",
    "- Serving: online (API real-time) y offline (batch predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed38e",
   "metadata": {},
   "source": [
    "## 2. Feature Store: concepto y práctica con Feast (demo local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: instala feast si quieres ejecutar (no en requirements por defecto)\n",
    "feast_demo = r'''\n",
    "# feature_repo/feature_definitions.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "source = FileSource(\n",
    "    path='data/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "cliente_fv = FeatureView(\n",
    "    name='cliente_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras', dtype=Float32),\n",
    "        Field(name='num_transacciones', dtype=Int64),\n",
    "    ],\n",
    "    source=source\n",
    ")\n",
    "# feast apply\n",
    "# feast materialize-incremental\n",
    "# store.get_online_features(...)\n",
    "'''\n",
    "print(feast_demo.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61f6f9",
   "metadata": {},
   "source": [
    "## 3. Versionado de datasets y experimentos con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf33f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_snippet = r'''\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('ventas_prediccion')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('model', 'xgboost')\n",
    "    mlflow.log_param('n_estimators', 100)\n",
    "    mlflow.log_metric('rmse', 0.85)\n",
    "    mlflow.log_artifact('model.pkl')\n",
    "'''\n",
    "print(mlflow_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e01bb",
   "metadata": {},
   "source": [
    "## 4. Reentrenamiento automatizado (Airflow + MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7556724",
   "metadata": {},
   "source": [
    "- DAG semanal: extraer nuevos datos, generar features, entrenar, evaluar.\n",
    "- Si métrica mejora sobre baseline → registrar modelo, promover a producción.\n",
    "- Si drift detectado → alerta y reentrenamiento fuera de calendario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d1c42",
   "metadata": {},
   "source": [
    "## 5. Serving online y batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85229297",
   "metadata": {},
   "source": [
    "- Online: endpoint REST (FastAPI + modelo en memoria o via MLflow Model Registry).\n",
    "- Batch: Spark job semanal para scoring de todo el catálogo/clientes.\n",
    "- Caché de features online (Redis) para latencia < 50ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59fa8b",
   "metadata": {},
   "source": [
    "## 6. Monitoreo de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa955c",
   "metadata": {},
   "source": [
    "- Data drift: distribución de features cambia (KS test, PSI).\n",
    "- Concept drift: relación X→Y cambia (performance degrada).\n",
    "- Métricas de negocio: precisión, recall, ROC-AUC, ingresos.\n",
    "- Alertas automáticas y rollback si threshold cruzado."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
