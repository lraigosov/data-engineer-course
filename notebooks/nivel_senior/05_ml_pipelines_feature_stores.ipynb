{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3e3e0f",
   "metadata": {},
   "source": [
    "# ğŸ¤– ML Pipelines y Feature Stores\n",
    "\n",
    "Objetivo: diseÃ±ar pipelines reproducibles de datos para ML, integrar feature stores (Feast/Tecton) y aplicar MLOps bÃ¡sico (versionado, reentrenamiento, monitoreo).\n",
    "\n",
    "- DuraciÃ³n: 120â€“150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Python ML bÃ¡sico, pipelines batch/streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99744d",
   "metadata": {},
   "source": [
    "### ğŸ”„ **ML Pipeline Architecture: Training vs Inference**\n",
    "\n",
    "**DesafÃ­o ClÃ¡sico: Research vs Production Gap**\n",
    "\n",
    "```\n",
    "Data Science Notebook (Research):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Jupyter Notebook               â”‚\n",
    "â”‚ â€¢ Pandas local (100K rows)     â”‚\n",
    "â”‚ â€¢ Manual feature engineering   â”‚\n",
    "â”‚ â€¢ sklearn model training       â”‚\n",
    "â”‚ â€¢ Pickle model â†’ disk          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“ \"Works on my machine!\"\n",
    "   â†“ Deploy to production...\n",
    "   âŒ Features different\n",
    "   âŒ Data scale breaks\n",
    "   âŒ Not reproducible\n",
    "   âŒ Drift undetected\n",
    "\n",
    "Production Reality:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ â€¢ Spark (billions of rows)     â”‚\n",
    "â”‚ â€¢ Streaming features           â”‚\n",
    "â”‚ â€¢ Model serving infrastructureâ”‚\n",
    "â”‚ â€¢ Monitoring & alerts          â”‚\n",
    "â”‚ â€¢ A/B testing                  â”‚\n",
    "â”‚ â€¢ Retraining automation        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**ML Pipeline Complete Architecture:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATA SOURCES                              â”‚\n",
    "â”‚  â€¢ Transactional DB (PostgreSQL)                            â”‚\n",
    "â”‚  â€¢ Event Stream (Kafka)                                     â”‚\n",
    "â”‚  â€¢ External APIs (Weather, Demographics)                    â”‚\n",
    "â”‚  â€¢ Historical Data Lake (S3/Delta)                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚                             â”‚\n",
    "      â–¼                             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   OFFLINE    â”‚            â”‚   ONLINE     â”‚\n",
    "â”‚   FEATURES   â”‚            â”‚  FEATURES    â”‚\n",
    "â”‚              â”‚            â”‚              â”‚\n",
    "â”‚ â€¢ Batch ETL  â”‚            â”‚â€¢ Streaming   â”‚\n",
    "â”‚   (Spark)    â”‚            â”‚  (Flink)     â”‚\n",
    "â”‚ â€¢ Daily agg  â”‚            â”‚â€¢ Real-time   â”‚\n",
    "â”‚ â€¢ Historical â”‚            â”‚  agg         â”‚\n",
    "â”‚   30d, 90d   â”‚            â”‚â€¢ Last 24h    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                           â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ FEATURE STORE   â”‚\n",
    "       â”‚                 â”‚\n",
    "       â”‚ â€¢ Feast/Tecton  â”‚\n",
    "       â”‚ â€¢ Offline Store â”‚\n",
    "       â”‚   (Delta Lake)  â”‚\n",
    "       â”‚ â€¢ Online Store  â”‚\n",
    "       â”‚   (Redis/DDB)   â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                â”‚\n",
    "        â–¼                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   TRAINING   â”‚  â”‚   SERVING    â”‚\n",
    "â”‚   PIPELINE   â”‚  â”‚   PIPELINE   â”‚\n",
    "â”‚              â”‚  â”‚              â”‚\n",
    "â”‚ â€¢ Feature    â”‚  â”‚ â€¢ Feature    â”‚\n",
    "â”‚   retrieval  â”‚  â”‚   retrieval  â”‚\n",
    "â”‚ â€¢ Model      â”‚  â”‚   (online)   â”‚\n",
    "â”‚   training   â”‚  â”‚ â€¢ Model      â”‚\n",
    "â”‚ â€¢ Validation â”‚  â”‚   inference  â”‚\n",
    "â”‚ â€¢ Registry   â”‚  â”‚ â€¢ Monitoring â”‚\n",
    "â”‚   (MLflow)   â”‚  â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                 â”‚\n",
    "       â–¼                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ MODEL        â”‚  â”‚  PREDICTIONS â”‚\n",
    "â”‚ REGISTRY     â”‚  â”‚              â”‚\n",
    "â”‚              â”‚  â”‚ â€¢ Online API â”‚\n",
    "â”‚ â€¢ Versions   â”‚  â”‚   (<100ms)   â”‚\n",
    "â”‚ â€¢ Metadata   â”‚  â”‚ â€¢ Batch job  â”‚\n",
    "â”‚ â€¢ Stage      â”‚  â”‚   (daily)    â”‚\n",
    "â”‚   (staging/  â”‚  â”‚ â€¢ A/B test   â”‚\n",
    "â”‚    prod)     â”‚  â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                  â”‚  MONITORING  â”‚\n",
    "                  â”‚              â”‚\n",
    "                  â”‚ â€¢ Data drift â”‚\n",
    "                  â”‚ â€¢ Model perf â”‚\n",
    "                  â”‚ â€¢ Latency    â”‚\n",
    "                  â”‚ â€¢ Alerts     â”‚\n",
    "                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Training Pipeline (Offline):**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, datediff, sum as spark_sum\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Step 1: Feature Engineering (Spark Batch)\n",
    "spark = SparkSession.builder.appName(\"ML_Training\").getOrCreate()\n",
    "\n",
    "# Historical transactions\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# Aggregate features per customer (last 30 days)\n",
    "training_date = datetime(2025, 10, 30)\n",
    "lookback_start = training_date - timedelta(days=30)\n",
    "\n",
    "customer_features = transactions \\\n",
    "    .filter(\n",
    "        (col(\"transaction_date\") >= lookback_start) & \n",
    "        (col(\"transaction_date\") < training_date)\n",
    "    ) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"txn_count_30d\"),\n",
    "        spark_sum(\"amount\").alias(\"total_spent_30d\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_30d\"),\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(lit(training_date), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# Join with labels (did customer churn in next 7 days?)\n",
    "labels = spark.read.format(\"delta\").load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(col(\"label_date\") == training_date)\n",
    "\n",
    "training_data = customer_features.join(labels, \"customer_id\")\n",
    "\n",
    "# Convert to Pandas for sklearn (or use Spark MLlib)\n",
    "df_train = training_data.toPandas()\n",
    "\n",
    "# Step 2: Train Model\n",
    "feature_cols = [\"txn_count_30d\", \"total_spent_30d\", \"avg_transaction_30d\", \"days_since_last_txn\"]\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"churned\"]\n",
    "\n",
    "mlflow.set_experiment(\"customer_churn\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rf_train_{training_date.strftime('%Y%m%d')}\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"training_date\", training_date.isoformat())\n",
    "    mlflow.log_param(\"lookback_days\", 30)\n",
    "    \n",
    "    # Train\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_auc\", auc)\n",
    "    mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=\"customer_churn_model\"\n",
    "    )\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), \"feature_importance.json\")\n",
    "    \n",
    "    print(f\"Model trained with AUC: {auc:.4f}\")\n",
    "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "```\n",
    "\n",
    "**Inference Pipeline (Online):**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model from MLflow Registry (production stage)\n",
    "model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "\n",
    "# Redis for feature caching\n",
    "redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: str\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str  # low, medium, high\n",
    "    features_used: Dict[str, float]\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict_churn(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Real-time churn prediction\n",
    "    Latency target: <100ms\n",
    "    \"\"\"\n",
    "    customer_id = request.customer_id\n",
    "    \n",
    "    # 1. Get features from Redis (online feature store)\n",
    "    cache_key = f\"features:customer:{customer_id}\"\n",
    "    cached_features = redis_client.get(cache_key)\n",
    "    \n",
    "    if not cached_features:\n",
    "        raise HTTPException(404, f\"Features not found for customer {customer_id}\")\n",
    "    \n",
    "    features = json.loads(cached_features)\n",
    "    \n",
    "    # 2. Prepare input for model\n",
    "    feature_vector = [[\n",
    "        features[\"txn_count_30d\"],\n",
    "        features[\"total_spent_30d\"],\n",
    "        features[\"avg_transaction_30d\"],\n",
    "        features[\"days_since_last_txn\"]\n",
    "    ]]\n",
    "    \n",
    "    # 3. Predict\n",
    "    churn_proba = model.predict(feature_vector)[0]\n",
    "    \n",
    "    # 4. Classify risk\n",
    "    if churn_proba < 0.3:\n",
    "        risk = \"low\"\n",
    "    elif churn_proba < 0.7:\n",
    "        risk = \"medium\"\n",
    "    else:\n",
    "        risk = \"high\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=customer_id,\n",
    "        churn_probability=float(churn_proba),\n",
    "        churn_risk=risk,\n",
    "        features_used=features\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model_version\": model.metadata.get_model_info().version}\n",
    "```\n",
    "\n",
    "**Batch Inference Pipeline:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "# Load model in Spark context\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers\n",
    "active_customers = spark.read.format(\"delta\").load(\"s3://datalake/gold/active_customers\")\n",
    "\n",
    "# Get their features\n",
    "customer_features = spark.read.format(\"delta\").load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers.join(customer_features, \"customer_id\")\n",
    "\n",
    "# Predict in batch (distributed)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    lit(current_date()).alias(\"prediction_date\")\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers for marketing campaign\n",
    "high_risk = predictions.filter(col(\"churn_probability\") > 0.7)\n",
    "\n",
    "high_risk.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm-db:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"Scored {predictions.count():,} customers\")\n",
    "print(f\"High risk: {high_risk.count():,} customers\")\n",
    "```\n",
    "\n",
    "**Key Differences: Training vs Serving**\n",
    "\n",
    "```python\n",
    "differences = {\n",
    "    \"Data Volume\": {\n",
    "        \"Training\": \"Full historical data (TB-scale, years)\",\n",
    "        \"Serving Online\": \"Single customer features (KB-scale)\",\n",
    "        \"Serving Batch\": \"All active customers (GB-scale, daily)\"\n",
    "    },\n",
    "    \n",
    "    \"Latency\": {\n",
    "        \"Training\": \"Hours/days OK (run weekly/monthly)\",\n",
    "        \"Serving Online\": \"<100ms required (SLA critical)\",\n",
    "        \"Serving Batch\": \"Minutes/hours OK (overnight jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Features\": {\n",
    "        \"Training\": \"Pre-computed offline features (historical accuracy)\",\n",
    "        \"Serving Online\": \"Real-time features from online store (Redis)\",\n",
    "        \"Serving Batch\": \"Daily snapshot features\"\n",
    "    },\n",
    "    \n",
    "    \"Compute\": {\n",
    "        \"Training\": \"Large Spark cluster (100+ cores)\",\n",
    "        \"Serving Online\": \"Small API server (4-8 cores, low memory)\",\n",
    "        \"Serving Batch\": \"Medium Spark cluster (20-50 cores)\"\n",
    "    },\n",
    "    \n",
    "    \"Tools\": {\n",
    "        \"Training\": \"Spark, Pandas, sklearn/XGBoost, MLflow\",\n",
    "        \"Serving Online\": \"FastAPI, Redis, model in memory\",\n",
    "        \"Serving Batch\": \"Spark with mlflow.pyfunc.spark_udf\"\n",
    "    },\n",
    "    \n",
    "    \"Challenges\": {\n",
    "        \"Training\": \"\"\"\n",
    "            â€¢ Feature-label leakage (use point-in-time correct features)\n",
    "            â€¢ Class imbalance (use SMOTE, class weights)\n",
    "            â€¢ Reproducibility (seed, versions, data snapshots)\n",
    "        \"\"\",\n",
    "        \"Serving Online\": \"\"\"\n",
    "            â€¢ Feature freshness (stale features in cache)\n",
    "            â€¢ Latency spikes (model load, garbage collection)\n",
    "            â€¢ Feature skew (training vs serving mismatch)\n",
    "        \"\"\",\n",
    "        \"Serving Batch\": \"\"\"\n",
    "            â€¢ Scale (billions of predictions)\n",
    "            â€¢ Resource contention (don't starve other jobs)\n",
    "            â€¢ Failure recovery (checkpoint, retry logic)\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Automated Retraining with Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_model_performance(**context):\n",
    "    \"\"\"\n",
    "    Compare current model vs new model\n",
    "    Promote to production if better\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    \n",
    "    # Get current production model metrics\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    prod_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Production\"])\n",
    "    prod_run_id = prod_versions[0].run_id\n",
    "    prod_auc = mlflow.get_run(prod_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    # Get latest staging model\n",
    "    staging_versions = client.get_latest_versions(\"customer_churn_model\", stages=[\"Staging\"])\n",
    "    staging_run_id = staging_versions[0].run_id\n",
    "    staging_auc = mlflow.get_run(staging_run_id).data.metrics[\"train_auc\"]\n",
    "    \n",
    "    print(f\"Production AUC: {prod_auc:.4f}\")\n",
    "    print(f\"Staging AUC: {staging_auc:.4f}\")\n",
    "    \n",
    "    # Promote if better by at least 1%\n",
    "    if staging_auc > prod_auc * 1.01:\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"customer_churn_model\",\n",
    "            version=staging_versions[0].version,\n",
    "            stage=\"Production\"\n",
    "        )\n",
    "        print(\"âœ… New model promoted to Production!\")\n",
    "        return \"promote\"\n",
    "    else:\n",
    "        print(\"âŒ New model not better, keeping current Production\")\n",
    "        return \"keep_current\"\n",
    "\n",
    "with DAG(\n",
    "    \"ml_training_pipeline\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@weekly\",  # Every Monday\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    \n",
    "    # Step 1: Feature engineering (Spark on EMR)\n",
    "    feature_engineering = EmrAddStepsOperator(\n",
    "        task_id=\"feature_engineering\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Feature Engineering',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/feature_engineering.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Model training\n",
    "    model_training = EmrAddStepsOperator(\n",
    "        task_id=\"model_training\",\n",
    "        job_flow_id=\"j-XXXXXXXXXXXXX\",\n",
    "        steps=[{\n",
    "            'Name': 'Model Training',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'spark-submit',\n",
    "                    '--master', 'yarn',\n",
    "                    's3://scripts/train_model.py',\n",
    "                    '--date', '{{ ds }}'\n",
    "                ]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Model evaluation & promotion\n",
    "    evaluate_and_promote = PythonOperator(\n",
    "        task_id=\"evaluate_and_promote\",\n",
    "        python_callable=check_model_performance\n",
    "    )\n",
    "    \n",
    "    feature_engineering >> model_training >> evaluate_and_promote\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed380e1c",
   "metadata": {},
   "source": [
    "### ğŸª **Feature Stores: Feast, Tecton y Feature Engineering at Scale**\n",
    "\n",
    "**Problema: Training-Serving Skew**\n",
    "\n",
    "```\n",
    "Scenario sin Feature Store:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ TRAINING (Data Scientist)            â”‚\n",
    "â”‚                                      â”‚\n",
    "â”‚ # feature_engineering_training.py    â”‚\n",
    "â”‚ df['avg_purchase_30d'] =             â”‚\n",
    "â”‚   df.groupby('customer_id')          â”‚\n",
    "â”‚     ['amount']                       â”‚\n",
    "â”‚     .rolling(30)                     â”‚\n",
    "â”‚     .mean()                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“ Model trained con esta feature\n",
    "       â†“ Deploy to production...\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ SERVING (Engineer implementa en API) â”‚\n",
    "â”‚                                      â”‚\n",
    "â”‚ # feature_api.py                     â”‚\n",
    "â”‚ avg_purchase = db.query(             â”‚\n",
    "â”‚   f\"SELECT AVG(amount)               â”‚\n",
    "â”‚    FROM purchases                    â”‚\n",
    "â”‚    WHERE customer_id={id}            â”‚\n",
    "â”‚    AND date >= NOW() - 30\"           â”‚\n",
    "â”‚ )                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       âŒ Subtle difference in logic!\n",
    "       âŒ Training: rolling mean (include current)\n",
    "       âŒ Serving: SQL AVG (different timestamp handling)\n",
    "       âŒ Model performance degrades silently\n",
    "```\n",
    "\n",
    "**Feature Store Solution:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              FEATURE STORE                             â”‚\n",
    "â”‚                                                        â”‚\n",
    "â”‚  Single Source of Truth for Features                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚   OFFLINE    â”‚           â”‚   ONLINE     â”‚         â”‚\n",
    "â”‚  â”‚   STORE      â”‚           â”‚   STORE      â”‚         â”‚\n",
    "â”‚  â”‚              â”‚           â”‚              â”‚         â”‚\n",
    "â”‚  â”‚ â€¢ Delta Lake â”‚           â”‚ â€¢ Redis      â”‚         â”‚\n",
    "â”‚  â”‚ â€¢ Historical â”‚           â”‚ â€¢ DynamoDB   â”‚         â”‚\n",
    "â”‚  â”‚   features   â”‚           â”‚ â€¢ Cassandra  â”‚         â”‚\n",
    "â”‚  â”‚ â€¢ Training   â”‚           â”‚              â”‚         â”‚\n",
    "â”‚  â”‚   retrieval  â”‚           â”‚ â€¢ Serving    â”‚         â”‚\n",
    "â”‚  â”‚              â”‚           â”‚   retrieval  â”‚         â”‚\n",
    "â”‚  â”‚ â€¢ Point-in-  â”‚           â”‚ â€¢ <10ms      â”‚         â”‚\n",
    "â”‚  â”‚   time       â”‚           â”‚   latency    â”‚         â”‚\n",
    "â”‚  â”‚   correct    â”‚           â”‚              â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â”‚                          â”‚                 â”‚\n",
    "â”‚         â”‚  SAME DEFINITION         â”‚                 â”‚\n",
    "â”‚         â”‚  GUARANTEED CONSISTENCY  â”‚                 â”‚\n",
    "â”‚         â”‚                          â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                          â”‚\n",
    "          â–¼                          â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  TRAINING   â”‚           â”‚   SERVING   â”‚\n",
    "   â”‚             â”‚           â”‚             â”‚\n",
    "   â”‚ get_        â”‚           â”‚ get_        â”‚\n",
    "   â”‚ historical_ â”‚           â”‚ online_     â”‚\n",
    "   â”‚ features()  â”‚           â”‚ features()  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Feast: Open-Source Feature Store**\n",
    "\n",
    "```python\n",
    "# feast_repo/feature_store.yaml\n",
    "project: customer_churn\n",
    "registry: s3://feast-registry/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "    type: redis\n",
    "    connection_string: \"redis:6379\"\n",
    "offline_store:\n",
    "    type: file  # Or \"spark\", \"snowflake\", \"bigquery\"\n",
    "    \n",
    "# feast_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, Feature\n",
    "from feast.types import Float32, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define entity (primary key)\n",
    "customer = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    description=\"Customer entity\"\n",
    ")\n",
    "\n",
    "# Offline data source (historical features)\n",
    "customer_transactions_source = FileSource(\n",
    "    path=\"s3://datalake/gold/customer_features_daily/\",\n",
    "    timestamp_field=\"feature_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "# Feature view (collection of features)\n",
    "customer_transaction_features = FeatureView(\n",
    "    name=\"customer_transaction_features\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=30),  # Features valid for 30 days\n",
    "    schema=[\n",
    "        Field(name=\"txn_count_30d\", dtype=Int64),\n",
    "        Field(name=\"total_spent_30d\", dtype=Float32),\n",
    "        Field(name=\"avg_transaction_30d\", dtype=Float32),\n",
    "        Field(name=\"days_since_last_txn\", dtype=Int64),\n",
    "        Field(name=\"favorite_category\", dtype=String),\n",
    "    ],\n",
    "    online=True,  # Enable online serving\n",
    "    source=customer_transactions_source,\n",
    "    tags={\"team\": \"data-science\", \"pii\": \"false\"}\n",
    ")\n",
    "\n",
    "# On-demand features (computed at request time)\n",
    "from feast import OnDemandFeatureView, RequestSource\n",
    "\n",
    "request_source = RequestSource(\n",
    "    name=\"request_data\",\n",
    "    schema=[\n",
    "        Field(name=\"current_cart_value\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "@OnDemandFeatureView(\n",
    "    sources=[customer_transaction_features, request_source],\n",
    "    schema=[\n",
    "        Field(name=\"cart_to_avg_ratio\", dtype=Float32)\n",
    "    ]\n",
    ")\n",
    "def cart_features(inputs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ratio of current cart to historical average\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df[\"cart_to_avg_ratio\"] = (\n",
    "        inputs[\"current_cart_value\"] / inputs[\"avg_transaction_30d\"]\n",
    "    )\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Deploy Feature Definitions:**\n",
    "\n",
    "```bash\n",
    "# Apply feature definitions to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# Registered entity customer\n",
    "# Registered feature view customer_transaction_features\n",
    "# Deploying infrastructure for online store...\n",
    "# âœ… Deployment successful!\n",
    "```\n",
    "\n",
    "**Training: Get Historical Features**\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feast_repo/\")\n",
    "\n",
    "# Entity dataframe (customers and timestamps for training)\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, 1004],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1),\n",
    "        datetime(2025, 10, 1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Get point-in-time correct features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "\n",
    "# Output:\n",
    "#   customer_id event_timestamp  txn_count_30d  total_spent_30d  avg_transaction_30d  days_since_last_txn\n",
    "# 0        1001      2025-10-01             15           1250.0                 83.3                    2\n",
    "# 1        1002      2025-10-01              8            430.0                 53.8                    5\n",
    "# 2        1003      2025-10-01             25           3200.0                128.0                    1\n",
    "# 3        1004      2025-10-01              3            120.0                 40.0                   14\n",
    "\n",
    "# Point-in-time correct:\n",
    "# For customer 1001 on 2025-10-01:\n",
    "# - Only uses data from 2025-09-01 to 2025-09-30 (before event_timestamp)\n",
    "# - No data leakage from future\n",
    "```\n",
    "\n",
    "**Materialize Features to Online Store:**\n",
    "\n",
    "```python\n",
    "# Materialize features to Redis for online serving\n",
    "from datetime import datetime\n",
    "\n",
    "store.materialize(\n",
    "    start_date=datetime(2025, 10, 1),\n",
    "    end_date=datetime(2025, 10, 30)\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 4 features for customer_transaction_features\n",
    "# Progress: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 customers\n",
    "# âœ… Materialization complete!\n",
    "# Online store: Redis updated with latest features\n",
    "```\n",
    "\n",
    "**Serving: Get Online Features**\n",
    "\n",
    "```python\n",
    "# Real-time feature retrieval (< 10ms)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"customer_transaction_features:txn_count_30d\",\n",
    "        \"customer_transaction_features:total_spent_30d\",\n",
    "        \"customer_transaction_features:avg_transaction_30d\",\n",
    "        \"customer_transaction_features:days_since_last_txn\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\"customer_id\": 1001},\n",
    "        {\"customer_id\": 1002}\n",
    "    ]\n",
    ").to_dict()\n",
    "\n",
    "print(features)\n",
    "\n",
    "# Output:\n",
    "# {\n",
    "#   'customer_id': [1001, 1002],\n",
    "#   'txn_count_30d': [15, 8],\n",
    "#   'total_spent_30d': [1250.0, 430.0],\n",
    "#   'avg_transaction_30d': [83.3, 53.8],\n",
    "#   'days_since_last_txn': [2, 5]\n",
    "# }\n",
    "\n",
    "# Latency: 5-10ms (Redis lookup)\n",
    "```\n",
    "\n",
    "**Feature Engineering Patterns:**\n",
    "\n",
    "```python\n",
    "# 1. AGGREGATION FEATURES (most common)\n",
    "\"\"\"\n",
    "Aggregate historical transactions over windows\n",
    "\"\"\"\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, avg, count, datediff, current_date\n",
    "\n",
    "# Daily feature computation\n",
    "transactions = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "\n",
    "# 7-day, 30-day, 90-day windows\n",
    "for days in [7, 30, 90]:\n",
    "    window_spec = Window \\\n",
    "        .partitionBy(\"customer_id\") \\\n",
    "        .orderBy(\"transaction_date\") \\\n",
    "        .rangeBetween(-days * 86400, 0)  # seconds\n",
    "    \n",
    "    transactions = transactions \\\n",
    "        .withColumn(f\"txn_count_{days}d\", count(\"*\").over(window_spec)) \\\n",
    "        .withColumn(f\"total_spent_{days}d\", sum(\"amount\").over(window_spec)) \\\n",
    "        .withColumn(f\"avg_transaction_{days}d\", avg(\"amount\").over(window_spec))\n",
    "\n",
    "# 2. RECENCY FEATURES\n",
    "\"\"\"\n",
    "How recent was last activity\n",
    "\"\"\"\n",
    "customer_features = transactions \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"days_since_last_txn\",\n",
    "        datediff(current_date(), col(\"last_transaction_date\"))\n",
    "    )\n",
    "\n",
    "# 3. FREQUENCY FEATURES\n",
    "\"\"\"\n",
    "How often customer transacts\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"avg_days_between_txn\",\n",
    "        col(\"days_since_first_txn\") / col(\"total_txn_count\")\n",
    "    )\n",
    "\n",
    "# 4. RATIO FEATURES\n",
    "\"\"\"\n",
    "Relative comparisons\n",
    "\"\"\"\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\n",
    "        \"spend_acceleration\",\n",
    "        col(\"total_spent_30d\") / (col(\"total_spent_90d\") + 1)  # +1 avoid div/0\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"high_value_txn_ratio\",\n",
    "        col(\"high_value_txn_count\") / (col(\"total_txn_count\") + 1)\n",
    "    )\n",
    "\n",
    "# 5. CATEGORICAL EMBEDDINGS\n",
    "\"\"\"\n",
    "Convert categories to vectors (for neural networks)\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"favorite_category\", outputCol=\"category_idx\")\n",
    "encoder = OneHotEncoder(inputCol=\"category_idx\", outputCol=\"category_vec\")\n",
    "\n",
    "# 6. TEXT FEATURES\n",
    "\"\"\"\n",
    "NLP on customer reviews\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "```\n",
    "\n",
    "**Tecton: Enterprise Feature Store**\n",
    "\n",
    "```python\n",
    "# Tecton advantages over Feast:\n",
    "tecton_features = {\n",
    "    \"1. Real-time Stream Features\": \"\"\"\n",
    "        Feast: Batch only (daily materialization)\n",
    "        Tecton: Native Flink/Spark Streaming\n",
    "        \n",
    "        @stream_feature_view(\n",
    "            source=KafkaStreamSource(topic=\"transactions\"),\n",
    "            aggregation_interval=timedelta(minutes=1)\n",
    "        )\n",
    "        def user_transaction_counts(transactions):\n",
    "            return f.count(transactions)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Monitoring Built-in\": \"\"\"\n",
    "        Tecton tracks:\n",
    "        - Data quality (nulls, outliers)\n",
    "        - Data drift (distribution changes)\n",
    "        - Feature freshness (lag time)\n",
    "        - Feature usage (which models use which features)\n",
    "        \n",
    "        Auto-alerts cuando features degradan\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Multi-tenant & Enterprise\": \"\"\"\n",
    "        - RBAC (role-based access control)\n",
    "        - Cost tracking per team\n",
    "        - SLA enforcement\n",
    "        - Audit logs\n",
    "        \n",
    "        Feast: Open-source, self-managed\n",
    "        Tecton: Managed SaaS ($$$)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Transformation Pushdown\": \"\"\"\n",
    "        Tecton optimiza transformaciones:\n",
    "        - Spark transformations compiled\n",
    "        - Predicate pushdown\n",
    "        - Partition pruning\n",
    "        \n",
    "        10x faster que Feast para large scale\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Tecton example (similar API)\n",
    "from tecton import Entity, BatchSource, FeatureView\n",
    "from tecton.types import Field, String, Float64, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "customer = Entity(name=\"customer\", join_keys=[\"customer_id\"])\n",
    "\n",
    "transactions_batch = BatchSource(\n",
    "    name=\"transactions\",\n",
    "    batch_config=SnowflakeBatchConfig(\n",
    "        database=\"ANALYTICS\",\n",
    "        schema=\"SILVER\",\n",
    "        table=\"TRANSACTIONS\",\n",
    "        timestamp_field=\"transaction_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "@batch_feature_view(\n",
    "    sources=[transactions_batch],\n",
    "    entities=[customer],\n",
    "    mode=\"spark_sql\",\n",
    "    online=True,\n",
    "    offline=True,\n",
    "    feature_start_time=datetime(2020, 1, 1),\n",
    "    batch_schedule=timedelta(days=1),\n",
    "    ttl=timedelta(days=30)\n",
    ")\n",
    "def customer_transaction_metrics(transactions):\n",
    "    return f\"\"\"\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            transaction_date as timestamp,\n",
    "            COUNT(*) as txn_count_30d,\n",
    "            SUM(amount) as total_spent_30d,\n",
    "            AVG(amount) as avg_transaction_30d\n",
    "        FROM {transactions}\n",
    "        WHERE transaction_date >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "        GROUP BY customer_id, transaction_date\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Feature Naming Convention\": \"\"\"\n",
    "        Pattern: {entity}_{aggregation}_{window}\n",
    "        \n",
    "        âœ… Good:\n",
    "        - customer_txn_count_30d\n",
    "        - product_view_count_7d\n",
    "        - session_avg_duration_1h\n",
    "        \n",
    "        âŒ Bad:\n",
    "        - feature1\n",
    "        - count\n",
    "        - customer_feature\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Feature Documentation\": \"\"\"\n",
    "        Every feature needs:\n",
    "        - Description: What it measures\n",
    "        - Business logic: How it's computed\n",
    "        - Owner: Who to contact\n",
    "        - Dependencies: Source tables\n",
    "        - SLA: Freshness, availability\n",
    "        \n",
    "        Store in catalog (DataHub, Amundsen)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Feature Versioning\": \"\"\"\n",
    "        Version features like code:\n",
    "        - customer_txn_count_30d_v1\n",
    "        - customer_txn_count_30d_v2 (changed logic)\n",
    "        \n",
    "        Old models use v1, new models use v2\n",
    "        Gradual migration, no breaking changes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Feature Reusability\": \"\"\"\n",
    "        Don't duplicate features:\n",
    "        - Search catalog before creating\n",
    "        - Reuse across models/teams\n",
    "        - Contribute back to central store\n",
    "        \n",
    "        Example: \"customer_txn_count_30d\" usado por:\n",
    "        - Churn model\n",
    "        - Recommendation model  \n",
    "        - Fraud detection model\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Point-in-Time Correctness\": \"\"\"\n",
    "        CRITICAL: No data leakage\n",
    "        \n",
    "        âŒ Wrong:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "        -- Uses ALL data including future!\n",
    "        \n",
    "        âœ… Correct:\n",
    "        SELECT AVG(amount) FROM transactions\n",
    "        WHERE customer_id = 123\n",
    "          AND transaction_date < :prediction_date\n",
    "        -- Only past data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"6. Monitoring & Alerting\": \"\"\"\n",
    "        Track metrics:\n",
    "        - Feature freshness (lag time)\n",
    "        - Null rates (data quality)\n",
    "        - Distribution shifts (drift)\n",
    "        - Query latency (performance)\n",
    "        \n",
    "        Alert if:\n",
    "        - Freshness > 2 hours (SLA violation)\n",
    "        - Null rate > 10% (data issue)\n",
    "        - P95 distribution shift > 2 std devs\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "\n",
    "```python\n",
    "# Online store costs can be HIGH\n",
    "cost_example = {\n",
    "    \"Scenario\": \"1M customers, 20 features, 8 bytes/feature\",\n",
    "    \n",
    "    \"Redis\": {\n",
    "        \"Storage\": \"1M Ã— 20 Ã— 8 bytes = 160 MB\",\n",
    "        \"Cost\": \"$0.023/GB/hour Ã— 0.16 GB Ã— 730 hours = $2.70/month\",\n",
    "        \"Latency\": \"1-5ms\",\n",
    "        \"Best for\": \"High QPS (>1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"DynamoDB\": {\n",
    "        \"Storage\": \"1M Ã— 20 Ã— 8 bytes = 160 MB = $0.04/month\",\n",
    "        \"Reads\": \"100K reads/day Ã— $0.25/million = $0.75/month\",\n",
    "        \"Cost\": \"$0.79/month\",\n",
    "        \"Latency\": \"5-20ms\",\n",
    "        \"Best for\": \"Medium QPS (100-1000 req/s)\"\n",
    "    },\n",
    "    \n",
    "    \"Optimization\": \"\"\"\n",
    "        1. TTL: Expire stale features (save storage)\n",
    "        2. Lazy loading: Only materialize requested features\n",
    "        3. Batch reads: Get features for multiple entities\n",
    "        4. Caching: Application-level cache (reduce lookups)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab5f5a",
   "metadata": {},
   "source": [
    "### ğŸ”§ **MLOps: Versionado, Experimentos y Model Registry**\n",
    "\n",
    "**ML Lifecycle Challenges:**\n",
    "\n",
    "```\n",
    "Traditional Software:\n",
    "Code â†’ Build â†’ Test â†’ Deploy\n",
    "âœ… Deterministic (same input = same output)\n",
    "âœ… Tests catch regressions\n",
    "âœ… Rollback = redeploy old code\n",
    "\n",
    "Machine Learning:\n",
    "Code + Data + Hyperparameters â†’ Train â†’ Validate â†’ Deploy\n",
    "âŒ Non-deterministic (randomness, data drift)\n",
    "âŒ Tests don't catch model degradation\n",
    "âŒ Rollback = redeploy old model + old features + old data pipeline\n",
    "```\n",
    "\n",
    "**MLOps Stack:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  ML LIFECYCLE                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                          â”‚\n",
    "â”‚  1. EXPERIMENTATION                                     â”‚\n",
    "â”‚     â€¢ Jupyter / VS Code                                 â”‚\n",
    "â”‚     â€¢ MLflow Tracking                                   â”‚\n",
    "â”‚     â€¢ Weights & Biases                                  â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  2. VERSIONING                                          â”‚\n",
    "â”‚     â€¢ Data: DVC, Delta Lake versions                    â”‚\n",
    "â”‚     â€¢ Code: Git                                         â”‚\n",
    "â”‚     â€¢ Models: MLflow Model Registry                     â”‚\n",
    "â”‚     â€¢ Features: Feature Store                           â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  3. TRAINING PIPELINE                                   â”‚\n",
    "â”‚     â€¢ Orchestration: Airflow, Kubeflow, Metaflow       â”‚\n",
    "â”‚     â€¢ Compute: Spark, Kubernetes, SageMaker            â”‚\n",
    "â”‚     â€¢ Hyperparameter tuning: Optuna, Ray Tune          â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  4. VALIDATION & TESTING                                â”‚\n",
    "â”‚     â€¢ Model validation: Great Expectations              â”‚\n",
    "â”‚     â€¢ A/B testing framework                             â”‚\n",
    "â”‚     â€¢ Shadow mode deployment                            â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  5. MODEL REGISTRY                                      â”‚\n",
    "â”‚     â€¢ Staging â†’ Production promotion                    â”‚\n",
    "â”‚     â€¢ Metadata: metrics, lineage, dependencies         â”‚\n",
    "â”‚     â€¢ RBAC: who can promote                            â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  6. DEPLOYMENT                                          â”‚\n",
    "â”‚     â€¢ Online: REST API (FastAPI, TorchServe)           â”‚\n",
    "â”‚     â€¢ Batch: Spark jobs                                â”‚\n",
    "â”‚     â€¢ Edge: TensorFlow Lite, ONNX                      â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  7. MONITORING                                          â”‚\n",
    "â”‚     â€¢ Data drift: Evidently, WhyLabs                   â”‚\n",
    "â”‚     â€¢ Model performance: Custom dashboards             â”‚\n",
    "â”‚     â€¢ Infrastructure: Prometheus, Datadog              â”‚\n",
    "â”‚     â†“                                                   â”‚\n",
    "â”‚  8. RETRAINING (back to step 3)                        â”‚\n",
    "â”‚     â€¢ Scheduled: weekly/monthly                        â”‚\n",
    "â”‚     â€¢ Triggered: drift detected                        â”‚\n",
    "â”‚                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**MLflow: Experiment Tracking**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Set tracking server (or use local)\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "mlflow.set_experiment(\"customer_churn_v2\")\n",
    "\n",
    "# Hyperparameter grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "best_auc = 0\n",
    "best_run_id = None\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for depth in param_grid['max_depth']:\n",
    "        for min_split in param_grid['min_samples_split']:\n",
    "            \n",
    "            # Start MLflow run\n",
    "            with mlflow.start_run(run_name=f\"rf_ne{n_est}_md{depth}_ms{min_split}\"):\n",
    "                \n",
    "                # Log parameters\n",
    "                mlflow.log_param(\"n_estimators\", n_est)\n",
    "                mlflow.log_param(\"max_depth\", depth)\n",
    "                mlflow.log_param(\"min_samples_split\", min_split)\n",
    "                mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "                mlflow.log_param(\"feature_version\", \"v2.1\")\n",
    "                mlflow.log_param(\"dataset_version\", \"2025-10-30\")\n",
    "                \n",
    "                # Train model\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_est,\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_split,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "                mlflow.log_metric(\"roc_auc\", auc)\n",
    "                mlflow.log_metric(\"test_size\", len(X_test))\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    \"model\",\n",
    "                    signature=mlflow.models.infer_signature(X_train, y_train)\n",
    "                )\n",
    "                \n",
    "                # Log artifacts\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "                mlflow.log_artifact(\"feature_importance.csv\")\n",
    "                \n",
    "                # Track best model\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    best_run_id = mlflow.active_run().info.run_id\n",
    "                    mlflow.set_tag(\"best_model\", \"true\")\n",
    "                \n",
    "                print(f\"Run: ne={n_est}, md={depth}, ms={min_split} â†’ AUC={auc:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ† Best model: {best_run_id} with AUC={best_auc:.4f}\")\n",
    "```\n",
    "\n",
    "**MLflow Model Registry: Staging â†’ Production**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Register best model\n",
    "model_name = \"customer_churn_model\"\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "model_version = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model registered: {model_name} version {model_version.version}\")\n",
    "\n",
    "# Transition to Staging for validation\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "# Add description and tags\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    description=f\"\"\"\n",
    "    Churn prediction model trained on 2025-10-30\n",
    "    Features: customer_transaction_features v2.1\n",
    "    Performance: AUC=0.89, F1=0.82\n",
    "    Training data: 100K customers, 30 days lookback\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    key=\"validation_status\",\n",
    "    value=\"pending\"\n",
    ")\n",
    "\n",
    "# Validation tests\n",
    "print(\"\\nğŸ§ª Running validation tests...\")\n",
    "\n",
    "# Test 1: Model can load\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/Staging\")\n",
    "print(\"âœ… Model loads successfully\")\n",
    "\n",
    "# Test 2: Predictions on validation set\n",
    "val_predictions = loaded_model.predict(X_val)\n",
    "val_auc = roc_auc_score(y_val, loaded_model.predict_proba(X_val)[:, 1])\n",
    "print(f\"âœ… Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Test 3: No feature drift\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for feature in feature_names:\n",
    "    train_dist = X_train[feature]\n",
    "    val_dist = X_val[feature]\n",
    "    ks_stat, p_value = ks_2samp(train_dist, val_dist)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"âš ï¸ Feature drift detected in {feature} (p={p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"âœ… {feature}: no drift\")\n",
    "\n",
    "# Test 4: Latency test\n",
    "import time\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = loaded_model.predict(X_val[:1])\n",
    "latency = (time.time() - start) / 1000 * 1000  # ms\n",
    "print(f\"âœ… Average latency: {latency:.2f}ms\")\n",
    "\n",
    "# If all tests pass, promote to Production\n",
    "if val_auc > 0.85 and latency < 100:\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive old Production\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        key=\"validation_status\",\n",
    "        value=\"passed\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸš€ Model promoted to Production!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Model failed validation, staying in Staging\")\n",
    "```\n",
    "\n",
    "**Data Versioning with DVC:**\n",
    "\n",
    "```python\n",
    "# dvc.yaml - Define data pipeline\n",
    "\"\"\"\n",
    "stages:\n",
    "  extract:\n",
    "    cmd: python src/extract_data.py\n",
    "    deps:\n",
    "      - src/extract_data.py\n",
    "    outs:\n",
    "      - data/raw/transactions.parquet\n",
    "  \n",
    "  features:\n",
    "    cmd: python src/generate_features.py\n",
    "    deps:\n",
    "      - src/generate_features.py\n",
    "      - data/raw/transactions.parquet\n",
    "    params:\n",
    "      - features.lookback_days\n",
    "      - features.aggregation_windows\n",
    "    outs:\n",
    "      - data/features/customer_features.parquet\n",
    "  \n",
    "  train:\n",
    "    cmd: python src/train_model.py\n",
    "    deps:\n",
    "      - src/train_model.py\n",
    "      - data/features/customer_features.parquet\n",
    "    params:\n",
    "      - model.n_estimators\n",
    "      - model.max_depth\n",
    "    metrics:\n",
    "      - metrics.json:\n",
    "          cache: false\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "\"\"\"\n",
    "\n",
    "# params.yaml - Hyperparameters\n",
    "\"\"\"\n",
    "features:\n",
    "  lookback_days: 30\n",
    "  aggregation_windows: [7, 30, 90]\n",
    "\n",
    "model:\n",
    "  n_estimators: 100\n",
    "  max_depth: 10\n",
    "  min_samples_split: 5\n",
    "\"\"\"\n",
    "\n",
    "# Track data version\n",
    "\"\"\"\n",
    "$ dvc add data/raw/transactions.parquet\n",
    "$ git add data/raw/transactions.parquet.dvc .gitignore\n",
    "$ git commit -m \"Add transactions data v1\"\n",
    "$ git tag -a \"data-v1\" -m \"Initial dataset\"\n",
    "\n",
    "$ dvc push  # Upload to S3/GCS\n",
    "\"\"\"\n",
    "\n",
    "# Reproduce experiment\n",
    "\"\"\"\n",
    "$ git checkout data-v1\n",
    "$ dvc checkout\n",
    "$ dvc repro  # Runs entire pipeline\n",
    "\n",
    "Output:\n",
    "  Running stage 'extract'\n",
    "  Running stage 'features'  \n",
    "  Running stage 'train'\n",
    "  âœ… Reproduced experiment from data-v1\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Continuous Training with Kubeflow:**\n",
    "\n",
    "```python\n",
    "# kubeflow_pipeline.py\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "@func_to_container_op\n",
    "def extract_data(output_path: str):\n",
    "    \"\"\"Extract training data\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    df = spark.read.format(\"delta\").load(\"s3://datalake/silver/transactions\")\n",
    "    df.write.parquet(output_path)\n",
    "\n",
    "@func_to_container_op\n",
    "def train_model(data_path: str, model_path: str) -> float:\n",
    "    \"\"\"Train model and return AUC\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import joblib\n",
    "    \n",
    "    df = pd.read_parquet(data_path)\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    return auc\n",
    "\n",
    "@func_to_container_op\n",
    "def deploy_model(model_path: str, auc: float):\n",
    "    \"\"\"Deploy if AUC > threshold\"\"\"\n",
    "    if auc > 0.85:\n",
    "        # Deploy to production\n",
    "        import mlflow\n",
    "        mlflow.register_model(f\"file://{model_path}\", \"churn_model\")\n",
    "        print(f\"âœ… Model deployed with AUC={auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"âŒ Model not deployed, AUC={auc:.4f} < 0.85\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Customer Churn Training Pipeline',\n",
    "    description='Extract, train, deploy churn model'\n",
    ")\n",
    "def training_pipeline():\n",
    "    # Extract\n",
    "    extract_op = extract_data(output_path=\"/data/training.parquet\")\n",
    "    \n",
    "    # Train\n",
    "    train_op = train_model(\n",
    "        data_path=extract_op.output,\n",
    "        model_path=\"/models/churn_model.pkl\"\n",
    "    )\n",
    "    \n",
    "    # Deploy\n",
    "    deploy_op = deploy_model(\n",
    "        model_path=train_op.outputs['model_path'],\n",
    "        auc=train_op.outputs['Output']\n",
    "    )\n",
    "\n",
    "# Compile and run\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(training_pipeline, 'pipeline.yaml')\n",
    "    \n",
    "    # Submit to Kubeflow\n",
    "    client = kfp.Client(host='http://kubeflow.example.com')\n",
    "    client.create_run_from_pipeline_func(\n",
    "        training_pipeline,\n",
    "        arguments={},\n",
    "        run_name='churn_training_2025_10_30'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Model Comparison Dashboard:**\n",
    "\n",
    "```python\n",
    "# Compare multiple model versions\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get all versions of model\n",
    "versions = client.search_model_versions(f\"name='customer_churn_model'\")\n",
    "\n",
    "comparison_data = []\n",
    "for version in versions:\n",
    "    run = mlflow.get_run(version.run_id)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'version': version.version,\n",
    "        'stage': version.current_stage,\n",
    "        'created': version.creation_timestamp,\n",
    "        'auc': run.data.metrics.get('roc_auc', 0),\n",
    "        'f1': run.data.metrics.get('f1_score', 0),\n",
    "        'accuracy': run.data.metrics.get('accuracy', 0),\n",
    "        'n_estimators': run.data.params.get('n_estimators', 0),\n",
    "        'max_depth': run.data.params.get('max_depth', 0),\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Plot comparison\n",
    "fig = px.line(\n",
    "    df_comparison,\n",
    "    x='version',\n",
    "    y=['auc', 'f1', 'accuracy'],\n",
    "    title='Model Performance Over Versions',\n",
    "    labels={'value': 'Score', 'variable': 'Metric'}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Production model details\n",
    "prod_version = df_comparison[df_comparison['stage'] == 'Production'].iloc[0]\n",
    "print(f\"\\nğŸ“Š Production Model (v{prod_version['version']}):\")\n",
    "print(f\"  AUC: {prod_version['auc']:.4f}\")\n",
    "print(f\"  F1: {prod_version['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {prod_version['accuracy']:.4f}\")\n",
    "print(f\"  Hyperparams: n_estimators={prod_version['n_estimators']}, max_depth={prod_version['max_depth']}\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "mlops_best_practices = {\n",
    "    \"1. Reproducibility\": \"\"\"\n",
    "        Track EVERYTHING:\n",
    "        - Code version (git commit hash)\n",
    "        - Data version (DVC, Delta version)\n",
    "        - Environment (Docker image, requirements.txt)\n",
    "        - Random seeds\n",
    "        - Hyperparameters\n",
    "        \n",
    "        Goal: Reproduce exact model 6 months later\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Automated Testing\": \"\"\"\n",
    "        Unit tests:\n",
    "        - Feature engineering logic\n",
    "        - Model preprocessing\n",
    "        - Prediction postprocessing\n",
    "        \n",
    "        Integration tests:\n",
    "        - End-to-end pipeline\n",
    "        - Model serving API\n",
    "        - Feature store integration\n",
    "        \n",
    "        Model tests:\n",
    "        - Minimum accuracy threshold\n",
    "        - Inference latency < 100ms\n",
    "        - No feature drift\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Gradual Rollout\": \"\"\"\n",
    "        Don't deploy to 100% traffic immediately\n",
    "        \n",
    "        Stage 1: Shadow mode (30 days)\n",
    "        - New model runs parallel to old\n",
    "        - Predictions logged but not used\n",
    "        - Compare performance\n",
    "        \n",
    "        Stage 2: A/B test (7 days)\n",
    "        - 10% traffic to new model\n",
    "        - Monitor metrics closely\n",
    "        - Rollback if issues\n",
    "        \n",
    "        Stage 3: Ramp up (14 days)\n",
    "        - 25% â†’ 50% â†’ 75% â†’ 100%\n",
    "        - Gradual increase\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Model Governance\": \"\"\"\n",
    "        RBAC for model promotion:\n",
    "        - Data Scientists: can register to Staging\n",
    "        - ML Engineers: can promote to Production\n",
    "        - Approvals: require 2+ reviewers\n",
    "        \n",
    "        Audit trail:\n",
    "        - Who deployed what when\n",
    "        - Why model was promoted\n",
    "        - What tests were run\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Rollback Plan\": \"\"\"\n",
    "        Always have rollback ready:\n",
    "        - Keep 2-3 previous versions in Production\n",
    "        - Canary deployment (route % traffic)\n",
    "        - Circuit breaker (auto-rollback if errors spike)\n",
    "        \n",
    "        Example:\n",
    "        v10 deployed â†’ errors 5% â†’ auto-rollback to v9\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564959f0",
   "metadata": {},
   "source": [
    "### ğŸš€ **Production Serving: Online, Batch y Model Monitoring**\n",
    "\n",
    "**Serving Strategies:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            MODEL SERVING PATTERNS                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                            â”‚\n",
    "â”‚  1. ONLINE SERVING (Real-time)                            â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
    "â”‚     â”‚   Client    â”‚                                       â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                       â”‚\n",
    "â”‚            â”‚ HTTP Request                                 â”‚\n",
    "â”‚            â–¼                                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚     â”‚  Load        â”‚        â”‚   Feature    â”‚            â”‚\n",
    "â”‚     â”‚  Balancer    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Store      â”‚            â”‚\n",
    "â”‚     â”‚  (Nginx)     â”‚        â”‚   (Redis)    â”‚            â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚            â”‚                        â”‚                     â”‚\n",
    "â”‚            â–¼                        â”‚                     â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                     â”‚\n",
    "â”‚     â”‚  API Server  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚     â”‚  (FastAPI)   â”‚                                      â”‚\n",
    "â”‚     â”‚              â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ Model in   â”‚                                      â”‚\n",
    "â”‚     â”‚   memory     â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ Features   â”‚                                      â”‚\n",
    "â”‚     â”‚   from Redis â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ <100ms     â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚  2. BATCH SERVING (Bulk predictions)                      â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚     â”‚   Scheduler  â”‚                                      â”‚\n",
    "â”‚     â”‚   (Airflow)  â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚            â”‚ Daily @2AM                                   â”‚\n",
    "â”‚            â–¼                                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚     â”‚  Spark Job   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Feature     â”‚            â”‚\n",
    "â”‚     â”‚              â”‚        â”‚  Store       â”‚            â”‚\n",
    "â”‚     â”‚ â€¢ Load model â”‚        â”‚  (Delta)     â”‚            â”‚\n",
    "â”‚     â”‚ â€¢ Get        â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚     â”‚   features   â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ Predict    â”‚                                      â”‚\n",
    "â”‚     â”‚   millions   â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚            â”‚                                               â”‚\n",
    "â”‚            â–¼                                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚     â”‚  Predictions â”‚                                      â”‚\n",
    "â”‚     â”‚  Delta Table â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚  3. STREAMING SERVING (Micro-batch)                       â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚     â”‚    Kafka     â”‚                                      â”‚\n",
    "â”‚     â”‚   (Events)   â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚            â”‚                                               â”‚\n",
    "â”‚            â–¼                                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚     â”‚ Flink/Spark  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Feature     â”‚            â”‚\n",
    "â”‚     â”‚ Streaming    â”‚        â”‚  Store       â”‚            â”‚\n",
    "â”‚     â”‚              â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚     â”‚ â€¢ Enrich     â”‚                                      â”‚\n",
    "â”‚     â”‚   with       â”‚                                      â”‚\n",
    "â”‚     â”‚   features   â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ Predict    â”‚                                      â”‚\n",
    "â”‚     â”‚ â€¢ <1s        â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚            â”‚                                               â”‚\n",
    "â”‚            â–¼                                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚     â”‚ Output Kafka â”‚                                      â”‚\n",
    "â”‚     â”‚   Topic      â”‚                                      â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚                                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Online Serving: FastAPI Production Setup**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "import mlflow.pyfunc\n",
    "import redis\n",
    "import json\n",
    "from typing import List\n",
    "import time\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Metrics\n",
    "PREDICTION_COUNT = Counter('prediction_requests_total', 'Total prediction requests')\n",
    "PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "PREDICTION_ERRORS = Counter('prediction_errors_total', 'Total prediction errors')\n",
    "\n",
    "# Global model variable\n",
    "model = None\n",
    "redis_client = None\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Load model once at startup (not per request)\n",
    "    \"\"\"\n",
    "    global model, redis_client\n",
    "    \n",
    "    # Load model from MLflow Registry\n",
    "    print(\"Loading model from MLflow...\")\n",
    "    model = mlflow.pyfunc.load_model(\"models:/customer_churn_model/Production\")\n",
    "    print(\"âœ… Model loaded\")\n",
    "    \n",
    "    # Connect to Redis\n",
    "    redis_client = redis.Redis(\n",
    "        host='redis',\n",
    "        port=6379,\n",
    "        decode_responses=True,\n",
    "        max_connections=50\n",
    "    )\n",
    "    print(\"âœ… Redis connected\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup on shutdown\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Churn Prediction API\",\n",
    "    description=\"Real-time customer churn prediction\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_ids: List[str] = Field(..., max_items=100)  # Batch up to 100\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    customer_id: str\n",
    "    churn_probability: float\n",
    "    churn_risk: str\n",
    "    features: dict\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "    latency_ms: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Batch prediction endpoint\n",
    "    Latency: <100ms for 100 customers\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        PREDICTION_COUNT.inc(len(request.customer_ids))\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get features from Redis (batch)\n",
    "        pipeline = redis_client.pipeline()\n",
    "        for customer_id in request.customer_ids:\n",
    "            pipeline.get(f\"features:customer:{customer_id}\")\n",
    "        \n",
    "        features_raw = pipeline.execute()\n",
    "        \n",
    "        # Prepare batch input\n",
    "        feature_vectors = []\n",
    "        valid_customers = []\n",
    "        \n",
    "        for customer_id, features_str in zip(request.customer_ids, features_raw):\n",
    "            if not features_str:\n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=0.5,  # Default\n",
    "                    churn_risk=\"unknown\",\n",
    "                    features={}\n",
    "                ))\n",
    "                continue\n",
    "            \n",
    "            features = json.loads(features_str)\n",
    "            feature_vectors.append([\n",
    "                features[\"txn_count_30d\"],\n",
    "                features[\"total_spent_30d\"],\n",
    "                features[\"avg_transaction_30d\"],\n",
    "                features[\"days_since_last_txn\"]\n",
    "            ])\n",
    "            valid_customers.append((customer_id, features))\n",
    "        \n",
    "        # Batch prediction (efficient!)\n",
    "        if feature_vectors:\n",
    "            churn_probas = model.predict(feature_vectors)\n",
    "            \n",
    "            for (customer_id, features), churn_proba in zip(valid_customers, churn_probas):\n",
    "                risk = \"low\" if churn_proba < 0.3 else \"medium\" if churn_proba < 0.7 else \"high\"\n",
    "                \n",
    "                predictions.append(Prediction(\n",
    "                    customer_id=customer_id,\n",
    "                    churn_probability=float(churn_proba),\n",
    "                    churn_risk=risk,\n",
    "                    features=features\n",
    "                ))\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        PREDICTION_LATENCY.observe(latency_ms / 1000)\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            predictions=predictions,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        PREDICTION_ERRORS.inc()\n",
    "        raise HTTPException(500, f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check\"\"\"\n",
    "    try:\n",
    "        redis_client.ping()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"model_version\": model.metadata.get_model_info().version,\n",
    "            \"redis\": \"connected\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Run with: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4\n",
    "```\n",
    "\n",
    "**Kubernetes Deployment:**\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: churn-prediction-api\n",
    "spec:\n",
    "  replicas: 4  # Horizontal scaling\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: churn-prediction\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: churn-prediction\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: api\n",
    "        image: myregistry/churn-prediction:v1.2.3\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "        env:\n",
    "        - name: MLFLOW_TRACKING_URI\n",
    "          value: \"http://mlflow:5000\"\n",
    "        - name: REDIS_HOST\n",
    "          value: \"redis\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: churn-prediction-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: churn-prediction\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: churn-prediction-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: churn-prediction-api\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "```\n",
    "\n",
    "**Batch Serving with Spark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct, col, current_date\n",
    "import mlflow.pyfunc\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPredictionBatch\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load model as Spark UDF\n",
    "model_uri = \"models:/customer_churn_model/Production\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "# Get all active customers (10M customers)\n",
    "active_customers = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/active_customers\") \\\n",
    "    .filter(col(\"is_active\") == True)\n",
    "\n",
    "print(f\"Scoring {active_customers.count():,} customers...\")\n",
    "\n",
    "# Get features\n",
    "customer_features = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/customer_features_daily\") \\\n",
    "    .filter(col(\"feature_date\") == current_date())\n",
    "\n",
    "# Join\n",
    "customers_with_features = active_customers \\\n",
    "    .join(customer_features, \"customer_id\", \"inner\")\n",
    "\n",
    "# Batch predict (distributed across cluster)\n",
    "predictions = customers_with_features.withColumn(\n",
    "    \"churn_probability\",\n",
    "    predict_udf(struct(\n",
    "        col(\"txn_count_30d\"),\n",
    "        col(\"total_spent_30d\"),\n",
    "        col(\"avg_transaction_30d\"),\n",
    "        col(\"days_since_last_txn\")\n",
    "    ))\n",
    ").withColumn(\n",
    "    \"churn_risk\",\n",
    "    when(col(\"churn_probability\") < 0.3, \"low\")\n",
    "    .when(col(\"churn_probability\") < 0.7, \"medium\")\n",
    "    .otherwise(\"high\")\n",
    ").withColumn(\n",
    "    \"prediction_date\",\n",
    "    current_date()\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "predictions.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"churn_risk\",\n",
    "    \"prediction_date\"\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"prediction_date\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"s3://datalake/gold/churn_predictions\")\n",
    "\n",
    "# High-risk customers â†’ CRM\n",
    "high_risk = predictions.filter(col(\"churn_risk\") == \"high\")\n",
    "\n",
    "high_risk.select(\n",
    "    \"customer_id\",\n",
    "    \"churn_probability\",\n",
    "    \"total_spent_30d\",\n",
    "    \"days_since_last_txn\"\n",
    ").write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://crm:5432/marketing\") \\\n",
    "    .option(\"dbtable\", \"high_churn_risk_customers\") \\\n",
    "    .option(\"user\", \"crm_user\") \\\n",
    "    .option(\"password\", \"***\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"âœ… Scored {predictions.count():,} customers\")\n",
    "print(f\"âš ï¸ High risk: {high_risk.count():,} customers\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Model Monitoring: Data Drift Detection**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "import mlflow\n",
    "\n",
    "# Reference data (training data distribution)\n",
    "reference_df = pd.read_parquet(\"s3://datalake/training/reference_data.parquet\")\n",
    "\n",
    "# Current data (production predictions last 7 days)\n",
    "current_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/prediction_inputs\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 7\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# Evidently report\n",
    "report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "    DataQualityPreset()\n",
    "])\n",
    "\n",
    "report.run(\n",
    "    reference_data=reference_df,\n",
    "    current_data=current_df,\n",
    "    column_mapping=None\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report.save_html(\"drift_report.html\")\n",
    "\n",
    "# Extract drift metrics\n",
    "drift_metrics = report.as_dict()\n",
    "\n",
    "# Check for drift\n",
    "features_with_drift = []\n",
    "for feature, metrics in drift_metrics['metrics'][0]['result']['drift_by_columns'].items():\n",
    "    if metrics['drift_detected']:\n",
    "        features_with_drift.append({\n",
    "            'feature': feature,\n",
    "            'drift_score': metrics['drift_score'],\n",
    "            'p_value': metrics.get('stattest_threshold', 0)\n",
    "        })\n",
    "\n",
    "if features_with_drift:\n",
    "    print(\"âš ï¸ DRIFT DETECTED in features:\")\n",
    "    for feature_drift in features_with_drift:\n",
    "        print(f\"  â€¢ {feature_drift['feature']}: score={feature_drift['drift_score']:.4f}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=\"drift_detection\"):\n",
    "        mlflow.log_metric(\"features_with_drift\", len(features_with_drift))\n",
    "        mlflow.log_artifact(\"drift_report.html\")\n",
    "    \n",
    "    # Send alert\n",
    "    send_alert(\n",
    "        title=\"âš ï¸ Data Drift Detected\",\n",
    "        message=f\"{len(features_with_drift)} features drifted\",\n",
    "        severity=\"warning\"\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ… No drift detected\")\n",
    "```\n",
    "\n",
    "**Model Performance Monitoring:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get predictions and actuals\n",
    "predictions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_predictions\") \\\n",
    "    .filter(\"prediction_date >= current_date() - 30\")\n",
    "\n",
    "# Join with actuals (churned or not in next 7 days)\n",
    "actuals = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"s3://datalake/gold/churn_labels\") \\\n",
    "    .filter(\"label_date >= current_date() - 30\")\n",
    "\n",
    "performance = predictions \\\n",
    "    .join(actuals, [\"customer_id\", \"prediction_date\"]) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Calculate metrics over time\n",
    "daily_metrics = []\n",
    "\n",
    "for date in pd.date_range(\n",
    "    start=datetime.now() - timedelta(days=30),\n",
    "    end=datetime.now(),\n",
    "    freq='D'\n",
    "):\n",
    "    day_data = performance[performance['prediction_date'] == date.date()]\n",
    "    \n",
    "    if len(day_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "    \n",
    "    auc = roc_auc_score(day_data['churned'], day_data['churn_probability'])\n",
    "    \n",
    "    # Classify at 0.7 threshold\n",
    "    predictions_binary = (day_data['churn_probability'] > 0.7).astype(int)\n",
    "    precision = precision_score(day_data['churned'], predictions_binary)\n",
    "    recall = recall_score(day_data['churned'], predictions_binary)\n",
    "    \n",
    "    daily_metrics.append({\n",
    "        'date': date,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'num_predictions': len(day_data)\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(daily_metrics)\n",
    "\n",
    "# Plot performance over time\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['auc'], name='AUC'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['precision'], name='Precision'))\n",
    "fig.add_trace(go.Scatter(x=df_metrics['date'], y=df_metrics['recall'], name='Recall'))\n",
    "\n",
    "# Add threshold line\n",
    "fig.add_hline(y=0.80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Minimum AUC\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Over Last 30 Days',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Score',\n",
    "    yaxis_range=[0, 1]\n",
    ")\n",
    "\n",
    "fig.write_html(\"performance_dashboard.html\")\n",
    "\n",
    "# Check if below threshold\n",
    "if df_metrics['auc'].iloc[-1] < 0.80:\n",
    "    print(\"âš ï¸ MODEL PERFORMANCE DEGRADED\")\n",
    "    print(f\"  Current AUC: {df_metrics['auc'].iloc[-1]:.4f}\")\n",
    "    print(f\"  Threshold: 0.80\")\n",
    "    \n",
    "    # Trigger retraining\n",
    "    from airflow.api.client.local_client import Client\n",
    "    client = Client(None, None)\n",
    "    client.trigger_dag(\n",
    "        dag_id='ml_training_pipeline',\n",
    "        conf={'reason': 'performance_degradation'}\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Retraining triggered\")\n",
    "else:\n",
    "    print(\"âœ… Model performance healthy\")\n",
    "```\n",
    "\n",
    "**A/B Testing Framework:**\n",
    "\n",
    "```python\n",
    "# Route traffic between model versions\n",
    "from fastapi import FastAPI, Request\n",
    "import random\n",
    "import mlflow\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load multiple model versions\n",
    "model_v1 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/1\")\n",
    "model_v2 = mlflow.pyfunc.load_model(\"models:/customer_churn_model/2\")\n",
    "\n",
    "# A/B test config\n",
    "AB_TEST_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"model_v1_traffic\": 0.9,  # 90% traffic\n",
    "    \"model_v2_traffic\": 0.1   # 10% traffic (new model)\n",
    "}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: Request, customer_id: str):\n",
    "    # Assign to variant\n",
    "    if AB_TEST_CONFIG[\"enabled\"]:\n",
    "        rand = random.random()\n",
    "        if rand < AB_TEST_CONFIG[\"model_v2_traffic\"]:\n",
    "            variant = \"v2\"\n",
    "            model = model_v2\n",
    "        else:\n",
    "            variant = \"v1\"\n",
    "            model = model_v1\n",
    "    else:\n",
    "        variant = \"v1\"\n",
    "        model = model_v1\n",
    "    \n",
    "    # Get features and predict\n",
    "    features = get_features(customer_id)\n",
    "    prediction = model.predict([features])[0]\n",
    "    \n",
    "    # Log for analysis\n",
    "    log_ab_test(\n",
    "        customer_id=customer_id,\n",
    "        variant=variant,\n",
    "        prediction=prediction,\n",
    "        timestamp=datetime.now()\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"churn_probability\": float(prediction),\n",
    "        \"model_variant\": variant  # For debugging\n",
    "    }\n",
    "\n",
    "# Analyze A/B test results\n",
    "def analyze_ab_test():\n",
    "    \"\"\"\n",
    "    Compare model v1 vs v2 performance\n",
    "    \"\"\"\n",
    "    results = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"s3://datalake/gold/ab_test_logs\") \\\n",
    "        .filter(\"timestamp >= current_timestamp() - interval 7 days\")\n",
    "    \n",
    "    # Group by variant\n",
    "    v1_results = results.filter(\"variant == 'v1'\")\n",
    "    v2_results = results.filter(\"variant == 'v2'\")\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    # Compare AUC\n",
    "    v1_auc = calculate_auc(v1_results)\n",
    "    v2_auc = calculate_auc(v2_results)\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(v1_results['auc'], v2_results['auc'])\n",
    "    \n",
    "    print(f\"Model v1 AUC: {v1_auc:.4f}\")\n",
    "    print(f\"Model v2 AUC: {v2_auc:.4f}\")\n",
    "    print(f\"Improvement: {(v2_auc - v1_auc) / v1_auc * 100:.2f}%\")\n",
    "    print(f\"Statistical significance: p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05 and v2_auc > v1_auc:\n",
    "        print(\"âœ… Model v2 is significantly better!\")\n",
    "        print(\"Recommendation: Ramp up v2 to 100% traffic\")\n",
    "    else:\n",
    "        print(\"âŒ No significant improvement, keep v1\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b644b1",
   "metadata": {},
   "source": [
    "## 1. Pipeline de datos para ML: ETL â†’ Feature Engineering â†’ Training â†’ Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02075bc",
   "metadata": {},
   "source": [
    "- Extract: fuentes raw (transacciones, logs, eventos).\n",
    "- Transform: limpieza, agregaciones por ventana, joins.\n",
    "- Feature Engineering: crear features (ratios, lags, embeddings).\n",
    "- Training: dataset versionado, experimentos (MLflow/Weights&Biases).\n",
    "- Serving: online (API real-time) y offline (batch predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed38e",
   "metadata": {},
   "source": [
    "## 2. Feature Store: concepto y prÃ¡ctica con Feast (demo local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aca25c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '# feature_repo/feature_definitions.py', 'from feast import Entity, FeatureView, Field, FileSource', 'from feast.types import Float32, Int64', 'from datetime import timedelta', '', \"cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\", '', 'source = FileSource(', \"    path='data/features.parquet',\", \"    timestamp_field='event_timestamp'\", ')', '', 'cliente_fv = FeatureView(', \"    name='cliente_features',\", '    entities=[cliente],', '    ttl=timedelta(days=30),', '    schema=[', \"        Field(name='total_compras', dtype=Float32),\", \"        Field(name='num_transacciones', dtype=Int64),\", '    ],', '    source=source', ')', '# feast apply', '# feast materialize-incremental']\n"
     ]
    }
   ],
   "source": [
    "# Nota: instala feast si quieres ejecutar (no en requirements por defecto)\n",
    "feast_demo = r'''\n",
    "# feature_repo/feature_definitions.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "source = FileSource(\n",
    "    path='data/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "cliente_fv = FeatureView(\n",
    "    name='cliente_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras', dtype=Float32),\n",
    "        Field(name='num_transacciones', dtype=Int64),\n",
    "    ],\n",
    "    source=source\n",
    ")\n",
    "# feast apply\n",
    "# feast materialize-incremental\n",
    "# store.get_online_features(...)\n",
    "'''\n",
    "print(feast_demo.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61f6f9",
   "metadata": {},
   "source": [
    "## 3. Versionado de datasets y experimentos con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f58e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencias bÃ¡sicas ya instaladas\n"
     ]
    }
   ],
   "source": [
    "# InstalaciÃ³n de dependencias\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    import numpy\n",
    "    import sklearn\n",
    "    print(\"âœ… Dependencias bÃ¡sicas ya instaladas\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Instalando dependencias...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\", \"scikit-learn\", \"--quiet\"])\n",
    "    print(\"âœ… Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f5ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LibrerÃ­as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Imports necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e38910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos generados:\n",
      "   - 10,000 clientes\n",
      "   - 100,000 transacciones\n",
      "   - Target: 6.9% clientes activos\n",
      "\n",
      "ğŸ“Š Muestra de clientes:\n",
      "   cliente_id  edad  antiguedad_meses     ciudad  segmento  activo_30d\n",
      "0           1    56                75       CDMX   Premium           0\n",
      "1           2    69                86  Monterrey   Premium           0\n",
      "2           3    46               118     BogotÃ¡  EstÃ¡ndar           0\n",
      "3           4    32                24  Monterrey    BÃ¡sico           0\n",
      "4           5    60                65  Monterrey   Premium           0\n"
     ]
    }
   ],
   "source": [
    "# 1. GeneraciÃ³n de datos sintÃ©ticos de clientes y transacciones\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar 10,000 clientes\n",
    "n_clientes = 10000\n",
    "clientes = pd.DataFrame({\n",
    "    'cliente_id': range(1, n_clientes + 1),\n",
    "    'edad': np.random.randint(18, 70, n_clientes),\n",
    "    'antiguedad_meses': np.random.randint(1, 120, n_clientes),\n",
    "    'ciudad': np.random.choice(['CDMX', 'Guadalajara', 'Monterrey', 'BogotÃ¡', 'Lima'], n_clientes),\n",
    "    'segmento': np.random.choice(['Premium', 'EstÃ¡ndar', 'BÃ¡sico'], n_clientes, p=[0.2, 0.5, 0.3])\n",
    "})\n",
    "\n",
    "# Generar 100,000 transacciones\n",
    "n_transacciones = 100000\n",
    "transacciones = pd.DataFrame({\n",
    "    'cliente_id': np.random.randint(1, n_clientes + 1, n_transacciones),\n",
    "    'fecha': pd.date_range(end=datetime.now(), periods=n_transacciones, freq='H'),\n",
    "    'monto': np.random.exponential(500, n_transacciones),\n",
    "    'categoria': np.random.choice(['ElectrÃ³nica', 'Ropa', 'Alimentos', 'Hogar'], n_transacciones),\n",
    "    'canal': np.random.choice(['Web', 'App', 'Tienda'], n_transacciones)\n",
    "})\n",
    "\n",
    "# Target: cliente activo en Ãºltimos 30 dÃ­as (churn prediction)\n",
    "transacciones_recientes = transacciones[transacciones['fecha'] >= (datetime.now() - timedelta(days=30))]\n",
    "clientes_activos = transacciones_recientes['cliente_id'].unique()\n",
    "clientes['activo_30d'] = clientes['cliente_id'].isin(clientes_activos).astype(int)\n",
    "\n",
    "print(f\"âœ… Datos generados:\")\n",
    "print(f\"   - {len(clientes):,} clientes\")\n",
    "print(f\"   - {len(transacciones):,} transacciones\")\n",
    "print(f\"   - Target: {clientes['activo_30d'].mean():.1%} clientes activos\")\n",
    "print(f\"\\nğŸ“Š Muestra de clientes:\")\n",
    "print(clientes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83cf5452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creando features agregadas...\n",
      "âœ… Features creadas: 24 columnas\n",
      "\n",
      "ğŸ“Š Lista de features:\n",
      "['cliente_id', 'edad', 'antiguedad_meses', 'ciudad', 'segmento', 'activo_30d', 'total_gastado', 'ticket_promedio', 'std_gasto', 'num_transacciones', 'dias_ultima_compra', 'gasto_7d', 'trans_7d', 'gasto_30d', 'trans_30d', 'gasto_90d', 'trans_90d', 'compras_app', 'compras_tienda', 'compras_web', 'cat_alimentos', 'cat_electrÃ³nica', 'cat_hogar', 'cat_ropa']\n",
      "\n",
      "ğŸ“ˆ EstadÃ­sticas:\n",
      "        cliente_id          edad  antiguedad_meses    activo_30d  \\\n",
      "count  10000.00000  10000.000000      10000.000000  10000.000000   \n",
      "mean    5000.50000     43.539400         59.988900      0.068800   \n",
      "std     2886.89568     14.911636         34.178002      0.253126   \n",
      "min        1.00000     18.000000          1.000000      0.000000   \n",
      "25%     2500.75000     31.000000         30.000000      0.000000   \n",
      "50%     5000.50000     43.000000         60.000000      0.000000   \n",
      "75%     7500.25000     56.000000         89.000000      0.000000   \n",
      "max    10000.00000     69.000000        119.000000      1.000000   \n",
      "\n",
      "       total_gastado  ticket_promedio     std_gasto  num_transacciones  \n",
      "count   10000.000000     10000.000000  10000.000000       10000.000000  \n",
      "mean     5013.120155       501.163748    459.167407          10.000000  \n",
      "std      2234.757362       167.905914    197.738562           3.132759  \n",
      "min        70.711138        35.355569      0.000000           1.000000  \n",
      "25%      3400.427543       383.397030    320.205053           8.000000  \n",
      "50%      4735.787554       483.207333    428.312145          10.000000  \n",
      "75%      6392.612696       598.618164    560.203773          12.000000  \n",
      "max     16104.252154      1694.339606   1913.838024          22.000000  \n",
      "âœ… Features creadas: 24 columnas\n",
      "\n",
      "ğŸ“Š Lista de features:\n",
      "['cliente_id', 'edad', 'antiguedad_meses', 'ciudad', 'segmento', 'activo_30d', 'total_gastado', 'ticket_promedio', 'std_gasto', 'num_transacciones', 'dias_ultima_compra', 'gasto_7d', 'trans_7d', 'gasto_30d', 'trans_30d', 'gasto_90d', 'trans_90d', 'compras_app', 'compras_tienda', 'compras_web', 'cat_alimentos', 'cat_electrÃ³nica', 'cat_hogar', 'cat_ropa']\n",
      "\n",
      "ğŸ“ˆ EstadÃ­sticas:\n",
      "        cliente_id          edad  antiguedad_meses    activo_30d  \\\n",
      "count  10000.00000  10000.000000      10000.000000  10000.000000   \n",
      "mean    5000.50000     43.539400         59.988900      0.068800   \n",
      "std     2886.89568     14.911636         34.178002      0.253126   \n",
      "min        1.00000     18.000000          1.000000      0.000000   \n",
      "25%     2500.75000     31.000000         30.000000      0.000000   \n",
      "50%     5000.50000     43.000000         60.000000      0.000000   \n",
      "75%     7500.25000     56.000000         89.000000      0.000000   \n",
      "max    10000.00000     69.000000        119.000000      1.000000   \n",
      "\n",
      "       total_gastado  ticket_promedio     std_gasto  num_transacciones  \n",
      "count   10000.000000     10000.000000  10000.000000       10000.000000  \n",
      "mean     5013.120155       501.163748    459.167407          10.000000  \n",
      "std      2234.757362       167.905914    197.738562           3.132759  \n",
      "min        70.711138        35.355569      0.000000           1.000000  \n",
      "25%      3400.427543       383.397030    320.205053           8.000000  \n",
      "50%      4735.787554       483.207333    428.312145          10.000000  \n",
      "75%      6392.612696       598.618164    560.203773          12.000000  \n",
      "max     16104.252154      1694.339606   1913.838024          22.000000  \n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Engineering: Crear features agregadas por cliente\n",
    "print(\"ğŸ”§ Creando features agregadas...\")\n",
    "\n",
    "# Features de transacciones\n",
    "features_trans = transacciones.groupby('cliente_id').agg({\n",
    "    'monto': ['sum', 'mean', 'std', 'count'],\n",
    "    'fecha': lambda x: (datetime.now() - x.max()).days  # dÃ­as desde Ãºltima compra\n",
    "}).reset_index()\n",
    "\n",
    "features_trans.columns = ['cliente_id', 'total_gastado', 'ticket_promedio', \n",
    "                          'std_gasto', 'num_transacciones', 'dias_ultima_compra']\n",
    "\n",
    "# Features de comportamiento por canal\n",
    "canal_features = transacciones.groupby(['cliente_id', 'canal']).size().unstack(fill_value=0)\n",
    "canal_features.columns = [f'compras_{col.lower()}' for col in canal_features.columns]\n",
    "canal_features = canal_features.reset_index()\n",
    "\n",
    "# Features de categorÃ­a\n",
    "categoria_features = transacciones.groupby(['cliente_id', 'categoria']).size().unstack(fill_value=0)\n",
    "categoria_features.columns = [f'cat_{col.lower().replace(\" \", \"_\")}' for col in categoria_features.columns]\n",
    "categoria_features = categoria_features.reset_index()\n",
    "\n",
    "# Features de tendencia temporal (Ãºltimos 7, 30, 90 dÃ­as)\n",
    "for dias in [7, 30, 90]:\n",
    "    fecha_corte = datetime.now() - timedelta(days=dias)\n",
    "    trans_periodo = transacciones[transacciones['fecha'] >= fecha_corte]\n",
    "    features_periodo = trans_periodo.groupby('cliente_id').agg({\n",
    "        'monto': 'sum',\n",
    "        'fecha': 'count'\n",
    "    }).reset_index()\n",
    "    features_periodo.columns = ['cliente_id', f'gasto_{dias}d', f'trans_{dias}d']\n",
    "    features_trans = features_trans.merge(features_periodo, on='cliente_id', how='left')\n",
    "\n",
    "# Unir todas las features\n",
    "features = clientes.merge(features_trans, on='cliente_id', how='left')\n",
    "features = features.merge(canal_features, on='cliente_id', how='left')\n",
    "features = features.merge(categoria_features, on='cliente_id', how='left')\n",
    "\n",
    "# Rellenar NaN con 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "print(f\"âœ… Features creadas: {len(features.columns)} columnas\")\n",
    "print(f\"\\nğŸ“Š Lista de features:\")\n",
    "print(features.columns.tolist())\n",
    "print(f\"\\nğŸ“ˆ EstadÃ­sticas:\")\n",
    "print(features.describe().iloc[:, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3543d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Preparando datos para entrenamiento...\n",
      "âœ… Datos preparados:\n",
      "   - Training set: 8,000 samples\n",
      "   - Test set: 2,000 samples\n",
      "   - Features: 20 columnas\n",
      "   - Balance target (train): 6.9% activos\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparar datos para entrenamiento\n",
    "print(\"ğŸ¯ Preparando datos para entrenamiento...\")\n",
    "\n",
    "# Separar features y target\n",
    "X = features.drop(['cliente_id', 'activo_30d', 'ciudad', 'segmento'], axis=1)\n",
    "y = features['activo_30d']\n",
    "\n",
    "# One-hot encoding para variables categÃ³ricas\n",
    "X = pd.get_dummies(X, columns=[], drop_first=True)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"âœ… Datos preparados:\")\n",
    "print(f\"   - Training set: {len(X_train):,} samples\")\n",
    "print(f\"   - Test set: {len(X_test):,} samples\")\n",
    "print(f\"   - Features: {X_train.shape[1]} columnas\")\n",
    "print(f\"   - Balance target (train): {y_train.mean():.1%} activos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf8b2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Entrenando modelo...\n",
      "âœ… Modelo entrenado:\n",
      "   - Accuracy (train): 1.0000\n",
      "   - Accuracy (test): 1.0000\n",
      "   - ROC-AUC: 1.0000\n",
      "\n",
      "ğŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Inactivo       1.00      1.00      1.00      1862\n",
      "      Activo       1.00      1.00      1.00       138\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "âœ… Modelo entrenado:\n",
      "   - Accuracy (train): 1.0000\n",
      "   - Accuracy (test): 1.0000\n",
      "   - ROC-AUC: 1.0000\n",
      "\n",
      "ğŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Inactivo       1.00      1.00      1.00      1862\n",
      "      Activo       1.00      1.00      1.00       138\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Entrenar modelo (Random Forest)\n",
    "print(\"ğŸ¤– Entrenando modelo...\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# MÃ©tricas\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "print(f\"âœ… Modelo entrenado:\")\n",
    "print(f\"   - Accuracy (train): {train_score:.4f}\")\n",
    "print(f\"   - Accuracy (test): {test_score:.4f}\")\n",
    "print(f\"   - ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"\\nğŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Inactivo', 'Activo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3780876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analizando importancia de features...\n",
      "\n",
      "ğŸ” Top 15 Features mÃ¡s importantes:\n",
      "           feature  importance\n",
      "dias_ultima_compra    0.324442\n",
      "         gasto_30d    0.279566\n",
      "         trans_30d    0.253452\n",
      "         gasto_90d    0.058778\n",
      "         trans_90d    0.035037\n",
      "          gasto_7d    0.023832\n",
      "          trans_7d    0.023089\n",
      "     total_gastado    0.000294\n",
      " num_transacciones    0.000284\n",
      "       compras_web    0.000217\n",
      "         std_gasto    0.000192\n",
      "     cat_alimentos    0.000190\n",
      "   cat_electrÃ³nica    0.000167\n",
      "              edad    0.000150\n",
      "   ticket_promedio    0.000150\n",
      "\n",
      "ğŸ“ˆ Importancia por categorÃ­a:\n",
      "   - Features de gasto: 0.363\n",
      "   - Features temporales: 0.998\n",
      "   - Features de canal: 0.000\n",
      "   - Features de categorÃ­a: 0.000\n"
     ]
    }
   ],
   "source": [
    "# 5. Feature Importance Analysis\n",
    "print(\"ğŸ“Š Analizando importancia de features...\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ” Top 15 Features mÃ¡s importantes:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Analizar features por categorÃ­a\n",
    "print(\"\\nğŸ“ˆ Importancia por categorÃ­a:\")\n",
    "print(f\"   - Features de gasto: {feature_importance[feature_importance['feature'].str.contains('gasto|total|ticket')]['importance'].sum():.3f}\")\n",
    "print(f\"   - Features temporales: {feature_importance[feature_importance['feature'].str.contains('_\\\\d+d|dias|ultima')]['importance'].sum():.3f}\")\n",
    "print(f\"   - Features de canal: {feature_importance[feature_importance['feature'].str.contains('compras_')]['importance'].sum():.3f}\")\n",
    "print(f\"   - Features de categorÃ­a: {feature_importance[feature_importance['feature'].str.contains('cat_')]['importance'].sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4817f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Simulando Feature Store (almacenamiento offline)...\n",
      "âœ… Feature Store Offline guardado:\n",
      "   - Archivo: feature_store/cliente_features_v1.parquet\n",
      "   - Registros: 10,000\n",
      "   - Features: 26 columnas\n",
      "\n",
      "ğŸš€ Feature Store Online (muestra):\n",
      "   - 100 clientes cargados en memoria\n",
      "   - Latencia tÃ­pica: <10ms\n",
      "   cliente_id  total_gastado  num_transacciones  dias_ultima_compra  gasto_30d\n",
      "0           1    5583.103975                 15                  33        0.0\n",
      "1           2    4634.874212                 11                 450        0.0\n",
      "2           3    4977.199431                  5                1965        0.0\n",
      "3           4    4487.541634                 13                 528        0.0\n",
      "4           5    5596.542061                  9                 310        0.0\n"
     ]
    }
   ],
   "source": [
    "# 6. SimulaciÃ³n de Feature Store: Guardar features offline\n",
    "print(\"ğŸ’¾ Simulando Feature Store (almacenamiento offline)...\")\n",
    "\n",
    "# En producciÃ³n esto irÃ­a a Delta Lake / Parquet con versionado\n",
    "feature_store_offline = features.copy()\n",
    "feature_store_offline['feature_timestamp'] = datetime.now()\n",
    "feature_store_offline['feature_version'] = 'v1.0'\n",
    "\n",
    "# Guardar en formato parquet (simulaciÃ³n de offline store)\n",
    "import os\n",
    "os.makedirs('../datasets/processed/feature_store', exist_ok=True)\n",
    "feature_store_offline.to_parquet(\n",
    "    '../datasets/processed/feature_store/cliente_features_v1.parquet',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Feature Store Offline guardado:\")\n",
    "print(f\"   - Archivo: feature_store/cliente_features_v1.parquet\")\n",
    "print(f\"   - Registros: {len(feature_store_offline):,}\")\n",
    "print(f\"   - Features: {len(feature_store_offline.columns)} columnas\")\n",
    "\n",
    "# SimulaciÃ³n de Online Store (en producciÃ³n serÃ­a Redis/DynamoDB)\n",
    "online_store_sample = feature_store_offline[['cliente_id', 'total_gastado', 'num_transacciones', \n",
    "                                              'dias_ultima_compra', 'gasto_30d']].head(100)\n",
    "print(f\"\\nğŸš€ Feature Store Online (muestra):\")\n",
    "print(f\"   - {len(online_store_sample)} clientes cargados en memoria\")\n",
    "print(f\"   - Latencia tÃ­pica: <10ms\")\n",
    "print(online_store_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d988cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Simulando inferencia en producciÃ³n...\n",
      "\n",
      "ğŸ“Š Predicciones para clientes de prueba:\n",
      "   Cliente 1: Inactivo (prob=0.00%, conf=100.00%)\n",
      "   Cliente 100: Inactivo (prob=0.00%, conf=100.00%)\n",
      "   Cliente 500: Inactivo (prob=0.00%, conf=100.00%)\n",
      "   Cliente 1000: Inactivo (prob=0.00%, conf=100.00%)\n",
      "   Cliente 5000: Inactivo (prob=0.00%, conf=100.00%)\n",
      "\n",
      "ğŸ“¦ Simulando batch scoring (todos los clientes)...\n",
      "   Cliente 1000: Inactivo (prob=0.00%, conf=100.00%)\n",
      "   Cliente 5000: Inactivo (prob=0.00%, conf=100.00%)\n",
      "\n",
      "ğŸ“¦ Simulando batch scoring (todos los clientes)...\n",
      "âœ… Batch scoring completado: 1,000 predicciones\n",
      "   - Clientes predichos activos: 64 (6.4%)\n",
      "   - Probabilidad promedio: 6.41%\n",
      "âœ… Batch scoring completado: 1,000 predicciones\n",
      "   - Clientes predichos activos: 64 (6.4%)\n",
      "   - Probabilidad promedio: 6.41%\n"
     ]
    }
   ],
   "source": [
    "# 7. SimulaciÃ³n de Inferencia en ProducciÃ³n\n",
    "print(\"ğŸ¯ Simulando inferencia en producciÃ³n...\")\n",
    "\n",
    "def predict_cliente_activo(cliente_id, model, feature_store):\n",
    "    \"\"\"\n",
    "    Simula predicciÃ³n online: obtiene features del store y hace inferencia\n",
    "    \"\"\"\n",
    "    # Obtener features del cliente\n",
    "    cliente_features = feature_store[feature_store['cliente_id'] == cliente_id]\n",
    "    \n",
    "    if len(cliente_features) == 0:\n",
    "        return {'error': 'Cliente no encontrado'}\n",
    "    \n",
    "    # Preparar features para el modelo\n",
    "    X_pred = cliente_features.drop(['cliente_id', 'activo_30d', 'ciudad', 'segmento', \n",
    "                                     'feature_timestamp', 'feature_version'], axis=1, errors='ignore')\n",
    "    \n",
    "    # PredicciÃ³n\n",
    "    pred_proba = model.predict_proba(X_pred)[0][1]\n",
    "    pred_class = model.predict(X_pred)[0]\n",
    "    \n",
    "    return {\n",
    "        'cliente_id': int(cliente_id),\n",
    "        'probabilidad_activo': round(float(pred_proba), 4),\n",
    "        'prediccion': 'Activo' if pred_class == 1 else 'Inactivo',\n",
    "        'confianza': round(float(max(model.predict_proba(X_pred)[0])), 4)\n",
    "    }\n",
    "\n",
    "# Probar con algunos clientes\n",
    "clientes_test = [1, 100, 500, 1000, 5000]\n",
    "print(\"\\nğŸ“Š Predicciones para clientes de prueba:\")\n",
    "for cid in clientes_test:\n",
    "    result = predict_cliente_activo(cid, model, feature_store_offline)\n",
    "    print(f\"   Cliente {cid}: {result['prediccion']} (prob={result['probabilidad_activo']:.2%}, conf={result['confianza']:.2%})\")\n",
    "\n",
    "# Simular batch scoring\n",
    "print(\"\\nğŸ“¦ Simulando batch scoring (todos los clientes)...\")\n",
    "all_predictions = []\n",
    "for cid in feature_store_offline['cliente_id'].head(1000):  # Primeros 1000\n",
    "    pred = predict_cliente_activo(cid, model, feature_store_offline)\n",
    "    all_predictions.append(pred)\n",
    "\n",
    "predictions_df = pd.DataFrame(all_predictions)\n",
    "print(f\"âœ… Batch scoring completado: {len(predictions_df):,} predicciones\")\n",
    "print(f\"   - Clientes predichos activos: {(predictions_df['prediccion'] == 'Activo').sum():,} ({(predictions_df['prediccion'] == 'Activo').mean():.1%})\")\n",
    "print(f\"   - Probabilidad promedio: {predictions_df['probabilidad_activo'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756a0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Simulando detecciÃ³n de Data Drift...\n",
      "\n",
      "ğŸ” Resultados de Data Drift Detection:\n",
      "          feature  ks_statistic  p_value  drift_detected\n",
      "             edad        0.1912      0.0            True\n",
      " antiguedad_meses        0.5059      0.0            True\n",
      "    total_gastado        0.8736      0.0            True\n",
      "num_transacciones        0.2009      0.0            True\n",
      "\n",
      "âš ï¸ ALERTA: Drift detectado en 4 features:\n",
      "   - edad: KS=0.1912, p-value=0.0\n",
      "   - antiguedad_meses: KS=0.5059, p-value=0.0\n",
      "   - total_gastado: KS=0.8736, p-value=0.0\n",
      "   - num_transacciones: KS=0.2009, p-value=0.0\n",
      "\n",
      "ğŸ’¡ AcciÃ³n recomendada: Reentrenar modelo con datos recientes\n",
      "\n",
      "ğŸ” Resultados de Data Drift Detection:\n",
      "          feature  ks_statistic  p_value  drift_detected\n",
      "             edad        0.1912      0.0            True\n",
      " antiguedad_meses        0.5059      0.0            True\n",
      "    total_gastado        0.8736      0.0            True\n",
      "num_transacciones        0.2009      0.0            True\n",
      "\n",
      "âš ï¸ ALERTA: Drift detectado en 4 features:\n",
      "   - edad: KS=0.1912, p-value=0.0\n",
      "   - antiguedad_meses: KS=0.5059, p-value=0.0\n",
      "   - total_gastado: KS=0.8736, p-value=0.0\n",
      "   - num_transacciones: KS=0.2009, p-value=0.0\n",
      "\n",
      "ğŸ’¡ AcciÃ³n recomendada: Reentrenar modelo con datos recientes\n"
     ]
    }
   ],
   "source": [
    "# 8. SimulaciÃ³n de Monitoreo: Data Drift Detection\n",
    "print(\"ğŸ“Š Simulando detecciÃ³n de Data Drift...\")\n",
    "\n",
    "# Simular nuevos datos con drift (distribuciÃ³n cambiada)\n",
    "np.random.seed(100)\n",
    "new_clientes = pd.DataFrame({\n",
    "    'cliente_id': range(n_clientes + 1, n_clientes + 1001),\n",
    "    'edad': np.random.randint(25, 60, 1000),  # DistribuciÃ³n diferente\n",
    "    'antiguedad_meses': np.random.randint(1, 60, 1000),  # MÃ¡s nuevos\n",
    "    'total_gastado': np.random.exponential(800, 1000),  # Mayor gasto promedio\n",
    "    'num_transacciones': np.random.randint(1, 20, 1000)\n",
    "})\n",
    "\n",
    "# Comparar distribuciones (KS test simplificado)\n",
    "from scipy import stats\n",
    "\n",
    "def detect_drift(train_data, new_data, feature, threshold=0.1):\n",
    "    \"\"\"Detecta drift usando Kolmogorov-Smirnov test\"\"\"\n",
    "    statistic, pvalue = stats.ks_2samp(train_data[feature], new_data[feature])\n",
    "    has_drift = pvalue < threshold\n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'ks_statistic': round(statistic, 4),\n",
    "        'p_value': round(pvalue, 4),\n",
    "        'drift_detected': has_drift\n",
    "    }\n",
    "\n",
    "# Analizar drift en features clave\n",
    "drift_results = []\n",
    "common_features = ['edad', 'antiguedad_meses', 'total_gastado', 'num_transacciones']\n",
    "\n",
    "for feat in common_features:\n",
    "    if feat in features.columns and feat in new_clientes.columns:\n",
    "        result = detect_drift(features, new_clientes, feat)\n",
    "        drift_results.append(result)\n",
    "\n",
    "drift_df = pd.DataFrame(drift_results)\n",
    "print(\"\\nğŸ” Resultados de Data Drift Detection:\")\n",
    "print(drift_df.to_string(index=False))\n",
    "\n",
    "drifted_features = drift_df[drift_df['drift_detected'] == True]\n",
    "if len(drifted_features) > 0:\n",
    "    print(f\"\\nâš ï¸ ALERTA: Drift detectado en {len(drifted_features)} features:\")\n",
    "    for _, row in drifted_features.iterrows():\n",
    "        print(f\"   - {row['feature']}: KS={row['ks_statistic']}, p-value={row['p_value']}\")\n",
    "    print(\"\\nğŸ’¡ AcciÃ³n recomendada: Reentrenar modelo con datos recientes\")\n",
    "else:\n",
    "    print(\"\\nâœ… No se detectÃ³ drift significativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455ce75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š RESUMEN DEL ML PIPELINE\n",
      "============================================================\n",
      "\n",
      "âœ… COMPONENTES IMPLEMENTADOS:\n",
      "   1. âœ“ GeneraciÃ³n de datos sintÃ©ticos (clientes + transacciones)\n",
      "   2. âœ“ Feature Engineering (20 features agregadas)\n",
      "   3. âœ“ Entrenamiento de modelo (Random Forest)\n",
      "   4. âœ“ EvaluaciÃ³n y mÃ©tricas (ROC-AUC: 1.00)\n",
      "   5. âœ“ Feature Store Offline (Parquet)\n",
      "   6. âœ“ Feature Store Online (simulaciÃ³n en memoria)\n",
      "   7. âœ“ Inferencia en producciÃ³n (online + batch)\n",
      "   8. âœ“ Monitoreo de Data Drift (KS test)\n",
      "\n",
      "ğŸ“ˆ MÃ‰TRICAS DEL MODELO:\n",
      "   - Accuracy: 100.0%\n",
      "   - ROC-AUC: 1.0000\n",
      "   - Features mÃ¡s importantes:\n",
      "     â€¢ dias_ultima_compra: 32.4%\n",
      "     â€¢ gasto_30d: 28.0%\n",
      "     â€¢ trans_30d: 25.3%\n",
      "\n",
      "ğŸ’¾ FEATURE STORE:\n",
      "   - Clientes: 10,000\n",
      "   - Features: 20 columnas\n",
      "   - VersiÃ³n: v1.0\n",
      "   - Formato: Parquet\n",
      "\n",
      "ğŸš€ INFERENCIA:\n",
      "   - Predicciones batch: 1,000 clientes\n",
      "   - Tasa de activaciÃ³n predicha: 6.4%\n",
      "   - Latencia tÃ­pica: <10ms (online)\n",
      "\n",
      "âš ï¸ DRIFT DETECTION:\n",
      "   - Features con drift: 4 de 4 analizadas\n",
      "   - RecomendaciÃ³n: Reentrenar modelo\n",
      "\n",
      "ğŸ“ CONCEPTOS CLAVE DEMOSTRADOS:\n",
      "   â€¢ Feature Engineering automatizado\n",
      "   â€¢ Feature Store (offline + online)\n",
      "   â€¢ Versionado de features\n",
      "   â€¢ Pipeline de inferencia\n",
      "   â€¢ Monitoreo de drift\n",
      "   â€¢ SimulaciÃ³n de MLOps workflow\n",
      "\n",
      "============================================================\n",
      "âœ… Pipeline completo ejecutado exitosamente\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 9. Resumen del Pipeline Completo\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š RESUMEN DEL ML PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nâœ… COMPONENTES IMPLEMENTADOS:\")\n",
    "print(\"   1. âœ“ GeneraciÃ³n de datos sintÃ©ticos (clientes + transacciones)\")\n",
    "print(\"   2. âœ“ Feature Engineering (20 features agregadas)\")\n",
    "print(\"   3. âœ“ Entrenamiento de modelo (Random Forest)\")\n",
    "print(\"   4. âœ“ EvaluaciÃ³n y mÃ©tricas (ROC-AUC: 1.00)\")\n",
    "print(\"   5. âœ“ Feature Store Offline (Parquet)\")\n",
    "print(\"   6. âœ“ Feature Store Online (simulaciÃ³n en memoria)\")\n",
    "print(\"   7. âœ“ Inferencia en producciÃ³n (online + batch)\")\n",
    "print(\"   8. âœ“ Monitoreo de Data Drift (KS test)\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ MÃ‰TRICAS DEL MODELO:\")\n",
    "print(f\"   - Accuracy: 100.0%\")\n",
    "print(f\"   - ROC-AUC: 1.0000\")\n",
    "print(f\"   - Features mÃ¡s importantes:\")\n",
    "print(f\"     â€¢ dias_ultima_compra: 32.4%\")\n",
    "print(f\"     â€¢ gasto_30d: 28.0%\")\n",
    "print(f\"     â€¢ trans_30d: 25.3%\")\n",
    "\n",
    "print(\"\\nğŸ’¾ FEATURE STORE:\")\n",
    "print(f\"   - Clientes: 10,000\")\n",
    "print(f\"   - Features: 20 columnas\")\n",
    "print(f\"   - VersiÃ³n: v1.0\")\n",
    "print(f\"   - Formato: Parquet\")\n",
    "\n",
    "print(\"\\nğŸš€ INFERENCIA:\")\n",
    "print(f\"   - Predicciones batch: 1,000 clientes\")\n",
    "print(f\"   - Tasa de activaciÃ³n predicha: 6.4%\")\n",
    "print(f\"   - Latencia tÃ­pica: <10ms (online)\")\n",
    "\n",
    "print(\"\\nâš ï¸ DRIFT DETECTION:\")\n",
    "print(f\"   - Features con drift: 4 de 4 analizadas\")\n",
    "print(f\"   - RecomendaciÃ³n: Reentrenar modelo\")\n",
    "\n",
    "print(\"\\nğŸ“ CONCEPTOS CLAVE DEMOSTRADOS:\")\n",
    "print(\"   â€¢ Feature Engineering automatizado\")\n",
    "print(\"   â€¢ Feature Store (offline + online)\")\n",
    "print(\"   â€¢ Versionado de features\")\n",
    "print(\"   â€¢ Pipeline de inferencia\")\n",
    "print(\"   â€¢ Monitoreo de drift\")\n",
    "print(\"   â€¢ SimulaciÃ³n de MLOps workflow\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Pipeline completo ejecutado exitosamente\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf33f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import mlflow\n",
      "mlflow.set_tracking_uri('http://localhost:5000')\n",
      "mlflow.set_experiment('ventas_prediccion')\n",
      "\n",
      "with mlflow.start_run():\n",
      "    mlflow.log_param('model', 'xgboost')\n",
      "    mlflow.log_param('n_estimators', 100)\n",
      "    mlflow.log_metric('rmse', 0.85)\n",
      "    mlflow.log_artifact('model.pkl')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow_snippet = r'''\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('ventas_prediccion')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('model', 'xgboost')\n",
    "    mlflow.log_param('n_estimators', 100)\n",
    "    mlflow.log_metric('rmse', 0.85)\n",
    "    mlflow.log_artifact('model.pkl')\n",
    "'''\n",
    "print(mlflow_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e01bb",
   "metadata": {},
   "source": [
    "## 4. Reentrenamiento automatizado (Airflow + MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7556724",
   "metadata": {},
   "source": [
    "- DAG semanal: extraer nuevos datos, generar features, entrenar, evaluar.\n",
    "- Si mÃ©trica mejora sobre baseline â†’ registrar modelo, promover a producciÃ³n.\n",
    "- Si drift detectado â†’ alerta y reentrenamiento fuera de calendario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d1c42",
   "metadata": {},
   "source": [
    "## 5. Serving online y batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85229297",
   "metadata": {},
   "source": [
    "- Online: endpoint REST (FastAPI + modelo en memoria o via MLflow Model Registry).\n",
    "- Batch: Spark job semanal para scoring de todo el catÃ¡logo/clientes.\n",
    "- CachÃ© de features online (Redis) para latencia < 50ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59fa8b",
   "metadata": {},
   "source": [
    "## 6. Monitoreo de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa955c",
   "metadata": {},
   "source": [
    "- Data drift: distribuciÃ³n de features cambia (KS test, PSI).\n",
    "- Concept drift: relaciÃ³n Xâ†’Y cambia (performance degrada).\n",
    "- MÃ©tricas de negocio: precisiÃ³n, recall, ROC-AUC, ingresos.\n",
    "- Alertas automÃ¡ticas y rollback si threshold cruzado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "\n",
    "**Siguiente â†’:** [ğŸ’° Cost Optimization y FinOps en la Nube â†’](06_cost_optimization_finops.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Senior:**\n",
    "- [ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [ğŸ—ï¸ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y prÃ¡ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [ğŸ“Š Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
