{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a08253",
   "metadata": {},
   "source": [
    "# 🔐 Seguridad, Compliance y Auditoría de Datos\n",
    "\n",
    "Objetivo: implementar controles de seguridad (IAM, cifrado, enmascaramiento), cumplir regulaciones (GDPR, HIPAA, SOC2) y establecer auditoría de accesos y linaje.\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Governance (Senior 01), experiencia con cloud IAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9003a8",
   "metadata": {},
   "source": [
    "### 🔒 **Defense in Depth: Multi-Layer Security Architecture**\n",
    "\n",
    "**Modelo de Seguridad en Capas para Data Platforms**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Layer 1: NETWORK SECURITY                              │\n",
    "│  • VPC isolation, private subnets                       │\n",
    "│  • Security Groups (stateful firewall)                  │\n",
    "│  • NACLs (Network ACLs - stateless)                     │\n",
    "│  • VPN/PrivateLink para conectividad                    │\n",
    "│  • DDoS protection (AWS Shield, CloudFlare)             │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  Layer 2: IDENTITY & ACCESS MANAGEMENT (IAM)            │\n",
    "│  • Least privilege principle                            │\n",
    "│  • Role-Based Access Control (RBAC)                     │\n",
    "│  • Multi-Factor Authentication (MFA)                    │\n",
    "│  • Service accounts con permisos mínimos                │\n",
    "│  • Temporary credentials (STS AssumeRole)               │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  Layer 3: DATA ENCRYPTION                               │\n",
    "│  • At-rest: KMS, customer-managed keys                  │\n",
    "│  • In-transit: TLS 1.3, mTLS                            │\n",
    "│  • Application-level: Field-level encryption            │\n",
    "│  • Key rotation (automática, 90 días)                   │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  Layer 4: APPLICATION SECURITY                          │\n",
    "│  • Input validation (SQL injection, XSS)                │\n",
    "│  • API authentication (JWT, OAuth2)                     │\n",
    "│  • Rate limiting, DDoS mitigation                       │\n",
    "│  • Security headers (HSTS, CSP)                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  Layer 5: DATA GOVERNANCE                               │\n",
    "│  • PII masking/tokenization                             │\n",
    "│  • Data classification (public, internal, confidential) │\n",
    "│  • Access logs y auditoría                              │\n",
    "│  • Data lineage tracking                                │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  Layer 6: MONITORING & INCIDENT RESPONSE                │\n",
    "│  • SIEM (Splunk, Datadog Security)                      │\n",
    "│  • Anomaly detection (ML-based)                         │\n",
    "│  • Security alerts (Slack, PagerDuty)                   │\n",
    "│  • Incident response playbooks                          │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**AWS IAM: Least Privilege Implementation**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# 1. ROLE-BASED ACCESS CONTROL (RBAC)\n",
    "\n",
    "# Data Engineer (Read raw, Write curated)\n",
    "data_engineer_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"ReadRawData\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::data-lake/raw/*\",\n",
    "                \"arn:aws:s3:::data-lake/raw\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"WriteCuratedData\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/curated/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"s3:x-amz-server-side-encryption\": \"aws:kms\"  # Enforce encryption\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"GlueJobExecution\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:glue:us-east-1:123456789012:job/etl-*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"DenyProductionDelete\",\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Action\": \"s3:DeleteObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/prod/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Data Scientist (Read-only curated + analytics)\n",
    "data_scientist_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::data-lake/curated/*\",\n",
    "                \"arn:aws:s3:::data-lake/analytics/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"athena:StartQueryExecution\",\n",
    "                \"athena:GetQueryExecution\",\n",
    "                \"athena:GetQueryResults\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"athena:WorkGroup\": \"data-science-workgroup\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. SERVICE ACCOUNT (EMR Cluster)\n",
    "emr_service_role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/processing/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kms:Decrypt\",\n",
    "                \"kms:Encrypt\",\n",
    "                \"kms:GenerateDataKey\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:kms:us-east-1:123456789012:key/data-lake-key\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create IAM role with policy\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "def create_data_role(role_name, policy_document, description):\n",
    "    \"\"\"Create IAM role with inline policy\"\"\"\n",
    "    \n",
    "    # Trust policy (who can assume this role)\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"glue.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create role\n",
    "    role = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=description,\n",
    "        MaxSessionDuration=3600,  # 1 hour\n",
    "        Tags=[\n",
    "            {'Key': 'Team', 'Value': 'DataEngineering'},\n",
    "            {'Key': 'Environment', 'Value': 'Production'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Attach inline policy\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=f'{role_name}-policy',\n",
    "        PolicyDocument=json.dumps(policy_document)\n",
    "    )\n",
    "    \n",
    "    return role['Role']['Arn']\n",
    "\n",
    "# 3. TEMPORARY CREDENTIALS (STS AssumeRole)\n",
    "sts = boto3.client('sts')\n",
    "\n",
    "def assume_data_engineer_role(role_arn, session_name):\n",
    "    \"\"\"Get temporary credentials\"\"\"\n",
    "    response = sts.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name,\n",
    "        DurationSeconds=3600,  # 1 hour\n",
    "        Tags=[\n",
    "            {'Key': 'User', 'Value': session_name}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Use temporary credentials\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "    \n",
    "    return s3\n",
    "\n",
    "# 4. MFA ENFORCEMENT\n",
    "mfa_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowListActions\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:ListUsers\",\n",
    "                \"iam:ListVirtualMFADevices\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"AllowIndividualUserToManageTheirOwnMFA\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:EnableMFADevice\",\n",
    "                \"iam:CreateVirtualMFADevice\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"DenyAllExceptListedIfNoMFA\",\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"NotAction\": [\n",
    "                \"iam:CreateVirtualMFADevice\",\n",
    "                \"iam:EnableMFADevice\",\n",
    "                \"iam:ListMFADevices\",\n",
    "                \"iam:ListUsers\",\n",
    "                \"iam:GetUser\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"BoolIfExists\": {\n",
    "                    \"aws:MultiFactorAuthPresent\": \"false\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Secrets Management: Rotación Automática**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "secretsmanager = boto3.client('secretsmanager')\n",
    "\n",
    "# 1. Create secret with automatic rotation\n",
    "def create_rotating_secret(secret_name, secret_value):\n",
    "    \"\"\"Create secret con rotación automática cada 30 días\"\"\"\n",
    "    \n",
    "    response = secretsmanager.create_secret(\n",
    "        Name=secret_name,\n",
    "        Description='Database password with auto-rotation',\n",
    "        SecretString=secret_value,\n",
    "        Tags=[\n",
    "            {'Key': 'Environment', 'Value': 'Production'},\n",
    "            {'Key': 'AutoRotate', 'Value': 'true'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Enable automatic rotation (Lambda-based)\n",
    "    secretsmanager.rotate_secret(\n",
    "        SecretId=secret_name,\n",
    "        RotationLambdaARN='arn:aws:lambda:us-east-1:123456789012:function:rotate-db-password',\n",
    "        RotationRules={\n",
    "            'AutomaticallyAfterDays': 30,  # Rotate every 30 days\n",
    "            'Duration': '2h',\n",
    "            'ScheduleExpression': 'rate(30 days)'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response['ARN']\n",
    "\n",
    "# 2. Retrieve secret in application\n",
    "def get_database_credentials(secret_name):\n",
    "    \"\"\"Get current secret value\"\"\"\n",
    "    response = secretsmanager.get_secret_value(SecretId=secret_name)\n",
    "    \n",
    "    import json\n",
    "    secret = json.loads(response['SecretString'])\n",
    "    \n",
    "    return {\n",
    "        'username': secret['username'],\n",
    "        'password': secret['password'],\n",
    "        'host': secret['host'],\n",
    "        'port': secret['port']\n",
    "    }\n",
    "\n",
    "# 3. Lambda function para rotación\n",
    "\"\"\"\n",
    "import boto3\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    '''Rotate database password'''\n",
    "    \n",
    "    service_client = boto3.client('secretsmanager')\n",
    "    arn = event['SecretId']\n",
    "    token = event['ClientRequestToken']\n",
    "    step = event['Step']\n",
    "    \n",
    "    if step == 'createSecret':\n",
    "        # Generate new password\n",
    "        new_password = generate_random_password()\n",
    "        \n",
    "        # Store pending secret\n",
    "        service_client.put_secret_value(\n",
    "            SecretId=arn,\n",
    "            ClientRequestToken=token,\n",
    "            SecretString=json.dumps({'password': new_password}),\n",
    "            VersionStages=['AWSPENDING']\n",
    "        )\n",
    "    \n",
    "    elif step == 'setSecret':\n",
    "        # Update database with new password\n",
    "        pending_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSPENDING'\n",
    "        )\n",
    "        \n",
    "        current_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSCURRENT'\n",
    "        )\n",
    "        \n",
    "        # Connect with current credentials\n",
    "        conn = psycopg2.connect(**json.loads(current_secret['SecretString']))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Update password\n",
    "        new_password = json.loads(pending_secret['SecretString'])['password']\n",
    "        cursor.execute(f\"ALTER USER myuser WITH PASSWORD '{new_password}'\")\n",
    "        conn.commit()\n",
    "    \n",
    "    elif step == 'testSecret':\n",
    "        # Verify new credentials work\n",
    "        pending_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSPENDING'\n",
    "        )\n",
    "        \n",
    "        # Test connection\n",
    "        conn = psycopg2.connect(**json.loads(pending_secret['SecretString']))\n",
    "        conn.close()\n",
    "    \n",
    "    elif step == 'finishSecret':\n",
    "        # Move AWSPENDING to AWSCURRENT\n",
    "        service_client.update_secret_version_stage(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSCURRENT',\n",
    "            MoveToVersionId=token\n",
    "        )\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Capital One Breach (2019)**\n",
    "\n",
    "**Vulnerabilidad**: Firewall mal configurado permitió acceso desde Internet a metadata service (169.254.169.254) que expuso IAM credentials.\n",
    "\n",
    "**Impacto**: 100M registros de clientes comprometidos, multa de $80M.\n",
    "\n",
    "**Lecciones**:\n",
    "1. ✅ **Network Segmentation**: Servidores en subnets privadas\n",
    "2. ✅ **IMDSv2**: Require token para metadata (AWS)\n",
    "3. ✅ **Security Groups**: Deny 169.254.169.254 desde apps\n",
    "4. ✅ **WAF**: Web Application Firewall con reglas OWASP\n",
    "5. ✅ **Least Privilege**: Limitar scope de IAM roles\n",
    "\n",
    "```python\n",
    "# IMDSv2 enforcement (require token)\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "ec2.modify_instance_metadata_options(\n",
    "    InstanceId='i-1234567890abcdef0',\n",
    "    HttpTokens='required',  # Require IMDSv2\n",
    "    HttpPutResponseHopLimit=1  # Prevent IP forwarding\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a6831",
   "metadata": {},
   "source": [
    "### 🔐 **Encryption: At-Rest, In-Transit y Application-Level**\n",
    "\n",
    "**Encryption Hierarchy**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│  1. AT-REST ENCRYPTION                               │\n",
    "│     • Storage: S3 (SSE-KMS), EBS, RDS                │\n",
    "│     • Databases: TDE (Transparent Data Encryption)   │\n",
    "│     • Backups: Encrypted snapshots                   │\n",
    "│     • Key Management: KMS, CloudHSM                  │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  2. IN-TRANSIT ENCRYPTION                            │\n",
    "│     • TLS 1.3 (HTTPS, gRPC)                          │\n",
    "│     • mTLS (mutual TLS) para service-to-service      │\n",
    "│     • VPN/PrivateLink para inter-VPC                 │\n",
    "│     • IPSec para site-to-site                        │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  3. APPLICATION-LEVEL ENCRYPTION                     │\n",
    "│     • Field-level: Encrypt specific columns (PII)    │\n",
    "│     • Envelope encryption (DEK + KEK)                │\n",
    "│     • Client-side encryption antes de upload         │\n",
    "│     • Tokenization (irreversible)                    │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**AWS KMS: Customer-Managed Keys**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "kms = boto3.client('kms')\n",
    "\n",
    "# 1. CREATE CUSTOMER-MANAGED KEY (CMK)\n",
    "def create_data_encryption_key():\n",
    "    \"\"\"Create KMS key con rotación automática\"\"\"\n",
    "    \n",
    "    response = kms.create_key(\n",
    "        Description='Data Lake encryption key',\n",
    "        KeyUsage='ENCRYPT_DECRYPT',\n",
    "        Origin='AWS_KMS',\n",
    "        MultiRegion=False,\n",
    "        Tags=[\n",
    "            {'TagKey': 'Environment', 'TagValue': 'Production'},\n",
    "            {'TagKey': 'Purpose', 'TagValue': 'DataLake'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    key_id = response['KeyMetadata']['KeyId']\n",
    "    \n",
    "    # Create alias\n",
    "    kms.create_alias(\n",
    "        AliasName='alias/data-lake-key',\n",
    "        TargetKeyId=key_id\n",
    "    )\n",
    "    \n",
    "    # Enable automatic key rotation (365 días)\n",
    "    kms.enable_key_rotation(KeyId=key_id)\n",
    "    \n",
    "    # Set key policy\n",
    "    key_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"Enable IAM User Permissions\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": \"arn:aws:iam::123456789012:root\"\n",
    "                },\n",
    "                \"Action\": \"kms:*\",\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"Allow use of the key for encryption\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": [\n",
    "                        \"arn:aws:iam::123456789012:role/DataEngineerRole\",\n",
    "                        \"arn:aws:iam::123456789012:role/EMRServiceRole\"\n",
    "                    ]\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"kms:Encrypt\",\n",
    "                    \"kms:Decrypt\",\n",
    "                    \"kms:GenerateDataKey\",\n",
    "                    \"kms:DescribeKey\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"Allow CloudWatch Logs\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"logs.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"kms:Encrypt\",\n",
    "                    \"kms:Decrypt\",\n",
    "                    \"kms:GenerateDataKey\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    kms.put_key_policy(\n",
    "        KeyId=key_id,\n",
    "        PolicyName='default',\n",
    "        Policy=json.dumps(key_policy)\n",
    "    )\n",
    "    \n",
    "    return key_id\n",
    "\n",
    "# 2. S3 ENCRYPTION (SSE-KMS)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_encrypted_file(bucket, key, data, kms_key_id):\n",
    "    \"\"\"Upload file con KMS encryption\"\"\"\n",
    "    \n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=data,\n",
    "        ServerSideEncryption='aws:kms',\n",
    "        SSEKMSKeyId=kms_key_id,\n",
    "        BucketKeyEnabled=True  # Reduce KMS API calls (cost)\n",
    "    )\n",
    "\n",
    "# Enforce encryption on bucket\n",
    "def enforce_bucket_encryption(bucket_name, kms_key_id):\n",
    "    \"\"\"Bucket policy: deny unencrypted uploads\"\"\"\n",
    "    \n",
    "    bucket_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"DenyUnencryptedObjectUploads\",\n",
    "                \"Effect\": \"Deny\",\n",
    "                \"Principal\": \"*\",\n",
    "                \"Action\": \"s3:PutObject\",\n",
    "                \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringNotEquals\": {\n",
    "                        \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"DenyIncorrectKMSKey\",\n",
    "                \"Effect\": \"Deny\",\n",
    "                \"Principal\": \"*\",\n",
    "                \"Action\": \"s3:PutObject\",\n",
    "                \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringNotEquals\": {\n",
    "                        \"s3:x-amz-server-side-encryption-aws-kms-key-id\": kms_key_id\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    s3.put_bucket_policy(\n",
    "        Bucket=bucket_name,\n",
    "        Policy=json.dumps(bucket_policy)\n",
    "    )\n",
    "\n",
    "# 3. RDS ENCRYPTION\n",
    "rds = boto3.client('rds')\n",
    "\n",
    "def create_encrypted_rds_instance():\n",
    "    \"\"\"Create RDS con encryption at-rest\"\"\"\n",
    "    \n",
    "    response = rds.create_db_instance(\n",
    "        DBInstanceIdentifier='data-warehouse',\n",
    "        DBInstanceClass='db.r5.2xlarge',\n",
    "        Engine='postgres',\n",
    "        MasterUsername='admin',\n",
    "        MasterUserPassword='SecurePassword123!',\n",
    "        AllocatedStorage=1000,\n",
    "        StorageType='gp3',\n",
    "        StorageEncrypted=True,  # Enable encryption\n",
    "        KmsKeyId='arn:aws:kms:us-east-1:123456789012:key/data-lake-key',\n",
    "        BackupRetentionPeriod=30,\n",
    "        CopyTagsToSnapshot=True,\n",
    "        EnableCloudwatchLogsExports=['postgresql'],\n",
    "        Tags=[\n",
    "            {'Key': 'Encrypted', 'Value': 'true'}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# 4. ENVELOPE ENCRYPTION (Data Encryption Key)\n",
    "def encrypt_large_file_with_envelope(file_path, kms_key_id):\n",
    "    \"\"\"Encrypt file usando envelope encryption pattern\"\"\"\n",
    "    \n",
    "    # Generate Data Encryption Key (DEK)\n",
    "    dek_response = kms.generate_data_key(\n",
    "        KeyId=kms_key_id,\n",
    "        KeySpec='AES_256'\n",
    "    )\n",
    "    \n",
    "    plaintext_dek = dek_response['Plaintext']  # Use this to encrypt\n",
    "    encrypted_dek = dek_response['CiphertextBlob']  # Store this\n",
    "    \n",
    "    # Encrypt file with DEK\n",
    "    cipher = Fernet(base64.urlsafe_b64encode(plaintext_dek[:32]))\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        plaintext_data = f.read()\n",
    "    \n",
    "    encrypted_data = cipher.encrypt(plaintext_data)\n",
    "    \n",
    "    # Upload encrypted file + encrypted DEK\n",
    "    s3.put_object(\n",
    "        Bucket='data-lake',\n",
    "        Key=f'encrypted/{file_path}',\n",
    "        Body=encrypted_data,\n",
    "        Metadata={\n",
    "            'x-amz-key': base64.b64encode(encrypted_dek).decode('utf-8')\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return encrypted_data\n",
    "\n",
    "def decrypt_envelope_encrypted_file(s3_key):\n",
    "    \"\"\"Decrypt file encrypted con envelope pattern\"\"\"\n",
    "    \n",
    "    # Get encrypted file + metadata\n",
    "    response = s3.get_object(Bucket='data-lake', Key=s3_key)\n",
    "    encrypted_data = response['Body'].read()\n",
    "    encrypted_dek = base64.b64decode(response['Metadata']['x-amz-key'])\n",
    "    \n",
    "    # Decrypt DEK usando KMS\n",
    "    dek_response = kms.decrypt(CiphertextBlob=encrypted_dek)\n",
    "    plaintext_dek = dek_response['Plaintext']\n",
    "    \n",
    "    # Decrypt file data usando DEK\n",
    "    cipher = Fernet(base64.urlsafe_b64encode(plaintext_dek[:32]))\n",
    "    plaintext_data = cipher.decrypt(encrypted_data)\n",
    "    \n",
    "    return plaintext_data\n",
    "```\n",
    "\n",
    "**TLS/mTLS Configuration**\n",
    "\n",
    "```python\n",
    "# FastAPI con TLS (HTTPS)\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Run with TLS\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8443,\n",
    "        ssl_keyfile=\"/path/to/private.key\",\n",
    "        ssl_certfile=\"/path/to/certificate.crt\",\n",
    "        ssl_ca_certs=\"/path/to/ca-bundle.crt\",  # For mTLS\n",
    "        ssl_cert_reqs=2  # CERT_REQUIRED (enforce client certs)\n",
    "    )\n",
    "\n",
    "# Nginx reverse proxy con TLS termination\n",
    "\"\"\"\n",
    "server {\n",
    "    listen 443 ssl http2;\n",
    "    server_name api.example.com;\n",
    "    \n",
    "    # TLS 1.3 only\n",
    "    ssl_protocols TLSv1.3;\n",
    "    ssl_certificate /etc/nginx/ssl/certificate.crt;\n",
    "    ssl_certificate_key /etc/nginx/ssl/private.key;\n",
    "    \n",
    "    # Modern cipher suite\n",
    "    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';\n",
    "    ssl_prefer_server_ciphers on;\n",
    "    \n",
    "    # HSTS (force HTTPS)\n",
    "    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n",
    "    \n",
    "    # OCSP Stapling\n",
    "    ssl_stapling on;\n",
    "    ssl_stapling_verify on;\n",
    "    \n",
    "    location / {\n",
    "        proxy_pass http://backend:8000;\n",
    "        proxy_set_header X-Forwarded-Proto https;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Python requests con client certificate (mTLS)\n",
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    'https://api.example.com/data',\n",
    "    cert=('/path/to/client.crt', '/path/to/client.key'),\n",
    "    verify='/path/to/ca-bundle.crt'  # Verify server cert\n",
    ")\n",
    "```\n",
    "\n",
    "**Field-Level Encryption (PII Protection)**\n",
    "\n",
    "```python\n",
    "from cryptography.fernet import Fernet\n",
    "import hashlib\n",
    "import hmac\n",
    "\n",
    "class PIIEncryptor:\n",
    "    \"\"\"Encrypt/decrypt PII fields\"\"\"\n",
    "    \n",
    "    def __init__(self, encryption_key):\n",
    "        self.cipher = Fernet(encryption_key)\n",
    "    \n",
    "    def encrypt_field(self, plaintext: str) -> str:\n",
    "        \"\"\"Encrypt single field\"\"\"\n",
    "        return self.cipher.encrypt(plaintext.encode()).decode()\n",
    "    \n",
    "    def decrypt_field(self, ciphertext: str) -> str:\n",
    "        \"\"\"Decrypt single field\"\"\"\n",
    "        return self.cipher.decrypt(ciphertext.encode()).decode()\n",
    "    \n",
    "    def tokenize(self, value: str, secret: str) -> str:\n",
    "        \"\"\"One-way tokenization (irreversible)\"\"\"\n",
    "        return hmac.new(\n",
    "            secret.encode(),\n",
    "            value.encode(),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "\n",
    "# Spark UDF para field-level encryption\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "encryptor = PIIEncryptor(encryption_key=b'your-32-byte-key-here...')\n",
    "\n",
    "def encrypt_pii_udf(value):\n",
    "    if value:\n",
    "        return encryptor.encrypt_field(value)\n",
    "    return None\n",
    "\n",
    "encrypt_udf = F.udf(encrypt_pii_udf, StringType())\n",
    "\n",
    "# Apply to DataFrame\n",
    "df_encrypted = df.withColumn(\n",
    "    'email_encrypted',\n",
    "    encrypt_udf(F.col('email'))\n",
    ").withColumn(\n",
    "    'ssn_tokenized',\n",
    "    F.sha2(F.col('ssn'), 256)  # One-way hash\n",
    ")\n",
    "\n",
    "# Save con partition por encrypted field (queryable)\n",
    "df_encrypted.write.partitionBy('date').parquet('s3://bucket/encrypted-data/')\n",
    "```\n",
    "\n",
    "**Caso Real: Equifax Breach (2017)**\n",
    "\n",
    "**Vulnerabilidad**: Apache Struts sin parchear + SSL certificate expirado (no detected).\n",
    "\n",
    "**Impacto**: 147M personas, $700M settlement.\n",
    "\n",
    "**Protección**:\n",
    "```python\n",
    "# 1. Automated vulnerability scanning\n",
    "\"\"\"\n",
    "# Trivy (container scanning)\n",
    "trivy image my-data-pipeline:latest --severity HIGH,CRITICAL\n",
    "\n",
    "# Snyk (dependency scanning)\n",
    "snyk test --all-projects\n",
    "\"\"\"\n",
    "\n",
    "# 2. Certificate monitoring\n",
    "import ssl\n",
    "import socket\n",
    "from datetime import datetime\n",
    "\n",
    "def check_ssl_expiry(hostname, port=443):\n",
    "    \"\"\"Alert si cert expira en <30 días\"\"\"\n",
    "    context = ssl.create_default_context()\n",
    "    with socket.create_connection((hostname, port)) as sock:\n",
    "        with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
    "            cert = ssock.getpeercert()\n",
    "            \n",
    "            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')\n",
    "            days_remaining = (not_after - datetime.now()).days\n",
    "            \n",
    "            if days_remaining < 30:\n",
    "                send_alert(f\"⚠️ SSL cert expires in {days_remaining} days\")\n",
    "            \n",
    "            return days_remaining\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2965a",
   "metadata": {},
   "source": [
    "### 🎭 **PII Protection: Masking, Tokenization & Anonymization**\n",
    "\n",
    "**PII Classification & Protection Strategies**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│  PII Category            Strategy                      │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  DIRECT IDENTIFIERS                                    │\n",
    "│  • SSN, Passport         → Tokenization (irreversible) │\n",
    "│  • Email                 → Masking (u***@domain.com)   │\n",
    "│  • Phone                 → Partial mask (***-***-1234) │\n",
    "│  • Name                  → Pseudonymization            │\n",
    "│                                                         │\n",
    "│  QUASI-IDENTIFIERS (combination = identity)            │\n",
    "│  • ZIP + Age + Gender    → k-anonymity (generalize)    │\n",
    "│  • IP Address            → Truncate last octet         │\n",
    "│  • Timestamp             → Round to hour/day           │\n",
    "│                                                         │\n",
    "│  SENSITIVE ATTRIBUTES                                  │\n",
    "│  • Health data           → Encryption + access control │\n",
    "│  • Financial data        → Field-level encryption      │\n",
    "│  • Biometrics            → Hash + salt                 │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Masking Techniques**\n",
    "\n",
    "```python\n",
    "import re\n",
    "import hashlib\n",
    "import hmac\n",
    "from typing import Optional\n",
    "\n",
    "class PIIMasker:\n",
    "    \"\"\"Comprehensive PII masking library\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenization_secret: str):\n",
    "        self.secret = tokenization_secret.encode()\n",
    "    \n",
    "    # 1. EMAIL MASKING\n",
    "    def mask_email(self, email: str) -> str:\n",
    "        \"\"\"user@example.com → u***@example.com\"\"\"\n",
    "        if not email or '@' not in email:\n",
    "            return email\n",
    "        \n",
    "        user, domain = email.split('@', 1)\n",
    "        if len(user) <= 2:\n",
    "            masked_user = user[0] + '*'\n",
    "        else:\n",
    "            masked_user = user[0] + '*' * (len(user) - 2) + user[-1]\n",
    "        \n",
    "        return f\"{masked_user}@{domain}\"\n",
    "    \n",
    "    # 2. PHONE MASKING\n",
    "    def mask_phone(self, phone: str) -> str:\n",
    "        \"\"\"(555) 123-4567 → ***-***-4567\"\"\"\n",
    "        digits = re.sub(r'\\D', '', phone)\n",
    "        if len(digits) >= 10:\n",
    "            return f\"***-***-{digits[-4:]}\"\n",
    "        return \"***-****\"\n",
    "    \n",
    "    # 3. SSN/ID MASKING\n",
    "    def mask_ssn(self, ssn: str) -> str:\n",
    "        \"\"\"123-45-6789 → ***-**-6789\"\"\"\n",
    "        digits = re.sub(r'\\D', '', ssn)\n",
    "        if len(digits) == 9:\n",
    "            return f\"***-**-{digits[-4:]}\"\n",
    "        return \"***-**-****\"\n",
    "    \n",
    "    # 4. CREDIT CARD MASKING\n",
    "    def mask_credit_card(self, cc: str) -> str:\n",
    "        \"\"\"4532-1234-5678-9010 → ****-****-****-9010\"\"\"\n",
    "        digits = re.sub(r'\\D', '', cc)\n",
    "        if len(digits) >= 12:\n",
    "            return f\"****-****-****-{digits[-4:]}\"\n",
    "        return \"****-****-****-****\"\n",
    "    \n",
    "    # 5. NAME MASKING\n",
    "    def mask_name(self, name: str) -> str:\n",
    "        \"\"\"John Smith → J*** S*****\"\"\"\n",
    "        words = name.split()\n",
    "        masked = []\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                masked.append(word[0] + '*' * (len(word) - 1))\n",
    "            else:\n",
    "                masked.append(word)\n",
    "        return ' '.join(masked)\n",
    "    \n",
    "    # 6. TOKENIZATION (deterministic, irreversible)\n",
    "    def tokenize(self, value: str) -> str:\n",
    "        \"\"\"Generate consistent token for same input\"\"\"\n",
    "        return hmac.new(\n",
    "            self.secret,\n",
    "            value.encode(),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "    \n",
    "    # 7. PSEUDONYMIZATION (reversible con key)\n",
    "    def pseudonymize(self, value: str, salt: str) -> str:\n",
    "        \"\"\"Create pseudonym usando hash + salt\"\"\"\n",
    "        return hashlib.sha256(f\"{value}{salt}\".encode()).hexdigest()[:16]\n",
    "    \n",
    "    # 8. IP ADDRESS MASKING\n",
    "    def mask_ip(self, ip: str) -> str:\n",
    "        \"\"\"192.168.1.100 → 192.168.1.0\"\"\"\n",
    "        parts = ip.split('.')\n",
    "        if len(parts) == 4:\n",
    "            parts[-1] = '0'\n",
    "            return '.'.join(parts)\n",
    "        return ip\n",
    "    \n",
    "    # 9. DATE GENERALIZATION\n",
    "    def generalize_date(self, date_str: str, precision: str = 'month') -> str:\n",
    "        \"\"\"2025-10-15 → 2025-10-01 (month precision)\"\"\"\n",
    "        from datetime import datetime\n",
    "        dt = datetime.fromisoformat(date_str)\n",
    "        \n",
    "        if precision == 'year':\n",
    "            return dt.strftime('%Y-01-01')\n",
    "        elif precision == 'month':\n",
    "            return dt.strftime('%Y-%m-01')\n",
    "        elif precision == 'week':\n",
    "            # Round to Monday\n",
    "            days_since_monday = dt.weekday()\n",
    "            week_start = dt - timedelta(days=days_since_monday)\n",
    "            return week_start.strftime('%Y-%m-%d')\n",
    "        return date_str\n",
    "\n",
    "# Spark UDF Implementation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "masker = PIIMasker(tokenization_secret='your-secret-key')\n",
    "\n",
    "# Register UDFs\n",
    "mask_email_udf = F.udf(masker.mask_email, StringType())\n",
    "mask_phone_udf = F.udf(masker.mask_phone, StringType())\n",
    "tokenize_udf = F.udf(masker.tokenize, StringType())\n",
    "\n",
    "# Apply to DataFrame\n",
    "df_masked = (\n",
    "    df\n",
    "    .withColumn('email_masked', mask_email_udf(F.col('email')))\n",
    "    .withColumn('phone_masked', mask_phone_udf(F.col('phone')))\n",
    "    .withColumn('ssn_token', tokenize_udf(F.col('ssn')))\n",
    "    .drop('email', 'phone', 'ssn')  # Remove original PII\n",
    ")\n",
    "\n",
    "# Ejemplo: Production vs Development masking\n",
    "def create_dev_dataset(prod_df):\n",
    "    \"\"\"Create dev dataset con PII masked\"\"\"\n",
    "    return (\n",
    "        prod_df\n",
    "        .withColumn('email', mask_email_udf(F.col('email')))\n",
    "        .withColumn('phone', F.lit('***-***-****'))  # Static mask\n",
    "        .withColumn('ssn', tokenize_udf(F.col('ssn')))  # Consistent token\n",
    "        .withColumn('name', mask_name_udf(F.col('name')))\n",
    "        .withColumn('created_at', F.date_trunc('month', F.col('created_at')))\n",
    "    )\n",
    "```\n",
    "\n",
    "**K-Anonymity: Quasi-Identifier Protection**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def k_anonymize(df: pd.DataFrame, quasi_identifiers: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    Generalize quasi-identifiers para achieve k-anonymity\n",
    "    (cada combinación aparece al menos k veces)\n",
    "    \"\"\"\n",
    "    \n",
    "    def generalize_age(age):\n",
    "        \"\"\"Age → age range\"\"\"\n",
    "        if age < 18:\n",
    "            return '<18'\n",
    "        elif age < 30:\n",
    "            return '18-29'\n",
    "        elif age < 40:\n",
    "            return '30-39'\n",
    "        elif age < 50:\n",
    "            return '40-49'\n",
    "        elif age < 60:\n",
    "            return '50-59'\n",
    "        else:\n",
    "            return '60+'\n",
    "    \n",
    "    def generalize_zipcode(zipcode):\n",
    "        \"\"\"12345 → 123**\"\"\"\n",
    "        return str(zipcode)[:3] + '**'\n",
    "    \n",
    "    # Apply generalizations\n",
    "    df_anon = df.copy()\n",
    "    \n",
    "    if 'age' in quasi_identifiers:\n",
    "        df_anon['age'] = df_anon['age'].apply(generalize_age)\n",
    "    \n",
    "    if 'zipcode' in quasi_identifiers:\n",
    "        df_anon['zipcode'] = df_anon['zipcode'].apply(generalize_zipcode)\n",
    "    \n",
    "    # Check k-anonymity\n",
    "    group_sizes = df_anon.groupby(quasi_identifiers).size()\n",
    "    \n",
    "    if (group_sizes < k).any():\n",
    "        # Suppress rows que no cumplen k-anonymity\n",
    "        valid_groups = group_sizes[group_sizes >= k].index\n",
    "        df_anon = df_anon.set_index(quasi_identifiers).loc[valid_groups].reset_index()\n",
    "    \n",
    "    return df_anon\n",
    "\n",
    "# Ejemplo\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [25, 27, 35, 42, 55],\n",
    "    'zipcode': [12345, 12346, 12347, 54321, 54322],\n",
    "    'disease': ['flu', 'diabetes', 'flu', 'hypertension', 'diabetes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply k-anonymity (k=2)\n",
    "df_k_anon = k_anonymize(df, quasi_identifiers=['age', 'zipcode'], k=2)\n",
    "\n",
    "\"\"\"\n",
    "Before:\n",
    "name      age  zipcode    disease\n",
    "Alice     25   12345      flu\n",
    "Bob       27   12346      diabetes\n",
    "\n",
    "After (k=2):\n",
    "name      age     zipcode    disease\n",
    "Alice     18-29   123**      flu\n",
    "Bob       18-29   123**      diabetes\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Differential Privacy: Statistical Noise**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class DifferentialPrivacy:\n",
    "    \"\"\"Add calibrated noise para preserve privacy\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon: float = 1.0):\n",
    "        \"\"\"\n",
    "        epsilon: privacy budget (lower = more private, less accurate)\n",
    "        - 0.1: Very private (high noise)\n",
    "        - 1.0: Reasonable trade-off\n",
    "        - 10: Minimal privacy (low noise)\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def add_laplace_noise(self, value: float, sensitivity: float = 1.0) -> float:\n",
    "        \"\"\"Add Laplacian noise\"\"\"\n",
    "        scale = sensitivity / self.epsilon\n",
    "        noise = np.random.laplace(0, scale)\n",
    "        return value + noise\n",
    "    \n",
    "    def private_count(self, count: int) -> int:\n",
    "        \"\"\"Noisy count query\"\"\"\n",
    "        noisy = self.add_laplace_noise(count, sensitivity=1.0)\n",
    "        return max(0, int(round(noisy)))  # Non-negative\n",
    "    \n",
    "    def private_mean(self, values: list, lower_bound: float, upper_bound: float) -> float:\n",
    "        \"\"\"Noisy mean (bounded values)\"\"\"\n",
    "        sensitivity = (upper_bound - lower_bound) / len(values)\n",
    "        true_mean = np.mean(values)\n",
    "        return self.add_laplace_noise(true_mean, sensitivity)\n",
    "\n",
    "# Spark implementation\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dp = DifferentialPrivacy(epsilon=1.0)\n",
    "\n",
    "def add_noise_udf(value):\n",
    "    \"\"\"UDF to add differential privacy noise\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    return dp.add_laplace_noise(float(value), sensitivity=1.0)\n",
    "\n",
    "add_noise = F.udf(add_noise_udf, DoubleType())\n",
    "\n",
    "# Apply to aggregations\n",
    "df_private = (\n",
    "    df.groupBy('city')\n",
    "    .agg(F.count('*').alias('count'))\n",
    "    .withColumn('noisy_count', add_noise(F.col('count')))\n",
    ")\n",
    "```\n",
    "\n",
    "**GDPR Right to Erasure (Right to Be Forgotten)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def gdpr_delete_user_data(user_id: str, tables: list):\n",
    "    \"\"\"\n",
    "    Delete all data for user (GDPR compliance)\n",
    "    Support Delta Lake time travel para audit\n",
    "    \"\"\"\n",
    "    \n",
    "    deletion_log = []\n",
    "    \n",
    "    for table_path in tables:\n",
    "        # Load Delta table\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # Count before\n",
    "        before_count = spark.read.format('delta').load(table_path) \\\n",
    "            .filter(F.col('user_id') == user_id).count()\n",
    "        \n",
    "        # Delete\n",
    "        delta_table.delete(condition=f\"user_id = '{user_id}'\")\n",
    "        \n",
    "        # Verify\n",
    "        after_count = spark.read.format('delta').load(table_path) \\\n",
    "            .filter(F.col('user_id') == user_id).count()\n",
    "        \n",
    "        deletion_log.append({\n",
    "            'table': table_path,\n",
    "            'user_id': user_id,\n",
    "            'records_deleted': before_count,\n",
    "            'records_remaining': after_count,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    # Log deletion para audit\n",
    "    log_df = spark.createDataFrame(deletion_log)\n",
    "    log_df.write.mode('append').format('delta').save('s3://audit/gdpr-deletions/')\n",
    "    \n",
    "    return deletion_log\n",
    "\n",
    "# Anonymize instead of delete (alternative)\n",
    "def gdpr_anonymize_user_data(user_id: str):\n",
    "    \"\"\"Replace PII with anonymized values\"\"\"\n",
    "    \n",
    "    masker = PIIMasker('secret-key')\n",
    "    \n",
    "    delta_table = DeltaTable.forPath(spark, 's3://data-lake/users/')\n",
    "    \n",
    "    delta_table.update(\n",
    "        condition=f\"user_id = '{user_id}'\",\n",
    "        set={\n",
    "            'email': F.lit(masker.tokenize(user_id)),\n",
    "            'name': F.lit('ANONYMIZED'),\n",
    "            'phone': F.lit('***-***-****'),\n",
    "            'address': F.lit(None),\n",
    "            'anonymized_at': F.current_timestamp()\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "**Caso Real: Facebook-Cambridge Analytica (2018)**\n",
    "\n",
    "**Problema**: 87M perfiles usados sin consent para political targeting.\n",
    "\n",
    "**GDPR Protección**:\n",
    "```python\n",
    "# 1. Consent management\n",
    "class ConsentManager:\n",
    "    def check_consent(self, user_id: str, purpose: str) -> bool:\n",
    "        \"\"\"Verify user consent antes de procesar\"\"\"\n",
    "        consent = get_user_consent(user_id)\n",
    "        return purpose in consent.get('purposes', [])\n",
    "    \n",
    "    def process_with_consent(self, user_id: str, purpose: str, data):\n",
    "        if not self.check_consent(user_id, purpose):\n",
    "            raise ValueError(f\"No consent for {purpose}\")\n",
    "        \n",
    "        # Process data\n",
    "        return transform(data)\n",
    "\n",
    "# 2. Data minimization\n",
    "def minimize_data_collection(user_data: dict, purpose: str):\n",
    "    \"\"\"Collect only necessary fields\"\"\"\n",
    "    \n",
    "    purpose_fields = {\n",
    "        'analytics': ['user_id', 'timestamp', 'event_type'],\n",
    "        'marketing': ['user_id', 'email', 'opt_in'],\n",
    "        'personalization': ['user_id', 'preferences']\n",
    "    }\n",
    "    \n",
    "    allowed_fields = purpose_fields.get(purpose, [])\n",
    "    return {k: v for k, v in user_data.items() if k in allowed_fields}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f32f1",
   "metadata": {},
   "source": [
    "### 📋 **Compliance Frameworks: GDPR, HIPAA, SOC2 & Audit**\n",
    "\n",
    "**Compliance Requirements Matrix**\n",
    "\n",
    "| Framework | Scope | Key Requirements | Data Engineer Responsibilities |\n",
    "|-----------|-------|------------------|--------------------------------|\n",
    "| **GDPR** | EU citizens | • Consent<br>• Right to erasure<br>• Data portability<br>• Breach notification (72h) | • Delete/anonymize on request<br>• Data residency (EU)<br>• Encryption at-rest<br>• Access logs |\n",
    "| **CCPA** | California residents | • Opt-out of sale<br>• Access to data<br>• Deletion rights | • Implement do-not-sell flag<br>• Data export API<br>• 45-day deletion window |\n",
    "| **HIPAA** | Healthcare (US) | • PHI encryption<br>• Audit trails<br>• Business Associate Agreements | • Encrypt PHI at-rest/in-transit<br>• Implement access controls<br>• Audit all PHI access |\n",
    "| **SOC2** | Service organizations | • Security controls<br>• Availability<br>• Confidentiality | • Implement monitoring<br>• Incident response<br>• Change management |\n",
    "| **PCI-DSS** | Payment cards | • Cardholder data encryption<br>• Secure networks<br>• Regular testing | • Tokenize cards<br>• Network segmentation<br>• Vulnerability scanning |\n",
    "\n",
    "**GDPR Implementation**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "class GDPRCompliance:\n",
    "    \"\"\"GDPR compliance toolkit\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "    \n",
    "    # 1. RIGHT TO ACCESS (Article 15)\n",
    "    def export_user_data(self, user_id: str, output_path: str):\n",
    "        \"\"\"\n",
    "        Export all user data en formato legible (JSON)\n",
    "        User can request their data\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [\n",
    "            's3://data-lake/users/',\n",
    "            's3://data-lake/transactions/',\n",
    "            's3://data-lake/events/',\n",
    "            's3://data-lake/preferences/'\n",
    "        ]\n",
    "        \n",
    "        user_data = {}\n",
    "        \n",
    "        for table_path in tables:\n",
    "            table_name = table_path.split('/')[-2]\n",
    "            \n",
    "            df = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id)\n",
    "            \n",
    "            # Convert to JSON\n",
    "            records = df.toJSON().collect()\n",
    "            user_data[table_name] = [json.loads(r) for r in records]\n",
    "        \n",
    "        # Add metadata\n",
    "        export_package = {\n",
    "            'user_id': user_id,\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'data': user_data,\n",
    "            'retention_policy': '7 years',\n",
    "            'data_controller': 'YourCompany Inc.'\n",
    "        }\n",
    "        \n",
    "        # Write to S3 (user can download)\n",
    "        import json\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(export_package, f, indent=2)\n",
    "        \n",
    "        # Log export request\n",
    "        self.log_gdpr_request('access', user_id)\n",
    "        \n",
    "        return export_package\n",
    "    \n",
    "    # 2. RIGHT TO ERASURE (Article 17)\n",
    "    def delete_user_data(self, user_id: str, reason: str = 'user_request'):\n",
    "        \"\"\"\n",
    "        Delete all user data (Right to be Forgotten)\n",
    "        Must complete within 30 days\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [\n",
    "            's3://data-lake/users/',\n",
    "            's3://data-lake/transactions/',\n",
    "            's3://data-lake/events/',\n",
    "            's3://data-lake/preferences/'\n",
    "        ]\n",
    "        \n",
    "        deletion_report = {\n",
    "            'user_id': user_id,\n",
    "            'request_date': datetime.now(),\n",
    "            'reason': reason,\n",
    "            'tables_processed': []\n",
    "        }\n",
    "        \n",
    "        for table_path in tables:\n",
    "            delta_table = DeltaTable.forPath(self.spark, table_path)\n",
    "            \n",
    "            # Count before deletion\n",
    "            before = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id).count()\n",
    "            \n",
    "            # Delete\n",
    "            delta_table.delete(condition=f\"user_id = '{user_id}'\")\n",
    "            \n",
    "            # Verify deletion\n",
    "            after = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id).count()\n",
    "            \n",
    "            deletion_report['tables_processed'].append({\n",
    "                'table': table_path,\n",
    "                'records_deleted': before,\n",
    "                'records_remaining': after,\n",
    "                'status': 'complete' if after == 0 else 'failed'\n",
    "            })\n",
    "        \n",
    "        # Purge from backups (async job)\n",
    "        self.schedule_backup_purge(user_id)\n",
    "        \n",
    "        # Log deletion\n",
    "        self.log_gdpr_request('erasure', user_id, deletion_report)\n",
    "        \n",
    "        return deletion_report\n",
    "    \n",
    "    # 3. DATA PORTABILITY (Article 20)\n",
    "    def export_user_data_structured(self, user_id: str) -> dict:\n",
    "        \"\"\"Export en formato machine-readable (CSV, JSON)\"\"\"\n",
    "        \n",
    "        # Export to multiple formats\n",
    "        formats = ['json', 'csv', 'parquet']\n",
    "        export_urls = {}\n",
    "        \n",
    "        for fmt in formats:\n",
    "            output_path = f's3://exports/{user_id}/data.{fmt}'\n",
    "            \n",
    "            df = self.spark.read.format('delta').load('s3://data-lake/users/') \\\n",
    "                .filter(F.col('user_id') == user_id)\n",
    "            \n",
    "            if fmt == 'parquet':\n",
    "                df.write.mode('overwrite').parquet(output_path)\n",
    "            elif fmt == 'csv':\n",
    "                df.write.mode('overwrite').option('header', True).csv(output_path)\n",
    "            else:  # json\n",
    "                df.write.mode('overwrite').json(output_path)\n",
    "            \n",
    "            export_urls[fmt] = output_path\n",
    "        \n",
    "        return export_urls\n",
    "    \n",
    "    # 4. CONSENT MANAGEMENT (Article 7)\n",
    "    def check_consent(self, user_id: str, purpose: str) -> bool:\n",
    "        \"\"\"Verify explicit consent for data processing\"\"\"\n",
    "        \n",
    "        consent_df = self.spark.read.format('delta').load('s3://data-lake/consents/') \\\n",
    "            .filter(\n",
    "                (F.col('user_id') == user_id) &\n",
    "                (F.col('purpose') == purpose) &\n",
    "                (F.col('consent_given') == True) &\n",
    "                (F.col('consent_withdrawn_at').isNull())\n",
    "            )\n",
    "        \n",
    "        return consent_df.count() > 0\n",
    "    \n",
    "    def withdraw_consent(self, user_id: str, purpose: str):\n",
    "        \"\"\"User withdraws consent\"\"\"\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, 's3://data-lake/consents/')\n",
    "        \n",
    "        delta_table.update(\n",
    "            condition=f\"user_id = '{user_id}' AND purpose = '{purpose}'\",\n",
    "            set={'consent_withdrawn_at': F.current_timestamp()}\n",
    "        )\n",
    "        \n",
    "        # Stop processing data for this purpose\n",
    "        self.log_gdpr_request('consent_withdrawal', user_id, {'purpose': purpose})\n",
    "    \n",
    "    # 5. BREACH NOTIFICATION (Article 33)\n",
    "    def detect_data_breach(self):\n",
    "        \"\"\"Detect suspicious access patterns\"\"\"\n",
    "        \n",
    "        # Query access logs\n",
    "        access_logs = self.spark.read.format('delta').load('s3://logs/access/')\n",
    "        \n",
    "        # Anomaly detection: unusual access volume\n",
    "        anomalies = access_logs.groupBy('user_id', 'date') \\\n",
    "            .agg(F.count('*').alias('access_count')) \\\n",
    "            .filter(F.col('access_count') > 1000)  # Threshold\n",
    "        \n",
    "        if anomalies.count() > 0:\n",
    "            self.trigger_breach_response(anomalies)\n",
    "    \n",
    "    def trigger_breach_response(self, anomalies):\n",
    "        \"\"\"72-hour notification requirement\"\"\"\n",
    "        \n",
    "        breach_report = {\n",
    "            'detected_at': datetime.now(),\n",
    "            'notification_deadline': datetime.now() + timedelta(hours=72),\n",
    "            'affected_users': anomalies.select('user_id').distinct().count(),\n",
    "            'actions_taken': [\n",
    "                'Blocked suspicious IPs',\n",
    "                'Reset affected user sessions',\n",
    "                'Notified security team'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Alert compliance team\n",
    "        send_alert(\n",
    "            title='⚠️ Potential Data Breach Detected',\n",
    "            message=f\"Affected users: {breach_report['affected_users']}\\n\"\n",
    "                    f\"Notification deadline: {breach_report['notification_deadline']}\",\n",
    "            channel='#security-incidents'\n",
    "        )\n",
    "        \n",
    "        # Log breach\n",
    "        self.log_gdpr_request('breach', 'system', breach_report)\n",
    "    \n",
    "    # 6. DATA RETENTION (Article 5)\n",
    "    def enforce_retention_policy(self, table_path: str, retention_days: int = 2555):\n",
    "        \"\"\"\n",
    "        Delete data older than retention period\n",
    "        GDPR: Keep data no longer than necessary\n",
    "        \"\"\"\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, table_path)\n",
    "        \n",
    "        cutoff_date = datetime.now() - timedelta(days=retention_days)\n",
    "        \n",
    "        # Delete old data\n",
    "        delta_table.delete(\n",
    "            condition=f\"created_at < '{cutoff_date.isoformat()}'\"\n",
    "        )\n",
    "        \n",
    "        # Vacuum to physically delete files\n",
    "        delta_table.vacuum(retentionHours=0)  # Immediate deletion\n",
    "    \n",
    "    # 7. AUDIT LOGGING\n",
    "    def log_gdpr_request(self, request_type: str, user_id: str, details: dict = None):\n",
    "        \"\"\"Log all GDPR-related operations\"\"\"\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'request_type': request_type,\n",
    "            'user_id': user_id,\n",
    "            'details': details or {},\n",
    "            'processed_by': 'gdpr_compliance_service'\n",
    "        }\n",
    "        \n",
    "        log_df = self.spark.createDataFrame([log_entry])\n",
    "        log_df.write.mode('append').format('delta').save('s3://audit/gdpr-requests/')\n",
    "```\n",
    "\n",
    "**HIPAA Compliance (Healthcare)**\n",
    "\n",
    "```python\n",
    "class HIPAACompliance:\n",
    "    \"\"\"Health Insurance Portability and Accountability Act\"\"\"\n",
    "    \n",
    "    # PHI = Protected Health Information\n",
    "    PHI_FIELDS = [\n",
    "        'name', 'address', 'ssn', 'medical_record_number',\n",
    "        'email', 'phone', 'ip_address', 'device_id',\n",
    "        'diagnosis', 'prescription', 'lab_results'\n",
    "    ]\n",
    "    \n",
    "    def encrypt_phi(self, df):\n",
    "        \"\"\"Encrypt all PHI fields\"\"\"\n",
    "        \n",
    "        for field in self.PHI_FIELDS:\n",
    "            if field in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    field,\n",
    "                    encrypt_udf(F.col(field))\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def audit_phi_access(self, user_id: str, patient_id: str, purpose: str):\n",
    "        \"\"\"Log every PHI access (HIPAA requirement)\"\"\"\n",
    "        \n",
    "        audit_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'user_id': user_id,\n",
    "            'patient_id': patient_id,\n",
    "            'purpose': purpose,\n",
    "            'ip_address': get_client_ip(),\n",
    "            'action': 'view_phi'\n",
    "        }\n",
    "        \n",
    "        # Write to immutable audit log\n",
    "        spark.createDataFrame([audit_entry]).write \\\n",
    "            .mode('append') \\\n",
    "            .format('delta') \\\n",
    "            .option('mergeSchema', False) \\\n",
    "            .save('s3://audit/phi-access/')\n",
    "    \n",
    "    def de_identify_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Remove 18 HIPAA identifiers para create de-identified dataset\n",
    "        Safe Harbor method\n",
    "        \"\"\"\n",
    "        \n",
    "        identifiers = [\n",
    "            'name', 'address', 'city', 'zip', 'phone', 'fax', 'email',\n",
    "            'ssn', 'medical_record_number', 'health_plan_number',\n",
    "            'account_number', 'certificate_number', 'vehicle_id',\n",
    "            'device_id', 'url', 'ip_address', 'biometric_id',\n",
    "            'photo', 'any_unique_code'\n",
    "        ]\n",
    "        \n",
    "        # Remove identifiers\n",
    "        df_deidentified = df.drop(*[col for col in identifiers if col in df.columns])\n",
    "        \n",
    "        # Generalize dates (only year)\n",
    "        date_columns = [col for col, dtype in df.dtypes if 'date' in dtype or 'timestamp' in dtype]\n",
    "        for col in date_columns:\n",
    "            df_deidentified = df_deidentified.withColumn(\n",
    "                col,\n",
    "                F.year(F.col(col))\n",
    "            )\n",
    "        \n",
    "        # Generalize age (>89 → 90+)\n",
    "        if 'age' in df_deidentified.columns:\n",
    "            df_deidentified = df_deidentified.withColumn(\n",
    "                'age',\n",
    "                F.when(F.col('age') > 89, 90).otherwise(F.col('age'))\n",
    "            )\n",
    "        \n",
    "        return df_deidentified\n",
    "```\n",
    "\n",
    "**SOC2 Audit Evidence Collection**\n",
    "\n",
    "```python\n",
    "# SOC2 Trust Service Criteria\n",
    "class SOC2Compliance:\n",
    "    \"\"\"System and Organization Controls Type 2\"\"\"\n",
    "    \n",
    "    def collect_audit_evidence(self, period_days: int = 90):\n",
    "        \"\"\"Gather evidence for SOC2 audit\"\"\"\n",
    "        \n",
    "        evidence = {\n",
    "            # CC6.1: Logical access controls\n",
    "            'access_controls': self.verify_access_controls(),\n",
    "            \n",
    "            # CC6.6: Vulnerability management\n",
    "            'vulnerability_scans': self.collect_vulnerability_scans(period_days),\n",
    "            \n",
    "            # CC7.2: System monitoring\n",
    "            'monitoring_alerts': self.collect_monitoring_data(period_days),\n",
    "            \n",
    "            # CC8.1: Change management\n",
    "            'code_changes': self.collect_git_commits(period_days),\n",
    "            \n",
    "            # A1.2: System availability\n",
    "            'uptime_metrics': self.calculate_uptime(period_days),\n",
    "        }\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def verify_access_controls(self):\n",
    "        \"\"\"Verify IAM policies follow least privilege\"\"\"\n",
    "        \n",
    "        iam = boto3.client('iam')\n",
    "        \n",
    "        # Check MFA enforcement\n",
    "        users = iam.list_users()['Users']\n",
    "        \n",
    "        mfa_status = []\n",
    "        for user in users:\n",
    "            mfa_devices = iam.list_mfa_devices(UserName=user['UserName'])\n",
    "            \n",
    "            mfa_status.append({\n",
    "                'user': user['UserName'],\n",
    "                'mfa_enabled': len(mfa_devices['MFADevices']) > 0\n",
    "            })\n",
    "        \n",
    "        # Check overly permissive policies\n",
    "        policies = iam.list_policies(Scope='Local')['Policies']\n",
    "        \n",
    "        risky_policies = []\n",
    "        for policy in policies:\n",
    "            policy_version = iam.get_policy_version(\n",
    "                PolicyArn=policy['Arn'],\n",
    "                VersionId=policy['DefaultVersionId']\n",
    "            )\n",
    "            \n",
    "            doc = policy_version['PolicyVersion']['Document']\n",
    "            \n",
    "            # Check for wildcard permissions\n",
    "            for statement in doc.get('Statement', []):\n",
    "                if statement.get('Effect') == 'Allow' and '*' in statement.get('Action', []):\n",
    "                    risky_policies.append(policy['PolicyName'])\n",
    "        \n",
    "        return {\n",
    "            'mfa_compliance': sum(1 for u in mfa_status if u['mfa_enabled']) / len(mfa_status),\n",
    "            'risky_policies': risky_policies\n",
    "        }\n",
    "```\n",
    "\n",
    "**Caso Real: Uber 2016 Breach (Concealed)**\n",
    "\n",
    "**Problema**: Breach de 57M usuarios, pagaron $100K a hackers para ocultarlo, no notificaron (violación GDPR/leyes estatales).\n",
    "\n",
    "**Resultado**: $148M multa, CEO criminally charged.\n",
    "\n",
    "**Compliance Checklist**:\n",
    "```python\n",
    "compliance_checklist = \"\"\"\n",
    "✅ Data Classification (public, internal, confidential, restricted)\n",
    "✅ Access Controls (RBAC, MFA)\n",
    "✅ Encryption (at-rest: AES-256, in-transit: TLS 1.3)\n",
    "✅ Data Retention Policy (automated deletion)\n",
    "✅ Breach Detection (SIEM, anomaly detection)\n",
    "✅ Incident Response Plan (72h notification for GDPR)\n",
    "✅ Audit Logs (immutable, centralized)\n",
    "✅ Third-Party Audits (annual SOC2, penetration testing)\n",
    "✅ Employee Training (security awareness, phishing)\n",
    "✅ Disaster Recovery (RTO < 4h, RPO < 1h)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b98e",
   "metadata": {},
   "source": [
    "## 1. IAM y principio de mínimo privilegio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a170b",
   "metadata": {},
   "source": [
    "- Roles por función: data-engineer-ro, data-scientist, admin.\n",
    "- Políticas granulares: lectura de bucket específico, escritura en tabla específica.\n",
    "- MFA obligatorio para operaciones sensibles (producción, eliminación).\n",
    "- Rotación automática de credenciales (secretos, keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7771f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_policy_example = r'''\n",
    "# AWS IAM policy para data engineer con acceso de lectura a raw y escritura a curated\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "      \"Resource\": [\"arn:aws:s3:::data-lake/raw/*\"]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "      \"Resource\": [\"arn:aws:s3:::data-lake/curated/*\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "print(iam_policy_example.splitlines()[:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6cc67",
   "metadata": {},
   "source": [
    "## 2. Cifrado en tránsito y at-rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971318a",
   "metadata": {},
   "source": [
    "- **At-rest**: S3 SSE-KMS, RDS encryption, disk encryption (EBS/GCS).\n",
    "- **In-transit**: TLS 1.2+ para todas las APIs, VPN/PrivateLink para conectividad interna.\n",
    "- Gestión de claves: AWS KMS, GCP Cloud KMS, Azure Key Vault con rotación automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd346c6f",
   "metadata": {},
   "source": [
    "## 3. Enmascaramiento y anonimización de PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92cb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def mask_email(email: str) -> str:\n",
    "    user, domain = email.split('@')\n",
    "    return f'{user[0]}***@{domain}'\n",
    "\n",
    "def hash_pii(value: str) -> str:\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "\n",
    "mask_email('usuario@ejemplo.com'), hash_pii('12345678A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c42f49",
   "metadata": {},
   "source": [
    "## 4. Cumplimiento normativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c128ca",
   "metadata": {},
   "source": [
    "### GDPR (Europa)\n",
    "- Derecho al olvido: implementar DELETE cascada y purga en backups.\n",
    "- Consentimiento explícito y auditable.\n",
    "- Data residency: almacenar en región EU.\n",
    "\n",
    "### HIPAA (salud, USA)\n",
    "- PHI cifrado, logs de acceso auditables.\n",
    "- Business Associate Agreements con proveedores cloud.\n",
    "\n",
    "### SOC2 (seguridad organizacional)\n",
    "- Controles de acceso, monitoreo, incident response.\n",
    "- Auditorías anuales por terceros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af392a5a",
   "metadata": {},
   "source": [
    "## 5. Auditoría de accesos y linaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a754d8",
   "metadata": {},
   "source": [
    "- CloudTrail (AWS), Cloud Audit Logs (GCP), Activity Log (Azure).\n",
    "- Registrar quién accedió qué dato, cuándo, desde dónde.\n",
    "- Linaje de datos: OpenLineage, DataHub, Marquez → rastrear transformaciones y uso.\n",
    "- Alertas ante accesos anómalos (SIEM: Splunk, Datadog Security)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef33df",
   "metadata": {},
   "source": [
    "## 6. Checklist de seguridad para pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b30d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = '''\n",
    "☑ IAM con mínimo privilegio y MFA\n",
    "☑ Cifrado at-rest (KMS) y in-transit (TLS)\n",
    "☑ Enmascaramiento de PII en logs y datasets de dev\n",
    "☑ Rotación automática de secretos (API keys, DB passwords)\n",
    "☑ Auditoría habilitada (CloudTrail, logs centralizados)\n",
    "☑ Vulnerability scanning de contenedores (Trivy, Clair)\n",
    "☑ Network segmentation (VPCs, subnets privadas)\n",
    "☑ Incident response plan documentado y probado\n",
    "'''\n",
    "print(checklist)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
