{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a08253",
   "metadata": {},
   "source": [
    "# ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos\n",
    "\n",
    "Objetivo: implementar controles de seguridad (IAM, cifrado, enmascaramiento), cumplir regulaciones (GDPR, HIPAA, SOC2) y establecer auditorÃ­a de accesos y linaje.\n",
    "\n",
    "- DuraciÃ³n: 120â€“150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Governance (Senior 01), experiencia con cloud IAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9003a8",
   "metadata": {},
   "source": [
    "### ğŸ”’ **Defense in Depth: Multi-Layer Security Architecture**\n",
    "\n",
    "**Modelo de Seguridad en Capas para Data Platforms**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Layer 1: NETWORK SECURITY                              â”‚\n",
    "â”‚  â€¢ VPC isolation, private subnets                       â”‚\n",
    "â”‚  â€¢ Security Groups (stateful firewall)                  â”‚\n",
    "â”‚  â€¢ NACLs (Network ACLs - stateless)                     â”‚\n",
    "â”‚  â€¢ VPN/PrivateLink para conectividad                    â”‚\n",
    "â”‚  â€¢ DDoS protection (AWS Shield, CloudFlare)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Layer 2: IDENTITY & ACCESS MANAGEMENT (IAM)            â”‚\n",
    "â”‚  â€¢ Least privilege principle                            â”‚\n",
    "â”‚  â€¢ Role-Based Access Control (RBAC)                     â”‚\n",
    "â”‚  â€¢ Multi-Factor Authentication (MFA)                    â”‚\n",
    "â”‚  â€¢ Service accounts con permisos mÃ­nimos                â”‚\n",
    "â”‚  â€¢ Temporary credentials (STS AssumeRole)               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Layer 3: DATA ENCRYPTION                               â”‚\n",
    "â”‚  â€¢ At-rest: KMS, customer-managed keys                  â”‚\n",
    "â”‚  â€¢ In-transit: TLS 1.3, mTLS                            â”‚\n",
    "â”‚  â€¢ Application-level: Field-level encryption            â”‚\n",
    "â”‚  â€¢ Key rotation (automÃ¡tica, 90 dÃ­as)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Layer 4: APPLICATION SECURITY                          â”‚\n",
    "â”‚  â€¢ Input validation (SQL injection, XSS)                â”‚\n",
    "â”‚  â€¢ API authentication (JWT, OAuth2)                     â”‚\n",
    "â”‚  â€¢ Rate limiting, DDoS mitigation                       â”‚\n",
    "â”‚  â€¢ Security headers (HSTS, CSP)                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Layer 5: DATA GOVERNANCE                               â”‚\n",
    "â”‚  â€¢ PII masking/tokenization                             â”‚\n",
    "â”‚  â€¢ Data classification (public, internal, confidential) â”‚\n",
    "â”‚  â€¢ Access logs y auditorÃ­a                              â”‚\n",
    "â”‚  â€¢ Data lineage tracking                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Layer 6: MONITORING & INCIDENT RESPONSE                â”‚\n",
    "â”‚  â€¢ SIEM (Splunk, Datadog Security)                      â”‚\n",
    "â”‚  â€¢ Anomaly detection (ML-based)                         â”‚\n",
    "â”‚  â€¢ Security alerts (Slack, PagerDuty)                   â”‚\n",
    "â”‚  â€¢ Incident response playbooks                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**AWS IAM: Least Privilege Implementation**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# 1. ROLE-BASED ACCESS CONTROL (RBAC)\n",
    "\n",
    "# Data Engineer (Read raw, Write curated)\n",
    "data_engineer_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"ReadRawData\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::data-lake/raw/*\",\n",
    "                \"arn:aws:s3:::data-lake/raw\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"WriteCuratedData\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/curated/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"s3:x-amz-server-side-encryption\": \"aws:kms\"  # Enforce encryption\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"GlueJobExecution\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:glue:us-east-1:123456789012:job/etl-*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"DenyProductionDelete\",\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Action\": \"s3:DeleteObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/prod/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Data Scientist (Read-only curated + analytics)\n",
    "data_scientist_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::data-lake/curated/*\",\n",
    "                \"arn:aws:s3:::data-lake/analytics/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"athena:StartQueryExecution\",\n",
    "                \"athena:GetQueryExecution\",\n",
    "                \"athena:GetQueryResults\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"athena:WorkGroup\": \"data-science-workgroup\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. SERVICE ACCOUNT (EMR Cluster)\n",
    "emr_service_role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::data-lake/processing/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kms:Decrypt\",\n",
    "                \"kms:Encrypt\",\n",
    "                \"kms:GenerateDataKey\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:kms:us-east-1:123456789012:key/data-lake-key\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create IAM role with policy\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "def create_data_role(role_name, policy_document, description):\n",
    "    \"\"\"Create IAM role with inline policy\"\"\"\n",
    "    \n",
    "    # Trust policy (who can assume this role)\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"glue.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create role\n",
    "    role = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=description,\n",
    "        MaxSessionDuration=3600,  # 1 hour\n",
    "        Tags=[\n",
    "            {'Key': 'Team', 'Value': 'DataEngineering'},\n",
    "            {'Key': 'Environment', 'Value': 'Production'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Attach inline policy\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=f'{role_name}-policy',\n",
    "        PolicyDocument=json.dumps(policy_document)\n",
    "    )\n",
    "    \n",
    "    return role['Role']['Arn']\n",
    "\n",
    "# 3. TEMPORARY CREDENTIALS (STS AssumeRole)\n",
    "sts = boto3.client('sts')\n",
    "\n",
    "def assume_data_engineer_role(role_arn, session_name):\n",
    "    \"\"\"Get temporary credentials\"\"\"\n",
    "    response = sts.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name,\n",
    "        DurationSeconds=3600,  # 1 hour\n",
    "        Tags=[\n",
    "            {'Key': 'User', 'Value': session_name}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Use temporary credentials\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "    \n",
    "    return s3\n",
    "\n",
    "# 4. MFA ENFORCEMENT\n",
    "mfa_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowListActions\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:ListUsers\",\n",
    "                \"iam:ListVirtualMFADevices\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"AllowIndividualUserToManageTheirOwnMFA\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:EnableMFADevice\",\n",
    "                \"iam:CreateVirtualMFADevice\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"DenyAllExceptListedIfNoMFA\",\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"NotAction\": [\n",
    "                \"iam:CreateVirtualMFADevice\",\n",
    "                \"iam:EnableMFADevice\",\n",
    "                \"iam:ListMFADevices\",\n",
    "                \"iam:ListUsers\",\n",
    "                \"iam:GetUser\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"BoolIfExists\": {\n",
    "                    \"aws:MultiFactorAuthPresent\": \"false\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Secrets Management: RotaciÃ³n AutomÃ¡tica**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "secretsmanager = boto3.client('secretsmanager')\n",
    "\n",
    "# 1. Create secret with automatic rotation\n",
    "def create_rotating_secret(secret_name, secret_value):\n",
    "    \"\"\"Create secret con rotaciÃ³n automÃ¡tica cada 30 dÃ­as\"\"\"\n",
    "    \n",
    "    response = secretsmanager.create_secret(\n",
    "        Name=secret_name,\n",
    "        Description='Database password with auto-rotation',\n",
    "        SecretString=secret_value,\n",
    "        Tags=[\n",
    "            {'Key': 'Environment', 'Value': 'Production'},\n",
    "            {'Key': 'AutoRotate', 'Value': 'true'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Enable automatic rotation (Lambda-based)\n",
    "    secretsmanager.rotate_secret(\n",
    "        SecretId=secret_name,\n",
    "        RotationLambdaARN='arn:aws:lambda:us-east-1:123456789012:function:rotate-db-password',\n",
    "        RotationRules={\n",
    "            'AutomaticallyAfterDays': 30,  # Rotate every 30 days\n",
    "            'Duration': '2h',\n",
    "            'ScheduleExpression': 'rate(30 days)'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response['ARN']\n",
    "\n",
    "# 2. Retrieve secret in application\n",
    "def get_database_credentials(secret_name):\n",
    "    \"\"\"Get current secret value\"\"\"\n",
    "    response = secretsmanager.get_secret_value(SecretId=secret_name)\n",
    "    \n",
    "    import json\n",
    "    secret = json.loads(response['SecretString'])\n",
    "    \n",
    "    return {\n",
    "        'username': secret['username'],\n",
    "        'password': secret['password'],\n",
    "        'host': secret['host'],\n",
    "        'port': secret['port']\n",
    "    }\n",
    "\n",
    "# 3. Lambda function para rotaciÃ³n\n",
    "\"\"\"\n",
    "import boto3\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    '''Rotate database password'''\n",
    "    \n",
    "    service_client = boto3.client('secretsmanager')\n",
    "    arn = event['SecretId']\n",
    "    token = event['ClientRequestToken']\n",
    "    step = event['Step']\n",
    "    \n",
    "    if step == 'createSecret':\n",
    "        # Generate new password\n",
    "        new_password = generate_random_password()\n",
    "        \n",
    "        # Store pending secret\n",
    "        service_client.put_secret_value(\n",
    "            SecretId=arn,\n",
    "            ClientRequestToken=token,\n",
    "            SecretString=json.dumps({'password': new_password}),\n",
    "            VersionStages=['AWSPENDING']\n",
    "        )\n",
    "    \n",
    "    elif step == 'setSecret':\n",
    "        # Update database with new password\n",
    "        pending_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSPENDING'\n",
    "        )\n",
    "        \n",
    "        current_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSCURRENT'\n",
    "        )\n",
    "        \n",
    "        # Connect with current credentials\n",
    "        conn = psycopg2.connect(**json.loads(current_secret['SecretString']))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Update password\n",
    "        new_password = json.loads(pending_secret['SecretString'])['password']\n",
    "        cursor.execute(f\"ALTER USER myuser WITH PASSWORD '{new_password}'\")\n",
    "        conn.commit()\n",
    "    \n",
    "    elif step == 'testSecret':\n",
    "        # Verify new credentials work\n",
    "        pending_secret = service_client.get_secret_value(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSPENDING'\n",
    "        )\n",
    "        \n",
    "        # Test connection\n",
    "        conn = psycopg2.connect(**json.loads(pending_secret['SecretString']))\n",
    "        conn.close()\n",
    "    \n",
    "    elif step == 'finishSecret':\n",
    "        # Move AWSPENDING to AWSCURRENT\n",
    "        service_client.update_secret_version_stage(\n",
    "            SecretId=arn,\n",
    "            VersionStage='AWSCURRENT',\n",
    "            MoveToVersionId=token\n",
    "        )\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Capital One Breach (2019)**\n",
    "\n",
    "**Vulnerabilidad**: Firewall mal configurado permitiÃ³ acceso desde Internet a metadata service (169.254.169.254) que expuso IAM credentials.\n",
    "\n",
    "**Impacto**: 100M registros de clientes comprometidos, multa de $80M.\n",
    "\n",
    "**Lecciones**:\n",
    "1. âœ… **Network Segmentation**: Servidores en subnets privadas\n",
    "2. âœ… **IMDSv2**: Require token para metadata (AWS)\n",
    "3. âœ… **Security Groups**: Deny 169.254.169.254 desde apps\n",
    "4. âœ… **WAF**: Web Application Firewall con reglas OWASP\n",
    "5. âœ… **Least Privilege**: Limitar scope de IAM roles\n",
    "\n",
    "```python\n",
    "# IMDSv2 enforcement (require token)\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "ec2.modify_instance_metadata_options(\n",
    "    InstanceId='i-1234567890abcdef0',\n",
    "    HttpTokens='required',  # Require IMDSv2\n",
    "    HttpPutResponseHopLimit=1  # Prevent IP forwarding\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca31920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” SECURITY & COMPLIANCE AUDIT SIMULATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset original (CON PII sensible):\n",
      "   â€¢ Total usuarios: 5,000\n",
      "   â€¢ Columnas con PII: email, full_name, ssn, credit_card, phone, address, date_of_birth\n",
      "   â€¢ Usuarios con consentimiento GDPR: 4,267 (85.3%)\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ DATA CLASSIFICATION - ClasificaciÃ³n por Sensibilidad\n",
      "================================================================================\n",
      "\n",
      "ğŸ” ClasificaciÃ³n de campos:\n",
      "\n",
      "   PUBLIC:\n",
      "      â€¢ user_id\n",
      "      â€¢ department\n",
      "      â€¢ data_residency\n",
      "\n",
      "   INTERNAL:\n",
      "      â€¢ email\n",
      "      â€¢ full_name\n",
      "      â€¢ phone\n",
      "      â€¢ created_at\n",
      "      â€¢ consent_gdpr\n",
      "\n",
      "   CONFIDENTIAL:\n",
      "      â€¢ salary\n",
      "      â€¢ address\n",
      "      â€¢ date_of_birth\n",
      "\n",
      "   RESTRICTED:\n",
      "      â€¢ ssn\n",
      "      â€¢ credit_card\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ PII MASKING & HASHING - ProtecciÃ³n de Datos Sensibles\n",
      "================================================================================\n",
      "\n",
      "ğŸ­ Ejemplos de masking:\n",
      "\n",
      "   Usuario U03346:\n",
      "      Email:        user3346@zkrbt.com â†’ u***@zkrbt.com\n",
      "      Credit Card:  4553-6855-1989-4625 â†’ ****-****-****-4625\n",
      "      SSN:          592-66-1227 â†’ ***-**-1227\n",
      "      Phone:        +1-692-475-5356 â†’ +1-***-***-****\n",
      "      Full Name:    Vygazdc Dvsijiti â†’ 68e1c2797258b553 (hashed)\n",
      "\n",
      "   Usuario U02158:\n",
      "      Email:        user2158@wgiia.com â†’ u***@wgiia.com\n",
      "      Credit Card:  4330-4575-4369-4681 â†’ ****-****-****-4681\n",
      "      SSN:          835-45-5273 â†’ ***-**-5273\n",
      "      Phone:        +1-386-987-6203 â†’ +1-***-***-****\n",
      "      Full Name:    Ckdla Atetbuzsgp â†’ b740268590b9318d (hashed)\n",
      "\n",
      "   Usuario U03561:\n",
      "      Email:        user3561@clkap.com â†’ u***@clkap.com\n",
      "      Credit Card:  4537-4823-7439-1590 â†’ ****-****-****-1590\n",
      "      SSN:          976-54-2016 â†’ ***-**-2016\n",
      "      Phone:        +1-453-703-6185 â†’ +1-***-***-****\n",
      "      Full Name:    Fmmwbk Fnxckn â†’ c1d671c3812d5dee (hashed)\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ ACCESS AUDIT LOG - Registro de Accesos\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset original (CON PII sensible):\n",
      "   â€¢ Total usuarios: 5,000\n",
      "   â€¢ Columnas con PII: email, full_name, ssn, credit_card, phone, address, date_of_birth\n",
      "   â€¢ Usuarios con consentimiento GDPR: 4,267 (85.3%)\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ DATA CLASSIFICATION - ClasificaciÃ³n por Sensibilidad\n",
      "================================================================================\n",
      "\n",
      "ğŸ” ClasificaciÃ³n de campos:\n",
      "\n",
      "   PUBLIC:\n",
      "      â€¢ user_id\n",
      "      â€¢ department\n",
      "      â€¢ data_residency\n",
      "\n",
      "   INTERNAL:\n",
      "      â€¢ email\n",
      "      â€¢ full_name\n",
      "      â€¢ phone\n",
      "      â€¢ created_at\n",
      "      â€¢ consent_gdpr\n",
      "\n",
      "   CONFIDENTIAL:\n",
      "      â€¢ salary\n",
      "      â€¢ address\n",
      "      â€¢ date_of_birth\n",
      "\n",
      "   RESTRICTED:\n",
      "      â€¢ ssn\n",
      "      â€¢ credit_card\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ PII MASKING & HASHING - ProtecciÃ³n de Datos Sensibles\n",
      "================================================================================\n",
      "\n",
      "ğŸ­ Ejemplos de masking:\n",
      "\n",
      "   Usuario U03346:\n",
      "      Email:        user3346@zkrbt.com â†’ u***@zkrbt.com\n",
      "      Credit Card:  4553-6855-1989-4625 â†’ ****-****-****-4625\n",
      "      SSN:          592-66-1227 â†’ ***-**-1227\n",
      "      Phone:        +1-692-475-5356 â†’ +1-***-***-****\n",
      "      Full Name:    Vygazdc Dvsijiti â†’ 68e1c2797258b553 (hashed)\n",
      "\n",
      "   Usuario U02158:\n",
      "      Email:        user2158@wgiia.com â†’ u***@wgiia.com\n",
      "      Credit Card:  4330-4575-4369-4681 â†’ ****-****-****-4681\n",
      "      SSN:          835-45-5273 â†’ ***-**-5273\n",
      "      Phone:        +1-386-987-6203 â†’ +1-***-***-****\n",
      "      Full Name:    Ckdla Atetbuzsgp â†’ b740268590b9318d (hashed)\n",
      "\n",
      "   Usuario U03561:\n",
      "      Email:        user3561@clkap.com â†’ u***@clkap.com\n",
      "      Credit Card:  4537-4823-7439-1590 â†’ ****-****-****-1590\n",
      "      SSN:          976-54-2016 â†’ ***-**-2016\n",
      "      Phone:        +1-453-703-6185 â†’ +1-***-***-****\n",
      "      Full Name:    Fmmwbk Fnxckn â†’ c1d671c3812d5dee (hashed)\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ ACCESS AUDIT LOG - Registro de Accesos\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Audit Log generado:\n",
      "   â€¢ Total accesos: 10,000\n",
      "   â€¢ PerÃ­odo: 2025-11-09 â†’ 2025-12-09\n",
      "   â€¢ Accesos exitosos: 9,451 (94.5%)\n",
      "   â€¢ Accesos fallidos: 549\n",
      "\n",
      "ğŸ“Š Top 5 recursos mÃ¡s accedidos:\n",
      "   â€¢ s3_bucket_curated: 2,072 accesos (20.7%)\n",
      "   â€¢ users_table: 2,072 accesos (20.7%)\n",
      "   â€¢ transactions_table: 1,979 accesos (19.8%)\n",
      "   â€¢ analytics_dashboard: 1,964 accesos (19.6%)\n",
      "   â€¢ s3_bucket_raw: 1,913 accesos (19.1%)\n",
      "\n",
      "ğŸ‘¥ Accesos por rol:\n",
      "   â€¢ data-engineer: 4,029 (40.3%)\n",
      "   â€¢ data-scientist: 2,939 (29.4%)\n",
      "   â€¢ analyst: 2,029 (20.3%)\n",
      "   â€¢ admin: 517 (5.2%)\n",
      "   â€¢ external-partner: 486 (4.9%)\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ ANOMALY DETECTION - DetecciÃ³n de Accesos Sospechosos\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ AnomalÃ­as detectadas: 148 (1.48%)\n",
      "\n",
      "ğŸš¨ Top 5 anomalÃ­as crÃ­ticas:\n",
      "\n",
      "   [2025-11-25 02:17]\n",
      "   â””â”€ Usuario: U00417 (rol: data-scientist)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_raw\n",
      "   â””â”€ IP: 56.203.56.2\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-15 14:17]\n",
      "   â””â”€ Usuario: U00188 (rol: external-partner)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_raw\n",
      "   â””â”€ IP: 37.222.170.144\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-12 03:17]\n",
      "   â””â”€ Usuario: U00072 (rol: data-scientist)\n",
      "   â””â”€ AcciÃ³n: DELETE en s3_bucket_raw\n",
      "   â””â”€ IP: 31.11.57.107\n",
      "   â””â”€ Estado: âŒ Fallido\n",
      "\n",
      "   [2025-11-26 04:17]\n",
      "   â””â”€ Usuario: U00222 (rol: analyst)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_curated\n",
      "   â””â”€ IP: 71.233.84.166\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-10 02:17]\n",
      "   â””â”€ Usuario: U00014 (rol: data-engineer)\n",
      "   â””â”€ AcciÃ³n: EXPORT en users_table\n",
      "   â””â”€ IP: 189.61.180.218\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "ğŸŒ™ Accesos fuera de horario (22:00-06:00): 3,345 (33.5%)\n",
      "\n",
      "================================================================================\n",
      "5ï¸âƒ£ GDPR COMPLIANCE - Cumplimiento Normativo\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ Usuarios SIN consentimiento GDPR: 733 (14.7%)\n",
      "   â†’ AcciÃ³n requerida: Solicitar consentimiento o eliminar datos\n",
      "\n",
      "ğŸŒ Data Residency:\n",
      "   â€¢ US: 2,053 usuarios (41.1%)\n",
      "   â€¢ EU: 1,965 usuarios (39.3%)\n",
      "   â€¢ APAC: 982 usuarios (19.6%)\n",
      "\n",
      "ğŸš¨ VIOLACIÃ“N GDPR: 290 usuarios en EU sin consentimiento\n",
      "   â†’ Riesgo legal: Multa hasta 4% del revenue anual global\n",
      "   â†’ Usuarios afectados: ['U00000', 'U00030', 'U00045', 'U00059', 'U00072']\n",
      "\n",
      "ğŸ—‘ï¸ Solicitudes de 'Derecho al Olvido' (Ãºltimos 30 dÃ­as): 37\n",
      "   â†’ Tiempo promedio de procesamiento: 15 dÃ­as\n",
      "   â†’ SLA GDPR: 30 dÃ­as mÃ¡ximo\n",
      "\n",
      "================================================================================\n",
      "6ï¸âƒ£ ENCRYPTION STATUS - Estado del Cifrado\n",
      "================================================================================\n",
      "\n",
      "ğŸ”’ Estado del cifrado:\n",
      "\n",
      "   S3 Buckets:\n",
      "      âœ… raw-data:\n",
      "         â””â”€ KMS: aws/s3\n",
      "         â””â”€ RotaciÃ³n: auto\n",
      "      âœ… curated-data:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ RotaciÃ³n: 90 days\n",
      "      âœ… logs:\n",
      "         â””â”€ KMS: aws/s3\n",
      "         â””â”€ RotaciÃ³n: auto\n",
      "      âŒ temp-data:\n",
      "         â””â”€ âš ï¸ NO CIFRADO - Riesgo de compliance\n",
      "\n",
      "   Databases:\n",
      "      âœ… prod-rds:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ TLS: âœ…\n",
      "      âœ… analytics-redshift:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ TLS: âœ…\n",
      "      âŒ dev-postgres:\n",
      "         â””â”€ âš ï¸ NO CIFRADO - Riesgo de compliance\n",
      "\n",
      "================================================================================\n",
      "7ï¸âƒ£ SECURITY SCORE - PuntuaciÃ³n de Seguridad\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Componentes del Security Score:\n",
      "\n",
      "   IAM & Access Control (peso: 25%):\n",
      "      âœ… MFA enabled: 85%\n",
      "      âš ï¸ Least privilege: 70%\n",
      "      â†’ Score ponderado: 19.4/100\n",
      "\n",
      "   Encryption (peso: 20%):\n",
      "      âœ… At-rest coverage: 80%\n",
      "      âœ… In-transit (TLS): 90%\n",
      "      â†’ Score ponderado: 17.0/100\n",
      "\n",
      "   PII Protection (peso: 20%):\n",
      "      âœ… Masking implemented: 100%\n",
      "      âœ… Data classification: 100%\n",
      "      â†’ Score ponderado: 20.0/100\n",
      "\n",
      "   Audit & Monitoring (peso: 15%):\n",
      "      âœ… Access logs: 100%\n",
      "      âš ï¸ Anomaly detection: 75%\n",
      "      â†’ Score ponderado: 13.1/100\n",
      "\n",
      "   Compliance (peso: 20%):\n",
      "      âœ… GDPR consent: 85%\n",
      "      âœ… Data residency: 100%\n",
      "      â†’ Score ponderado: 18.5/100\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ SECURITY SCORE TOTAL: 88.0/100\n",
      "   CalificaciÃ³n: B (Bueno)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Audit Log generado:\n",
      "   â€¢ Total accesos: 10,000\n",
      "   â€¢ PerÃ­odo: 2025-11-09 â†’ 2025-12-09\n",
      "   â€¢ Accesos exitosos: 9,451 (94.5%)\n",
      "   â€¢ Accesos fallidos: 549\n",
      "\n",
      "ğŸ“Š Top 5 recursos mÃ¡s accedidos:\n",
      "   â€¢ s3_bucket_curated: 2,072 accesos (20.7%)\n",
      "   â€¢ users_table: 2,072 accesos (20.7%)\n",
      "   â€¢ transactions_table: 1,979 accesos (19.8%)\n",
      "   â€¢ analytics_dashboard: 1,964 accesos (19.6%)\n",
      "   â€¢ s3_bucket_raw: 1,913 accesos (19.1%)\n",
      "\n",
      "ğŸ‘¥ Accesos por rol:\n",
      "   â€¢ data-engineer: 4,029 (40.3%)\n",
      "   â€¢ data-scientist: 2,939 (29.4%)\n",
      "   â€¢ analyst: 2,029 (20.3%)\n",
      "   â€¢ admin: 517 (5.2%)\n",
      "   â€¢ external-partner: 486 (4.9%)\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ ANOMALY DETECTION - DetecciÃ³n de Accesos Sospechosos\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ AnomalÃ­as detectadas: 148 (1.48%)\n",
      "\n",
      "ğŸš¨ Top 5 anomalÃ­as crÃ­ticas:\n",
      "\n",
      "   [2025-11-25 02:17]\n",
      "   â””â”€ Usuario: U00417 (rol: data-scientist)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_raw\n",
      "   â””â”€ IP: 56.203.56.2\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-15 14:17]\n",
      "   â””â”€ Usuario: U00188 (rol: external-partner)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_raw\n",
      "   â””â”€ IP: 37.222.170.144\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-12 03:17]\n",
      "   â””â”€ Usuario: U00072 (rol: data-scientist)\n",
      "   â””â”€ AcciÃ³n: DELETE en s3_bucket_raw\n",
      "   â””â”€ IP: 31.11.57.107\n",
      "   â””â”€ Estado: âŒ Fallido\n",
      "\n",
      "   [2025-11-26 04:17]\n",
      "   â””â”€ Usuario: U00222 (rol: analyst)\n",
      "   â””â”€ AcciÃ³n: EXPORT en s3_bucket_curated\n",
      "   â””â”€ IP: 71.233.84.166\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "   [2025-11-10 02:17]\n",
      "   â””â”€ Usuario: U00014 (rol: data-engineer)\n",
      "   â””â”€ AcciÃ³n: EXPORT en users_table\n",
      "   â””â”€ IP: 189.61.180.218\n",
      "   â””â”€ Estado: âœ… Exitoso\n",
      "\n",
      "ğŸŒ™ Accesos fuera de horario (22:00-06:00): 3,345 (33.5%)\n",
      "\n",
      "================================================================================\n",
      "5ï¸âƒ£ GDPR COMPLIANCE - Cumplimiento Normativo\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ Usuarios SIN consentimiento GDPR: 733 (14.7%)\n",
      "   â†’ AcciÃ³n requerida: Solicitar consentimiento o eliminar datos\n",
      "\n",
      "ğŸŒ Data Residency:\n",
      "   â€¢ US: 2,053 usuarios (41.1%)\n",
      "   â€¢ EU: 1,965 usuarios (39.3%)\n",
      "   â€¢ APAC: 982 usuarios (19.6%)\n",
      "\n",
      "ğŸš¨ VIOLACIÃ“N GDPR: 290 usuarios en EU sin consentimiento\n",
      "   â†’ Riesgo legal: Multa hasta 4% del revenue anual global\n",
      "   â†’ Usuarios afectados: ['U00000', 'U00030', 'U00045', 'U00059', 'U00072']\n",
      "\n",
      "ğŸ—‘ï¸ Solicitudes de 'Derecho al Olvido' (Ãºltimos 30 dÃ­as): 37\n",
      "   â†’ Tiempo promedio de procesamiento: 15 dÃ­as\n",
      "   â†’ SLA GDPR: 30 dÃ­as mÃ¡ximo\n",
      "\n",
      "================================================================================\n",
      "6ï¸âƒ£ ENCRYPTION STATUS - Estado del Cifrado\n",
      "================================================================================\n",
      "\n",
      "ğŸ”’ Estado del cifrado:\n",
      "\n",
      "   S3 Buckets:\n",
      "      âœ… raw-data:\n",
      "         â””â”€ KMS: aws/s3\n",
      "         â””â”€ RotaciÃ³n: auto\n",
      "      âœ… curated-data:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ RotaciÃ³n: 90 days\n",
      "      âœ… logs:\n",
      "         â””â”€ KMS: aws/s3\n",
      "         â””â”€ RotaciÃ³n: auto\n",
      "      âŒ temp-data:\n",
      "         â””â”€ âš ï¸ NO CIFRADO - Riesgo de compliance\n",
      "\n",
      "   Databases:\n",
      "      âœ… prod-rds:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ TLS: âœ…\n",
      "      âœ… analytics-redshift:\n",
      "         â””â”€ KMS: customer-managed\n",
      "         â””â”€ TLS: âœ…\n",
      "      âŒ dev-postgres:\n",
      "         â””â”€ âš ï¸ NO CIFRADO - Riesgo de compliance\n",
      "\n",
      "================================================================================\n",
      "7ï¸âƒ£ SECURITY SCORE - PuntuaciÃ³n de Seguridad\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Componentes del Security Score:\n",
      "\n",
      "   IAM & Access Control (peso: 25%):\n",
      "      âœ… MFA enabled: 85%\n",
      "      âš ï¸ Least privilege: 70%\n",
      "      â†’ Score ponderado: 19.4/100\n",
      "\n",
      "   Encryption (peso: 20%):\n",
      "      âœ… At-rest coverage: 80%\n",
      "      âœ… In-transit (TLS): 90%\n",
      "      â†’ Score ponderado: 17.0/100\n",
      "\n",
      "   PII Protection (peso: 20%):\n",
      "      âœ… Masking implemented: 100%\n",
      "      âœ… Data classification: 100%\n",
      "      â†’ Score ponderado: 20.0/100\n",
      "\n",
      "   Audit & Monitoring (peso: 15%):\n",
      "      âœ… Access logs: 100%\n",
      "      âš ï¸ Anomaly detection: 75%\n",
      "      â†’ Score ponderado: 13.1/100\n",
      "\n",
      "   Compliance (peso: 20%):\n",
      "      âœ… GDPR consent: 85%\n",
      "      âœ… Data residency: 100%\n",
      "      â†’ Score ponderado: 18.5/100\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ SECURITY SCORE TOTAL: 88.0/100\n",
      "   CalificaciÃ³n: B (Bueno)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” SimulaciÃ³n: AnÃ¡lisis de Seguridad y Compliance (Data Platform)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” SECURITY & COMPLIANCE AUDIT SIMULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= DATASET CON PII =============\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Generar dataset con informaciÃ³n sensible\n",
    "users = []\n",
    "for i in range(5000):\n",
    "    users.append({\n",
    "        'user_id': f'U{i:05d}',\n",
    "        'email': f'user{i}@{\"\".join(np.random.choice(list(\"abcdefghijklmnopqrstuvwxyz\"), 5))}.com',\n",
    "        'full_name': f'{\"\".join(np.random.choice(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"), 1))}'\n",
    "                     f'{\"\".join(np.random.choice(list(\"abcdefghijklmnopqrstuvwxyz\"), np.random.randint(4, 8)))} '\n",
    "                     f'{\"\".join(np.random.choice(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"), 1))}'\n",
    "                     f'{\"\".join(np.random.choice(list(\"abcdefghijklmnopqrstuvwxyz\"), np.random.randint(5, 10)))}',\n",
    "        'ssn': f'{np.random.randint(100, 999)}-{np.random.randint(10, 99)}-{np.random.randint(1000, 9999)}',\n",
    "        'credit_card': f'{np.random.randint(4000, 4999)}-{np.random.randint(1000, 9999)}-{np.random.randint(1000, 9999)}-{np.random.randint(1000, 9999)}',\n",
    "        'phone': f'+1-{np.random.randint(200, 999)}-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}',\n",
    "        'address': f'{np.random.randint(100, 9999)} Main St, City, State {np.random.randint(10000, 99999)}',\n",
    "        'date_of_birth': (datetime(1950, 1, 1) + timedelta(days=np.random.randint(0, 25000))).strftime('%Y-%m-%d'),\n",
    "        'salary': np.random.randint(30000, 250000),\n",
    "        'department': np.random.choice(['Engineering', 'Sales', 'HR', 'Finance', 'Marketing']),\n",
    "        'consent_gdpr': np.random.choice([True, False], p=[0.85, 0.15]),\n",
    "        'data_residency': np.random.choice(['EU', 'US', 'APAC'], p=[0.4, 0.4, 0.2]),\n",
    "        'created_at': now - timedelta(days=np.random.randint(0, 730))\n",
    "    })\n",
    "\n",
    "df_users = pd.DataFrame(users)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset original (CON PII sensible):\")\n",
    "print(f\"   â€¢ Total usuarios: {len(df_users):,}\")\n",
    "print(f\"   â€¢ Columnas con PII: email, full_name, ssn, credit_card, phone, address, date_of_birth\")\n",
    "print(f\"   â€¢ Usuarios con consentimiento GDPR: {df_users['consent_gdpr'].sum():,} ({df_users['consent_gdpr'].mean()*100:.1f}%)\")\n",
    "\n",
    "# ============= 1. CLASIFICACIÃ“N DE DATOS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1ï¸âƒ£ DATA CLASSIFICATION - ClasificaciÃ³n por Sensibilidad\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_classification = {\n",
    "    'PUBLIC': ['user_id', 'department', 'data_residency'],\n",
    "    'INTERNAL': ['email', 'full_name', 'phone', 'created_at', 'consent_gdpr'],\n",
    "    'CONFIDENTIAL': ['salary', 'address', 'date_of_birth'],\n",
    "    'RESTRICTED': ['ssn', 'credit_card']\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ” ClasificaciÃ³n de campos:\")\n",
    "for level, fields in data_classification.items():\n",
    "    print(f\"\\n   {level}:\")\n",
    "    for field in fields:\n",
    "        print(f\"      â€¢ {field}\")\n",
    "\n",
    "# ============= 2. PII MASKING & HASHING =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2ï¸âƒ£ PII MASKING & HASHING - ProtecciÃ³n de Datos Sensibles\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def mask_email(email):\n",
    "    \"\"\"Enmascara email manteniendo primer caracter y dominio\"\"\"\n",
    "    if '@' not in email:\n",
    "        return email\n",
    "    user, domain = email.split('@')\n",
    "    return f'{user[0]}***@{domain}'\n",
    "\n",
    "def mask_credit_card(cc):\n",
    "    \"\"\"Muestra solo Ãºltimos 4 dÃ­gitos\"\"\"\n",
    "    return f'****-****-****-{cc.split(\"-\")[-1]}'\n",
    "\n",
    "def mask_ssn(ssn):\n",
    "    \"\"\"Muestra solo Ãºltimos 4 dÃ­gitos\"\"\"\n",
    "    return f'***-**-{ssn.split(\"-\")[-1]}'\n",
    "\n",
    "def mask_phone(phone):\n",
    "    \"\"\"Muestra solo cÃ³digo de Ã¡rea\"\"\"\n",
    "    parts = phone.split('-')\n",
    "    return f'{parts[0]}-***-***-****' if len(parts) == 4 else '***'\n",
    "\n",
    "def hash_pii(value):\n",
    "    \"\"\"Hash SHA256 para anonimizaciÃ³n irreversible\"\"\"\n",
    "    return hashlib.sha256(str(value).encode()).hexdigest()[:16]\n",
    "\n",
    "# Aplicar masking\n",
    "df_masked = df_users.copy()\n",
    "df_masked['email_masked'] = df_masked['email'].apply(mask_email)\n",
    "df_masked['credit_card_masked'] = df_masked['credit_card'].apply(mask_credit_card)\n",
    "df_masked['ssn_masked'] = df_masked['ssn'].apply(mask_ssn)\n",
    "df_masked['phone_masked'] = df_masked['phone'].apply(mask_phone)\n",
    "df_masked['full_name_hashed'] = df_masked['full_name'].apply(hash_pii)\n",
    "\n",
    "print(\"\\nğŸ­ Ejemplos de masking:\")\n",
    "sample = df_users.sample(3)\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"\\n   Usuario {row['user_id']}:\")\n",
    "    print(f\"      Email:        {row['email']} â†’ {mask_email(row['email'])}\")\n",
    "    print(f\"      Credit Card:  {row['credit_card']} â†’ {mask_credit_card(row['credit_card'])}\")\n",
    "    print(f\"      SSN:          {row['ssn']} â†’ {mask_ssn(row['ssn'])}\")\n",
    "    print(f\"      Phone:        {row['phone']} â†’ {mask_phone(row['phone'])}\")\n",
    "    print(f\"      Full Name:    {row['full_name']} â†’ {hash_pii(row['full_name'])} (hashed)\")\n",
    "\n",
    "# ============= 3. ACCESS AUDIT LOG =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3ï¸âƒ£ ACCESS AUDIT LOG - Registro de Accesos\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular logs de acceso\n",
    "access_logs = []\n",
    "roles = ['data-engineer', 'data-scientist', 'analyst', 'admin', 'external-partner']\n",
    "actions = ['READ', 'WRITE', 'DELETE', 'EXPORT']\n",
    "resources = ['users_table', 'transactions_table', 'analytics_dashboard', 's3_bucket_raw', 's3_bucket_curated']\n",
    "\n",
    "for i in range(10000):\n",
    "    log_time = now - timedelta(days=np.random.randint(0, 30), \n",
    "                                hours=np.random.randint(0, 24))\n",
    "    \n",
    "    role = np.random.choice(roles, p=[0.4, 0.3, 0.2, 0.05, 0.05])\n",
    "    action = np.random.choice(actions, p=[0.7, 0.2, 0.05, 0.05])\n",
    "    resource = np.random.choice(resources)\n",
    "    \n",
    "    # Generar anomalÃ­as (accesos sospechosos)\n",
    "    is_anomaly = False\n",
    "    if role == 'external-partner' and action in ['DELETE', 'EXPORT']:\n",
    "        is_anomaly = True\n",
    "    elif log_time.hour < 6 or log_time.hour > 22:  # Fuera de horario\n",
    "        if action in ['DELETE', 'EXPORT'] and np.random.random() > 0.7:\n",
    "            is_anomaly = True\n",
    "    \n",
    "    access_logs.append({\n",
    "        'timestamp': log_time,\n",
    "        'user_id': f'U{np.random.randint(0, 500):05d}',\n",
    "        'role': role,\n",
    "        'action': action,\n",
    "        'resource': resource,\n",
    "        'ip_address': f'{np.random.randint(10, 192)}.{np.random.randint(0, 255)}.{np.random.randint(0, 255)}.{np.random.randint(1, 254)}',\n",
    "        'success': np.random.choice([True, False], p=[0.95, 0.05]),\n",
    "        'is_anomaly': is_anomaly\n",
    "    })\n",
    "\n",
    "df_access = pd.DataFrame(access_logs)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Audit Log generado:\")\n",
    "print(f\"   â€¢ Total accesos: {len(df_access):,}\")\n",
    "print(f\"   â€¢ PerÃ­odo: {df_access['timestamp'].min().date()} â†’ {df_access['timestamp'].max().date()}\")\n",
    "print(f\"   â€¢ Accesos exitosos: {df_access['success'].sum():,} ({df_access['success'].mean()*100:.1f}%)\")\n",
    "print(f\"   â€¢ Accesos fallidos: {(~df_access['success']).sum():,}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 5 recursos mÃ¡s accedidos:\")\n",
    "top_resources = df_access['resource'].value_counts().head(5)\n",
    "for resource, count in top_resources.items():\n",
    "    pct = count / len(df_access) * 100\n",
    "    print(f\"   â€¢ {resource}: {count:,} accesos ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ‘¥ Accesos por rol:\")\n",
    "by_role = df_access.groupby('role').size().sort_values(ascending=False)\n",
    "for role, count in by_role.items():\n",
    "    pct = count / len(df_access) * 100\n",
    "    print(f\"   â€¢ {role}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# ============= 4. ANOMALY DETECTION =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4ï¸âƒ£ ANOMALY DETECTION - DetecciÃ³n de Accesos Sospechosos\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anomalies = df_access[df_access['is_anomaly'] == True]\n",
    "\n",
    "print(f\"\\nâš ï¸ AnomalÃ­as detectadas: {len(anomalies):,} ({len(anomalies)/len(df_access)*100:.2f}%)\")\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(f\"\\nğŸš¨ Top 5 anomalÃ­as crÃ­ticas:\")\n",
    "    critical = anomalies[anomalies['action'].isin(['DELETE', 'EXPORT'])].head(5)\n",
    "    for idx, row in critical.iterrows():\n",
    "        print(f\"\\n   [{row['timestamp'].strftime('%Y-%m-%d %H:%M')}]\")\n",
    "        print(f\"   â””â”€ Usuario: {row['user_id']} (rol: {row['role']})\")\n",
    "        print(f\"   â””â”€ AcciÃ³n: {row['action']} en {row['resource']}\")\n",
    "        print(f\"   â””â”€ IP: {row['ip_address']}\")\n",
    "        print(f\"   â””â”€ Estado: {'âœ… Exitoso' if row['success'] else 'âŒ Fallido'}\")\n",
    "\n",
    "# Accesos fuera de horario\n",
    "off_hours = df_access[df_access['timestamp'].dt.hour.isin(range(0, 6)) | df_access['timestamp'].dt.hour.isin(range(22, 24))]\n",
    "print(f\"\\nğŸŒ™ Accesos fuera de horario (22:00-06:00): {len(off_hours):,} ({len(off_hours)/len(df_access)*100:.1f}%)\")\n",
    "\n",
    "# ============= 5. GDPR COMPLIANCE =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5ï¸âƒ£ GDPR COMPLIANCE - Cumplimiento Normativo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Usuarios sin consentimiento\n",
    "no_consent = df_users[df_users['consent_gdpr'] == False]\n",
    "print(f\"\\nâš ï¸ Usuarios SIN consentimiento GDPR: {len(no_consent):,} ({len(no_consent)/len(df_users)*100:.1f}%)\")\n",
    "print(f\"   â†’ AcciÃ³n requerida: Solicitar consentimiento o eliminar datos\")\n",
    "\n",
    "# Data residency\n",
    "print(f\"\\nğŸŒ Data Residency:\")\n",
    "residency = df_users['data_residency'].value_counts()\n",
    "for region, count in residency.items():\n",
    "    pct = count / len(df_users) * 100\n",
    "    print(f\"   â€¢ {region}: {count:,} usuarios ({pct:.1f}%)\")\n",
    "\n",
    "# Usuarios EU sin consentimiento (violaciÃ³n GDPR)\n",
    "eu_no_consent = df_users[(df_users['data_residency'] == 'EU') & (df_users['consent_gdpr'] == False)]\n",
    "print(f\"\\nğŸš¨ VIOLACIÃ“N GDPR: {len(eu_no_consent):,} usuarios en EU sin consentimiento\")\n",
    "if len(eu_no_consent) > 0:\n",
    "    print(f\"   â†’ Riesgo legal: Multa hasta 4% del revenue anual global\")\n",
    "    print(f\"   â†’ Usuarios afectados: {eu_no_consent['user_id'].head(5).tolist()}\")\n",
    "\n",
    "# Derecho al olvido (Right to be Forgotten)\n",
    "rtf_requests = np.random.randint(10, 50)\n",
    "print(f\"\\nğŸ—‘ï¸ Solicitudes de 'Derecho al Olvido' (Ãºltimos 30 dÃ­as): {rtf_requests}\")\n",
    "print(f\"   â†’ Tiempo promedio de procesamiento: 15 dÃ­as\")\n",
    "print(f\"   â†’ SLA GDPR: 30 dÃ­as mÃ¡ximo\")\n",
    "\n",
    "# ============= 6. ENCRYPTION STATUS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6ï¸âƒ£ ENCRYPTION STATUS - Estado del Cifrado\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "encryption_status = {\n",
    "    'S3 Buckets': {\n",
    "        'raw-data': {'encrypted': True, 'kms_key': 'aws/s3', 'rotation': 'auto'},\n",
    "        'curated-data': {'encrypted': True, 'kms_key': 'customer-managed', 'rotation': '90 days'},\n",
    "        'logs': {'encrypted': True, 'kms_key': 'aws/s3', 'rotation': 'auto'},\n",
    "        'temp-data': {'encrypted': False, 'kms_key': None, 'rotation': None}  # âš ï¸ No cifrado\n",
    "    },\n",
    "    'Databases': {\n",
    "        'prod-rds': {'encrypted': True, 'kms_key': 'customer-managed', 'tls': True},\n",
    "        'analytics-redshift': {'encrypted': True, 'kms_key': 'customer-managed', 'tls': True},\n",
    "        'dev-postgres': {'encrypted': False, 'kms_key': None, 'tls': False}  # âš ï¸ No cifrado\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ”’ Estado del cifrado:\")\n",
    "for category, resources in encryption_status.items():\n",
    "    print(f\"\\n   {category}:\")\n",
    "    for resource, config in resources.items():\n",
    "        status = 'âœ…' if config['encrypted'] else 'âŒ'\n",
    "        print(f\"      {status} {resource}:\")\n",
    "        if config['encrypted']:\n",
    "            print(f\"         â””â”€ KMS: {config['kms_key']}\")\n",
    "            if category == 'S3 Buckets':\n",
    "                print(f\"         â””â”€ RotaciÃ³n: {config['rotation']}\")\n",
    "            else:\n",
    "                print(f\"         â””â”€ TLS: {'âœ…' if config['tls'] else 'âŒ'}\")\n",
    "        else:\n",
    "            print(f\"         â””â”€ âš ï¸ NO CIFRADO - Riesgo de compliance\")\n",
    "\n",
    "# ============= 7. SECURITY SCORE =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"7ï¸âƒ£ SECURITY SCORE - PuntuaciÃ³n de Seguridad\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular score\n",
    "score_components = {\n",
    "    'IAM & Access Control': {\n",
    "        'MFA enabled': 85,  # 85% de usuarios con MFA\n",
    "        'Least privilege': 70,  # 70% de roles siguen principio\n",
    "        'Weight': 0.25\n",
    "    },\n",
    "    'Encryption': {\n",
    "        'At-rest coverage': 80,  # 80% de recursos cifrados\n",
    "        'In-transit (TLS)': 90,  # 90% usa TLS\n",
    "        'Weight': 0.20\n",
    "    },\n",
    "    'PII Protection': {\n",
    "        'Masking implemented': 100,  # Masking implementado\n",
    "        'Data classification': 100,  # ClasificaciÃ³n completa\n",
    "        'Weight': 0.20\n",
    "    },\n",
    "    'Audit & Monitoring': {\n",
    "        'Access logs': 100,  # Logs completos\n",
    "        'Anomaly detection': 75,  # 75% de anomalÃ­as detectadas\n",
    "        'Weight': 0.15\n",
    "    },\n",
    "    'Compliance': {\n",
    "        'GDPR consent': 85,  # 85% con consentimiento\n",
    "        'Data residency': 100,  # Correctamente ubicado\n",
    "        'Weight': 0.20\n",
    "    }\n",
    "}\n",
    "\n",
    "total_score = 0\n",
    "print(\"\\nğŸ“Š Componentes del Security Score:\\n\")\n",
    "for component, metrics in score_components.items():\n",
    "    weight = metrics.pop('Weight')\n",
    "    avg_score = sum(metrics.values()) / len(metrics)\n",
    "    weighted_score = avg_score * weight\n",
    "    total_score += weighted_score\n",
    "    \n",
    "    print(f\"   {component} (peso: {weight*100:.0f}%):\")\n",
    "    for metric, score in metrics.items():\n",
    "        status = 'âœ…' if score >= 80 else ('âš ï¸' if score >= 60 else 'âŒ')\n",
    "        print(f\"      {status} {metric}: {score}%\")\n",
    "    print(f\"      â†’ Score ponderado: {weighted_score:.1f}/100\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ğŸ¯ SECURITY SCORE TOTAL: {total_score:.1f}/100\")\n",
    "if total_score >= 90:\n",
    "    grade = 'A (Excelente)'\n",
    "elif total_score >= 80:\n",
    "    grade = 'B (Bueno)'\n",
    "elif total_score >= 70:\n",
    "    grade = 'C (Aceptable)'\n",
    "else:\n",
    "    grade = 'D (Requiere mejoras urgentes)'\n",
    "print(f\"   CalificaciÃ³n: {grade}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a6831",
   "metadata": {},
   "source": [
    "### ğŸ” **Encryption: At-Rest, In-Transit y Application-Level**\n",
    "\n",
    "**Encryption Hierarchy**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. AT-REST ENCRYPTION                               â”‚\n",
    "â”‚     â€¢ Storage: S3 (SSE-KMS), EBS, RDS                â”‚\n",
    "â”‚     â€¢ Databases: TDE (Transparent Data Encryption)   â”‚\n",
    "â”‚     â€¢ Backups: Encrypted snapshots                   â”‚\n",
    "â”‚     â€¢ Key Management: KMS, CloudHSM                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  2. IN-TRANSIT ENCRYPTION                            â”‚\n",
    "â”‚     â€¢ TLS 1.3 (HTTPS, gRPC)                          â”‚\n",
    "â”‚     â€¢ mTLS (mutual TLS) para service-to-service      â”‚\n",
    "â”‚     â€¢ VPN/PrivateLink para inter-VPC                 â”‚\n",
    "â”‚     â€¢ IPSec para site-to-site                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  3. APPLICATION-LEVEL ENCRYPTION                     â”‚\n",
    "â”‚     â€¢ Field-level: Encrypt specific columns (PII)    â”‚\n",
    "â”‚     â€¢ Envelope encryption (DEK + KEK)                â”‚\n",
    "â”‚     â€¢ Client-side encryption antes de upload         â”‚\n",
    "â”‚     â€¢ Tokenization (irreversible)                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**AWS KMS: Customer-Managed Keys**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "kms = boto3.client('kms')\n",
    "\n",
    "# 1. CREATE CUSTOMER-MANAGED KEY (CMK)\n",
    "def create_data_encryption_key():\n",
    "    \"\"\"Create KMS key con rotaciÃ³n automÃ¡tica\"\"\"\n",
    "    \n",
    "    response = kms.create_key(\n",
    "        Description='Data Lake encryption key',\n",
    "        KeyUsage='ENCRYPT_DECRYPT',\n",
    "        Origin='AWS_KMS',\n",
    "        MultiRegion=False,\n",
    "        Tags=[\n",
    "            {'TagKey': 'Environment', 'TagValue': 'Production'},\n",
    "            {'TagKey': 'Purpose', 'TagValue': 'DataLake'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    key_id = response['KeyMetadata']['KeyId']\n",
    "    \n",
    "    # Create alias\n",
    "    kms.create_alias(\n",
    "        AliasName='alias/data-lake-key',\n",
    "        TargetKeyId=key_id\n",
    "    )\n",
    "    \n",
    "    # Enable automatic key rotation (365 dÃ­as)\n",
    "    kms.enable_key_rotation(KeyId=key_id)\n",
    "    \n",
    "    # Set key policy\n",
    "    key_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"Enable IAM User Permissions\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": \"arn:aws:iam::123456789012:root\"\n",
    "                },\n",
    "                \"Action\": \"kms:*\",\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"Allow use of the key for encryption\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": [\n",
    "                        \"arn:aws:iam::123456789012:role/DataEngineerRole\",\n",
    "                        \"arn:aws:iam::123456789012:role/EMRServiceRole\"\n",
    "                    ]\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"kms:Encrypt\",\n",
    "                    \"kms:Decrypt\",\n",
    "                    \"kms:GenerateDataKey\",\n",
    "                    \"kms:DescribeKey\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"Allow CloudWatch Logs\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"logs.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"kms:Encrypt\",\n",
    "                    \"kms:Decrypt\",\n",
    "                    \"kms:GenerateDataKey\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    kms.put_key_policy(\n",
    "        KeyId=key_id,\n",
    "        PolicyName='default',\n",
    "        Policy=json.dumps(key_policy)\n",
    "    )\n",
    "    \n",
    "    return key_id\n",
    "\n",
    "# 2. S3 ENCRYPTION (SSE-KMS)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_encrypted_file(bucket, key, data, kms_key_id):\n",
    "    \"\"\"Upload file con KMS encryption\"\"\"\n",
    "    \n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=data,\n",
    "        ServerSideEncryption='aws:kms',\n",
    "        SSEKMSKeyId=kms_key_id,\n",
    "        BucketKeyEnabled=True  # Reduce KMS API calls (cost)\n",
    "    )\n",
    "\n",
    "# Enforce encryption on bucket\n",
    "def enforce_bucket_encryption(bucket_name, kms_key_id):\n",
    "    \"\"\"Bucket policy: deny unencrypted uploads\"\"\"\n",
    "    \n",
    "    bucket_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"DenyUnencryptedObjectUploads\",\n",
    "                \"Effect\": \"Deny\",\n",
    "                \"Principal\": \"*\",\n",
    "                \"Action\": \"s3:PutObject\",\n",
    "                \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringNotEquals\": {\n",
    "                        \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"DenyIncorrectKMSKey\",\n",
    "                \"Effect\": \"Deny\",\n",
    "                \"Principal\": \"*\",\n",
    "                \"Action\": \"s3:PutObject\",\n",
    "                \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringNotEquals\": {\n",
    "                        \"s3:x-amz-server-side-encryption-aws-kms-key-id\": kms_key_id\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    s3.put_bucket_policy(\n",
    "        Bucket=bucket_name,\n",
    "        Policy=json.dumps(bucket_policy)\n",
    "    )\n",
    "\n",
    "# 3. RDS ENCRYPTION\n",
    "rds = boto3.client('rds')\n",
    "\n",
    "def create_encrypted_rds_instance():\n",
    "    \"\"\"Create RDS con encryption at-rest\"\"\"\n",
    "    \n",
    "    response = rds.create_db_instance(\n",
    "        DBInstanceIdentifier='data-warehouse',\n",
    "        DBInstanceClass='db.r5.2xlarge',\n",
    "        Engine='postgres',\n",
    "        MasterUsername='admin',\n",
    "        MasterUserPassword='SecurePassword123!',\n",
    "        AllocatedStorage=1000,\n",
    "        StorageType='gp3',\n",
    "        StorageEncrypted=True,  # Enable encryption\n",
    "        KmsKeyId='arn:aws:kms:us-east-1:123456789012:key/data-lake-key',\n",
    "        BackupRetentionPeriod=30,\n",
    "        CopyTagsToSnapshot=True,\n",
    "        EnableCloudwatchLogsExports=['postgresql'],\n",
    "        Tags=[\n",
    "            {'Key': 'Encrypted', 'Value': 'true'}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# 4. ENVELOPE ENCRYPTION (Data Encryption Key)\n",
    "def encrypt_large_file_with_envelope(file_path, kms_key_id):\n",
    "    \"\"\"Encrypt file usando envelope encryption pattern\"\"\"\n",
    "    \n",
    "    # Generate Data Encryption Key (DEK)\n",
    "    dek_response = kms.generate_data_key(\n",
    "        KeyId=kms_key_id,\n",
    "        KeySpec='AES_256'\n",
    "    )\n",
    "    \n",
    "    plaintext_dek = dek_response['Plaintext']  # Use this to encrypt\n",
    "    encrypted_dek = dek_response['CiphertextBlob']  # Store this\n",
    "    \n",
    "    # Encrypt file with DEK\n",
    "    cipher = Fernet(base64.urlsafe_b64encode(plaintext_dek[:32]))\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        plaintext_data = f.read()\n",
    "    \n",
    "    encrypted_data = cipher.encrypt(plaintext_data)\n",
    "    \n",
    "    # Upload encrypted file + encrypted DEK\n",
    "    s3.put_object(\n",
    "        Bucket='data-lake',\n",
    "        Key=f'encrypted/{file_path}',\n",
    "        Body=encrypted_data,\n",
    "        Metadata={\n",
    "            'x-amz-key': base64.b64encode(encrypted_dek).decode('utf-8')\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return encrypted_data\n",
    "\n",
    "def decrypt_envelope_encrypted_file(s3_key):\n",
    "    \"\"\"Decrypt file encrypted con envelope pattern\"\"\"\n",
    "    \n",
    "    # Get encrypted file + metadata\n",
    "    response = s3.get_object(Bucket='data-lake', Key=s3_key)\n",
    "    encrypted_data = response['Body'].read()\n",
    "    encrypted_dek = base64.b64decode(response['Metadata']['x-amz-key'])\n",
    "    \n",
    "    # Decrypt DEK usando KMS\n",
    "    dek_response = kms.decrypt(CiphertextBlob=encrypted_dek)\n",
    "    plaintext_dek = dek_response['Plaintext']\n",
    "    \n",
    "    # Decrypt file data usando DEK\n",
    "    cipher = Fernet(base64.urlsafe_b64encode(plaintext_dek[:32]))\n",
    "    plaintext_data = cipher.decrypt(encrypted_data)\n",
    "    \n",
    "    return plaintext_data\n",
    "```\n",
    "\n",
    "**TLS/mTLS Configuration**\n",
    "\n",
    "```python\n",
    "# FastAPI con TLS (HTTPS)\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Run with TLS\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8443,\n",
    "        ssl_keyfile=\"/path/to/private.key\",\n",
    "        ssl_certfile=\"/path/to/certificate.crt\",\n",
    "        ssl_ca_certs=\"/path/to/ca-bundle.crt\",  # For mTLS\n",
    "        ssl_cert_reqs=2  # CERT_REQUIRED (enforce client certs)\n",
    "    )\n",
    "\n",
    "# Nginx reverse proxy con TLS termination\n",
    "\"\"\"\n",
    "server {\n",
    "    listen 443 ssl http2;\n",
    "    server_name api.example.com;\n",
    "    \n",
    "    # TLS 1.3 only\n",
    "    ssl_protocols TLSv1.3;\n",
    "    ssl_certificate /etc/nginx/ssl/certificate.crt;\n",
    "    ssl_certificate_key /etc/nginx/ssl/private.key;\n",
    "    \n",
    "    # Modern cipher suite\n",
    "    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';\n",
    "    ssl_prefer_server_ciphers on;\n",
    "    \n",
    "    # HSTS (force HTTPS)\n",
    "    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n",
    "    \n",
    "    # OCSP Stapling\n",
    "    ssl_stapling on;\n",
    "    ssl_stapling_verify on;\n",
    "    \n",
    "    location / {\n",
    "        proxy_pass http://backend:8000;\n",
    "        proxy_set_header X-Forwarded-Proto https;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Python requests con client certificate (mTLS)\n",
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    'https://api.example.com/data',\n",
    "    cert=('/path/to/client.crt', '/path/to/client.key'),\n",
    "    verify='/path/to/ca-bundle.crt'  # Verify server cert\n",
    ")\n",
    "```\n",
    "\n",
    "**Field-Level Encryption (PII Protection)**\n",
    "\n",
    "```python\n",
    "from cryptography.fernet import Fernet\n",
    "import hashlib\n",
    "import hmac\n",
    "\n",
    "class PIIEncryptor:\n",
    "    \"\"\"Encrypt/decrypt PII fields\"\"\"\n",
    "    \n",
    "    def __init__(self, encryption_key):\n",
    "        self.cipher = Fernet(encryption_key)\n",
    "    \n",
    "    def encrypt_field(self, plaintext: str) -> str:\n",
    "        \"\"\"Encrypt single field\"\"\"\n",
    "        return self.cipher.encrypt(plaintext.encode()).decode()\n",
    "    \n",
    "    def decrypt_field(self, ciphertext: str) -> str:\n",
    "        \"\"\"Decrypt single field\"\"\"\n",
    "        return self.cipher.decrypt(ciphertext.encode()).decode()\n",
    "    \n",
    "    def tokenize(self, value: str, secret: str) -> str:\n",
    "        \"\"\"One-way tokenization (irreversible)\"\"\"\n",
    "        return hmac.new(\n",
    "            secret.encode(),\n",
    "            value.encode(),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "\n",
    "# Spark UDF para field-level encryption\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "encryptor = PIIEncryptor(encryption_key=b'your-32-byte-key-here...')\n",
    "\n",
    "def encrypt_pii_udf(value):\n",
    "    if value:\n",
    "        return encryptor.encrypt_field(value)\n",
    "    return None\n",
    "\n",
    "encrypt_udf = F.udf(encrypt_pii_udf, StringType())\n",
    "\n",
    "# Apply to DataFrame\n",
    "df_encrypted = df.withColumn(\n",
    "    'email_encrypted',\n",
    "    encrypt_udf(F.col('email'))\n",
    ").withColumn(\n",
    "    'ssn_tokenized',\n",
    "    F.sha2(F.col('ssn'), 256)  # One-way hash\n",
    ")\n",
    "\n",
    "# Save con partition por encrypted field (queryable)\n",
    "df_encrypted.write.partitionBy('date').parquet('s3://bucket/encrypted-data/')\n",
    "```\n",
    "\n",
    "**Caso Real: Equifax Breach (2017)**\n",
    "\n",
    "**Vulnerabilidad**: Apache Struts sin parchear + SSL certificate expirado (no detected).\n",
    "\n",
    "**Impacto**: 147M personas, $700M settlement.\n",
    "\n",
    "**ProtecciÃ³n**:\n",
    "```python\n",
    "# 1. Automated vulnerability scanning\n",
    "\"\"\"\n",
    "# Trivy (container scanning)\n",
    "trivy image my-data-pipeline:latest --severity HIGH,CRITICAL\n",
    "\n",
    "# Snyk (dependency scanning)\n",
    "snyk test --all-projects\n",
    "\"\"\"\n",
    "\n",
    "# 2. Certificate monitoring\n",
    "import ssl\n",
    "import socket\n",
    "from datetime import datetime\n",
    "\n",
    "def check_ssl_expiry(hostname, port=443):\n",
    "    \"\"\"Alert si cert expira en <30 dÃ­as\"\"\"\n",
    "    context = ssl.create_default_context()\n",
    "    with socket.create_connection((hostname, port)) as sock:\n",
    "        with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
    "            cert = ssock.getpeercert()\n",
    "            \n",
    "            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')\n",
    "            days_remaining = (not_after - datetime.now()).days\n",
    "            \n",
    "            if days_remaining < 30:\n",
    "                send_alert(f\"âš ï¸ SSL cert expires in {days_remaining} days\")\n",
    "            \n",
    "            return days_remaining\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2965a",
   "metadata": {},
   "source": [
    "### ğŸ­ **PII Protection: Masking, Tokenization & Anonymization**\n",
    "\n",
    "**PII Classification & Protection Strategies**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PII Category            Strategy                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  DIRECT IDENTIFIERS                                    â”‚\n",
    "â”‚  â€¢ SSN, Passport         â†’ Tokenization (irreversible) â”‚\n",
    "â”‚  â€¢ Email                 â†’ Masking (u***@domain.com)   â”‚\n",
    "â”‚  â€¢ Phone                 â†’ Partial mask (***-***-1234) â”‚\n",
    "â”‚  â€¢ Name                  â†’ Pseudonymization            â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  QUASI-IDENTIFIERS (combination = identity)            â”‚\n",
    "â”‚  â€¢ ZIP + Age + Gender    â†’ k-anonymity (generalize)    â”‚\n",
    "â”‚  â€¢ IP Address            â†’ Truncate last octet         â”‚\n",
    "â”‚  â€¢ Timestamp             â†’ Round to hour/day           â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  SENSITIVE ATTRIBUTES                                  â”‚\n",
    "â”‚  â€¢ Health data           â†’ Encryption + access control â”‚\n",
    "â”‚  â€¢ Financial data        â†’ Field-level encryption      â”‚\n",
    "â”‚  â€¢ Biometrics            â†’ Hash + salt                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Masking Techniques**\n",
    "\n",
    "```python\n",
    "import re\n",
    "import hashlib\n",
    "import hmac\n",
    "from typing import Optional\n",
    "\n",
    "class PIIMasker:\n",
    "    \"\"\"Comprehensive PII masking library\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenization_secret: str):\n",
    "        self.secret = tokenization_secret.encode()\n",
    "    \n",
    "    # 1. EMAIL MASKING\n",
    "    def mask_email(self, email: str) -> str:\n",
    "        \"\"\"user@example.com â†’ u***@example.com\"\"\"\n",
    "        if not email or '@' not in email:\n",
    "            return email\n",
    "        \n",
    "        user, domain = email.split('@', 1)\n",
    "        if len(user) <= 2:\n",
    "            masked_user = user[0] + '*'\n",
    "        else:\n",
    "            masked_user = user[0] + '*' * (len(user) - 2) + user[-1]\n",
    "        \n",
    "        return f\"{masked_user}@{domain}\"\n",
    "    \n",
    "    # 2. PHONE MASKING\n",
    "    def mask_phone(self, phone: str) -> str:\n",
    "        \"\"\"(555) 123-4567 â†’ ***-***-4567\"\"\"\n",
    "        digits = re.sub(r'\\D', '', phone)\n",
    "        if len(digits) >= 10:\n",
    "            return f\"***-***-{digits[-4:]}\"\n",
    "        return \"***-****\"\n",
    "    \n",
    "    # 3. SSN/ID MASKING\n",
    "    def mask_ssn(self, ssn: str) -> str:\n",
    "        \"\"\"123-45-6789 â†’ ***-**-6789\"\"\"\n",
    "        digits = re.sub(r'\\D', '', ssn)\n",
    "        if len(digits) == 9:\n",
    "            return f\"***-**-{digits[-4:]}\"\n",
    "        return \"***-**-****\"\n",
    "    \n",
    "    # 4. CREDIT CARD MASKING\n",
    "    def mask_credit_card(self, cc: str) -> str:\n",
    "        \"\"\"4532-1234-5678-9010 â†’ ****-****-****-9010\"\"\"\n",
    "        digits = re.sub(r'\\D', '', cc)\n",
    "        if len(digits) >= 12:\n",
    "            return f\"****-****-****-{digits[-4:]}\"\n",
    "        return \"****-****-****-****\"\n",
    "    \n",
    "    # 5. NAME MASKING\n",
    "    def mask_name(self, name: str) -> str:\n",
    "        \"\"\"John Smith â†’ J*** S*****\"\"\"\n",
    "        words = name.split()\n",
    "        masked = []\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                masked.append(word[0] + '*' * (len(word) - 1))\n",
    "            else:\n",
    "                masked.append(word)\n",
    "        return ' '.join(masked)\n",
    "    \n",
    "    # 6. TOKENIZATION (deterministic, irreversible)\n",
    "    def tokenize(self, value: str) -> str:\n",
    "        \"\"\"Generate consistent token for same input\"\"\"\n",
    "        return hmac.new(\n",
    "            self.secret,\n",
    "            value.encode(),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "    \n",
    "    # 7. PSEUDONYMIZATION (reversible con key)\n",
    "    def pseudonymize(self, value: str, salt: str) -> str:\n",
    "        \"\"\"Create pseudonym usando hash + salt\"\"\"\n",
    "        return hashlib.sha256(f\"{value}{salt}\".encode()).hexdigest()[:16]\n",
    "    \n",
    "    # 8. IP ADDRESS MASKING\n",
    "    def mask_ip(self, ip: str) -> str:\n",
    "        \"\"\"192.168.1.100 â†’ 192.168.1.0\"\"\"\n",
    "        parts = ip.split('.')\n",
    "        if len(parts) == 4:\n",
    "            parts[-1] = '0'\n",
    "            return '.'.join(parts)\n",
    "        return ip\n",
    "    \n",
    "    # 9. DATE GENERALIZATION\n",
    "    def generalize_date(self, date_str: str, precision: str = 'month') -> str:\n",
    "        \"\"\"2025-10-15 â†’ 2025-10-01 (month precision)\"\"\"\n",
    "        from datetime import datetime\n",
    "        dt = datetime.fromisoformat(date_str)\n",
    "        \n",
    "        if precision == 'year':\n",
    "            return dt.strftime('%Y-01-01')\n",
    "        elif precision == 'month':\n",
    "            return dt.strftime('%Y-%m-01')\n",
    "        elif precision == 'week':\n",
    "            # Round to Monday\n",
    "            days_since_monday = dt.weekday()\n",
    "            week_start = dt - timedelta(days=days_since_monday)\n",
    "            return week_start.strftime('%Y-%m-%d')\n",
    "        return date_str\n",
    "\n",
    "# Spark UDF Implementation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "masker = PIIMasker(tokenization_secret='your-secret-key')\n",
    "\n",
    "# Register UDFs\n",
    "mask_email_udf = F.udf(masker.mask_email, StringType())\n",
    "mask_phone_udf = F.udf(masker.mask_phone, StringType())\n",
    "tokenize_udf = F.udf(masker.tokenize, StringType())\n",
    "\n",
    "# Apply to DataFrame\n",
    "df_masked = (\n",
    "    df\n",
    "    .withColumn('email_masked', mask_email_udf(F.col('email')))\n",
    "    .withColumn('phone_masked', mask_phone_udf(F.col('phone')))\n",
    "    .withColumn('ssn_token', tokenize_udf(F.col('ssn')))\n",
    "    .drop('email', 'phone', 'ssn')  # Remove original PII\n",
    ")\n",
    "\n",
    "# Ejemplo: Production vs Development masking\n",
    "def create_dev_dataset(prod_df):\n",
    "    \"\"\"Create dev dataset con PII masked\"\"\"\n",
    "    return (\n",
    "        prod_df\n",
    "        .withColumn('email', mask_email_udf(F.col('email')))\n",
    "        .withColumn('phone', F.lit('***-***-****'))  # Static mask\n",
    "        .withColumn('ssn', tokenize_udf(F.col('ssn')))  # Consistent token\n",
    "        .withColumn('name', mask_name_udf(F.col('name')))\n",
    "        .withColumn('created_at', F.date_trunc('month', F.col('created_at')))\n",
    "    )\n",
    "```\n",
    "\n",
    "**K-Anonymity: Quasi-Identifier Protection**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def k_anonymize(df: pd.DataFrame, quasi_identifiers: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    Generalize quasi-identifiers para achieve k-anonymity\n",
    "    (cada combinaciÃ³n aparece al menos k veces)\n",
    "    \"\"\"\n",
    "    \n",
    "    def generalize_age(age):\n",
    "        \"\"\"Age â†’ age range\"\"\"\n",
    "        if age < 18:\n",
    "            return '<18'\n",
    "        elif age < 30:\n",
    "            return '18-29'\n",
    "        elif age < 40:\n",
    "            return '30-39'\n",
    "        elif age < 50:\n",
    "            return '40-49'\n",
    "        elif age < 60:\n",
    "            return '50-59'\n",
    "        else:\n",
    "            return '60+'\n",
    "    \n",
    "    def generalize_zipcode(zipcode):\n",
    "        \"\"\"12345 â†’ 123**\"\"\"\n",
    "        return str(zipcode)[:3] + '**'\n",
    "    \n",
    "    # Apply generalizations\n",
    "    df_anon = df.copy()\n",
    "    \n",
    "    if 'age' in quasi_identifiers:\n",
    "        df_anon['age'] = df_anon['age'].apply(generalize_age)\n",
    "    \n",
    "    if 'zipcode' in quasi_identifiers:\n",
    "        df_anon['zipcode'] = df_anon['zipcode'].apply(generalize_zipcode)\n",
    "    \n",
    "    # Check k-anonymity\n",
    "    group_sizes = df_anon.groupby(quasi_identifiers).size()\n",
    "    \n",
    "    if (group_sizes < k).any():\n",
    "        # Suppress rows que no cumplen k-anonymity\n",
    "        valid_groups = group_sizes[group_sizes >= k].index\n",
    "        df_anon = df_anon.set_index(quasi_identifiers).loc[valid_groups].reset_index()\n",
    "    \n",
    "    return df_anon\n",
    "\n",
    "# Ejemplo\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [25, 27, 35, 42, 55],\n",
    "    'zipcode': [12345, 12346, 12347, 54321, 54322],\n",
    "    'disease': ['flu', 'diabetes', 'flu', 'hypertension', 'diabetes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply k-anonymity (k=2)\n",
    "df_k_anon = k_anonymize(df, quasi_identifiers=['age', 'zipcode'], k=2)\n",
    "\n",
    "\"\"\"\n",
    "Before:\n",
    "name      age  zipcode    disease\n",
    "Alice     25   12345      flu\n",
    "Bob       27   12346      diabetes\n",
    "\n",
    "After (k=2):\n",
    "name      age     zipcode    disease\n",
    "Alice     18-29   123**      flu\n",
    "Bob       18-29   123**      diabetes\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Differential Privacy: Statistical Noise**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class DifferentialPrivacy:\n",
    "    \"\"\"Add calibrated noise para preserve privacy\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon: float = 1.0):\n",
    "        \"\"\"\n",
    "        epsilon: privacy budget (lower = more private, less accurate)\n",
    "        - 0.1: Very private (high noise)\n",
    "        - 1.0: Reasonable trade-off\n",
    "        - 10: Minimal privacy (low noise)\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def add_laplace_noise(self, value: float, sensitivity: float = 1.0) -> float:\n",
    "        \"\"\"Add Laplacian noise\"\"\"\n",
    "        scale = sensitivity / self.epsilon\n",
    "        noise = np.random.laplace(0, scale)\n",
    "        return value + noise\n",
    "    \n",
    "    def private_count(self, count: int) -> int:\n",
    "        \"\"\"Noisy count query\"\"\"\n",
    "        noisy = self.add_laplace_noise(count, sensitivity=1.0)\n",
    "        return max(0, int(round(noisy)))  # Non-negative\n",
    "    \n",
    "    def private_mean(self, values: list, lower_bound: float, upper_bound: float) -> float:\n",
    "        \"\"\"Noisy mean (bounded values)\"\"\"\n",
    "        sensitivity = (upper_bound - lower_bound) / len(values)\n",
    "        true_mean = np.mean(values)\n",
    "        return self.add_laplace_noise(true_mean, sensitivity)\n",
    "\n",
    "# Spark implementation\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dp = DifferentialPrivacy(epsilon=1.0)\n",
    "\n",
    "def add_noise_udf(value):\n",
    "    \"\"\"UDF to add differential privacy noise\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    return dp.add_laplace_noise(float(value), sensitivity=1.0)\n",
    "\n",
    "add_noise = F.udf(add_noise_udf, DoubleType())\n",
    "\n",
    "# Apply to aggregations\n",
    "df_private = (\n",
    "    df.groupBy('city')\n",
    "    .agg(F.count('*').alias('count'))\n",
    "    .withColumn('noisy_count', add_noise(F.col('count')))\n",
    ")\n",
    "```\n",
    "\n",
    "**GDPR Right to Erasure (Right to Be Forgotten)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def gdpr_delete_user_data(user_id: str, tables: list):\n",
    "    \"\"\"\n",
    "    Delete all data for user (GDPR compliance)\n",
    "    Support Delta Lake time travel para audit\n",
    "    \"\"\"\n",
    "    \n",
    "    deletion_log = []\n",
    "    \n",
    "    for table_path in tables:\n",
    "        # Load Delta table\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # Count before\n",
    "        before_count = spark.read.format('delta').load(table_path) \\\n",
    "            .filter(F.col('user_id') == user_id).count()\n",
    "        \n",
    "        # Delete\n",
    "        delta_table.delete(condition=f\"user_id = '{user_id}'\")\n",
    "        \n",
    "        # Verify\n",
    "        after_count = spark.read.format('delta').load(table_path) \\\n",
    "            .filter(F.col('user_id') == user_id).count()\n",
    "        \n",
    "        deletion_log.append({\n",
    "            'table': table_path,\n",
    "            'user_id': user_id,\n",
    "            'records_deleted': before_count,\n",
    "            'records_remaining': after_count,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    # Log deletion para audit\n",
    "    log_df = spark.createDataFrame(deletion_log)\n",
    "    log_df.write.mode('append').format('delta').save('s3://audit/gdpr-deletions/')\n",
    "    \n",
    "    return deletion_log\n",
    "\n",
    "# Anonymize instead of delete (alternative)\n",
    "def gdpr_anonymize_user_data(user_id: str):\n",
    "    \"\"\"Replace PII with anonymized values\"\"\"\n",
    "    \n",
    "    masker = PIIMasker('secret-key')\n",
    "    \n",
    "    delta_table = DeltaTable.forPath(spark, 's3://data-lake/users/')\n",
    "    \n",
    "    delta_table.update(\n",
    "        condition=f\"user_id = '{user_id}'\",\n",
    "        set={\n",
    "            'email': F.lit(masker.tokenize(user_id)),\n",
    "            'name': F.lit('ANONYMIZED'),\n",
    "            'phone': F.lit('***-***-****'),\n",
    "            'address': F.lit(None),\n",
    "            'anonymized_at': F.current_timestamp()\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "**Caso Real: Facebook-Cambridge Analytica (2018)**\n",
    "\n",
    "**Problema**: 87M perfiles usados sin consent para political targeting.\n",
    "\n",
    "**GDPR ProtecciÃ³n**:\n",
    "```python\n",
    "# 1. Consent management\n",
    "class ConsentManager:\n",
    "    def check_consent(self, user_id: str, purpose: str) -> bool:\n",
    "        \"\"\"Verify user consent antes de procesar\"\"\"\n",
    "        consent = get_user_consent(user_id)\n",
    "        return purpose in consent.get('purposes', [])\n",
    "    \n",
    "    def process_with_consent(self, user_id: str, purpose: str, data):\n",
    "        if not self.check_consent(user_id, purpose):\n",
    "            raise ValueError(f\"No consent for {purpose}\")\n",
    "        \n",
    "        # Process data\n",
    "        return transform(data)\n",
    "\n",
    "# 2. Data minimization\n",
    "def minimize_data_collection(user_data: dict, purpose: str):\n",
    "    \"\"\"Collect only necessary fields\"\"\"\n",
    "    \n",
    "    purpose_fields = {\n",
    "        'analytics': ['user_id', 'timestamp', 'event_type'],\n",
    "        'marketing': ['user_id', 'email', 'opt_in'],\n",
    "        'personalization': ['user_id', 'preferences']\n",
    "    }\n",
    "    \n",
    "    allowed_fields = purpose_fields.get(purpose, [])\n",
    "    return {k: v for k, v in user_data.items() if k in allowed_fields}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f32f1",
   "metadata": {},
   "source": [
    "### ğŸ“‹ **Compliance Frameworks: GDPR, HIPAA, SOC2 & Audit**\n",
    "\n",
    "**Compliance Requirements Matrix**\n",
    "\n",
    "| Framework | Scope | Key Requirements | Data Engineer Responsibilities |\n",
    "|-----------|-------|------------------|--------------------------------|\n",
    "| **GDPR** | EU citizens | â€¢ Consent<br>â€¢ Right to erasure<br>â€¢ Data portability<br>â€¢ Breach notification (72h) | â€¢ Delete/anonymize on request<br>â€¢ Data residency (EU)<br>â€¢ Encryption at-rest<br>â€¢ Access logs |\n",
    "| **CCPA** | California residents | â€¢ Opt-out of sale<br>â€¢ Access to data<br>â€¢ Deletion rights | â€¢ Implement do-not-sell flag<br>â€¢ Data export API<br>â€¢ 45-day deletion window |\n",
    "| **HIPAA** | Healthcare (US) | â€¢ PHI encryption<br>â€¢ Audit trails<br>â€¢ Business Associate Agreements | â€¢ Encrypt PHI at-rest/in-transit<br>â€¢ Implement access controls<br>â€¢ Audit all PHI access |\n",
    "| **SOC2** | Service organizations | â€¢ Security controls<br>â€¢ Availability<br>â€¢ Confidentiality | â€¢ Implement monitoring<br>â€¢ Incident response<br>â€¢ Change management |\n",
    "| **PCI-DSS** | Payment cards | â€¢ Cardholder data encryption<br>â€¢ Secure networks<br>â€¢ Regular testing | â€¢ Tokenize cards<br>â€¢ Network segmentation<br>â€¢ Vulnerability scanning |\n",
    "\n",
    "**GDPR Implementation**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "class GDPRCompliance:\n",
    "    \"\"\"GDPR compliance toolkit\"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "    \n",
    "    # 1. RIGHT TO ACCESS (Article 15)\n",
    "    def export_user_data(self, user_id: str, output_path: str):\n",
    "        \"\"\"\n",
    "        Export all user data en formato legible (JSON)\n",
    "        User can request their data\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [\n",
    "            's3://data-lake/users/',\n",
    "            's3://data-lake/transactions/',\n",
    "            's3://data-lake/events/',\n",
    "            's3://data-lake/preferences/'\n",
    "        ]\n",
    "        \n",
    "        user_data = {}\n",
    "        \n",
    "        for table_path in tables:\n",
    "            table_name = table_path.split('/')[-2]\n",
    "            \n",
    "            df = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id)\n",
    "            \n",
    "            # Convert to JSON\n",
    "            records = df.toJSON().collect()\n",
    "            user_data[table_name] = [json.loads(r) for r in records]\n",
    "        \n",
    "        # Add metadata\n",
    "        export_package = {\n",
    "            'user_id': user_id,\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'data': user_data,\n",
    "            'retention_policy': '7 years',\n",
    "            'data_controller': 'YourCompany Inc.'\n",
    "        }\n",
    "        \n",
    "        # Write to S3 (user can download)\n",
    "        import json\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(export_package, f, indent=2)\n",
    "        \n",
    "        # Log export request\n",
    "        self.log_gdpr_request('access', user_id)\n",
    "        \n",
    "        return export_package\n",
    "    \n",
    "    # 2. RIGHT TO ERASURE (Article 17)\n",
    "    def delete_user_data(self, user_id: str, reason: str = 'user_request'):\n",
    "        \"\"\"\n",
    "        Delete all user data (Right to be Forgotten)\n",
    "        Must complete within 30 days\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = [\n",
    "            's3://data-lake/users/',\n",
    "            's3://data-lake/transactions/',\n",
    "            's3://data-lake/events/',\n",
    "            's3://data-lake/preferences/'\n",
    "        ]\n",
    "        \n",
    "        deletion_report = {\n",
    "            'user_id': user_id,\n",
    "            'request_date': datetime.now(),\n",
    "            'reason': reason,\n",
    "            'tables_processed': []\n",
    "        }\n",
    "        \n",
    "        for table_path in tables:\n",
    "            delta_table = DeltaTable.forPath(self.spark, table_path)\n",
    "            \n",
    "            # Count before deletion\n",
    "            before = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id).count()\n",
    "            \n",
    "            # Delete\n",
    "            delta_table.delete(condition=f\"user_id = '{user_id}'\")\n",
    "            \n",
    "            # Verify deletion\n",
    "            after = self.spark.read.format('delta').load(table_path) \\\n",
    "                .filter(F.col('user_id') == user_id).count()\n",
    "            \n",
    "            deletion_report['tables_processed'].append({\n",
    "                'table': table_path,\n",
    "                'records_deleted': before,\n",
    "                'records_remaining': after,\n",
    "                'status': 'complete' if after == 0 else 'failed'\n",
    "            })\n",
    "        \n",
    "        # Purge from backups (async job)\n",
    "        self.schedule_backup_purge(user_id)\n",
    "        \n",
    "        # Log deletion\n",
    "        self.log_gdpr_request('erasure', user_id, deletion_report)\n",
    "        \n",
    "        return deletion_report\n",
    "    \n",
    "    # 3. DATA PORTABILITY (Article 20)\n",
    "    def export_user_data_structured(self, user_id: str) -> dict:\n",
    "        \"\"\"Export en formato machine-readable (CSV, JSON)\"\"\"\n",
    "        \n",
    "        # Export to multiple formats\n",
    "        formats = ['json', 'csv', 'parquet']\n",
    "        export_urls = {}\n",
    "        \n",
    "        for fmt in formats:\n",
    "            output_path = f's3://exports/{user_id}/data.{fmt}'\n",
    "            \n",
    "            df = self.spark.read.format('delta').load('s3://data-lake/users/') \\\n",
    "                .filter(F.col('user_id') == user_id)\n",
    "            \n",
    "            if fmt == 'parquet':\n",
    "                df.write.mode('overwrite').parquet(output_path)\n",
    "            elif fmt == 'csv':\n",
    "                df.write.mode('overwrite').option('header', True).csv(output_path)\n",
    "            else:  # json\n",
    "                df.write.mode('overwrite').json(output_path)\n",
    "            \n",
    "            export_urls[fmt] = output_path\n",
    "        \n",
    "        return export_urls\n",
    "    \n",
    "    # 4. CONSENT MANAGEMENT (Article 7)\n",
    "    def check_consent(self, user_id: str, purpose: str) -> bool:\n",
    "        \"\"\"Verify explicit consent for data processing\"\"\"\n",
    "        \n",
    "        consent_df = self.spark.read.format('delta').load('s3://data-lake/consents/') \\\n",
    "            .filter(\n",
    "                (F.col('user_id') == user_id) &\n",
    "                (F.col('purpose') == purpose) &\n",
    "                (F.col('consent_given') == True) &\n",
    "                (F.col('consent_withdrawn_at').isNull())\n",
    "            )\n",
    "        \n",
    "        return consent_df.count() > 0\n",
    "    \n",
    "    def withdraw_consent(self, user_id: str, purpose: str):\n",
    "        \"\"\"User withdraws consent\"\"\"\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, 's3://data-lake/consents/')\n",
    "        \n",
    "        delta_table.update(\n",
    "            condition=f\"user_id = '{user_id}' AND purpose = '{purpose}'\",\n",
    "            set={'consent_withdrawn_at': F.current_timestamp()}\n",
    "        )\n",
    "        \n",
    "        # Stop processing data for this purpose\n",
    "        self.log_gdpr_request('consent_withdrawal', user_id, {'purpose': purpose})\n",
    "    \n",
    "    # 5. BREACH NOTIFICATION (Article 33)\n",
    "    def detect_data_breach(self):\n",
    "        \"\"\"Detect suspicious access patterns\"\"\"\n",
    "        \n",
    "        # Query access logs\n",
    "        access_logs = self.spark.read.format('delta').load('s3://logs/access/')\n",
    "        \n",
    "        # Anomaly detection: unusual access volume\n",
    "        anomalies = access_logs.groupBy('user_id', 'date') \\\n",
    "            .agg(F.count('*').alias('access_count')) \\\n",
    "            .filter(F.col('access_count') > 1000)  # Threshold\n",
    "        \n",
    "        if anomalies.count() > 0:\n",
    "            self.trigger_breach_response(anomalies)\n",
    "    \n",
    "    def trigger_breach_response(self, anomalies):\n",
    "        \"\"\"72-hour notification requirement\"\"\"\n",
    "        \n",
    "        breach_report = {\n",
    "            'detected_at': datetime.now(),\n",
    "            'notification_deadline': datetime.now() + timedelta(hours=72),\n",
    "            'affected_users': anomalies.select('user_id').distinct().count(),\n",
    "            'actions_taken': [\n",
    "                'Blocked suspicious IPs',\n",
    "                'Reset affected user sessions',\n",
    "                'Notified security team'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Alert compliance team\n",
    "        send_alert(\n",
    "            title='âš ï¸ Potential Data Breach Detected',\n",
    "            message=f\"Affected users: {breach_report['affected_users']}\\n\"\n",
    "                    f\"Notification deadline: {breach_report['notification_deadline']}\",\n",
    "            channel='#security-incidents'\n",
    "        )\n",
    "        \n",
    "        # Log breach\n",
    "        self.log_gdpr_request('breach', 'system', breach_report)\n",
    "    \n",
    "    # 6. DATA RETENTION (Article 5)\n",
    "    def enforce_retention_policy(self, table_path: str, retention_days: int = 2555):\n",
    "        \"\"\"\n",
    "        Delete data older than retention period\n",
    "        GDPR: Keep data no longer than necessary\n",
    "        \"\"\"\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, table_path)\n",
    "        \n",
    "        cutoff_date = datetime.now() - timedelta(days=retention_days)\n",
    "        \n",
    "        # Delete old data\n",
    "        delta_table.delete(\n",
    "            condition=f\"created_at < '{cutoff_date.isoformat()}'\"\n",
    "        )\n",
    "        \n",
    "        # Vacuum to physically delete files\n",
    "        delta_table.vacuum(retentionHours=0)  # Immediate deletion\n",
    "    \n",
    "    # 7. AUDIT LOGGING\n",
    "    def log_gdpr_request(self, request_type: str, user_id: str, details: dict = None):\n",
    "        \"\"\"Log all GDPR-related operations\"\"\"\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'request_type': request_type,\n",
    "            'user_id': user_id,\n",
    "            'details': details or {},\n",
    "            'processed_by': 'gdpr_compliance_service'\n",
    "        }\n",
    "        \n",
    "        log_df = self.spark.createDataFrame([log_entry])\n",
    "        log_df.write.mode('append').format('delta').save('s3://audit/gdpr-requests/')\n",
    "```\n",
    "\n",
    "**HIPAA Compliance (Healthcare)**\n",
    "\n",
    "```python\n",
    "class HIPAACompliance:\n",
    "    \"\"\"Health Insurance Portability and Accountability Act\"\"\"\n",
    "    \n",
    "    # PHI = Protected Health Information\n",
    "    PHI_FIELDS = [\n",
    "        'name', 'address', 'ssn', 'medical_record_number',\n",
    "        'email', 'phone', 'ip_address', 'device_id',\n",
    "        'diagnosis', 'prescription', 'lab_results'\n",
    "    ]\n",
    "    \n",
    "    def encrypt_phi(self, df):\n",
    "        \"\"\"Encrypt all PHI fields\"\"\"\n",
    "        \n",
    "        for field in self.PHI_FIELDS:\n",
    "            if field in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    field,\n",
    "                    encrypt_udf(F.col(field))\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def audit_phi_access(self, user_id: str, patient_id: str, purpose: str):\n",
    "        \"\"\"Log every PHI access (HIPAA requirement)\"\"\"\n",
    "        \n",
    "        audit_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'user_id': user_id,\n",
    "            'patient_id': patient_id,\n",
    "            'purpose': purpose,\n",
    "            'ip_address': get_client_ip(),\n",
    "            'action': 'view_phi'\n",
    "        }\n",
    "        \n",
    "        # Write to immutable audit log\n",
    "        spark.createDataFrame([audit_entry]).write \\\n",
    "            .mode('append') \\\n",
    "            .format('delta') \\\n",
    "            .option('mergeSchema', False) \\\n",
    "            .save('s3://audit/phi-access/')\n",
    "    \n",
    "    def de_identify_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Remove 18 HIPAA identifiers para create de-identified dataset\n",
    "        Safe Harbor method\n",
    "        \"\"\"\n",
    "        \n",
    "        identifiers = [\n",
    "            'name', 'address', 'city', 'zip', 'phone', 'fax', 'email',\n",
    "            'ssn', 'medical_record_number', 'health_plan_number',\n",
    "            'account_number', 'certificate_number', 'vehicle_id',\n",
    "            'device_id', 'url', 'ip_address', 'biometric_id',\n",
    "            'photo', 'any_unique_code'\n",
    "        ]\n",
    "        \n",
    "        # Remove identifiers\n",
    "        df_deidentified = df.drop(*[col for col in identifiers if col in df.columns])\n",
    "        \n",
    "        # Generalize dates (only year)\n",
    "        date_columns = [col for col, dtype in df.dtypes if 'date' in dtype or 'timestamp' in dtype]\n",
    "        for col in date_columns:\n",
    "            df_deidentified = df_deidentified.withColumn(\n",
    "                col,\n",
    "                F.year(F.col(col))\n",
    "            )\n",
    "        \n",
    "        # Generalize age (>89 â†’ 90+)\n",
    "        if 'age' in df_deidentified.columns:\n",
    "            df_deidentified = df_deidentified.withColumn(\n",
    "                'age',\n",
    "                F.when(F.col('age') > 89, 90).otherwise(F.col('age'))\n",
    "            )\n",
    "        \n",
    "        return df_deidentified\n",
    "```\n",
    "\n",
    "**SOC2 Audit Evidence Collection**\n",
    "\n",
    "```python\n",
    "# SOC2 Trust Service Criteria\n",
    "class SOC2Compliance:\n",
    "    \"\"\"System and Organization Controls Type 2\"\"\"\n",
    "    \n",
    "    def collect_audit_evidence(self, period_days: int = 90):\n",
    "        \"\"\"Gather evidence for SOC2 audit\"\"\"\n",
    "        \n",
    "        evidence = {\n",
    "            # CC6.1: Logical access controls\n",
    "            'access_controls': self.verify_access_controls(),\n",
    "            \n",
    "            # CC6.6: Vulnerability management\n",
    "            'vulnerability_scans': self.collect_vulnerability_scans(period_days),\n",
    "            \n",
    "            # CC7.2: System monitoring\n",
    "            'monitoring_alerts': self.collect_monitoring_data(period_days),\n",
    "            \n",
    "            # CC8.1: Change management\n",
    "            'code_changes': self.collect_git_commits(period_days),\n",
    "            \n",
    "            # A1.2: System availability\n",
    "            'uptime_metrics': self.calculate_uptime(period_days),\n",
    "        }\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def verify_access_controls(self):\n",
    "        \"\"\"Verify IAM policies follow least privilege\"\"\"\n",
    "        \n",
    "        iam = boto3.client('iam')\n",
    "        \n",
    "        # Check MFA enforcement\n",
    "        users = iam.list_users()['Users']\n",
    "        \n",
    "        mfa_status = []\n",
    "        for user in users:\n",
    "            mfa_devices = iam.list_mfa_devices(UserName=user['UserName'])\n",
    "            \n",
    "            mfa_status.append({\n",
    "                'user': user['UserName'],\n",
    "                'mfa_enabled': len(mfa_devices['MFADevices']) > 0\n",
    "            })\n",
    "        \n",
    "        # Check overly permissive policies\n",
    "        policies = iam.list_policies(Scope='Local')['Policies']\n",
    "        \n",
    "        risky_policies = []\n",
    "        for policy in policies:\n",
    "            policy_version = iam.get_policy_version(\n",
    "                PolicyArn=policy['Arn'],\n",
    "                VersionId=policy['DefaultVersionId']\n",
    "            )\n",
    "            \n",
    "            doc = policy_version['PolicyVersion']['Document']\n",
    "            \n",
    "            # Check for wildcard permissions\n",
    "            for statement in doc.get('Statement', []):\n",
    "                if statement.get('Effect') == 'Allow' and '*' in statement.get('Action', []):\n",
    "                    risky_policies.append(policy['PolicyName'])\n",
    "        \n",
    "        return {\n",
    "            'mfa_compliance': sum(1 for u in mfa_status if u['mfa_enabled']) / len(mfa_status),\n",
    "            'risky_policies': risky_policies\n",
    "        }\n",
    "```\n",
    "\n",
    "**Caso Real: Uber 2016 Breach (Concealed)**\n",
    "\n",
    "**Problema**: Breach de 57M usuarios, pagaron $100K a hackers para ocultarlo, no notificaron (violaciÃ³n GDPR/leyes estatales).\n",
    "\n",
    "**Resultado**: $148M multa, CEO criminally charged.\n",
    "\n",
    "**Compliance Checklist**:\n",
    "```python\n",
    "compliance_checklist = \"\"\"\n",
    "âœ… Data Classification (public, internal, confidential, restricted)\n",
    "âœ… Access Controls (RBAC, MFA)\n",
    "âœ… Encryption (at-rest: AES-256, in-transit: TLS 1.3)\n",
    "âœ… Data Retention Policy (automated deletion)\n",
    "âœ… Breach Detection (SIEM, anomaly detection)\n",
    "âœ… Incident Response Plan (72h notification for GDPR)\n",
    "âœ… Audit Logs (immutable, centralized)\n",
    "âœ… Third-Party Audits (annual SOC2, penetration testing)\n",
    "âœ… Employee Training (security awareness, phishing)\n",
    "âœ… Disaster Recovery (RTO < 4h, RPO < 1h)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101b98e",
   "metadata": {},
   "source": [
    "## 1. IAM y principio de mÃ­nimo privilegio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a170b",
   "metadata": {},
   "source": [
    "- Roles por funciÃ³n: data-engineer-ro, data-scientist, admin.\n",
    "- PolÃ­ticas granulares: lectura de bucket especÃ­fico, escritura en tabla especÃ­fica.\n",
    "- MFA obligatorio para operaciones sensibles (producciÃ³n, eliminaciÃ³n).\n",
    "- RotaciÃ³n automÃ¡tica de credenciales (secretos, keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7771f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '# AWS IAM policy para data engineer con acceso de lectura a raw y escritura a curated', '{', '  \"Version\": \"2012-10-17\",', '  \"Statement\": [', '    {', '      \"Effect\": \"Allow\",', '      \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],', '      \"Resource\": [\"arn:aws:s3:::data-lake/raw/*\"]', '    },', '    {', '      \"Effect\": \"Allow\",', '      \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],', '      \"Resource\": [\"arn:aws:s3:::data-lake/curated/*\"]', '    }', '  ]', '}']\n"
     ]
    }
   ],
   "source": [
    "iam_policy_example = r'''\n",
    "# AWS IAM policy para data engineer con acceso de lectura a raw y escritura a curated\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "      \"Resource\": [\"arn:aws:s3:::data-lake/raw/*\"]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "      \"Resource\": [\"arn:aws:s3:::data-lake/curated/*\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "print(iam_policy_example.splitlines()[:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6cc67",
   "metadata": {},
   "source": [
    "## 2. Cifrado en trÃ¡nsito y at-rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971318a",
   "metadata": {},
   "source": [
    "- **At-rest**: S3 SSE-KMS, RDS encryption, disk encryption (EBS/GCS).\n",
    "- **In-transit**: TLS 1.2+ para todas las APIs, VPN/PrivateLink para conectividad interna.\n",
    "- GestiÃ³n de claves: AWS KMS, GCP Cloud KMS, Azure Key Vault con rotaciÃ³n automÃ¡tica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd346c6f",
   "metadata": {},
   "source": [
    "## 3. Enmascaramiento y anonimizaciÃ³n de PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92cb911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u***@ejemplo.com',\n",
       " 'cb6f3ba16a6ddc7ae4f79f410abcd4187de72f269908baad3b3849e415271cd2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "def mask_email(email: str) -> str:\n",
    "    user, domain = email.split('@')\n",
    "    return f'{user[0]}***@{domain}'\n",
    "\n",
    "def hash_pii(value: str) -> str:\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "\n",
    "mask_email('usuario@ejemplo.com'), hash_pii('12345678A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c42f49",
   "metadata": {},
   "source": [
    "## 4. Cumplimiento normativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c128ca",
   "metadata": {},
   "source": [
    "### GDPR (Europa)\n",
    "- Derecho al olvido: implementar DELETE cascada y purga en backups.\n",
    "- Consentimiento explÃ­cito y auditable.\n",
    "- Data residency: almacenar en regiÃ³n EU.\n",
    "\n",
    "### HIPAA (salud, USA)\n",
    "- PHI cifrado, logs de acceso auditables.\n",
    "- Business Associate Agreements con proveedores cloud.\n",
    "\n",
    "### SOC2 (seguridad organizacional)\n",
    "- Controles de acceso, monitoreo, incident response.\n",
    "- AuditorÃ­as anuales por terceros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af392a5a",
   "metadata": {},
   "source": [
    "## 5. AuditorÃ­a de accesos y linaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a754d8",
   "metadata": {},
   "source": [
    "- CloudTrail (AWS), Cloud Audit Logs (GCP), Activity Log (Azure).\n",
    "- Registrar quiÃ©n accediÃ³ quÃ© dato, cuÃ¡ndo, desde dÃ³nde.\n",
    "- Linaje de datos: OpenLineage, DataHub, Marquez â†’ rastrear transformaciones y uso.\n",
    "- Alertas ante accesos anÃ³malos (SIEM: Splunk, Datadog Security)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef33df",
   "metadata": {},
   "source": [
    "## 6. Checklist de seguridad para pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b30d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â˜‘ IAM con mÃ­nimo privilegio y MFA\n",
      "â˜‘ Cifrado at-rest (KMS) y in-transit (TLS)\n",
      "â˜‘ Enmascaramiento de PII en logs y datasets de dev\n",
      "â˜‘ RotaciÃ³n automÃ¡tica de secretos (API keys, DB passwords)\n",
      "â˜‘ AuditorÃ­a habilitada (CloudTrail, logs centralizados)\n",
      "â˜‘ Vulnerability scanning de contenedores (Trivy, Clair)\n",
      "â˜‘ Network segmentation (VPCs, subnets privadas)\n",
      "â˜‘ Incident response plan documentado y probado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checklist = '''\n",
    "â˜‘ IAM con mÃ­nimo privilegio y MFA\n",
    "â˜‘ Cifrado at-rest (KMS) y in-transit (TLS)\n",
    "â˜‘ Enmascaramiento de PII en logs y datasets de dev\n",
    "â˜‘ RotaciÃ³n automÃ¡tica de secretos (API keys, DB passwords)\n",
    "â˜‘ AuditorÃ­a habilitada (CloudTrail, logs centralizados)\n",
    "â˜‘ Vulnerability scanning de contenedores (Trivy, Clair)\n",
    "â˜‘ Network segmentation (VPCs, subnets privadas)\n",
    "â˜‘ Incident response plan documentado y probado\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "\n",
    "**Siguiente â†’:** [ğŸ“Š Observabilidad y Linaje de Datos â†’](08_observabilidad_linaje.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Senior:**\n",
    "- [ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [ğŸ—ï¸ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y prÃ¡ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos](07_seguridad_compliance.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [ğŸ“Š Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
