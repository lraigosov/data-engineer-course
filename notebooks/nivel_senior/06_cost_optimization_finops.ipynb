{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6ed643",
   "metadata": {},
   "source": [
    "# 💰 Cost Optimization y FinOps en la Nube\n",
    "\n",
    "Objetivo: aplicar técnicas de optimización de costos cloud (AWS/GCP/Azure), establecer presupuestos, alertas y benchmarks, con métricas de eficiencia por workload.\n",
    "\n",
    "- Duración: 120 min\n",
    "- Dificultidad: Alta\n",
    "- Prerrequisitos: Mid 03 (AWS), experiencia con pipelines en cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b51785",
   "metadata": {},
   "source": [
    "### 💡 **FinOps Framework: Cloud Cost Management como Práctica Estratégica**\n",
    "\n",
    "**¿Qué es FinOps?**\n",
    "\n",
    "FinOps (Financial Operations) es la práctica de traer accountability financiera al modelo variable de gasto en la nube, combinando finanzas, tecnología y negocios.\n",
    "\n",
    "**Fases del Ciclo FinOps:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│               FinOps Lifecycle                          │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│  1. INFORM (Visibilidad)                                │\n",
    "│     • Cost allocation y tagging                         │\n",
    "│     • Showback/Chargeback por equipo                    │\n",
    "│     • Anomaly detection                                 │\n",
    "│                                                          │\n",
    "│  2. OPTIMIZE (Eficiencia)                               │\n",
    "│     • Rightsizing (reduce over-provisioning)            │\n",
    "│     • Reserved Instances / Savings Plans                │\n",
    "│     • Spot/Preemptible instances                        │\n",
    "│     • Storage lifecycle policies                        │\n",
    "│                                                          │\n",
    "│  3. OPERATE (Cultura)                                   │\n",
    "│     • Budget owners por dominio                         │\n",
    "│     • Cost awareness en desarrollo                      │\n",
    "│     • Continuous improvement                            │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Principios Fundamentales:**\n",
    "\n",
    "1. **Teams Need to Collaborate**: Ingeniería, Finanzas, Product\n",
    "2. **Everyone Takes Ownership**: Cada equipo responsable de su gasto\n",
    "3. **Centralized Team Drives FinOps**: FinOps team como facilitador\n",
    "4. **Reports Should Be Accessible**: Dashboards self-service\n",
    "5. **Decisions Driven by Business Value**: Costo vs impacto en negocio\n",
    "6. **Take Advantage of Variable Cost**: Elasticidad como ventaja\n",
    "\n",
    "**Modelo de Responsabilidad:**\n",
    "\n",
    "| Rol | Responsabilidades |\n",
    "|-----|-------------------|\n",
    "| **FinOps Team** | Políticas, herramientas, reporting, training |\n",
    "| **Engineers** | Implementar optimizaciones, cost-aware design |\n",
    "| **Finance** | Forecasting, budgets, business case analysis |\n",
    "| **Executives** | Aprobar inversiones, cost targets estratégicos |\n",
    "| **Product** | Priorizar features vs costo, unit economics |\n",
    "\n",
    "**Cost Allocation Strategy (Tagging):**\n",
    "\n",
    "```python\n",
    "# AWS Resource Tagging Standard\n",
    "tagging_strategy = {\n",
    "    # Mandatory tags (billing)\n",
    "    \"Environment\": [\"dev\", \"staging\", \"prod\"],\n",
    "    \"CostCenter\": [\"engineering\", \"data\", \"marketing\"],\n",
    "    \"Project\": [\"data-platform\", \"ml-ops\", \"analytics\"],\n",
    "    \"Owner\": \"email@company.com\",\n",
    "    \n",
    "    # Optional (technical)\n",
    "    \"Application\": \"spark-etl\",\n",
    "    \"ManagedBy\": \"terraform\",\n",
    "    \"Compliance\": [\"gdpr\", \"sox\", \"hipaa\"],\n",
    "}\n",
    "\n",
    "# Terraform enforcement\n",
    "resource \"aws_s3_bucket\" \"data_lake\" {\n",
    "  bucket = \"my-data-lake\"\n",
    "  \n",
    "  tags = {\n",
    "    Environment = \"prod\"\n",
    "    CostCenter  = \"data\"\n",
    "    Project     = \"data-platform\"\n",
    "    Owner       = \"data-team@company.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tag compliance policy (AWS Config)\n",
    "aws configservice put-config-rule --config-rule '{\n",
    "  \"ConfigRuleName\": \"required-tags\",\n",
    "  \"Source\": {\n",
    "    \"Owner\": \"AWS\",\n",
    "    \"SourceIdentifier\": \"REQUIRED_TAGS\"\n",
    "  },\n",
    "  \"InputParameters\": \"{\\\"tag1Key\\\":\\\"CostCenter\\\",\\\"tag2Key\\\":\\\"Project\\\"}\"\n",
    "}'\n",
    "```\n",
    "\n",
    "**Showback vs Chargeback:**\n",
    "\n",
    "```python\n",
    "# Showback: Informar costos (no cobrar)\n",
    "monthly_report = {\n",
    "    \"team\": \"Data Engineering\",\n",
    "    \"month\": \"2025-10\",\n",
    "    \"breakdown\": {\n",
    "        \"compute\": {\"ec2\": 1200, \"emr\": 3500},\n",
    "        \"storage\": {\"s3\": 800, \"ebs\": 400},\n",
    "        \"networking\": {\"data_transfer\": 300},\n",
    "        \"total\": 6200\n",
    "    },\n",
    "    \"trend\": \"+15% vs last month\",\n",
    "    \"top_resources\": [\n",
    "        {\"resource\": \"emr-cluster-prod\", \"cost\": 2800},\n",
    "        {\"resource\": \"ec2-spark-workers\", \"cost\": 900}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Chargeback: Cobrar internamente a cada equipo\n",
    "chargeback_policy = {\n",
    "    \"data_team\": {\n",
    "        \"budget\": 10000,\n",
    "        \"actual\": 6200,\n",
    "        \"remaining\": 3800,\n",
    "        \"action\": \"Approved\" if 6200 < 10000 else \"Budget Review\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Unit Economics para Data Platforms:**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────┐\n",
    "│  Métricas de Eficiencia                        │\n",
    "├────────────────────────────────────────────────┤\n",
    "│  • Cost per TB Processed:  $5 - $50            │\n",
    "│    (depende de transformación complexity)      │\n",
    "│                                                 │\n",
    "│  • Cost per Million Events: $0.10 - $5.00      │\n",
    "│    (streaming platforms)                       │\n",
    "│                                                 │\n",
    "│  • Cost per Query: $0.001 - $1.00              │\n",
    "│    (BigQuery, Athena)                          │\n",
    "│                                                 │\n",
    "│  • Cost per ML Training Run: $10 - $500        │\n",
    "│    (model size, GPU hours)                     │\n",
    "│                                                 │\n",
    "│  • Storage Cost per TB/month: $20 - $200       │\n",
    "│    (Standard → Glacier Deep Archive)           │\n",
    "└────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Caso Real: Netflix FinOps**\n",
    "\n",
    "Netflix procesa ~15 PB/día con presupuesto de ~$300M/año en AWS:\n",
    "\n",
    "- **Tagging Strategy**: 100% de recursos taggeados con show/movie ID\n",
    "- **Spot Instances**: 80% de workloads batch en Spot (ahorro 70%)\n",
    "- **S3 Lifecycle**: 90% de contenido en Glacier (ahorro $50M/año)\n",
    "- **Regional Optimization**: Data replication solo en regiones activas\n",
    "- **Custom Metrics**: Costo por hora de streaming por usuario\n",
    "\n",
    "**Dashboard FinOps (Grafana):**\n",
    "\n",
    "```python\n",
    "# Prometheus queries para dashboard\n",
    "queries = {\n",
    "    \"total_monthly_cost\": 'sum(aws_cost_total{period=\"monthly\"})',\n",
    "    \"cost_by_service\": 'sum by (service) (aws_cost_total)',\n",
    "    \"cost_by_team\": 'sum by (tag_costcenter) (aws_cost_total)',\n",
    "    \"savings_opportunity\": 'sum(aws_rightsizing_recommendations)',\n",
    "    \"budget_burn_rate\": 'rate(aws_cost_total[7d]) * 30',\n",
    "}\n",
    "\n",
    "# Alertas\n",
    "alerts = {\n",
    "    \"budget_80_percent\": \"Monthly cost > 80% of budget\",\n",
    "    \"cost_spike\": \"Cost increase > 50% vs last week\",\n",
    "    \"untagged_resources\": \"Resources without CostCenter tag > 5%\",\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a509f",
   "metadata": {},
   "source": [
    "### ⚡ **Compute Optimization: Rightsizing, Spot & Autoscaling**\n",
    "\n",
    "**Rightsizing: Eliminar Over-Provisioning**\n",
    "\n",
    "El 30-40% de recursos cloud están sobre-aprovisionados. Rightsizing ajusta instancias al uso real.\n",
    "\n",
    "**AWS EC2 Rightsizing Process:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# CloudWatch metrics para rightsizing\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "ce = boto3.client('ce')  # Cost Explorer\n",
    "\n",
    "def get_instance_utilization(instance_id, days=14):\n",
    "    \"\"\"Analiza CPU/Memory/Network de instancia\"\"\"\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=days)\n",
    "    \n",
    "    # CPU utilization\n",
    "    cpu_response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/EC2',\n",
    "        MetricName='CPUUtilization',\n",
    "        Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n",
    "        StartTime=start,\n",
    "        EndTime=end,\n",
    "        Period=3600,  # 1 hora\n",
    "        Statistics=['Average', 'Maximum']\n",
    "    )\n",
    "    \n",
    "    cpu_avg = sum(dp['Average'] for dp in cpu_response['Datapoints']) / len(cpu_response['Datapoints'])\n",
    "    cpu_max = max(dp['Maximum'] for dp in cpu_response['Datapoints'])\n",
    "    \n",
    "    return {\n",
    "        'instance_id': instance_id,\n",
    "        'cpu_avg': cpu_avg,\n",
    "        'cpu_max': cpu_max,\n",
    "        'recommendation': 'downsize' if cpu_avg < 20 else 'keep' if cpu_avg < 70 else 'upsize'\n",
    "    }\n",
    "\n",
    "# Rightsizing recommendations\n",
    "def generate_rightsizing_report(ec2_client):\n",
    "    instances = ec2_client.describe_instances()\n",
    "    \n",
    "    recommendations = []\n",
    "    for reservation in instances['Reservations']:\n",
    "        for instance in reservation['Instances']:\n",
    "            inst_id = instance['InstanceId']\n",
    "            inst_type = instance['InstanceType']\n",
    "            \n",
    "            metrics = get_instance_utilization(inst_id)\n",
    "            \n",
    "            # Lógica de recomendación\n",
    "            if metrics['cpu_avg'] < 20 and metrics['cpu_max'] < 50:\n",
    "                # Over-provisioned\n",
    "                current_price = get_instance_price(inst_type)\n",
    "                recommended_type = get_smaller_instance(inst_type)\n",
    "                recommended_price = get_instance_price(recommended_type)\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'instance_id': inst_id,\n",
    "                    'current_type': inst_type,\n",
    "                    'current_cost': current_price * 730,  # mensual\n",
    "                    'recommended_type': recommended_type,\n",
    "                    'recommended_cost': recommended_price * 730,\n",
    "                    'savings_pct': ((current_price - recommended_price) / current_price) * 100\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Ejemplo output\n",
    "\"\"\"\n",
    "instance_id      current_type  current_cost  recommended_type  recommended_cost  savings_pct\n",
    "i-0123456789   m5.4xlarge     $500          m5.2xlarge        $250              50%\n",
    "i-abcdef1234   c5.9xlarge     $1200         c5.4xlarge        $600              50%\n",
    "Total potential savings: $700/month ($8,400/year)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Spot Instances: 70-90% Ahorro**\n",
    "\n",
    "Spot instances son capacidad ociosa de AWS/GCP/Azure con descuentos masivos, ideal para workloads tolerantes a interrupciones.\n",
    "\n",
    "**Casos de Uso Spot:**\n",
    "- ✅ Batch ETL (reintentable)\n",
    "- ✅ ML Training (checkpoints)\n",
    "- ✅ Data analytics (stateless)\n",
    "- ✅ CI/CD builds\n",
    "- ❌ Databases (stateful)\n",
    "- ❌ Real-time APIs (baja latencia)\n",
    "\n",
    "**EMR con Spot Instances:**\n",
    "\n",
    "```python\n",
    "# AWS EMR cluster con 80% Spot + 20% On-Demand\n",
    "emr_cluster_config = {\n",
    "    \"Name\": \"etl-cluster-spot\",\n",
    "    \"Instances\": {\n",
    "        \"MasterInstanceGroup\": {\n",
    "            \"InstanceType\": \"m5.xlarge\",\n",
    "            \"InstanceCount\": 1,\n",
    "            \"Market\": \"ON_DEMAND\"  # Master siempre On-Demand\n",
    "        },\n",
    "        \"CoreInstanceGroup\": {\n",
    "            \"InstanceType\": \"r5.2xlarge\",\n",
    "            \"InstanceCount\": 2,\n",
    "            \"Market\": \"ON_DEMAND\",  # Core On-Demand (HDFS)\n",
    "            \"BidPrice\": \"auto\"\n",
    "        },\n",
    "        \"TaskInstanceGroups\": [\n",
    "            {\n",
    "                \"Name\": \"Task - Spot\",\n",
    "                \"InstanceType\": \"r5.4xlarge\",\n",
    "                \"InstanceCount\": 8,\n",
    "                \"Market\": \"SPOT\",\n",
    "                \"BidPrice\": \"auto\"  # AWS auto-pricing\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"Applications\": [{\"Name\": \"Spark\"}],\n",
    "    \"Configurations\": [\n",
    "        {\n",
    "            \"Classification\": \"spark-defaults\",\n",
    "            \"Properties\": {\n",
    "                \"spark.speculation\": \"true\",  # Re-run tasks si Spot termina\n",
    "                \"spark.speculation.multiplier\": \"2\",\n",
    "                \"spark.task.maxFailures\": \"8\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Costo comparison\n",
    "costs = {\n",
    "    \"on_demand\": {\n",
    "        \"master\": 0.192 * 1,\n",
    "        \"core\": 0.504 * 2,\n",
    "        \"task\": 1.008 * 8,\n",
    "        \"total_hourly\": 9.240\n",
    "    },\n",
    "    \"spot_mix\": {\n",
    "        \"master\": 0.192 * 1,\n",
    "        \"core\": 0.504 * 2,\n",
    "        \"task\": 0.303 * 8,  # ~70% discount\n",
    "        \"total_hourly\": 3.624\n",
    "    },\n",
    "    \"savings\": (9.240 - 3.624) / 9.240 * 100  # 60.8% ahorro\n",
    "}\n",
    "```\n",
    "\n",
    "**Spot Interruption Handling:**\n",
    "\n",
    "```python\n",
    "# Graceful termination con 2-minute warning\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "def check_spot_termination():\n",
    "    \"\"\"EC2 Instance Metadata para detectar interruption\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            'http://169.254.169.254/latest/meta-data/spot/instance-action',\n",
    "            timeout=1\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            # Spot instance será terminada en 2 minutos\n",
    "            action = response.json()\n",
    "            print(f\"⚠️ Spot termination: {action['action']} at {action['time']}\")\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            save_checkpoint()\n",
    "            \n",
    "            # Graceful shutdown\n",
    "            shutdown_gracefully()\n",
    "            \n",
    "            return True\n",
    "    except:\n",
    "        # No termination scheduled\n",
    "        return False\n",
    "\n",
    "# Spark checkpoint para resilience\n",
    "spark_conf = {\n",
    "    \"spark.speculation\": \"true\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "    \"spark.shuffle.service.enabled\": \"true\",\n",
    "    \"spark.checkpoint.dir\": \"s3://bucket/checkpoints/\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Autoscaling: Dynamic Capacity**\n",
    "\n",
    "```python\n",
    "# AWS Auto Scaling Group con target tracking\n",
    "asg_config = {\n",
    "    \"AutoScalingGroupName\": \"data-workers-asg\",\n",
    "    \"MinSize\": 2,\n",
    "    \"MaxSize\": 20,\n",
    "    \"DesiredCapacity\": 5,\n",
    "    \"LaunchTemplate\": {\n",
    "        \"LaunchTemplateId\": \"lt-xxx\",\n",
    "        \"Version\": \"$Latest\"\n",
    "    },\n",
    "    \"VPCZoneIdentifier\": \"subnet-a,subnet-b,subnet-c\",\n",
    "    \"HealthCheckType\": \"ELB\",\n",
    "    \"HealthCheckGracePeriod\": 300,\n",
    "    \"TargetGroupARNs\": [\"arn:aws:elasticloadbalancing:...\"],\n",
    "    \n",
    "    # Scaling policies\n",
    "    \"TargetTrackingConfiguration\": [\n",
    "        {\n",
    "            \"PredefinedMetricSpecification\": {\n",
    "                \"PredefinedMetricType\": \"ASGAverageCPUUtilization\"\n",
    "            },\n",
    "            \"TargetValue\": 70.0  # Scale cuando CPU > 70%\n",
    "        },\n",
    "        {\n",
    "            # Custom metric: SQS queue depth\n",
    "            \"CustomizedMetricSpecification\": {\n",
    "                \"MetricName\": \"ApproximateNumberOfMessagesVisible\",\n",
    "                \"Namespace\": \"AWS/SQS\",\n",
    "                \"Statistic\": \"Average\",\n",
    "                \"Unit\": \"Count\"\n",
    "            },\n",
    "            \"TargetValue\": 100  # Scale cuando queue > 100 msgs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Kubernetes HPA (Horizontal Pod Autoscaler)\n",
    "\"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: spark-driver\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: spark-driver\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: pending_tasks\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"100\"\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5min before scale-down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50  # Scale down max 50% at a time\n",
    "        periodSeconds: 60\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Lyft - 75% Spot Coverage**\n",
    "\n",
    "Lyft ejecuta toda su analítica en EMR con:\n",
    "- **Task Nodes**: 100% Spot (10,000+ instancias)\n",
    "- **Core Nodes**: On-Demand (HDFS persistence)\n",
    "- **Checkpointing**: S3 cada 5 minutos\n",
    "- **Interruption Rate**: ~10% (tolerado con retry)\n",
    "- **Ahorro anual**: ~$20M vs full On-Demand\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Diversify Instance Types**: Spot pools (c5, m5, r5)\n",
    "2. **Capacity Rebalancing**: AWS auto-reemplaza antes de termination\n",
    "3. **Allocation Strategy**: `price-capacity-optimized`\n",
    "4. **Checkpointing**: Guardar estado cada N minutos\n",
    "5. **Monitoring**: CloudWatch para interruption rate\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8427511",
   "metadata": {},
   "source": [
    "### 💾 **Storage Optimization: Lifecycle, Compression & Deduplication**\n",
    "\n",
    "**S3 Storage Classes: Cost vs Access Trade-off**\n",
    "\n",
    "AWS S3 ofrece 7 storage classes con diferentes precios y latencias:\n",
    "\n",
    "| Storage Class | Use Case | Latency | Cost/GB/month | Retrieval Cost |\n",
    "|---------------|----------|---------|---------------|----------------|\n",
    "| **S3 Standard** | Hot data (frecuente) | ms | $0.023 | $0 |\n",
    "| **S3 Intelligent-Tiering** | Auto-optimization | ms | $0.023-$0.0125 | $0 |\n",
    "| **S3 Standard-IA** | Infrequent access | ms | $0.0125 | $0.01/GB |\n",
    "| **S3 One Zone-IA** | Non-critical, infrequent | ms | $0.01 | $0.01/GB |\n",
    "| **S3 Glacier Instant** | Archive, instant | ms | $0.004 | $0.03/GB |\n",
    "| **S3 Glacier Flexible** | Archive, minutos-horas | 1-5 min | $0.0036 | $0.02/GB |\n",
    "| **S3 Glacier Deep Archive** | Long-term, 12h | 12 hours | $0.00099 | $0.02/GB |\n",
    "\n",
    "**Lifecycle Policy (Automated Tiering):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "lifecycle_policy = {\n",
    "    'Rules': [\n",
    "        {\n",
    "            'Id': 'data-lake-lifecycle',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {'Prefix': 'raw/'},\n",
    "            'Transitions': [\n",
    "                {\n",
    "                    'Days': 30,\n",
    "                    'StorageClass': 'STANDARD_IA'  # Después de 30 días\n",
    "                },\n",
    "                {\n",
    "                    'Days': 90,\n",
    "                    'StorageClass': 'GLACIER_IR'  # Después de 90 días\n",
    "                },\n",
    "                {\n",
    "                    'Days': 365,\n",
    "                    'StorageClass': 'DEEP_ARCHIVE'  # Después de 1 año\n",
    "                }\n",
    "            ],\n",
    "            'Expiration': {\n",
    "                'Days': 2555  # Delete después de 7 años (compliance)\n",
    "            },\n",
    "            'NoncurrentVersionTransitions': [\n",
    "                {\n",
    "                    'NoncurrentDays': 30,\n",
    "                    'StorageClass': 'GLACIER_IR'\n",
    "                }\n",
    "            ],\n",
    "            'NoncurrentVersionExpiration': {\n",
    "                'NoncurrentDays': 90\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Id': 'delete-incomplete-multipart-uploads',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {},\n",
    "            'AbortIncompleteMultipartUpload': {\n",
    "                'DaysAfterInitiation': 7  # Limpia uploads fallidos\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Id': 'logs-retention',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {'Prefix': 'logs/'},\n",
    "            'Expiration': {\n",
    "                'Days': 90  # Logs solo 90 días\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_lifecycle_configuration(\n",
    "    Bucket='my-data-lake',\n",
    "    LifecycleConfiguration=lifecycle_policy\n",
    ")\n",
    "\n",
    "# Costo Calculation\n",
    "storage_breakdown = {\n",
    "    \"raw_data\": {\n",
    "        \"size_tb\": 100,\n",
    "        \"access_pattern\": \"write-once, read-rarely\",\n",
    "        \"standard_cost\": 100 * 1024 * 0.023,  # $2,355/month\n",
    "        \"with_lifecycle\": {\n",
    "            \"standard_30d\": 100 * 1024 * 0.023 * (30/365),\n",
    "            \"ia_60d\": 100 * 1024 * 0.0125 * (60/365),\n",
    "            \"glacier_275d\": 100 * 1024 * 0.00099 * (275/365),\n",
    "            \"total\": 295  # $295/month (87% ahorro!)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Compression: Reduce Storage & Transfer Costs**\n",
    "\n",
    "Elegir codec y formato impacta significativamente en costos y performance.\n",
    "\n",
    "**Compression Codecs Comparison:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Dataset: 1 billion rows, 10 columns\n",
    "sample_data = pd.DataFrame({\n",
    "    'user_id': range(1_000_000_000),\n",
    "    'timestamp': pd.date_range('2020-01-01', periods=1_000_000_000, freq='s'),\n",
    "    'event_type': ['click'] * 500_000_000 + ['purchase'] * 500_000_000,\n",
    "    'amount': [10.5] * 1_000_000_000,\n",
    "    # ... 6 more columns\n",
    "})\n",
    "\n",
    "# Benchmark different codecs\n",
    "codecs = ['snappy', 'gzip', 'zstd', 'lz4']\n",
    "results = []\n",
    "\n",
    "for codec in codecs:\n",
    "    # Write Parquet\n",
    "    table = pa.Table.from_pandas(sample_data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        f'output_{codec}.parquet',\n",
    "        compression=codec,\n",
    "        compression_level=3 if codec == 'zstd' else None\n",
    "    )\n",
    "    \n",
    "    # Measure\n",
    "    import os\n",
    "    file_size = os.path.getsize(f'output_{codec}.parquet') / (1024**3)  # GB\n",
    "    \n",
    "    # Read benchmark\n",
    "    import time\n",
    "    start = time.time()\n",
    "    df = pq.read_table(f'output_{codec}.parquet').to_pandas()\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'codec': codec,\n",
    "        'file_size_gb': file_size,\n",
    "        'compression_ratio': 100 / file_size,  # Original 100 GB\n",
    "        'read_time_sec': read_time,\n",
    "        'storage_cost_monthly': file_size * 0.023  # S3 Standard\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\"\"\"\n",
    "codec    file_size_gb  compression_ratio  read_time_sec  storage_cost_monthly\n",
    "snappy   15.2          6.6x               45             $0.35\n",
    "gzip     8.4           11.9x              120            $0.19\n",
    "zstd     7.1           14.1x              55             $0.16  ← Winner\n",
    "lz4      16.8          5.9x               38             $0.39\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Recomendaciones por Codec:**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│  SNAPPY                                                │\n",
    "│  • Uso: Default Spark, buen balance                   │\n",
    "│  • Compresión: 6-8x (moderada)                        │\n",
    "│  • CPU: Bajo overhead                                 │\n",
    "│  • Splittable: ✅ (con Parquet)                       │\n",
    "│  • Caso: ETL daily, query frecuente                   │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  ZSTD (Zstandard)                                      │\n",
    "│  • Uso: Mejor ratio, good speed                       │\n",
    "│  • Compresión: 12-15x (alta)                          │\n",
    "│  • CPU: Moderado                                      │\n",
    "│  • Splittable: ✅                                     │\n",
    "│  • Caso: Archives, long-term storage                  │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  GZIP                                                  │\n",
    "│  • Uso: Legacy, alta compresión                       │\n",
    "│  • Compresión: 10-12x                                 │\n",
    "│  • CPU: Alto overhead (lento decode)                  │\n",
    "│  • Splittable: ❌ (sin Parquet)                       │\n",
    "│  • Caso: Logs, texto plano                            │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  LZ4                                                   │\n",
    "│  • Uso: Ultra-fast, baja compresión                   │\n",
    "│  • Compresión: 4-6x                                   │\n",
    "│  • CPU: Muy bajo                                      │\n",
    "│  • Splittable: ✅                                     │\n",
    "│  • Caso: Streaming, low-latency queries               │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Parquet Optimization (Row Groups & Page Size):**\n",
    "\n",
    "```python\n",
    "# Optimal Parquet configuration\n",
    "pq.write_table(\n",
    "    table,\n",
    "    'optimized.parquet',\n",
    "    compression='zstd',\n",
    "    compression_level=3,\n",
    "    row_group_size=128 * 1024 * 1024,  # 128 MB row groups (default: 64MB)\n",
    "    data_page_size=1024 * 1024,  # 1 MB pages (default: 1MB)\n",
    "    use_dictionary=True,  # Dictionary encoding (columnas repetitivas)\n",
    "    column_encoding={\n",
    "        'user_id': 'PLAIN',  # High cardinality\n",
    "        'event_type': 'RLE_DICTIONARY',  # Low cardinality\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ahorro con dictionary encoding\n",
    "\"\"\"\n",
    "Column: event_type (1B rows, 10 unique values)\n",
    "Without dict: 1B * 20 bytes = 20 GB\n",
    "With dict:    10 * 20 bytes + 1B * 4 bytes (indices) = 4 GB (80% ahorro!)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Deduplication: Eliminar Redundancia**\n",
    "\n",
    "```python\n",
    "# Delta Lake deduplication\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Merge con deduplication\n",
    "delta_table = DeltaTable.forPath(spark, 's3://bucket/delta-table')\n",
    "\n",
    "new_data = spark.read.parquet('s3://bucket/new-data/')\n",
    "\n",
    "# Deduplicate antes de insertar\n",
    "delta_table.alias('old').merge(\n",
    "    new_data.alias('new'),\n",
    "    'old.id = new.id AND old.timestamp = new.timestamp'\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Z-Order para reduce file scanning (performance + cost)\n",
    "delta_table.optimize().executeZOrderBy('date', 'customer_id')\n",
    "\n",
    "# Vacuum old versions (reduce storage)\n",
    "delta_table.vacuum(retentionHours=168)  # 7 días retention\n",
    "```\n",
    "\n",
    "**Caso Real: Pinterest - $20M Storage Savings**\n",
    "\n",
    "Pinterest redujo costos de S3 de $50M → $30M/año:\n",
    "\n",
    "1. **Lifecycle Policies**: 70% datos → Glacier (90 días)\n",
    "2. **Compression**: CSV → Parquet+ZSTD (10x compression)\n",
    "3. **Deduplication**: Delta Lake merge (elimina 30% duplicados)\n",
    "4. **Intelligent Tiering**: Auto-move cold data (ahorro 15%)\n",
    "5. **Vacuum**: Delete old versions (retiene 7 días vs 30)\n",
    "\n",
    "**Terraform IaC para Lifecycle:**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"data_lake\" {\n",
    "  bucket = aws_s3_bucket.data_lake.id\n",
    "\n",
    "  rule {\n",
    "    id     = \"intelligent-tiering\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    transition {\n",
    "      days          = 30\n",
    "      storage_class = \"STANDARD_IA\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 90\n",
    "      storage_class = \"GLACIER_IR\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 365\n",
    "      storage_class = \"DEEP_ARCHIVE\"\n",
    "    }\n",
    "\n",
    "    expiration {\n",
    "      days = 2555  # 7 years\n",
    "    }\n",
    "  }\n",
    "\n",
    "  rule {\n",
    "    id     = \"abort-incomplete-uploads\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    abort_incomplete_multipart_upload {\n",
    "      days_after_initiation = 7\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Monitoring Query (AWS Cost Explorer API):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "ce = boto3.client('ce')\n",
    "\n",
    "response = ce.get_cost_and_usage(\n",
    "    TimePeriod={\n",
    "        'Start': '2025-10-01',\n",
    "        'End': '2025-10-31'\n",
    "    },\n",
    "    Granularity='DAILY',\n",
    "    Metrics=['UnblendedCost'],\n",
    "    GroupBy=[\n",
    "        {'Type': 'DIMENSION', 'Key': 'SERVICE'},\n",
    "        {'Type': 'DIMENSION', 'Key': 'USAGE_TYPE'}\n",
    "    ],\n",
    "    Filter={\n",
    "        'Dimensions': {\n",
    "            'Key': 'SERVICE',\n",
    "            'Values': ['Amazon Simple Storage Service']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Analiza costos por storage class\n",
    "for result in response['ResultsByTime']:\n",
    "    print(f\"Date: {result['TimePeriod']['Start']}\")\n",
    "    for group in result['Groups']:\n",
    "        service, usage = group['Keys']\n",
    "        cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "        if cost > 0:\n",
    "            print(f\"  {usage}: ${cost:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd347931",
   "metadata": {},
   "source": [
    "### 📊 **Cost Monitoring & Anomaly Detection: Observability con FinOps**\n",
    "\n",
    "**Cost Observability Stack**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│  Cost Data Sources                                   │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  • AWS Cost & Usage Report (CUR) → S3 → Athena      │\n",
    "│  • GCP Billing Export → BigQuery                     │\n",
    "│  • Azure Cost Management → Export to Storage         │\n",
    "│                                                       │\n",
    "│  ↓                                                    │\n",
    "│  Data Processing                                     │\n",
    "│  • Spark/Airflow ETL (enrich with tags)             │\n",
    "│  • Cost allocation (chargeback logic)                │\n",
    "│  • Anomaly detection (ML models)                     │\n",
    "│                                                       │\n",
    "│  ↓                                                    │\n",
    "│  Visualization & Alerting                            │\n",
    "│  • Grafana dashboards (real-time)                    │\n",
    "│  • Slack/PagerDuty alerts                            │\n",
    "│  • Weekly reports (email)                            │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**AWS Cost & Usage Report (CUR) Setup:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Enable CUR\n",
    "cur = boto3.client('cur', region_name='us-east-1')\n",
    "\n",
    "cur.put_report_definition(\n",
    "    ReportDefinition={\n",
    "        'ReportName': 'data-platform-cur',\n",
    "        'TimeUnit': 'HOURLY',  # Granularidad horaria\n",
    "        'Format': 'Parquet',  # Mejor que CSV\n",
    "        'Compression': 'Parquet',\n",
    "        'AdditionalSchemaElements': ['RESOURCES', 'SPLIT_COST_ALLOCATION_DATA'],\n",
    "        'S3Bucket': 'cost-reports-bucket',\n",
    "        'S3Prefix': 'cur/',\n",
    "        'S3Region': 'us-east-1',\n",
    "        'AdditionalArtifacts': ['ATHENA'],  # Auto-crea Athena table\n",
    "        'RefreshClosedReports': True,\n",
    "        'ReportVersioning': 'OVERWRITE_REPORT'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Athena query para analizar CUR\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    line_item_usage_account_id,\n",
    "    product_servicename,\n",
    "    resource_tags_user_cost_center,\n",
    "    resource_tags_user_project,\n",
    "    DATE(line_item_usage_start_date) as date,\n",
    "    SUM(line_item_unblended_cost) as daily_cost\n",
    "FROM cur_database.cost_usage_report\n",
    "WHERE year = '2025' \n",
    "  AND month = '10'\n",
    "  AND line_item_line_item_type = 'Usage'\n",
    "GROUP BY 1, 2, 3, 4, 5\n",
    "ORDER BY daily_cost DESC\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={'Database': 'cur_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://query-results/'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Anomaly Detection con Machine Learning:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load historical cost data\n",
    "cost_history = pd.read_parquet('s3://cost-data/daily_costs.parquet')\n",
    "\n",
    "# Feature engineering\n",
    "cost_history['day_of_week'] = pd.to_datetime(cost_history['date']).dt.dayofweek\n",
    "cost_history['week_of_year'] = pd.to_datetime(cost_history['date']).dt.isocalendar().week\n",
    "\n",
    "# Train anomaly detector\n",
    "features = ['daily_cost', 'day_of_week', 'week_of_year']\n",
    "X = cost_history[features]\n",
    "\n",
    "model = IsolationForest(\n",
    "    contamination=0.05,  # 5% anomalías esperadas\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies para hoy\n",
    "today_cost = get_today_cost()\n",
    "today_features = [today_cost, datetime.now().weekday(), datetime.now().isocalendar()[1]]\n",
    "\n",
    "anomaly_score = model.decision_function([today_features])[0]\n",
    "is_anomaly = model.predict([today_features])[0] == -1\n",
    "\n",
    "if is_anomaly:\n",
    "    # Calculate deviation\n",
    "    expected_cost = cost_history[\n",
    "        (cost_history['day_of_week'] == today_features[1]) &\n",
    "        (cost_history['week_of_year'] == today_features[2])\n",
    "    ]['daily_cost'].mean()\n",
    "    \n",
    "    deviation_pct = ((today_cost - expected_cost) / expected_cost) * 100\n",
    "    \n",
    "    send_alert(\n",
    "        title=f\"⚠️ Cost Anomaly Detected\",\n",
    "        message=f\"Today's cost: ${today_cost:.2f}\\n\"\n",
    "                f\"Expected: ${expected_cost:.2f}\\n\"\n",
    "                f\"Deviation: {deviation_pct:+.1f}%\",\n",
    "        severity='high' if abs(deviation_pct) > 50 else 'medium'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Prophet (Facebook) para Forecasting:**\n",
    "\n",
    "```python\n",
    "from prophet import Prophet\n",
    "\n",
    "# Prepare data\n",
    "df = cost_history[['date', 'daily_cost']].rename(columns={'date': 'ds', 'daily_cost': 'y'})\n",
    "\n",
    "# Train model\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False\n",
    ")\n",
    "model.fit(df)\n",
    "\n",
    "# Forecast next 30 días\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Budget alert\n",
    "monthly_budget = 10000\n",
    "forecasted_month_cost = forecast[forecast['ds'].dt.month == datetime.now().month]['yhat'].sum()\n",
    "\n",
    "if forecasted_month_cost > monthly_budget:\n",
    "    send_alert(\n",
    "        title=\"📈 Budget Overrun Forecasted\",\n",
    "        message=f\"Forecasted: ${forecasted_month_cost:.2f}\\n\"\n",
    "                f\"Budget: ${monthly_budget:.2f}\\n\"\n",
    "                f\"Overrun: ${forecasted_month_cost - monthly_budget:.2f}\"\n",
    "    )\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "```\n",
    "\n",
    "**Real-Time Alerting con CloudWatch + Lambda:**\n",
    "\n",
    "```python\n",
    "# Lambda function triggered por CloudWatch Alarm\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Alert cuando costo diario > threshold\"\"\"\n",
    "    \n",
    "    # Parse CloudWatch alarm\n",
    "    message = json.loads(event['Records'][0]['Sns']['Message'])\n",
    "    alarm_name = message['AlarmName']\n",
    "    new_state = message['NewStateValue']\n",
    "    reason = message['NewStateReason']\n",
    "    \n",
    "    if new_state == 'ALARM':\n",
    "        # Get current cost\n",
    "        ce = boto3.client('ce')\n",
    "        response = ce.get_cost_and_usage(\n",
    "            TimePeriod={\n",
    "                'Start': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'End': (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            },\n",
    "            Granularity='DAILY',\n",
    "            Metrics=['UnblendedCost']\n",
    "        )\n",
    "        \n",
    "        current_cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])\n",
    "        \n",
    "        # Get top services\n",
    "        top_services = ce.get_cost_and_usage(\n",
    "            TimePeriod={\n",
    "                'Start': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'End': (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            },\n",
    "            Granularity='DAILY',\n",
    "            Metrics=['UnblendedCost'],\n",
    "            GroupBy=[{'Type': 'DIMENSION', 'Key': 'SERVICE'}]\n",
    "        )\n",
    "        \n",
    "        # Format Slack message\n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\"type\": \"plain_text\", \"text\": \"⚠️ Cost Spike Detected\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"fields\": [\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Current Cost:*\\n${current_cost:.2f}\"},\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Alarm:*\\n{alarm_name}\"},\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Reason:*\\n{reason}\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": \"*Top Services:*\\n\" + \"\\n\".join([\n",
    "                        f\"• {g['Keys'][0]}: ${float(g['Metrics']['UnblendedCost']['Amount']):.2f}\"\n",
    "                        for g in sorted(\n",
    "                            top_services['ResultsByTime'][0]['Groups'],\n",
    "                            key=lambda x: float(x['Metrics']['UnblendedCost']['Amount']),\n",
    "                            reverse=True\n",
    "                        )[:5]\n",
    "                    ])\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"actions\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"button\",\n",
    "                        \"text\": {\"type\": \"plain_text\", \"text\": \"View in Cost Explorer\"},\n",
    "                        \"url\": \"https://console.aws.amazon.com/cost-management/home\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Send to Slack\n",
    "        requests.post(\n",
    "            'https://hooks.slack.com/services/YOUR/WEBHOOK/URL',\n",
    "            json={\"blocks\": blocks}\n",
    "        )\n",
    "\n",
    "# CloudWatch Alarm (Terraform)\n",
    "\"\"\"\n",
    "resource \"aws_cloudwatch_metric_alarm\" \"daily_cost_spike\" {\n",
    "  alarm_name          = \"daily-cost-spike\"\n",
    "  comparison_operator = \"GreaterThanThreshold\"\n",
    "  evaluation_periods  = \"1\"\n",
    "  metric_name         = \"EstimatedCharges\"\n",
    "  namespace           = \"AWS/Billing\"\n",
    "  period              = \"21600\"  # 6 horas\n",
    "  statistic           = \"Maximum\"\n",
    "  threshold           = \"500\"  # $500/día\n",
    "  alarm_description   = \"Alert cuando costo diario > $500\"\n",
    "  alarm_actions       = [aws_sns_topic.cost_alerts.arn]\n",
    "\n",
    "  dimensions = {\n",
    "    Currency = \"USD\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Grafana Dashboard (FinOps):**\n",
    "\n",
    "```python\n",
    "# Prometheus queries\n",
    "grafana_dashboard = {\n",
    "    \"panels\": [\n",
    "        {\n",
    "            \"title\": \"Total Monthly Cost\",\n",
    "            \"query\": 'sum(aws_billing_estimated_charges{currency=\"USD\"})',\n",
    "            \"type\": \"stat\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Daily Cost Trend\",\n",
    "            \"query\": 'sum by (service) (rate(aws_cost_total[1d]))',\n",
    "            \"type\": \"graph\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Cost by Team\",\n",
    "            \"query\": 'sum by (tag_costcenter) (aws_cost_total)',\n",
    "            \"type\": \"piechart\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Budget Burn Rate\",\n",
    "            \"query\": 'rate(aws_cost_total[7d]) * 30',\n",
    "            \"type\": \"gauge\",\n",
    "            \"thresholds\": [8000, 9000, 10000]  # Budget: $10k\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Top 10 Recursos\",\n",
    "            \"query\": 'topk(10, sum by (resource_id) (aws_cost_total))',\n",
    "            \"type\": \"table\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Savings Opportunities\",\n",
    "            \"query\": 'sum(aws_rightsizing_savings_potential)',\n",
    "            \"type\": \"stat\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Export to JSON\n",
    "import json\n",
    "with open('finops_dashboard.json', 'w') as f:\n",
    "    json.dump(grafana_dashboard, f, indent=2)\n",
    "```\n",
    "\n",
    "**Weekly Cost Report (Automated):**\n",
    "\n",
    "```python\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "\n",
    "def generate_weekly_report():\n",
    "    \"\"\"Genera reporte semanal de costos\"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    last_week_cost = get_cost_for_period(days=7)\n",
    "    previous_week_cost = get_cost_for_period(days=14, offset=7)\n",
    "    \n",
    "    wow_change = ((last_week_cost - previous_week_cost) / previous_week_cost) * 100\n",
    "    \n",
    "    # Top services\n",
    "    top_services = get_cost_by_service(days=7)\n",
    "    \n",
    "    # Top resources\n",
    "    top_resources = get_cost_by_resource(days=7)\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations = get_cost_recommendations()\n",
    "    \n",
    "    # Generate HTML report\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "      <body>\n",
    "        <h1>📊 Weekly Cost Report</h1>\n",
    "        <h2>Overview</h2>\n",
    "        <p><strong>Last Week:</strong> ${last_week_cost:.2f}</p>\n",
    "        <p><strong>Previous Week:</strong> ${previous_week_cost:.2f}</p>\n",
    "        <p><strong>Change:</strong> {wow_change:+.1f}%</p>\n",
    "        \n",
    "        <h2>Top Services</h2>\n",
    "        <table border=\"1\">\n",
    "          <tr><th>Service</th><th>Cost</th><th>% of Total</th></tr>\n",
    "          {''.join([f\"<tr><td>{s['name']}</td><td>${s['cost']:.2f}</td><td>{s['pct']:.1f}%</td></tr>\" for s in top_services[:5]])}\n",
    "        </table>\n",
    "        \n",
    "        <h2>💰 Savings Opportunities</h2>\n",
    "        <ul>\n",
    "          {''.join([f\"<li>{r['description']}: <strong>${r['savings']:.2f}/month</strong></li>\" for r in recommendations])}\n",
    "        </ul>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send email\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = f\"Weekly Cost Report - ${last_week_cost:.2f}\"\n",
    "    msg['From'] = 'finops@company.com'\n",
    "    msg['To'] = 'team@company.com'\n",
    "    \n",
    "    msg.attach(MIMEText(html, 'html'))\n",
    "    \n",
    "    smtp = smtplib.SMTP('smtp.company.com')\n",
    "    smtp.send_message(msg)\n",
    "    smtp.quit()\n",
    "\n",
    "# Airflow DAG para schedule\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    'weekly_cost_report',\n",
    "    schedule_interval='0 9 * * MON',  # Lunes 9am\n",
    "    default_args={'owner': 'finops'}\n",
    ")\n",
    "\n",
    "PythonOperator(\n",
    "    task_id='generate_report',\n",
    "    python_callable=generate_weekly_report,\n",
    "    dag=dag\n",
    ")\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Airbnb - Cost Attribution System**\n",
    "\n",
    "Airbnb construyó un sistema interno de FinOps:\n",
    "- **Real-time Dashboards**: Grafana con costo por servicio cada 15 min\n",
    "- **ML Anomaly Detection**: Prophet + IsolationForest\n",
    "- **Chargeback**: Cada equipo ve su gasto en tiempo real\n",
    "- **Budget Enforcement**: Alertas cuando equipo > 80% budget\n",
    "- **Resultado**: 30% reducción en waste, $100M+ ahorros/año\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b3f3b",
   "metadata": {},
   "source": [
    "## 1. Principios de FinOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bffde",
   "metadata": {},
   "source": [
    "- Visibilidad: tagging de recursos, cost allocation por proyecto/equipo.\n",
    "- Accountability: cada equipo/dominio responsable de su presupuesto.\n",
    "- Optimización continua: rightsizing, reservas, spot instances, lifecycle policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d3813",
   "metadata": {},
   "source": [
    "## 2. Estrategias de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed4168",
   "metadata": {},
   "source": [
    "### 2.1 Compute\n",
    "- Spot/Preemptible para workloads tolerantes a interrupciones (batch, training).\n",
    "- Reserved/Committed para cargas predecibles (> 1 año).\n",
    "- Auto-scaling por métricas (CPU, latencia, queue depth).\n",
    "\n",
    "### 2.2 Storage\n",
    "- Lifecycle policies: S3 Standard → IA → Glacier → Deep Archive.\n",
    "- Compresión y formatos eficientes (Parquet/ORC con Snappy/ZSTD).\n",
    "- Borrado de snapshots y versiones antiguas.\n",
    "\n",
    "### 2.3 Networking\n",
    "- Minimizar data transfer inter-región.\n",
    "- Usar VPC endpoints y PrivateLink para evitar egress a internet.\n",
    "- CDN (CloudFront/Cloud CDN) para datos frecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7843c",
   "metadata": {},
   "source": [
    "## 3. Ejemplo: benchmark de costo por TB procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29453b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "  {'Pipeline':'ETL Diario', 'TB_procesados':5.2, 'Costo_USD':42.0, 'USD_per_TB':8.08},\n",
    "  {'Pipeline':'Streaming Kafka', 'TB_procesados':1.8, 'Costo_USD':95.0, 'USD_per_TB':52.78},\n",
    "  {'Pipeline':'ML Training', 'TB_procesados':0.3, 'Costo_USD':120.0, 'USD_per_TB':400.0},\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cc1a0",
   "metadata": {},
   "source": [
    "## 4. Alertas y presupuestos (AWS Budgets ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_demo = r'''\n",
    "# AWS CLI para crear presupuesto mensual de $1000 con alertas al 80% y 100%\n",
    "aws budgets create-budget \\\n",
    "  --account-id 123456789012 \\\n",
    "  --budget file://budget.json \\\n",
    "  --notifications-with-subscribers file://notifications.json\n",
    "\n",
    "# budget.json\n",
    "{\n",
    "  \"BudgetName\": \"data-platform-monthly\",\n",
    "  \"BudgetLimit\": {\"Amount\": \"1000\", \"Unit\": \"USD\"},\n",
    "  \"TimeUnit\": \"MONTHLY\",\n",
    "  \"BudgetType\": \"COST\"\n",
    "}\n",
    "\n",
    "# notifications.json (alertas al 80% y 100%)\n",
    "'''\n",
    "print(budget_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacab03",
   "metadata": {},
   "source": [
    "## 5. Métricas clave de FinOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac981e",
   "metadata": {},
   "source": [
    "- **Costo por TB procesado** (USD/TB).\n",
    "- **Costo por evento** (USD/millón eventos).\n",
    "- **Utilización de recursos** (% CPU/RAM usados vs aprovisionados).\n",
    "- **Savings por optimización** (Spot, RI, rightsizing).\n",
    "- **Trend mensual** (crecimiento de gasto vs negocio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eac008",
   "metadata": {},
   "source": [
    "## 6. Herramientas y dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c62932",
   "metadata": {},
   "source": [
    "- AWS Cost Explorer, GCP Billing Reports, Azure Cost Management.\n",
    "- Terraform/CDK para IaC con políticas de costos.\n",
    "- Terceros: Cloudability, CloudHealth, Vantage.\n",
    "- Dashboard custom con Grafana + Prometheus exporters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
