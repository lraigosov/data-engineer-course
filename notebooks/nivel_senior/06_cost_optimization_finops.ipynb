{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6ed643",
   "metadata": {},
   "source": [
    "# ğŸ’° Cost Optimization y FinOps en la Nube\n",
    "\n",
    "Objetivo: aplicar tÃ©cnicas de optimizaciÃ³n de costos cloud (AWS/GCP/Azure), establecer presupuestos, alertas y benchmarks, con mÃ©tricas de eficiencia por workload.\n",
    "\n",
    "- DuraciÃ³n: 120 min\n",
    "- Dificultidad: Alta\n",
    "- Prerrequisitos: Mid 03 (AWS), experiencia con pipelines en cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b51785",
   "metadata": {},
   "source": [
    "### ğŸ’¡ **FinOps Framework: Cloud Cost Management como PrÃ¡ctica EstratÃ©gica**\n",
    "\n",
    "**Â¿QuÃ© es FinOps?**\n",
    "\n",
    "FinOps (Financial Operations) es la prÃ¡ctica de traer accountability financiera al modelo variable de gasto en la nube, combinando finanzas, tecnologÃ­a y negocios.\n",
    "\n",
    "**Fases del Ciclo FinOps:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚               FinOps Lifecycle                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. INFORM (Visibilidad)                                â”‚\n",
    "â”‚     â€¢ Cost allocation y tagging                         â”‚\n",
    "â”‚     â€¢ Showback/Chargeback por equipo                    â”‚\n",
    "â”‚     â€¢ Anomaly detection                                 â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  2. OPTIMIZE (Eficiencia)                               â”‚\n",
    "â”‚     â€¢ Rightsizing (reduce over-provisioning)            â”‚\n",
    "â”‚     â€¢ Reserved Instances / Savings Plans                â”‚\n",
    "â”‚     â€¢ Spot/Preemptible instances                        â”‚\n",
    "â”‚     â€¢ Storage lifecycle policies                        â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  3. OPERATE (Cultura)                                   â”‚\n",
    "â”‚     â€¢ Budget owners por dominio                         â”‚\n",
    "â”‚     â€¢ Cost awareness en desarrollo                      â”‚\n",
    "â”‚     â€¢ Continuous improvement                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Principios Fundamentales:**\n",
    "\n",
    "1. **Teams Need to Collaborate**: IngenierÃ­a, Finanzas, Product\n",
    "2. **Everyone Takes Ownership**: Cada equipo responsable de su gasto\n",
    "3. **Centralized Team Drives FinOps**: FinOps team como facilitador\n",
    "4. **Reports Should Be Accessible**: Dashboards self-service\n",
    "5. **Decisions Driven by Business Value**: Costo vs impacto en negocio\n",
    "6. **Take Advantage of Variable Cost**: Elasticidad como ventaja\n",
    "\n",
    "**Modelo de Responsabilidad:**\n",
    "\n",
    "| Rol | Responsabilidades |\n",
    "|-----|-------------------|\n",
    "| **FinOps Team** | PolÃ­ticas, herramientas, reporting, training |\n",
    "| **Engineers** | Implementar optimizaciones, cost-aware design |\n",
    "| **Finance** | Forecasting, budgets, business case analysis |\n",
    "| **Executives** | Aprobar inversiones, cost targets estratÃ©gicos |\n",
    "| **Product** | Priorizar features vs costo, unit economics |\n",
    "\n",
    "**Cost Allocation Strategy (Tagging):**\n",
    "\n",
    "```python\n",
    "# AWS Resource Tagging Standard\n",
    "tagging_strategy = {\n",
    "    # Mandatory tags (billing)\n",
    "    \"Environment\": [\"dev\", \"staging\", \"prod\"],\n",
    "    \"CostCenter\": [\"engineering\", \"data\", \"marketing\"],\n",
    "    \"Project\": [\"data-platform\", \"ml-ops\", \"analytics\"],\n",
    "    \"Owner\": \"email@company.com\",\n",
    "    \n",
    "    # Optional (technical)\n",
    "    \"Application\": \"spark-etl\",\n",
    "    \"ManagedBy\": \"terraform\",\n",
    "    \"Compliance\": [\"gdpr\", \"sox\", \"hipaa\"],\n",
    "}\n",
    "\n",
    "# Terraform enforcement\n",
    "resource \"aws_s3_bucket\" \"data_lake\" {\n",
    "  bucket = \"my-data-lake\"\n",
    "  \n",
    "  tags = {\n",
    "    Environment = \"prod\"\n",
    "    CostCenter  = \"data\"\n",
    "    Project     = \"data-platform\"\n",
    "    Owner       = \"data-team@company.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tag compliance policy (AWS Config)\n",
    "aws configservice put-config-rule --config-rule '{\n",
    "  \"ConfigRuleName\": \"required-tags\",\n",
    "  \"Source\": {\n",
    "    \"Owner\": \"AWS\",\n",
    "    \"SourceIdentifier\": \"REQUIRED_TAGS\"\n",
    "  },\n",
    "  \"InputParameters\": \"{\\\"tag1Key\\\":\\\"CostCenter\\\",\\\"tag2Key\\\":\\\"Project\\\"}\"\n",
    "}'\n",
    "```\n",
    "\n",
    "**Showback vs Chargeback:**\n",
    "\n",
    "```python\n",
    "# Showback: Informar costos (no cobrar)\n",
    "monthly_report = {\n",
    "    \"team\": \"Data Engineering\",\n",
    "    \"month\": \"2025-10\",\n",
    "    \"breakdown\": {\n",
    "        \"compute\": {\"ec2\": 1200, \"emr\": 3500},\n",
    "        \"storage\": {\"s3\": 800, \"ebs\": 400},\n",
    "        \"networking\": {\"data_transfer\": 300},\n",
    "        \"total\": 6200\n",
    "    },\n",
    "    \"trend\": \"+15% vs last month\",\n",
    "    \"top_resources\": [\n",
    "        {\"resource\": \"emr-cluster-prod\", \"cost\": 2800},\n",
    "        {\"resource\": \"ec2-spark-workers\", \"cost\": 900}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Chargeback: Cobrar internamente a cada equipo\n",
    "chargeback_policy = {\n",
    "    \"data_team\": {\n",
    "        \"budget\": 10000,\n",
    "        \"actual\": 6200,\n",
    "        \"remaining\": 3800,\n",
    "        \"action\": \"Approved\" if 6200 < 10000 else \"Budget Review\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Unit Economics para Data Platforms:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  MÃ©tricas de Eficiencia                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ Cost per TB Processed:  $5 - $50            â”‚\n",
    "â”‚    (depende de transformaciÃ³n complexity)      â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â€¢ Cost per Million Events: $0.10 - $5.00      â”‚\n",
    "â”‚    (streaming platforms)                       â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â€¢ Cost per Query: $0.001 - $1.00              â”‚\n",
    "â”‚    (BigQuery, Athena)                          â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â€¢ Cost per ML Training Run: $10 - $500        â”‚\n",
    "â”‚    (model size, GPU hours)                     â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â€¢ Storage Cost per TB/month: $20 - $200       â”‚\n",
    "â”‚    (Standard â†’ Glacier Deep Archive)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Caso Real: Netflix FinOps**\n",
    "\n",
    "Netflix procesa ~15 PB/dÃ­a con presupuesto de ~$300M/aÃ±o en AWS:\n",
    "\n",
    "- **Tagging Strategy**: 100% de recursos taggeados con show/movie ID\n",
    "- **Spot Instances**: 80% de workloads batch en Spot (ahorro 70%)\n",
    "- **S3 Lifecycle**: 90% de contenido en Glacier (ahorro $50M/aÃ±o)\n",
    "- **Regional Optimization**: Data replication solo en regiones activas\n",
    "- **Custom Metrics**: Costo por hora de streaming por usuario\n",
    "\n",
    "**Dashboard FinOps (Grafana):**\n",
    "\n",
    "```python\n",
    "# Prometheus queries para dashboard\n",
    "queries = {\n",
    "    \"total_monthly_cost\": 'sum(aws_cost_total{period=\"monthly\"})',\n",
    "    \"cost_by_service\": 'sum by (service) (aws_cost_total)',\n",
    "    \"cost_by_team\": 'sum by (tag_costcenter) (aws_cost_total)',\n",
    "    \"savings_opportunity\": 'sum(aws_rightsizing_recommendations)',\n",
    "    \"budget_burn_rate\": 'rate(aws_cost_total[7d]) * 30',\n",
    "}\n",
    "\n",
    "# Alertas\n",
    "alerts = {\n",
    "    \"budget_80_percent\": \"Monthly cost > 80% of budget\",\n",
    "    \"cost_spike\": \"Cost increase > 50% vs last week\",\n",
    "    \"untagged_resources\": \"Resources without CostCenter tag > 5%\",\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdbe05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ’° FINOPS SIMULATION - DATA PLATFORM COST ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset de costos generado:\n",
      "   â€¢ PerÃ­odo: 2025-09-10 â†’ 2025-12-08\n",
      "   â€¢ Registros: 1,440\n",
      "   â€¢ Servicios: 8\n",
      "   â€¢ Equipos: 4\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ RESUMEN MENSUAL DE COSTOS\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¸ Costos Totales:\n",
      "   â€¢ Ãšltimos 90 dÃ­as: $1,368,147.92\n",
      "   â€¢ Promedio mensual: $456,049.31\n",
      "   â€¢ Promedio diario: $15,201.64\n",
      "\n",
      "ğŸ“¦ Top 5 Servicios MÃ¡s Costosos (90 dÃ­as):\n",
      "   1. EMR: $432,913.37 (31.6%)\n",
      "   2. Redshift: $361,717.81 (26.4%)\n",
      "   3. EC2: $252,607.13 (18.5%)\n",
      "   4. S3: $100,013.39 (7.3%)\n",
      "   5. Glue: $92,314.81 (6.7%)\n",
      "\n",
      "ğŸ‘¥ Costos por Equipo:\n",
      "   â€¢ Business Intelligence: $351,108.17 (25.7%)\n",
      "   â€¢ Analytics: $347,465.74 (25.4%)\n",
      "   â€¢ ML Team: $335,956.70 (24.6%)\n",
      "   â€¢ Data Engineering: $333,617.31 (24.4%)\n",
      "\n",
      "ğŸŒ Costos por Ambiente:\n",
      "   â€¢ prod: $1,325,968.51 (96.9%)\n",
      "   â€¢ dev: $21,882.28 (1.6%)\n",
      "   â€¢ staging: $20,297.14 (1.5%)\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ UNIT ECONOMICS - MÃ©tricas de Eficiencia\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š MÃ©tricas de eficiencia:\n",
      "   â€¢ Cost per TB processed: $3040.33\n",
      "   â€¢ Cost per Million Events: $16.0959\n",
      "   â€¢ Cost per Query: $1.1401\n",
      "\n",
      "ğŸ“ˆ VolÃºmenes procesados (90 dÃ­as):\n",
      "   â€¢ Data processed: 450 TB\n",
      "   â€¢ Events ingested: 85,000,000,000 eventos\n",
      "   â€¢ Queries executed: 1,200,000 queries\n",
      "\n",
      "ğŸ¯ Benchmarks de la industria:\n",
      "          MÃ©trica Tu valor    Benchmark     Estado\n",
      "      Cost per TB $3040.33       $5-$50 âš ï¸ Revisar\n",
      "Cost per M Events $16.0959  $0.10-$5.00 âš ï¸ Revisar\n",
      "   Cost per Query  $1.1401 $0.001-$1.00 âš ï¸ Revisar\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ ANOMALY DETECTION - DetecciÃ³n de Picos de Costo\n",
      "================================================================================\n",
      "\n",
      "ğŸ” AnÃ¡lisis de anomalÃ­as:\n",
      "   â€¢ Costo diario promedio: $15,201.64\n",
      "   â€¢ DesviaciÃ³n estÃ¡ndar: $3,277.91\n",
      "   â€¢ Threshold de alerta (mean + 2Ïƒ): $21,757.47\n",
      "   â€¢ DÃ­as con anomalÃ­as: 2 de 90 (2.2%)\n",
      "\n",
      "âš ï¸ Top 3 dÃ­as con mayor costo anÃ³malo:\n",
      "   â€¢ 2025-09-14 (Sunday): $25,888.10 (+70.3% vs promedio)\n",
      "   â€¢ 2025-10-16 (Thursday): $22,744.08 (+49.6% vs promedio)\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ SAVINGS OPPORTUNITIES - Oportunidades de Ahorro\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Oportunidades identificadas:\n",
      "\n",
      "   EMR Spot Instances:\n",
      "   â””â”€ Ahorro: $303,039.36 (22.1% del total)\n",
      "   â””â”€ CÃ³mo: Migrar workloads batch a Spot\n",
      "   â””â”€ Complejidad: Alta\n",
      "\n",
      "   Redshift Reserved Instances:\n",
      "   â””â”€ Ahorro: $144,687.12 (10.6% del total)\n",
      "   â””â”€ CÃ³mo: 1-year RI commitment\n",
      "   â””â”€ Complejidad: Baja\n",
      "\n",
      "   EC2 Rightsizing:\n",
      "   â””â”€ Ahorro: $63,151.78 (4.6% del total)\n",
      "   â””â”€ CÃ³mo: Reducir instancias sobre-aprovisionadas 25%\n",
      "   â””â”€ Complejidad: Media\n",
      "\n",
      "   S3 Lifecycle Policies:\n",
      "   â””â”€ Ahorro: $60,008.03 (4.4% del total)\n",
      "   â””â”€ CÃ³mo: Mover data >90 dÃ­as a Glacier\n",
      "   â””â”€ Complejidad: Baja\n",
      "\n",
      "================================================================================\n",
      "ğŸ’° AHORRO TOTAL POTENCIAL: $570,886.30\n",
      "   â€¢ ReducciÃ³n de costos: 41.7%\n",
      "   â€¢ Nuevo costo mensual: $265,753.87\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’° SimulaciÃ³n: AnÃ¡lisis de Costos Cloud (Data Platform)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ’° FINOPS SIMULATION - DATA PLATFORM COST ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= GENERAR DATOS DE COSTOS =============\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Simular 90 dÃ­as de costos por servicio\n",
    "services = ['S3', 'EMR', 'EC2', 'Glue', 'Athena', 'Redshift', 'Data Transfer', 'Lambda']\n",
    "teams = ['Data Engineering', 'Analytics', 'ML Team', 'Business Intelligence']\n",
    "environments = ['dev', 'staging', 'prod']\n",
    "\n",
    "cost_data = []\n",
    "for day in range(90):\n",
    "    date = now - timedelta(days=90-day)\n",
    "    \n",
    "    for service in services:\n",
    "        for team in np.random.choice(teams, size=2, replace=False):\n",
    "            env = np.random.choice(environments, p=[0.2, 0.1, 0.7])  # Prod mÃ¡s costoso\n",
    "            \n",
    "            # Base cost por servicio\n",
    "            base_costs = {\n",
    "                'S3': np.random.uniform(200, 400),\n",
    "                'EMR': np.random.uniform(500, 2000),\n",
    "                'EC2': np.random.uniform(300, 1200),\n",
    "                'Glue': np.random.uniform(100, 500),\n",
    "                'Athena': np.random.uniform(50, 200),\n",
    "                'Redshift': np.random.uniform(800, 1500),\n",
    "                'Data Transfer': np.random.uniform(100, 300),\n",
    "                'Lambda': np.random.uniform(20, 100)\n",
    "            }\n",
    "            \n",
    "            daily_cost = base_costs[service]\n",
    "            \n",
    "            # Prod cuesta mÃ¡s\n",
    "            if env == 'prod':\n",
    "                daily_cost *= 2.5\n",
    "            elif env == 'staging':\n",
    "                daily_cost *= 0.3\n",
    "            else:  # dev\n",
    "                daily_cost *= 0.15\n",
    "            \n",
    "            # Agregar ruido\n",
    "            daily_cost *= np.random.uniform(0.8, 1.2)\n",
    "            \n",
    "            cost_data.append({\n",
    "                'date': date,\n",
    "                'service': service,\n",
    "                'team': team,\n",
    "                'environment': env,\n",
    "                'cost_usd': daily_cost\n",
    "            })\n",
    "\n",
    "df_costs = pd.DataFrame(cost_data)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset de costos generado:\")\n",
    "print(f\"   â€¢ PerÃ­odo: {df_costs['date'].min().date()} â†’ {df_costs['date'].max().date()}\")\n",
    "print(f\"   â€¢ Registros: {len(df_costs):,}\")\n",
    "print(f\"   â€¢ Servicios: {len(services)}\")\n",
    "print(f\"   â€¢ Equipos: {len(teams)}\")\n",
    "\n",
    "# ============= RESUMEN DE COSTOS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1ï¸âƒ£ RESUMEN MENSUAL DE COSTOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_cost_90d = df_costs['cost_usd'].sum()\n",
    "monthly_cost = total_cost_90d / 3\n",
    "daily_avg = total_cost_90d / 90\n",
    "\n",
    "print(f\"\\nğŸ’¸ Costos Totales:\")\n",
    "print(f\"   â€¢ Ãšltimos 90 dÃ­as: ${total_cost_90d:,.2f}\")\n",
    "print(f\"   â€¢ Promedio mensual: ${monthly_cost:,.2f}\")\n",
    "print(f\"   â€¢ Promedio diario: ${daily_avg:,.2f}\")\n",
    "\n",
    "# Por servicio\n",
    "print(f\"\\nğŸ“¦ Top 5 Servicios MÃ¡s Costosos (90 dÃ­as):\")\n",
    "by_service = df_costs.groupby('service')['cost_usd'].sum().sort_values(ascending=False)\n",
    "for idx, (service, cost) in enumerate(by_service.head(5).items(), 1):\n",
    "    pct = cost / total_cost_90d * 100\n",
    "    print(f\"   {idx}. {service}: ${cost:,.2f} ({pct:.1f}%)\")\n",
    "\n",
    "# Por equipo\n",
    "print(f\"\\nğŸ‘¥ Costos por Equipo:\")\n",
    "by_team = df_costs.groupby('team')['cost_usd'].sum().sort_values(ascending=False)\n",
    "for team, cost in by_team.items():\n",
    "    pct = cost / total_cost_90d * 100\n",
    "    print(f\"   â€¢ {team}: ${cost:,.2f} ({pct:.1f}%)\")\n",
    "\n",
    "# Por ambiente\n",
    "print(f\"\\nğŸŒ Costos por Ambiente:\")\n",
    "by_env = df_costs.groupby('environment')['cost_usd'].sum().sort_values(ascending=False)\n",
    "for env, cost in by_env.items():\n",
    "    pct = cost / total_cost_90d * 100\n",
    "    print(f\"   â€¢ {env}: ${cost:,.2f} ({pct:.1f}%)\")\n",
    "\n",
    "# ============= UNIT ECONOMICS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2ï¸âƒ£ UNIT ECONOMICS - MÃ©tricas de Eficiencia\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular volumen de datos procesados\n",
    "tb_processed_90d = 450  # TB\n",
    "events_processed_90d = 85_000_000_000  # 85B eventos\n",
    "queries_executed_90d = 1_200_000  # 1.2M queries\n",
    "\n",
    "cost_per_tb = total_cost_90d / tb_processed_90d\n",
    "cost_per_million_events = total_cost_90d / (events_processed_90d / 1_000_000)\n",
    "cost_per_query = total_cost_90d / queries_executed_90d\n",
    "\n",
    "print(f\"\\nğŸ“Š MÃ©tricas de eficiencia:\")\n",
    "print(f\"   â€¢ Cost per TB processed: ${cost_per_tb:.2f}\")\n",
    "print(f\"   â€¢ Cost per Million Events: ${cost_per_million_events:.4f}\")\n",
    "print(f\"   â€¢ Cost per Query: ${cost_per_query:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ VolÃºmenes procesados (90 dÃ­as):\")\n",
    "print(f\"   â€¢ Data processed: {tb_processed_90d:,} TB\")\n",
    "print(f\"   â€¢ Events ingested: {events_processed_90d:,} eventos\")\n",
    "print(f\"   â€¢ Queries executed: {queries_executed_90d:,} queries\")\n",
    "\n",
    "# Benchmarks\n",
    "print(f\"\\nğŸ¯ Benchmarks de la industria:\")\n",
    "benchmarks = pd.DataFrame([\n",
    "    {'MÃ©trica': 'Cost per TB', 'Tu valor': f'${cost_per_tb:.2f}', 'Benchmark': '$5-$50', 'Estado': 'âœ… Ã“ptimo' if cost_per_tb < 50 else 'âš ï¸ Revisar'},\n",
    "    {'MÃ©trica': 'Cost per M Events', 'Tu valor': f'${cost_per_million_events:.4f}', 'Benchmark': '$0.10-$5.00', 'Estado': 'âœ… Ã“ptimo' if cost_per_million_events < 5 else 'âš ï¸ Revisar'},\n",
    "    {'MÃ©trica': 'Cost per Query', 'Tu valor': f'${cost_per_query:.4f}', 'Benchmark': '$0.001-$1.00', 'Estado': 'âœ… Ã“ptimo' if cost_per_query < 1 else 'âš ï¸ Revisar'},\n",
    "])\n",
    "print(benchmarks.to_string(index=False))\n",
    "\n",
    "# ============= ANOMALY DETECTION =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3ï¸âƒ£ ANOMALY DETECTION - DetecciÃ³n de Picos de Costo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Agrupar por dÃ­a\n",
    "daily_costs = df_costs.groupby('date')['cost_usd'].sum().reset_index()\n",
    "daily_costs['day_of_week'] = daily_costs['date'].dt.day_name()\n",
    "\n",
    "# Calcular estadÃ­sticas\n",
    "mean_daily = daily_costs['cost_usd'].mean()\n",
    "std_daily = daily_costs['cost_usd'].std()\n",
    "threshold = mean_daily + (2 * std_daily)\n",
    "\n",
    "# Detectar anomalÃ­as (>2 std dev)\n",
    "anomalies = daily_costs[daily_costs['cost_usd'] > threshold]\n",
    "\n",
    "print(f\"\\nğŸ” AnÃ¡lisis de anomalÃ­as:\")\n",
    "print(f\"   â€¢ Costo diario promedio: ${mean_daily:,.2f}\")\n",
    "print(f\"   â€¢ DesviaciÃ³n estÃ¡ndar: ${std_daily:,.2f}\")\n",
    "print(f\"   â€¢ Threshold de alerta (mean + 2Ïƒ): ${threshold:,.2f}\")\n",
    "print(f\"   â€¢ DÃ­as con anomalÃ­as: {len(anomalies)} de {len(daily_costs)} ({len(anomalies)/len(daily_costs)*100:.1f}%)\")\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(f\"\\nâš ï¸ Top 3 dÃ­as con mayor costo anÃ³malo:\")\n",
    "    for idx, row in anomalies.nlargest(3, 'cost_usd').iterrows():\n",
    "        pct_over = ((row['cost_usd'] - mean_daily) / mean_daily) * 100\n",
    "        print(f\"   â€¢ {row['date'].date()} ({row['day_of_week']}): ${row['cost_usd']:,.2f} (+{pct_over:.1f}% vs promedio)\")\n",
    "\n",
    "# ============= SAVINGS OPPORTUNITIES =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4ï¸âƒ£ SAVINGS OPPORTUNITIES - Oportunidades de Ahorro\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular oportunidades\n",
    "savings_opportunities = []\n",
    "\n",
    "# 1. Rightsizing\n",
    "over_provisioned_ec2 = df_costs[df_costs['service'] == 'EC2']['cost_usd'].sum() * 0.25\n",
    "savings_opportunities.append({\n",
    "    'Oportunidad': 'EC2 Rightsizing',\n",
    "    'Ahorro Potencial': over_provisioned_ec2,\n",
    "    'ImplementaciÃ³n': 'Reducir instancias sobre-aprovisionadas 25%',\n",
    "    'Complejidad': 'Media'\n",
    "})\n",
    "\n",
    "# 2. Spot Instances\n",
    "emr_cost = df_costs[df_costs['service'] == 'EMR']['cost_usd'].sum()\n",
    "spot_savings = emr_cost * 0.70  # 70% ahorro con Spot\n",
    "savings_opportunities.append({\n",
    "    'Oportunidad': 'EMR Spot Instances',\n",
    "    'Ahorro Potencial': spot_savings,\n",
    "    'ImplementaciÃ³n': 'Migrar workloads batch a Spot',\n",
    "    'Complejidad': 'Alta'\n",
    "})\n",
    "\n",
    "# 3. S3 Lifecycle\n",
    "s3_cost = df_costs[df_costs['service'] == 'S3']['cost_usd'].sum()\n",
    "lifecycle_savings = s3_cost * 0.60  # 60% ahorro con Glacier\n",
    "savings_opportunities.append({\n",
    "    'Oportunidad': 'S3 Lifecycle Policies',\n",
    "    'Ahorro Potencial': lifecycle_savings,\n",
    "    'ImplementaciÃ³n': 'Mover data >90 dÃ­as a Glacier',\n",
    "    'Complejidad': 'Baja'\n",
    "})\n",
    "\n",
    "# 4. Reserved Instances\n",
    "redshift_cost = df_costs[df_costs['service'] == 'Redshift']['cost_usd'].sum()\n",
    "ri_savings = redshift_cost * 0.40  # 40% ahorro con RI\n",
    "savings_opportunities.append({\n",
    "    'Oportunidad': 'Redshift Reserved Instances',\n",
    "    'Ahorro Potencial': ri_savings,\n",
    "    'ImplementaciÃ³n': '1-year RI commitment',\n",
    "    'Complejidad': 'Baja'\n",
    "})\n",
    "\n",
    "df_savings = pd.DataFrame(savings_opportunities)\n",
    "df_savings = df_savings.sort_values('Ahorro Potencial', ascending=False)\n",
    "\n",
    "total_savings_potential = df_savings['Ahorro Potencial'].sum()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Oportunidades identificadas:\")\n",
    "for idx, row in df_savings.iterrows():\n",
    "    pct_savings = (row['Ahorro Potencial'] / total_cost_90d) * 100\n",
    "    print(f\"\\n   {row['Oportunidad']}:\")\n",
    "    print(f\"   â””â”€ Ahorro: ${row['Ahorro Potencial']:,.2f} ({pct_savings:.1f}% del total)\")\n",
    "    print(f\"   â””â”€ CÃ³mo: {row['ImplementaciÃ³n']}\")\n",
    "    print(f\"   â””â”€ Complejidad: {row['Complejidad']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ’° AHORRO TOTAL POTENCIAL: ${total_savings_potential:,.2f}\")\n",
    "print(f\"   â€¢ ReducciÃ³n de costos: {(total_savings_potential/total_cost_90d)*100:.1f}%\")\n",
    "print(f\"   â€¢ Nuevo costo mensual: ${monthly_cost - (total_savings_potential/3):,.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a509f",
   "metadata": {},
   "source": [
    "### âš¡ **Compute Optimization: Rightsizing, Spot & Autoscaling**\n",
    "\n",
    "**Rightsizing: Eliminar Over-Provisioning**\n",
    "\n",
    "El 30-40% de recursos cloud estÃ¡n sobre-aprovisionados. Rightsizing ajusta instancias al uso real.\n",
    "\n",
    "**AWS EC2 Rightsizing Process:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# CloudWatch metrics para rightsizing\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "ce = boto3.client('ce')  # Cost Explorer\n",
    "\n",
    "def get_instance_utilization(instance_id, days=14):\n",
    "    \"\"\"Analiza CPU/Memory/Network de instancia\"\"\"\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=days)\n",
    "    \n",
    "    # CPU utilization\n",
    "    cpu_response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/EC2',\n",
    "        MetricName='CPUUtilization',\n",
    "        Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n",
    "        StartTime=start,\n",
    "        EndTime=end,\n",
    "        Period=3600,  # 1 hora\n",
    "        Statistics=['Average', 'Maximum']\n",
    "    )\n",
    "    \n",
    "    cpu_avg = sum(dp['Average'] for dp in cpu_response['Datapoints']) / len(cpu_response['Datapoints'])\n",
    "    cpu_max = max(dp['Maximum'] for dp in cpu_response['Datapoints'])\n",
    "    \n",
    "    return {\n",
    "        'instance_id': instance_id,\n",
    "        'cpu_avg': cpu_avg,\n",
    "        'cpu_max': cpu_max,\n",
    "        'recommendation': 'downsize' if cpu_avg < 20 else 'keep' if cpu_avg < 70 else 'upsize'\n",
    "    }\n",
    "\n",
    "# Rightsizing recommendations\n",
    "def generate_rightsizing_report(ec2_client):\n",
    "    instances = ec2_client.describe_instances()\n",
    "    \n",
    "    recommendations = []\n",
    "    for reservation in instances['Reservations']:\n",
    "        for instance in reservation['Instances']:\n",
    "            inst_id = instance['InstanceId']\n",
    "            inst_type = instance['InstanceType']\n",
    "            \n",
    "            metrics = get_instance_utilization(inst_id)\n",
    "            \n",
    "            # LÃ³gica de recomendaciÃ³n\n",
    "            if metrics['cpu_avg'] < 20 and metrics['cpu_max'] < 50:\n",
    "                # Over-provisioned\n",
    "                current_price = get_instance_price(inst_type)\n",
    "                recommended_type = get_smaller_instance(inst_type)\n",
    "                recommended_price = get_instance_price(recommended_type)\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'instance_id': inst_id,\n",
    "                    'current_type': inst_type,\n",
    "                    'current_cost': current_price * 730,  # mensual\n",
    "                    'recommended_type': recommended_type,\n",
    "                    'recommended_cost': recommended_price * 730,\n",
    "                    'savings_pct': ((current_price - recommended_price) / current_price) * 100\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Ejemplo output\n",
    "\"\"\"\n",
    "instance_id      current_type  current_cost  recommended_type  recommended_cost  savings_pct\n",
    "i-0123456789   m5.4xlarge     $500          m5.2xlarge        $250              50%\n",
    "i-abcdef1234   c5.9xlarge     $1200         c5.4xlarge        $600              50%\n",
    "Total potential savings: $700/month ($8,400/year)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Spot Instances: 70-90% Ahorro**\n",
    "\n",
    "Spot instances son capacidad ociosa de AWS/GCP/Azure con descuentos masivos, ideal para workloads tolerantes a interrupciones.\n",
    "\n",
    "**Casos de Uso Spot:**\n",
    "- âœ… Batch ETL (reintentable)\n",
    "- âœ… ML Training (checkpoints)\n",
    "- âœ… Data analytics (stateless)\n",
    "- âœ… CI/CD builds\n",
    "- âŒ Databases (stateful)\n",
    "- âŒ Real-time APIs (baja latencia)\n",
    "\n",
    "**EMR con Spot Instances:**\n",
    "\n",
    "```python\n",
    "# AWS EMR cluster con 80% Spot + 20% On-Demand\n",
    "emr_cluster_config = {\n",
    "    \"Name\": \"etl-cluster-spot\",\n",
    "    \"Instances\": {\n",
    "        \"MasterInstanceGroup\": {\n",
    "            \"InstanceType\": \"m5.xlarge\",\n",
    "            \"InstanceCount\": 1,\n",
    "            \"Market\": \"ON_DEMAND\"  # Master siempre On-Demand\n",
    "        },\n",
    "        \"CoreInstanceGroup\": {\n",
    "            \"InstanceType\": \"r5.2xlarge\",\n",
    "            \"InstanceCount\": 2,\n",
    "            \"Market\": \"ON_DEMAND\",  # Core On-Demand (HDFS)\n",
    "            \"BidPrice\": \"auto\"\n",
    "        },\n",
    "        \"TaskInstanceGroups\": [\n",
    "            {\n",
    "                \"Name\": \"Task - Spot\",\n",
    "                \"InstanceType\": \"r5.4xlarge\",\n",
    "                \"InstanceCount\": 8,\n",
    "                \"Market\": \"SPOT\",\n",
    "                \"BidPrice\": \"auto\"  # AWS auto-pricing\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"Applications\": [{\"Name\": \"Spark\"}],\n",
    "    \"Configurations\": [\n",
    "        {\n",
    "            \"Classification\": \"spark-defaults\",\n",
    "            \"Properties\": {\n",
    "                \"spark.speculation\": \"true\",  # Re-run tasks si Spot termina\n",
    "                \"spark.speculation.multiplier\": \"2\",\n",
    "                \"spark.task.maxFailures\": \"8\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Costo comparison\n",
    "costs = {\n",
    "    \"on_demand\": {\n",
    "        \"master\": 0.192 * 1,\n",
    "        \"core\": 0.504 * 2,\n",
    "        \"task\": 1.008 * 8,\n",
    "        \"total_hourly\": 9.240\n",
    "    },\n",
    "    \"spot_mix\": {\n",
    "        \"master\": 0.192 * 1,\n",
    "        \"core\": 0.504 * 2,\n",
    "        \"task\": 0.303 * 8,  # ~70% discount\n",
    "        \"total_hourly\": 3.624\n",
    "    },\n",
    "    \"savings\": (9.240 - 3.624) / 9.240 * 100  # 60.8% ahorro\n",
    "}\n",
    "```\n",
    "\n",
    "**Spot Interruption Handling:**\n",
    "\n",
    "```python\n",
    "# Graceful termination con 2-minute warning\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "def check_spot_termination():\n",
    "    \"\"\"EC2 Instance Metadata para detectar interruption\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            'http://169.254.169.254/latest/meta-data/spot/instance-action',\n",
    "            timeout=1\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            # Spot instance serÃ¡ terminada en 2 minutos\n",
    "            action = response.json()\n",
    "            print(f\"âš ï¸ Spot termination: {action['action']} at {action['time']}\")\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            save_checkpoint()\n",
    "            \n",
    "            # Graceful shutdown\n",
    "            shutdown_gracefully()\n",
    "            \n",
    "            return True\n",
    "    except:\n",
    "        # No termination scheduled\n",
    "        return False\n",
    "\n",
    "# Spark checkpoint para resilience\n",
    "spark_conf = {\n",
    "    \"spark.speculation\": \"true\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "    \"spark.shuffle.service.enabled\": \"true\",\n",
    "    \"spark.checkpoint.dir\": \"s3://bucket/checkpoints/\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Autoscaling: Dynamic Capacity**\n",
    "\n",
    "```python\n",
    "# AWS Auto Scaling Group con target tracking\n",
    "asg_config = {\n",
    "    \"AutoScalingGroupName\": \"data-workers-asg\",\n",
    "    \"MinSize\": 2,\n",
    "    \"MaxSize\": 20,\n",
    "    \"DesiredCapacity\": 5,\n",
    "    \"LaunchTemplate\": {\n",
    "        \"LaunchTemplateId\": \"lt-xxx\",\n",
    "        \"Version\": \"$Latest\"\n",
    "    },\n",
    "    \"VPCZoneIdentifier\": \"subnet-a,subnet-b,subnet-c\",\n",
    "    \"HealthCheckType\": \"ELB\",\n",
    "    \"HealthCheckGracePeriod\": 300,\n",
    "    \"TargetGroupARNs\": [\"arn:aws:elasticloadbalancing:...\"],\n",
    "    \n",
    "    # Scaling policies\n",
    "    \"TargetTrackingConfiguration\": [\n",
    "        {\n",
    "            \"PredefinedMetricSpecification\": {\n",
    "                \"PredefinedMetricType\": \"ASGAverageCPUUtilization\"\n",
    "            },\n",
    "            \"TargetValue\": 70.0  # Scale cuando CPU > 70%\n",
    "        },\n",
    "        {\n",
    "            # Custom metric: SQS queue depth\n",
    "            \"CustomizedMetricSpecification\": {\n",
    "                \"MetricName\": \"ApproximateNumberOfMessagesVisible\",\n",
    "                \"Namespace\": \"AWS/SQS\",\n",
    "                \"Statistic\": \"Average\",\n",
    "                \"Unit\": \"Count\"\n",
    "            },\n",
    "            \"TargetValue\": 100  # Scale cuando queue > 100 msgs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Kubernetes HPA (Horizontal Pod Autoscaler)\n",
    "\"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: spark-driver\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: spark-driver\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: pending_tasks\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"100\"\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5min before scale-down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50  # Scale down max 50% at a time\n",
    "        periodSeconds: 60\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Lyft - 75% Spot Coverage**\n",
    "\n",
    "Lyft ejecuta toda su analÃ­tica en EMR con:\n",
    "- **Task Nodes**: 100% Spot (10,000+ instancias)\n",
    "- **Core Nodes**: On-Demand (HDFS persistence)\n",
    "- **Checkpointing**: S3 cada 5 minutos\n",
    "- **Interruption Rate**: ~10% (tolerado con retry)\n",
    "- **Ahorro anual**: ~$20M vs full On-Demand\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Diversify Instance Types**: Spot pools (c5, m5, r5)\n",
    "2. **Capacity Rebalancing**: AWS auto-reemplaza antes de termination\n",
    "3. **Allocation Strategy**: `price-capacity-optimized`\n",
    "4. **Checkpointing**: Guardar estado cada N minutos\n",
    "5. **Monitoring**: CloudWatch para interruption rate\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8427511",
   "metadata": {},
   "source": [
    "### ğŸ’¾ **Storage Optimization: Lifecycle, Compression & Deduplication**\n",
    "\n",
    "**S3 Storage Classes: Cost vs Access Trade-off**\n",
    "\n",
    "AWS S3 ofrece 7 storage classes con diferentes precios y latencias:\n",
    "\n",
    "| Storage Class | Use Case | Latency | Cost/GB/month | Retrieval Cost |\n",
    "|---------------|----------|---------|---------------|----------------|\n",
    "| **S3 Standard** | Hot data (frecuente) | ms | $0.023 | $0 |\n",
    "| **S3 Intelligent-Tiering** | Auto-optimization | ms | $0.023-$0.0125 | $0 |\n",
    "| **S3 Standard-IA** | Infrequent access | ms | $0.0125 | $0.01/GB |\n",
    "| **S3 One Zone-IA** | Non-critical, infrequent | ms | $0.01 | $0.01/GB |\n",
    "| **S3 Glacier Instant** | Archive, instant | ms | $0.004 | $0.03/GB |\n",
    "| **S3 Glacier Flexible** | Archive, minutos-horas | 1-5 min | $0.0036 | $0.02/GB |\n",
    "| **S3 Glacier Deep Archive** | Long-term, 12h | 12 hours | $0.00099 | $0.02/GB |\n",
    "\n",
    "**Lifecycle Policy (Automated Tiering):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "lifecycle_policy = {\n",
    "    'Rules': [\n",
    "        {\n",
    "            'Id': 'data-lake-lifecycle',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {'Prefix': 'raw/'},\n",
    "            'Transitions': [\n",
    "                {\n",
    "                    'Days': 30,\n",
    "                    'StorageClass': 'STANDARD_IA'  # DespuÃ©s de 30 dÃ­as\n",
    "                },\n",
    "                {\n",
    "                    'Days': 90,\n",
    "                    'StorageClass': 'GLACIER_IR'  # DespuÃ©s de 90 dÃ­as\n",
    "                },\n",
    "                {\n",
    "                    'Days': 365,\n",
    "                    'StorageClass': 'DEEP_ARCHIVE'  # DespuÃ©s de 1 aÃ±o\n",
    "                }\n",
    "            ],\n",
    "            'Expiration': {\n",
    "                'Days': 2555  # Delete despuÃ©s de 7 aÃ±os (compliance)\n",
    "            },\n",
    "            'NoncurrentVersionTransitions': [\n",
    "                {\n",
    "                    'NoncurrentDays': 30,\n",
    "                    'StorageClass': 'GLACIER_IR'\n",
    "                }\n",
    "            ],\n",
    "            'NoncurrentVersionExpiration': {\n",
    "                'NoncurrentDays': 90\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Id': 'delete-incomplete-multipart-uploads',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {},\n",
    "            'AbortIncompleteMultipartUpload': {\n",
    "                'DaysAfterInitiation': 7  # Limpia uploads fallidos\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Id': 'logs-retention',\n",
    "            'Status': 'Enabled',\n",
    "            'Filter': {'Prefix': 'logs/'},\n",
    "            'Expiration': {\n",
    "                'Days': 90  # Logs solo 90 dÃ­as\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_lifecycle_configuration(\n",
    "    Bucket='my-data-lake',\n",
    "    LifecycleConfiguration=lifecycle_policy\n",
    ")\n",
    "\n",
    "# Costo Calculation\n",
    "storage_breakdown = {\n",
    "    \"raw_data\": {\n",
    "        \"size_tb\": 100,\n",
    "        \"access_pattern\": \"write-once, read-rarely\",\n",
    "        \"standard_cost\": 100 * 1024 * 0.023,  # $2,355/month\n",
    "        \"with_lifecycle\": {\n",
    "            \"standard_30d\": 100 * 1024 * 0.023 * (30/365),\n",
    "            \"ia_60d\": 100 * 1024 * 0.0125 * (60/365),\n",
    "            \"glacier_275d\": 100 * 1024 * 0.00099 * (275/365),\n",
    "            \"total\": 295  # $295/month (87% ahorro!)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Compression: Reduce Storage & Transfer Costs**\n",
    "\n",
    "Elegir codec y formato impacta significativamente en costos y performance.\n",
    "\n",
    "**Compression Codecs Comparison:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Dataset: 1 billion rows, 10 columns\n",
    "sample_data = pd.DataFrame({\n",
    "    'user_id': range(1_000_000_000),\n",
    "    'timestamp': pd.date_range('2020-01-01', periods=1_000_000_000, freq='s'),\n",
    "    'event_type': ['click'] * 500_000_000 + ['purchase'] * 500_000_000,\n",
    "    'amount': [10.5] * 1_000_000_000,\n",
    "    # ... 6 more columns\n",
    "})\n",
    "\n",
    "# Benchmark different codecs\n",
    "codecs = ['snappy', 'gzip', 'zstd', 'lz4']\n",
    "results = []\n",
    "\n",
    "for codec in codecs:\n",
    "    # Write Parquet\n",
    "    table = pa.Table.from_pandas(sample_data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        f'output_{codec}.parquet',\n",
    "        compression=codec,\n",
    "        compression_level=3 if codec == 'zstd' else None\n",
    "    )\n",
    "    \n",
    "    # Measure\n",
    "    import os\n",
    "    file_size = os.path.getsize(f'output_{codec}.parquet') / (1024**3)  # GB\n",
    "    \n",
    "    # Read benchmark\n",
    "    import time\n",
    "    start = time.time()\n",
    "    df = pq.read_table(f'output_{codec}.parquet').to_pandas()\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'codec': codec,\n",
    "        'file_size_gb': file_size,\n",
    "        'compression_ratio': 100 / file_size,  # Original 100 GB\n",
    "        'read_time_sec': read_time,\n",
    "        'storage_cost_monthly': file_size * 0.023  # S3 Standard\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\"\"\"\n",
    "codec    file_size_gb  compression_ratio  read_time_sec  storage_cost_monthly\n",
    "snappy   15.2          6.6x               45             $0.35\n",
    "gzip     8.4           11.9x              120            $0.19\n",
    "zstd     7.1           14.1x              55             $0.16  â† Winner\n",
    "lz4      16.8          5.9x               38             $0.39\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Recomendaciones por Codec:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  SNAPPY                                                â”‚\n",
    "â”‚  â€¢ Uso: Default Spark, buen balance                   â”‚\n",
    "â”‚  â€¢ CompresiÃ³n: 6-8x (moderada)                        â”‚\n",
    "â”‚  â€¢ CPU: Bajo overhead                                 â”‚\n",
    "â”‚  â€¢ Splittable: âœ… (con Parquet)                       â”‚\n",
    "â”‚  â€¢ Caso: ETL daily, query frecuente                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ZSTD (Zstandard)                                      â”‚\n",
    "â”‚  â€¢ Uso: Mejor ratio, good speed                       â”‚\n",
    "â”‚  â€¢ CompresiÃ³n: 12-15x (alta)                          â”‚\n",
    "â”‚  â€¢ CPU: Moderado                                      â”‚\n",
    "â”‚  â€¢ Splittable: âœ…                                     â”‚\n",
    "â”‚  â€¢ Caso: Archives, long-term storage                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  GZIP                                                  â”‚\n",
    "â”‚  â€¢ Uso: Legacy, alta compresiÃ³n                       â”‚\n",
    "â”‚  â€¢ CompresiÃ³n: 10-12x                                 â”‚\n",
    "â”‚  â€¢ CPU: Alto overhead (lento decode)                  â”‚\n",
    "â”‚  â€¢ Splittable: âŒ (sin Parquet)                       â”‚\n",
    "â”‚  â€¢ Caso: Logs, texto plano                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  LZ4                                                   â”‚\n",
    "â”‚  â€¢ Uso: Ultra-fast, baja compresiÃ³n                   â”‚\n",
    "â”‚  â€¢ CompresiÃ³n: 4-6x                                   â”‚\n",
    "â”‚  â€¢ CPU: Muy bajo                                      â”‚\n",
    "â”‚  â€¢ Splittable: âœ…                                     â”‚\n",
    "â”‚  â€¢ Caso: Streaming, low-latency queries               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Parquet Optimization (Row Groups & Page Size):**\n",
    "\n",
    "```python\n",
    "# Optimal Parquet configuration\n",
    "pq.write_table(\n",
    "    table,\n",
    "    'optimized.parquet',\n",
    "    compression='zstd',\n",
    "    compression_level=3,\n",
    "    row_group_size=128 * 1024 * 1024,  # 128 MB row groups (default: 64MB)\n",
    "    data_page_size=1024 * 1024,  # 1 MB pages (default: 1MB)\n",
    "    use_dictionary=True,  # Dictionary encoding (columnas repetitivas)\n",
    "    column_encoding={\n",
    "        'user_id': 'PLAIN',  # High cardinality\n",
    "        'event_type': 'RLE_DICTIONARY',  # Low cardinality\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ahorro con dictionary encoding\n",
    "\"\"\"\n",
    "Column: event_type (1B rows, 10 unique values)\n",
    "Without dict: 1B * 20 bytes = 20 GB\n",
    "With dict:    10 * 20 bytes + 1B * 4 bytes (indices) = 4 GB (80% ahorro!)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Deduplication: Eliminar Redundancia**\n",
    "\n",
    "```python\n",
    "# Delta Lake deduplication\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Merge con deduplication\n",
    "delta_table = DeltaTable.forPath(spark, 's3://bucket/delta-table')\n",
    "\n",
    "new_data = spark.read.parquet('s3://bucket/new-data/')\n",
    "\n",
    "# Deduplicate antes de insertar\n",
    "delta_table.alias('old').merge(\n",
    "    new_data.alias('new'),\n",
    "    'old.id = new.id AND old.timestamp = new.timestamp'\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Z-Order para reduce file scanning (performance + cost)\n",
    "delta_table.optimize().executeZOrderBy('date', 'customer_id')\n",
    "\n",
    "# Vacuum old versions (reduce storage)\n",
    "delta_table.vacuum(retentionHours=168)  # 7 dÃ­as retention\n",
    "```\n",
    "\n",
    "**Caso Real: Pinterest - $20M Storage Savings**\n",
    "\n",
    "Pinterest redujo costos de S3 de $50M â†’ $30M/aÃ±o:\n",
    "\n",
    "1. **Lifecycle Policies**: 70% datos â†’ Glacier (90 dÃ­as)\n",
    "2. **Compression**: CSV â†’ Parquet+ZSTD (10x compression)\n",
    "3. **Deduplication**: Delta Lake merge (elimina 30% duplicados)\n",
    "4. **Intelligent Tiering**: Auto-move cold data (ahorro 15%)\n",
    "5. **Vacuum**: Delete old versions (retiene 7 dÃ­as vs 30)\n",
    "\n",
    "**Terraform IaC para Lifecycle:**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"data_lake\" {\n",
    "  bucket = aws_s3_bucket.data_lake.id\n",
    "\n",
    "  rule {\n",
    "    id     = \"intelligent-tiering\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    transition {\n",
    "      days          = 30\n",
    "      storage_class = \"STANDARD_IA\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 90\n",
    "      storage_class = \"GLACIER_IR\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 365\n",
    "      storage_class = \"DEEP_ARCHIVE\"\n",
    "    }\n",
    "\n",
    "    expiration {\n",
    "      days = 2555  # 7 years\n",
    "    }\n",
    "  }\n",
    "\n",
    "  rule {\n",
    "    id     = \"abort-incomplete-uploads\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    abort_incomplete_multipart_upload {\n",
    "      days_after_initiation = 7\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Monitoring Query (AWS Cost Explorer API):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "ce = boto3.client('ce')\n",
    "\n",
    "response = ce.get_cost_and_usage(\n",
    "    TimePeriod={\n",
    "        'Start': '2025-10-01',\n",
    "        'End': '2025-10-31'\n",
    "    },\n",
    "    Granularity='DAILY',\n",
    "    Metrics=['UnblendedCost'],\n",
    "    GroupBy=[\n",
    "        {'Type': 'DIMENSION', 'Key': 'SERVICE'},\n",
    "        {'Type': 'DIMENSION', 'Key': 'USAGE_TYPE'}\n",
    "    ],\n",
    "    Filter={\n",
    "        'Dimensions': {\n",
    "            'Key': 'SERVICE',\n",
    "            'Values': ['Amazon Simple Storage Service']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Analiza costos por storage class\n",
    "for result in response['ResultsByTime']:\n",
    "    print(f\"Date: {result['TimePeriod']['Start']}\")\n",
    "    for group in result['Groups']:\n",
    "        service, usage = group['Keys']\n",
    "        cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "        if cost > 0:\n",
    "            print(f\"  {usage}: ${cost:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd347931",
   "metadata": {},
   "source": [
    "### ğŸ“Š **Cost Monitoring & Anomaly Detection: Observability con FinOps**\n",
    "\n",
    "**Cost Observability Stack**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Cost Data Sources                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ AWS Cost & Usage Report (CUR) â†’ S3 â†’ Athena      â”‚\n",
    "â”‚  â€¢ GCP Billing Export â†’ BigQuery                     â”‚\n",
    "â”‚  â€¢ Azure Cost Management â†’ Export to Storage         â”‚\n",
    "â”‚                                                       â”‚\n",
    "â”‚  â†“                                                    â”‚\n",
    "â”‚  Data Processing                                     â”‚\n",
    "â”‚  â€¢ Spark/Airflow ETL (enrich with tags)             â”‚\n",
    "â”‚  â€¢ Cost allocation (chargeback logic)                â”‚\n",
    "â”‚  â€¢ Anomaly detection (ML models)                     â”‚\n",
    "â”‚                                                       â”‚\n",
    "â”‚  â†“                                                    â”‚\n",
    "â”‚  Visualization & Alerting                            â”‚\n",
    "â”‚  â€¢ Grafana dashboards (real-time)                    â”‚\n",
    "â”‚  â€¢ Slack/PagerDuty alerts                            â”‚\n",
    "â”‚  â€¢ Weekly reports (email)                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**AWS Cost & Usage Report (CUR) Setup:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Enable CUR\n",
    "cur = boto3.client('cur', region_name='us-east-1')\n",
    "\n",
    "cur.put_report_definition(\n",
    "    ReportDefinition={\n",
    "        'ReportName': 'data-platform-cur',\n",
    "        'TimeUnit': 'HOURLY',  # Granularidad horaria\n",
    "        'Format': 'Parquet',  # Mejor que CSV\n",
    "        'Compression': 'Parquet',\n",
    "        'AdditionalSchemaElements': ['RESOURCES', 'SPLIT_COST_ALLOCATION_DATA'],\n",
    "        'S3Bucket': 'cost-reports-bucket',\n",
    "        'S3Prefix': 'cur/',\n",
    "        'S3Region': 'us-east-1',\n",
    "        'AdditionalArtifacts': ['ATHENA'],  # Auto-crea Athena table\n",
    "        'RefreshClosedReports': True,\n",
    "        'ReportVersioning': 'OVERWRITE_REPORT'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Athena query para analizar CUR\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    line_item_usage_account_id,\n",
    "    product_servicename,\n",
    "    resource_tags_user_cost_center,\n",
    "    resource_tags_user_project,\n",
    "    DATE(line_item_usage_start_date) as date,\n",
    "    SUM(line_item_unblended_cost) as daily_cost\n",
    "FROM cur_database.cost_usage_report\n",
    "WHERE year = '2025' \n",
    "  AND month = '10'\n",
    "  AND line_item_line_item_type = 'Usage'\n",
    "GROUP BY 1, 2, 3, 4, 5\n",
    "ORDER BY daily_cost DESC\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={'Database': 'cur_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://query-results/'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Anomaly Detection con Machine Learning:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load historical cost data\n",
    "cost_history = pd.read_parquet('s3://cost-data/daily_costs.parquet')\n",
    "\n",
    "# Feature engineering\n",
    "cost_history['day_of_week'] = pd.to_datetime(cost_history['date']).dt.dayofweek\n",
    "cost_history['week_of_year'] = pd.to_datetime(cost_history['date']).dt.isocalendar().week\n",
    "\n",
    "# Train anomaly detector\n",
    "features = ['daily_cost', 'day_of_week', 'week_of_year']\n",
    "X = cost_history[features]\n",
    "\n",
    "model = IsolationForest(\n",
    "    contamination=0.05,  # 5% anomalÃ­as esperadas\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies para hoy\n",
    "today_cost = get_today_cost()\n",
    "today_features = [today_cost, datetime.now().weekday(), datetime.now().isocalendar()[1]]\n",
    "\n",
    "anomaly_score = model.decision_function([today_features])[0]\n",
    "is_anomaly = model.predict([today_features])[0] == -1\n",
    "\n",
    "if is_anomaly:\n",
    "    # Calculate deviation\n",
    "    expected_cost = cost_history[\n",
    "        (cost_history['day_of_week'] == today_features[1]) &\n",
    "        (cost_history['week_of_year'] == today_features[2])\n",
    "    ]['daily_cost'].mean()\n",
    "    \n",
    "    deviation_pct = ((today_cost - expected_cost) / expected_cost) * 100\n",
    "    \n",
    "    send_alert(\n",
    "        title=f\"âš ï¸ Cost Anomaly Detected\",\n",
    "        message=f\"Today's cost: ${today_cost:.2f}\\n\"\n",
    "                f\"Expected: ${expected_cost:.2f}\\n\"\n",
    "                f\"Deviation: {deviation_pct:+.1f}%\",\n",
    "        severity='high' if abs(deviation_pct) > 50 else 'medium'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Prophet (Facebook) para Forecasting:**\n",
    "\n",
    "```python\n",
    "from prophet import Prophet\n",
    "\n",
    "# Prepare data\n",
    "df = cost_history[['date', 'daily_cost']].rename(columns={'date': 'ds', 'daily_cost': 'y'})\n",
    "\n",
    "# Train model\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False\n",
    ")\n",
    "model.fit(df)\n",
    "\n",
    "# Forecast next 30 dÃ­as\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Budget alert\n",
    "monthly_budget = 10000\n",
    "forecasted_month_cost = forecast[forecast['ds'].dt.month == datetime.now().month]['yhat'].sum()\n",
    "\n",
    "if forecasted_month_cost > monthly_budget:\n",
    "    send_alert(\n",
    "        title=\"ğŸ“ˆ Budget Overrun Forecasted\",\n",
    "        message=f\"Forecasted: ${forecasted_month_cost:.2f}\\n\"\n",
    "                f\"Budget: ${monthly_budget:.2f}\\n\"\n",
    "                f\"Overrun: ${forecasted_month_cost - monthly_budget:.2f}\"\n",
    "    )\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "```\n",
    "\n",
    "**Real-Time Alerting con CloudWatch + Lambda:**\n",
    "\n",
    "```python\n",
    "# Lambda function triggered por CloudWatch Alarm\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Alert cuando costo diario > threshold\"\"\"\n",
    "    \n",
    "    # Parse CloudWatch alarm\n",
    "    message = json.loads(event['Records'][0]['Sns']['Message'])\n",
    "    alarm_name = message['AlarmName']\n",
    "    new_state = message['NewStateValue']\n",
    "    reason = message['NewStateReason']\n",
    "    \n",
    "    if new_state == 'ALARM':\n",
    "        # Get current cost\n",
    "        ce = boto3.client('ce')\n",
    "        response = ce.get_cost_and_usage(\n",
    "            TimePeriod={\n",
    "                'Start': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'End': (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            },\n",
    "            Granularity='DAILY',\n",
    "            Metrics=['UnblendedCost']\n",
    "        )\n",
    "        \n",
    "        current_cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])\n",
    "        \n",
    "        # Get top services\n",
    "        top_services = ce.get_cost_and_usage(\n",
    "            TimePeriod={\n",
    "                'Start': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'End': (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            },\n",
    "            Granularity='DAILY',\n",
    "            Metrics=['UnblendedCost'],\n",
    "            GroupBy=[{'Type': 'DIMENSION', 'Key': 'SERVICE'}]\n",
    "        )\n",
    "        \n",
    "        # Format Slack message\n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\"type\": \"plain_text\", \"text\": \"âš ï¸ Cost Spike Detected\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"fields\": [\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Current Cost:*\\n${current_cost:.2f}\"},\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Alarm:*\\n{alarm_name}\"},\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*Reason:*\\n{reason}\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": \"*Top Services:*\\n\" + \"\\n\".join([\n",
    "                        f\"â€¢ {g['Keys'][0]}: ${float(g['Metrics']['UnblendedCost']['Amount']):.2f}\"\n",
    "                        for g in sorted(\n",
    "                            top_services['ResultsByTime'][0]['Groups'],\n",
    "                            key=lambda x: float(x['Metrics']['UnblendedCost']['Amount']),\n",
    "                            reverse=True\n",
    "                        )[:5]\n",
    "                    ])\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"actions\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"button\",\n",
    "                        \"text\": {\"type\": \"plain_text\", \"text\": \"View in Cost Explorer\"},\n",
    "                        \"url\": \"https://console.aws.amazon.com/cost-management/home\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Send to Slack\n",
    "        requests.post(\n",
    "            'https://hooks.slack.com/services/YOUR/WEBHOOK/URL',\n",
    "            json={\"blocks\": blocks}\n",
    "        )\n",
    "\n",
    "# CloudWatch Alarm (Terraform)\n",
    "\"\"\"\n",
    "resource \"aws_cloudwatch_metric_alarm\" \"daily_cost_spike\" {\n",
    "  alarm_name          = \"daily-cost-spike\"\n",
    "  comparison_operator = \"GreaterThanThreshold\"\n",
    "  evaluation_periods  = \"1\"\n",
    "  metric_name         = \"EstimatedCharges\"\n",
    "  namespace           = \"AWS/Billing\"\n",
    "  period              = \"21600\"  # 6 horas\n",
    "  statistic           = \"Maximum\"\n",
    "  threshold           = \"500\"  # $500/dÃ­a\n",
    "  alarm_description   = \"Alert cuando costo diario > $500\"\n",
    "  alarm_actions       = [aws_sns_topic.cost_alerts.arn]\n",
    "\n",
    "  dimensions = {\n",
    "    Currency = \"USD\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Grafana Dashboard (FinOps):**\n",
    "\n",
    "```python\n",
    "# Prometheus queries\n",
    "grafana_dashboard = {\n",
    "    \"panels\": [\n",
    "        {\n",
    "            \"title\": \"Total Monthly Cost\",\n",
    "            \"query\": 'sum(aws_billing_estimated_charges{currency=\"USD\"})',\n",
    "            \"type\": \"stat\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Daily Cost Trend\",\n",
    "            \"query\": 'sum by (service) (rate(aws_cost_total[1d]))',\n",
    "            \"type\": \"graph\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Cost by Team\",\n",
    "            \"query\": 'sum by (tag_costcenter) (aws_cost_total)',\n",
    "            \"type\": \"piechart\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Budget Burn Rate\",\n",
    "            \"query\": 'rate(aws_cost_total[7d]) * 30',\n",
    "            \"type\": \"gauge\",\n",
    "            \"thresholds\": [8000, 9000, 10000]  # Budget: $10k\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Top 10 Recursos\",\n",
    "            \"query\": 'topk(10, sum by (resource_id) (aws_cost_total))',\n",
    "            \"type\": \"table\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Savings Opportunities\",\n",
    "            \"query\": 'sum(aws_rightsizing_savings_potential)',\n",
    "            \"type\": \"stat\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Export to JSON\n",
    "import json\n",
    "with open('finops_dashboard.json', 'w') as f:\n",
    "    json.dump(grafana_dashboard, f, indent=2)\n",
    "```\n",
    "\n",
    "**Weekly Cost Report (Automated):**\n",
    "\n",
    "```python\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "\n",
    "def generate_weekly_report():\n",
    "    \"\"\"Genera reporte semanal de costos\"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    last_week_cost = get_cost_for_period(days=7)\n",
    "    previous_week_cost = get_cost_for_period(days=14, offset=7)\n",
    "    \n",
    "    wow_change = ((last_week_cost - previous_week_cost) / previous_week_cost) * 100\n",
    "    \n",
    "    # Top services\n",
    "    top_services = get_cost_by_service(days=7)\n",
    "    \n",
    "    # Top resources\n",
    "    top_resources = get_cost_by_resource(days=7)\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations = get_cost_recommendations()\n",
    "    \n",
    "    # Generate HTML report\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "      <body>\n",
    "        <h1>ğŸ“Š Weekly Cost Report</h1>\n",
    "        <h2>Overview</h2>\n",
    "        <p><strong>Last Week:</strong> ${last_week_cost:.2f}</p>\n",
    "        <p><strong>Previous Week:</strong> ${previous_week_cost:.2f}</p>\n",
    "        <p><strong>Change:</strong> {wow_change:+.1f}%</p>\n",
    "        \n",
    "        <h2>Top Services</h2>\n",
    "        <table border=\"1\">\n",
    "          <tr><th>Service</th><th>Cost</th><th>% of Total</th></tr>\n",
    "          {''.join([f\"<tr><td>{s['name']}</td><td>${s['cost']:.2f}</td><td>{s['pct']:.1f}%</td></tr>\" for s in top_services[:5]])}\n",
    "        </table>\n",
    "        \n",
    "        <h2>ğŸ’° Savings Opportunities</h2>\n",
    "        <ul>\n",
    "          {''.join([f\"<li>{r['description']}: <strong>${r['savings']:.2f}/month</strong></li>\" for r in recommendations])}\n",
    "        </ul>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send email\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = f\"Weekly Cost Report - ${last_week_cost:.2f}\"\n",
    "    msg['From'] = 'finops@company.com'\n",
    "    msg['To'] = 'team@company.com'\n",
    "    \n",
    "    msg.attach(MIMEText(html, 'html'))\n",
    "    \n",
    "    smtp = smtplib.SMTP('smtp.company.com')\n",
    "    smtp.send_message(msg)\n",
    "    smtp.quit()\n",
    "\n",
    "# Airflow DAG para schedule\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    'weekly_cost_report',\n",
    "    schedule_interval='0 9 * * MON',  # Lunes 9am\n",
    "    default_args={'owner': 'finops'}\n",
    ")\n",
    "\n",
    "PythonOperator(\n",
    "    task_id='generate_report',\n",
    "    python_callable=generate_weekly_report,\n",
    "    dag=dag\n",
    ")\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Caso Real: Airbnb - Cost Attribution System**\n",
    "\n",
    "Airbnb construyÃ³ un sistema interno de FinOps:\n",
    "- **Real-time Dashboards**: Grafana con costo por servicio cada 15 min\n",
    "- **ML Anomaly Detection**: Prophet + IsolationForest\n",
    "- **Chargeback**: Cada equipo ve su gasto en tiempo real\n",
    "- **Budget Enforcement**: Alertas cuando equipo > 80% budget\n",
    "- **Resultado**: 30% reducciÃ³n en waste, $100M+ ahorros/aÃ±o\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b3f3b",
   "metadata": {},
   "source": [
    "## 1. Principios de FinOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bffde",
   "metadata": {},
   "source": [
    "- Visibilidad: tagging de recursos, cost allocation por proyecto/equipo.\n",
    "- Accountability: cada equipo/dominio responsable de su presupuesto.\n",
    "- OptimizaciÃ³n continua: rightsizing, reservas, spot instances, lifecycle policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d3813",
   "metadata": {},
   "source": [
    "## 2. Estrategias de optimizaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed4168",
   "metadata": {},
   "source": [
    "### 2.1 Compute\n",
    "- Spot/Preemptible para workloads tolerantes a interrupciones (batch, training).\n",
    "- Reserved/Committed para cargas predecibles (> 1 aÃ±o).\n",
    "- Auto-scaling por mÃ©tricas (CPU, latencia, queue depth).\n",
    "\n",
    "### 2.2 Storage\n",
    "- Lifecycle policies: S3 Standard â†’ IA â†’ Glacier â†’ Deep Archive.\n",
    "- CompresiÃ³n y formatos eficientes (Parquet/ORC con Snappy/ZSTD).\n",
    "- Borrado de snapshots y versiones antiguas.\n",
    "\n",
    "### 2.3 Networking\n",
    "- Minimizar data transfer inter-regiÃ³n.\n",
    "- Usar VPC endpoints y PrivateLink para evitar egress a internet.\n",
    "- CDN (CloudFront/Cloud CDN) para datos frecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7843c",
   "metadata": {},
   "source": [
    "## 3. Ejemplo: benchmark de costo por TB procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29453b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pipeline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TB_procesados",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Costo_USD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "USD_per_TB",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c971b422-e1b1-4cc1-99ea-cf3b7b97a52a",
       "rows": [
        [
         "0",
         "ETL Diario",
         "5.2",
         "42.0",
         "8.08"
        ],
        [
         "1",
         "Streaming Kafka",
         "1.8",
         "95.0",
         "52.78"
        ],
        [
         "2",
         "ML Training",
         "0.3",
         "120.0",
         "400.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>TB_procesados</th>\n",
       "      <th>Costo_USD</th>\n",
       "      <th>USD_per_TB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ETL Diario</td>\n",
       "      <td>5.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Streaming Kafka</td>\n",
       "      <td>1.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>52.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML Training</td>\n",
       "      <td>0.3</td>\n",
       "      <td>120.0</td>\n",
       "      <td>400.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pipeline  TB_procesados  Costo_USD  USD_per_TB\n",
       "0       ETL Diario            5.2       42.0        8.08\n",
       "1  Streaming Kafka            1.8       95.0       52.78\n",
       "2      ML Training            0.3      120.0      400.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "  {'Pipeline':'ETL Diario', 'TB_procesados':5.2, 'Costo_USD':42.0, 'USD_per_TB':8.08},\n",
    "  {'Pipeline':'Streaming Kafka', 'TB_procesados':1.8, 'Costo_USD':95.0, 'USD_per_TB':52.78},\n",
    "  {'Pipeline':'ML Training', 'TB_procesados':0.3, 'Costo_USD':120.0, 'USD_per_TB':400.0},\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cc1a0",
   "metadata": {},
   "source": [
    "## 4. Alertas y presupuestos (AWS Budgets ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6214eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '# AWS CLI para crear presupuesto mensual de $1000 con alertas al 80% y 100%', 'aws budgets create-budget \\\\', '  --account-id 123456789012 \\\\', '  --budget file://budget.json \\\\', '  --notifications-with-subscribers file://notifications.json', '', '# budget.json', '{', '  \"BudgetName\": \"data-platform-monthly\",', '  \"BudgetLimit\": {\"Amount\": \"1000\", \"Unit\": \"USD\"},', '  \"TimeUnit\": \"MONTHLY\",', '  \"BudgetType\": \"COST\"', '}', '']\n"
     ]
    }
   ],
   "source": [
    "budget_demo = r'''\n",
    "# AWS CLI para crear presupuesto mensual de $1000 con alertas al 80% y 100%\n",
    "aws budgets create-budget \\\n",
    "  --account-id 123456789012 \\\n",
    "  --budget file://budget.json \\\n",
    "  --notifications-with-subscribers file://notifications.json\n",
    "\n",
    "# budget.json\n",
    "{\n",
    "  \"BudgetName\": \"data-platform-monthly\",\n",
    "  \"BudgetLimit\": {\"Amount\": \"1000\", \"Unit\": \"USD\"},\n",
    "  \"TimeUnit\": \"MONTHLY\",\n",
    "  \"BudgetType\": \"COST\"\n",
    "}\n",
    "\n",
    "# notifications.json (alertas al 80% y 100%)\n",
    "'''\n",
    "print(budget_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacab03",
   "metadata": {},
   "source": [
    "## 5. MÃ©tricas clave de FinOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac981e",
   "metadata": {},
   "source": [
    "- **Costo por TB procesado** (USD/TB).\n",
    "- **Costo por evento** (USD/millÃ³n eventos).\n",
    "- **UtilizaciÃ³n de recursos** (% CPU/RAM usados vs aprovisionados).\n",
    "- **Savings por optimizaciÃ³n** (Spot, RI, rightsizing).\n",
    "- **Trend mensual** (crecimiento de gasto vs negocio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eac008",
   "metadata": {},
   "source": [
    "## 6. Herramientas y dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c62932",
   "metadata": {},
   "source": [
    "- AWS Cost Explorer, GCP Billing Reports, Azure Cost Management.\n",
    "- Terraform/CDK para IaC con polÃ­ticas de costos.\n",
    "- Terceros: Cloudability, CloudHealth, Vantage.\n",
    "- Dashboard custom con Grafana + Prometheus exporters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "\n",
    "**Siguiente â†’:** [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos â†’](07_seguridad_compliance.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Senior:**\n",
    "- [ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [ğŸ—ï¸ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y prÃ¡ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [ğŸ“Š Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
