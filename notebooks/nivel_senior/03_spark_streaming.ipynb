{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc763fbf",
   "metadata": {},
   "source": [
    "# Apache Spark Streaming: Procesamiento en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar Spark Structured Streaming\n",
    "- Implementar procesamiento de ventanas y agregaciones\n",
    "- Integrar con Kafka y otras fuentes de streaming\n",
    "- Manejar estado y checkpointing\n",
    "- Optimizar rendimiento en streaming\n",
    "\n",
    "## Requisitos\n",
    "- PySpark 3.x\n",
    "- Python 3.8+\n",
    "- Kafka (opcional)\n",
    "- Delta Lake (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci\u00f3n de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, count, sum as spark_sum, avg, max as spark_max,\n",
    "    current_timestamp, to_json, from_json, struct, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"Librer\u00edas importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425e406",
   "metadata": {},
   "source": [
    "### \ud83c\udf0a **Spark Structured Streaming: Arquitectura y Micro-batches**\n",
    "\n",
    "**Evoluci\u00f3n del Streaming en Spark:**\n",
    "\n",
    "```\n",
    "Gen 1: Spark Streaming (DStreams) - 2013\n",
    "\u251c\u2500\u2500 RDDs en micro-batches\n",
    "\u251c\u2500\u2500 API de bajo nivel\n",
    "\u2514\u2500\u2500 \u274c Complejo, no unificado con batch\n",
    "\n",
    "Gen 2: Structured Streaming - 2016\n",
    "\u251c\u2500\u2500 DataFrames/Datasets unificados\n",
    "\u251c\u2500\u2500 Event-time processing\n",
    "\u251c\u2500\u2500 Exactly-once semantics\n",
    "\u2514\u2500\u2500 \u2705 API declarativa, tolerancia a fallos\n",
    "```\n",
    "\n",
    "**Micro-batch vs Continuous Processing:**\n",
    "\n",
    "| Aspecto | Micro-batch (default) | Continuous (experimental) |\n",
    "|---------|----------------------|---------------------------|\n",
    "| **Latencia** | ~100ms - 1s | ~1ms |\n",
    "| **Throughput** | \u2705 Alto (10K+ events/s) | \u26a0\ufe0f Medio |\n",
    "| **Garant\u00edas** | \u2705 Exactly-once | \u26a0\ufe0f At-least-once |\n",
    "| **Estado** | \u2705 Full support | \u26a0\ufe0f Limitado |\n",
    "| **Uso** | Analytics, aggregations | Ultra-low latency alerts |\n",
    "\n",
    "**Arquitectura de Micro-batch:**\n",
    "\n",
    "```python\n",
    "# Streaming Query = Infinite Table\n",
    "# Cada batch procesa un \"snapshot\" incremental\n",
    "\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Input Source (Kafka, Kinesis, Files)   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502 Batch 0: Records [1-100]\n",
    "               \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Incremental Query Engine                \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502 Batch 0: Transform + Aggregate     \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502 Write Batch 0 Results\n",
    "               \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Output Sink (Delta, Kafka, Console)    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502\n",
    "               \u2502 Trigger interval (e.g., 10s)\n",
    "               \u25bc\n",
    "         (Repeat Batch 1, 2, 3...)\n",
    "```\n",
    "\n",
    "**Triggers (Frecuencia de Ejecuci\u00f3n):**\n",
    "\n",
    "```python\n",
    "# 1. ProcessingTime: Ejecutar cada N segundos\n",
    "query = df.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# 2. Once: Ejecutar una sola vez (\u00fatil para testing/backfill)\n",
    "query = df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# 3. Continuous: Latencia ultra-baja (~1ms)\n",
    "query = df.writeStream \\\n",
    "    .trigger(continuous='1 second') \\\n",
    "    .start()\n",
    "\n",
    "# 4. AvailableNow: Procesar todos los datos disponibles (Spark 3.3+)\n",
    "query = df.writeStream \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "```python\n",
    "# 1. Append: Solo nuevos registros (default)\n",
    "# \u2705 Uso: Raw logs, eventos sin agregaciones\n",
    "# \u274c Limitaci\u00f3n: No updates/deletes\n",
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .start(\"/data/events\")\n",
    "\n",
    "# 2. Complete: Toda la tabla resultado (re-escribir completo)\n",
    "# \u2705 Uso: Agregaciones peque\u00f1as, dashboards\n",
    "# \u274c Limitaci\u00f3n: No escala, solo con aggregations\n",
    "df.groupBy(\"category\").count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"category_counts\") \\\n",
    "    .start()\n",
    "\n",
    "# 3. Update: Solo registros modificados\n",
    "# \u2705 Uso: Agregaciones con watermark, upserts\n",
    "# \u26a1 Best practice: Balanceo append vs complete\n",
    "df.groupBy(window(\"timestamp\", \"1 hour\"), \"user_id\") \\\n",
    "    .agg(count(\"*\").alias(\"events\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .start(\"/data/user_hourly_stats\")\n",
    "```\n",
    "\n",
    "**Event-time vs Processing-time:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# \u274c Processing-time: Cuando Spark procesa el evento\n",
    "df_processing_time = df.withColumn(\"processed_at\", current_timestamp())\n",
    "# Problema: Late data processed in wrong time window\n",
    "\n",
    "# \u2705 Event-time: Timestamp del evento original\n",
    "df_event_time = df.select(\"event_timestamp\", \"user_id\", \"action\")\n",
    "# Benefit: Correcta agregaci\u00f3n temporal incluso con retrasos\n",
    "\n",
    "# Ejemplo: Click en app a las 10:00 AM\n",
    "# - Dispositivo offline hasta 10:30 AM\n",
    "# - Processing-time: 10:30 AM \u274c (wrong window)\n",
    "# - Event-time: 10:00 AM \u2705 (correct window)\n",
    "```\n",
    "\n",
    "**Backpressure y Rate Limiting:**\n",
    "\n",
    "```python\n",
    "# Limitar ingesta para evitar overwhelm\n",
    "spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\  # Max 10K records/batch\n",
    "    .option(\"minPartitions\", 4) \\              # Paralelismo m\u00ednimo\n",
    "    .load()\n",
    "\n",
    "# Backpressure autom\u00e1tico (Spark 3.2+)\n",
    "spark.conf.set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.backpressure.initialRate\", 1000)\n",
    "```\n",
    "\n",
    "**Ejemplo Real: E-commerce Click Stream**\n",
    "\n",
    "```python\n",
    "# Fuente: Kafka con eventos de clicks\n",
    "clicks = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-clicks\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON payload\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "clicks_parsed = clicks.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Agregaci\u00f3n por ventanas de 5 minutos\n",
    "clicks_per_5min = clicks_parsed \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"clicks\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    )\n",
    "\n",
    "# Escribir a Delta Lake con ACID guarantees\n",
    "query = clicks_per_5min.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/clicks-5min\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/clicks_aggregated\")\n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(f\"Last Progress: {query.lastProgress}\")\n",
    "```\n",
    "\n",
    "**M\u00e9tricas de Performance:**\n",
    "\n",
    "```python\n",
    "# Acceder a m\u00e9tricas del stream\n",
    "progress = query.lastProgress\n",
    "\n",
    "key_metrics = {\n",
    "    \"batchId\": progress[\"batchId\"],\n",
    "    \"inputRowsPerSecond\": progress[\"inputRowsPerSecond\"],\n",
    "    \"processedRowsPerSecond\": progress[\"processedRowsPerSecond\"],\n",
    "    \"batchDuration\": progress[\"batchDuration\"],  # ms\n",
    "    \"numInputRows\": progress[\"numInputRows\"],\n",
    "    \"stateOperators\": progress[\"stateOperators\"]  # Estado acumulado\n",
    "}\n",
    "\n",
    "# Alertas si procesamiento es m\u00e1s lento que ingesta\n",
    "if progress[\"inputRowsPerSecond\"] > progress[\"processedRowsPerSecond\"]:\n",
    "    print(\"\u26a0\ufe0f WARNING: Falling behind! Increase parallelism\")\n",
    "```\n",
    "\n",
    "**Comparaci\u00f3n con Flink:**\n",
    "\n",
    "| Caracter\u00edstica | Spark Streaming | Apache Flink |\n",
    "|----------------|-----------------|--------------|\n",
    "| **Modelo** | Micro-batch | True streaming (record-at-a-time) |\n",
    "| **Latencia** | 100ms - 1s | 10ms - 100ms |\n",
    "| **Throughput** | \u2705 Muy alto | \u2705 Alto |\n",
    "| **Estado** | RocksDB, memory | RocksDB native |\n",
    "| **SQL Support** | \u2705 Excellent | \u2705 Good |\n",
    "| **Ecosystem** | \u2705 Spark ML, Delta | \u26a0\ufe0f Limitado |\n",
    "| **Learning Curve** | \u26a1 F\u00e1cil (si conoces Spark) | \u26a0\ufe0f Steeper |\n",
    "| **Best For** | Analytics, ML, unified batch+stream | Ultra-low latency, complex CEP |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5369d8",
   "metadata": {},
   "source": [
    "### \u23f0 **Watermarking: Manejo de Late Data y Event-time Windows**\n",
    "\n",
    "**Problema: Late Arriving Data**\n",
    "\n",
    "```\n",
    "Timeline:\n",
    "10:00 AM: User clicks product (event_timestamp = 10:00)\n",
    "10:15 AM: Network issues, event buffered\n",
    "10:30 AM: Event arrives at Spark (processing_time = 10:30)\n",
    "\n",
    "Sin watermark:\n",
    "- Evento procesado en ventana 10:30-10:35 \u274c (wrong)\n",
    "- Estado crece infinitamente (memory leak)\n",
    "\n",
    "Con watermark:\n",
    "- Evento procesado en ventana 10:00-10:05 \u2705 (correct)\n",
    "- Estado antiguo limpiado autom\u00e1ticamente\n",
    "```\n",
    "\n",
    "**Watermark = \"Cu\u00e1nto retraso tolerar\"**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "# Watermark de 10 minutos: Eventos con >10 min retraso se descartan\n",
    "df_with_watermark = df \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# C\u00f3mo funciona:\n",
    "# 1. Spark trackea max(event_timestamp) visto hasta ahora\n",
    "# 2. Watermark = max(event_timestamp) - threshold (10 min)\n",
    "# 3. Eventos con timestamp < watermark se descartan\n",
    "# 4. Estado de ventanas < watermark se elimina\n",
    "\n",
    "# Ejemplo num\u00e9rico:\n",
    "# Batch 1: max_event_time = 10:15, watermark = 10:05\n",
    "#   \u2192 Mantiene ventanas [10:00-10:05, 10:05-10:10, 10:10-10:15]\n",
    "# Batch 2: max_event_time = 10:25, watermark = 10:15\n",
    "#   \u2192 Elimina ventana [10:00-10:05], mantiene [10:05-10:25]\n",
    "```\n",
    "\n",
    "**Tipos de Ventanas (Windows):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, session_window\n",
    "\n",
    "# 1. TUMBLING WINDOW (no solapamiento)\n",
    "# Uso: M\u00e9tricas cada N minutos sin duplicar\n",
    "tumbling = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\")  # [00:00-00:10), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 2. SLIDING WINDOW (solapamiento)\n",
    "# Uso: Promedios m\u00f3viles, tendencias\n",
    "sliding = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\", \"5 minutes\")\n",
    "    # [00:00-00:10), [00:05-00:15), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 3. SESSION WINDOW (gap-based)\n",
    "# Uso: Sesiones de usuario, actividad continua\n",
    "session = df.groupBy(\n",
    "    \"user_id\",\n",
    "    session_window(\"timestamp\", \"30 minutes\")  # Gap de inactividad\n",
    ").count()\n",
    "# Si user activo 00:00, 00:05, 00:40 \u2192 2 sesiones:\n",
    "#   Session 1: [00:00-00:35] (last activity 00:05 + 30min gap)\n",
    "#   Session 2: [00:40-...]\n",
    "```\n",
    "\n",
    "**Configuraci\u00f3n de Watermark \u00d3ptima:**\n",
    "\n",
    "```python\n",
    "# \u274c Watermark muy corto (1 minuto)\n",
    "df.withWatermark(\"timestamp\", \"1 minute\")\n",
    "# Problema: Late data descartado agresivamente\n",
    "# Uso: Solo si latencia de red <1 min garantizada\n",
    "\n",
    "# \u2705 Watermark balanceado (10-30 minutos)\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "# Beneficio: 99% eventos capturados, estado razonable\n",
    "\n",
    "# \u26a0\ufe0f Watermark muy largo (2 horas)\n",
    "df.withWatermark(\"timestamp\", \"2 hours\")\n",
    "# Problema: Estado crece demasiado (OutOfMemory)\n",
    "# Uso: Solo si retrasos son realmente >1 hora\n",
    "\n",
    "# \ud83c\udfaf Regla de oro:\n",
    "# Watermark = P99 latency de tus datos + buffer\n",
    "# Ejemplo: Si 99% eventos llegan en <5 min \u2192 watermark 10 min\n",
    "```\n",
    "\n",
    "**Manejo de Late Data con Output Modes:**\n",
    "\n",
    "```python\n",
    "# Output Mode + Watermark interacci\u00f3n:\n",
    "\n",
    "# 1. APPEND + Watermark\n",
    "# \u2705 Escribe ventanas finalizadas (despu\u00e9s de watermark)\n",
    "# \u26a1 Best practice: Inmutable, no updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] escrita cuando watermark > 10:05\n",
    "\n",
    "# 2. UPDATE + Watermark\n",
    "# \u2705 Escribe ventanas activas + finalizadas\n",
    "# \u26a1 Uso: Dashboards que necesitan updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] puede actualizarse hasta watermark\n",
    "\n",
    "# 3. COMPLETE (no necesita watermark)\n",
    "# \u274c Re-escribe toda la tabla cada batch\n",
    "# Uso: Agregaciones peque\u00f1as (<1M registros)\n",
    "```\n",
    "\n",
    "**Caso Real: M\u00e9tricas de IoT Devices**\n",
    "\n",
    "```python\n",
    "# Devices env\u00edan telemetr\u00eda cada 1 minuto\n",
    "# Red celular puede tener latencia variable (1s - 5 min)\n",
    "\n",
    "iot_metrics = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"iot-telemetry\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), iot_schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "# Agregaci\u00f3n por device + ventana de 5 min\n",
    "device_stats = iot_metrics \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\  # Tolerar 10 min retraso\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"device_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        max(\"temperature\").alias(\"max_temp\"),\n",
    "        count(\"*\").alias(\"num_readings\")\n",
    "    ) \\\n",
    "    .filter(col(\"max_temp\") > 80)  # Alertas de sobrecalentamiento\n",
    "\n",
    "# Escribir alertas a Kafka para acci\u00f3n inmediata\n",
    "alerts_query = device_stats.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"device-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/iot-alerts\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Debugging Late Data:**\n",
    "\n",
    "```python\n",
    "# Agregar columnas de diagn\u00f3stico\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df_debug = df \\\n",
    "    .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "    .withColumn(\"latency_seconds\", \n",
    "        unix_timestamp(\"processing_time\") - unix_timestamp(\"event_timestamp\")\n",
    "    )\n",
    "\n",
    "# Analizar distribuci\u00f3n de latencias\n",
    "latency_stats = df_debug.groupBy(\n",
    "    window(\"processing_time\", \"1 minute\")\n",
    ").agg(\n",
    "    avg(\"latency_seconds\").alias(\"avg_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.95)\").alias(\"p95_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.99)\").alias(\"p99_latency\"),\n",
    "    max(\"latency_seconds\").alias(\"max_latency\")\n",
    ")\n",
    "\n",
    "# Si p99_latency = 300s (5 min) \u2192 watermark debe ser \u226510 min\n",
    "```\n",
    "\n",
    "**Watermark con Joins:**\n",
    "\n",
    "```python\n",
    "# Join entre streams requiere watermark en AMBOS\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "impressions = impressions_stream.withWatermark(\"impression_time\", \"10 minutes\")\n",
    "\n",
    "# Inner join con time constraint\n",
    "joined = clicks.join(\n",
    "    impressions,\n",
    "    expr(\"\"\"\n",
    "        click_user_id = impression_user_id AND\n",
    "        click_time >= impression_time AND\n",
    "        click_time <= impression_time + interval 1 hour\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Watermark resultante = min(5 min, 10 min) = 5 min\n",
    "# Estado mantenido: 1 hora (time constraint) + 5 min (watermark)\n",
    "```\n",
    "\n",
    "**Monitoreo de Watermark:**\n",
    "\n",
    "```python\n",
    "# Ver watermark actual en query progress\n",
    "progress = query.lastProgress\n",
    "print(f\"Current watermark: {progress['watermark']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "# {\n",
    "#   \"eventTime\": {\n",
    "#     \"avg\": \"2025-10-30T14:25:00.000Z\",\n",
    "#     \"max\": \"2025-10-30T14:30:00.000Z\",\n",
    "#     \"min\": \"2025-10-30T14:20:00.000Z\",\n",
    "#     \"watermark\": \"2025-10-30T14:20:00.000Z\"  # max - 10 min\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Alertas si watermark se retrasa mucho\n",
    "if progress['watermark'] < (current_time - timedelta(hours=1)):\n",
    "    send_alert(\"Watermark lagging! Check data source\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. \u2705 **Siempre usar watermark** con agregaciones event-time\n",
    "2. \u2705 **Medir P99 latency** de tus datos antes de configurar\n",
    "3. \u2705 **Append mode** con watermark para inmutabilidad\n",
    "4. \u2705 **Session windows** para an\u00e1lisis de comportamiento\n",
    "5. \u26a0\ufe0f **Sliding windows** consumen m\u00e1s estado (overlap)\n",
    "6. \u274c **No watermark infinito** (sin cleanup de estado)\n",
    "7. \u26a1 **Buffer 2-3x P99** para evitar drops\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06a65",
   "metadata": {},
   "source": [
    "### \ud83d\udcbe **State Management y Checkpointing: Fault Tolerance**\n",
    "\n",
    "**\u00bfQu\u00e9 es el Estado (State)?**\n",
    "\n",
    "```\n",
    "Stateless Processing (sin estado):\n",
    "Input: {\"user\": \"A\", \"action\": \"click\"}\n",
    "Output: {\"user\": \"A\", \"action\": \"click\", \"processed\": true}\n",
    "\u2705 Cada evento independiente\n",
    "\n",
    "Stateful Processing (con estado):\n",
    "Input Batch 1: {\"user\": \"A\", \"action\": \"click\"}\n",
    "State: {A: 1 click}\n",
    "Input Batch 2: {\"user\": \"A\", \"action\": \"purchase\"}\n",
    "State: {A: 1 click, 1 purchase}\n",
    "Output: {A: total_events=2, conversion_rate=1.0}\n",
    "\u2705 Memoria entre batches\n",
    "```\n",
    "\n",
    "**Operaciones Stateful en Spark:**\n",
    "\n",
    "```python\n",
    "# 1. Aggregations (groupBy)\n",
    "# Estado: Valores acumulados por key\n",
    "user_stats = df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\")\n",
    "    )\n",
    "# Estado almacenado: {user_123: {count: 45, revenue: 1200.50}}\n",
    "\n",
    "# 2. Windowed Aggregations\n",
    "# Estado: Valores por ventana + key\n",
    "window_stats = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "# Estado: {(window_10:00-10:05, product_42): 150}\n",
    "\n",
    "# 3. Stream-Stream Joins\n",
    "# Estado: Buffered events de ambos streams\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "views = views_stream.withWatermark(\"view_time\", \"5 minutes\")\n",
    "joined = clicks.join(views, \"session_id\")\n",
    "# Estado: Eventos no matcheados dentro de watermark\n",
    "\n",
    "# 4. Deduplication\n",
    "# Estado: Claves \u00fanicas vistas\n",
    "deduplicated = df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "# Estado: {event_456, event_789, ...}\n",
    "\n",
    "# 5. mapGroupsWithState / flatMapGroupsWithState\n",
    "# Estado: Customizado por usuario\n",
    "# Ejemplo: Sesiones complejas, m\u00e1quinas de estado\n",
    "```\n",
    "\n",
    "**State Store Backends:**\n",
    "\n",
    "```python\n",
    "# 1. Memory (default para testing)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"\n",
    ")\n",
    "# \u26a0\ufe0f Limitado por executor memory\n",
    "# Uso: Testing local, datasets peque\u00f1os\n",
    "\n",
    "# 2. RocksDB (producci\u00f3n)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "# \u2705 Disk-backed, escala a TB de estado\n",
    "# Uso: Producci\u00f3n, estado grande\n",
    "\n",
    "# 3. Custom (empresarial)\n",
    "# Ejemplo: Redis, Cassandra para estado compartido\n",
    "```\n",
    "\n",
    "**Checkpointing: Write-Ahead Log (WAL)**\n",
    "\n",
    "```\n",
    "Checkpoint Directory Structure:\n",
    "/checkpoints/my-query/\n",
    "\u251c\u2500\u2500 commits/\n",
    "\u2502   \u251c\u2500\u2500 0                    # Batch 0 metadata\n",
    "\u2502   \u251c\u2500\u2500 1\n",
    "\u2502   \u2514\u2500\u2500 2\n",
    "\u251c\u2500\u2500 offsets/\n",
    "\u2502   \u251c\u2500\u2500 0                    # Kafka offsets batch 0\n",
    "\u2502   \u251c\u2500\u2500 1\n",
    "\u2502   \u2514\u2500\u2500 2\n",
    "\u251c\u2500\u2500 sources/\n",
    "\u2502   \u2514\u2500\u2500 0/\n",
    "\u2502       \u2514\u2500\u2500 0               # Source info\n",
    "\u2514\u2500\u2500 state/\n",
    "    \u251c\u2500\u2500 0/\n",
    "    \u2502   \u251c\u2500\u2500 0/\n",
    "    \u2502   \u2502   \u251c\u2500\u2500 1.delta     # State snapshots\n",
    "    \u2502   \u2502   \u2514\u2500\u2500 1.snapshot\n",
    "    \u2502   \u2514\u2500\u2500 1/\n",
    "    \u2514\u2500\u2500 1/\n",
    "```\n",
    "\n",
    "**Configuraci\u00f3n de Checkpointing:**\n",
    "\n",
    "```python\n",
    "# Checkpoint obligatorio para stateful queries\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-stats\") \\  # REQUERIDO\n",
    "    .start(\"/delta/user_stats\")\n",
    "\n",
    "# Sin checkpoint \u2192 Error:\n",
    "# \"checkpointLocation must be specified either through option(\"checkpointLocation\", ...)\n",
    "# or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...)\"\n",
    "\n",
    "# Configuraci\u00f3n global (no recomendado)\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/checkpoints/default\")\n",
    "```\n",
    "\n",
    "**Recovery Process:**\n",
    "\n",
    "```python\n",
    "# Escenario: Executor falla durante Batch 5\n",
    "\n",
    "# 1. Query detecta falla\n",
    "# 2. Lee \u00faltimo checkpoint exitoso (Batch 4)\n",
    "#    - Offsets: Kafka partition 0 offset 1000, partition 1 offset 850\n",
    "#    - State: Usuario A \u2192 45 eventos, Usuario B \u2192 23 eventos\n",
    "# 3. Replay desde Batch 5\n",
    "#    - Lee desde offset 1000, 850 en Kafka\n",
    "#    - Restaura estado de Batch 4\n",
    "#    - Procesa Batch 5 con estado correcto\n",
    "# 4. Contin\u00faa normalmente\n",
    "\n",
    "# Garant\u00eda: Exactly-once processing\n",
    "# Kafka offsets + Estado + Output escritos at\u00f3micamente\n",
    "```\n",
    "\n",
    "**Ejemplo: Session Analytics con Estado Custom**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.streaming import GroupState, GroupStateTimeout\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "# Modelo de sesi\u00f3n\n",
    "@dataclass\n",
    "class SessionData:\n",
    "    user_id: str\n",
    "    start_time: datetime\n",
    "    last_activity: datetime\n",
    "    events: List[str]\n",
    "    total_revenue: float\n",
    "\n",
    "def update_session_state(\n",
    "    key: Tuple[str],\n",
    "    events: Iterator[pd.DataFrame],\n",
    "    state: GroupState\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Custom state management para sesiones\n",
    "    \"\"\"\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Leer estado anterior\n",
    "    if state.exists:\n",
    "        session = state.get()\n",
    "    else:\n",
    "        session = SessionData(\n",
    "            user_id=user_id,\n",
    "            start_time=None,\n",
    "            last_activity=None,\n",
    "            events=[],\n",
    "            total_revenue=0.0\n",
    "        )\n",
    "    \n",
    "    # Procesar eventos del batch\n",
    "    for event_batch in events:\n",
    "        for _, event in event_batch.iterrows():\n",
    "            if session.start_time is None:\n",
    "                session.start_time = event['timestamp']\n",
    "            \n",
    "            session.last_activity = event['timestamp']\n",
    "            session.events.append(event['event_type'])\n",
    "            session.total_revenue += event.get('revenue', 0.0)\n",
    "    \n",
    "    # Timeout si inactividad >30 min\n",
    "    if (datetime.now() - session.last_activity).seconds > 1800:\n",
    "        state.remove()  # Limpiar estado\n",
    "        # Emitir sesi\u00f3n finalizada\n",
    "        return iter([pd.DataFrame([{\n",
    "            'user_id': user_id,\n",
    "            'session_duration': (session.last_activity - session.start_time).seconds,\n",
    "            'num_events': len(session.events),\n",
    "            'total_revenue': session.total_revenue\n",
    "        }])])\n",
    "    else:\n",
    "        state.update(session)  # Guardar estado actualizado\n",
    "        state.setTimeoutDuration(\"30 minutes\")\n",
    "        return iter([])  # No output a\u00fan\n",
    "\n",
    "# Aplicar state management\n",
    "sessions = df.groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        update_session_state,\n",
    "        outputStructType=session_output_schema,\n",
    "        stateStructType=session_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "```\n",
    "\n",
    "**State Size Monitoring:**\n",
    "\n",
    "```python\n",
    "# M\u00e9tricas de estado en lastProgress\n",
    "progress = query.lastProgress\n",
    "\n",
    "state_info = progress[\"stateOperators\"][0]  # First stateful operator\n",
    "print(f\"Num state rows: {state_info['numRowsTotal']}\")\n",
    "print(f\"State memory (MB): {state_info['memoryUsedBytes'] / 1024 / 1024}\")\n",
    "print(f\"Custom metrics: {state_info['customMetrics']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "{\n",
    "  \"numRowsTotal\": 1234567,\n",
    "  \"numRowsUpdated\": 5678,\n",
    "  \"memoryUsedBytes\": 524288000,  # ~500 MB\n",
    "  \"customMetrics\": {\n",
    "    \"loadedMapCacheHitCount\": 1000,\n",
    "    \"loadedMapCacheMissCount\": 50,\n",
    "    \"stateOnCurrentVersionSizeBytes\": 450000000\n",
    "  }\n",
    "}\n",
    "\n",
    "# Alertas si estado crece sin control\n",
    "if state_info['numRowsTotal'] > 10_000_000:\n",
    "    print(\"\u26a0\ufe0f WARNING: State size >10M rows, consider:\")\n",
    "    print(\"  1. Reduce watermark threshold\")\n",
    "    print(\"  2. Increase parallelism (more partitions)\")\n",
    "    print(\"  3. Use state TTL (time-to-live)\")\n",
    "```\n",
    "\n",
    "**State Cleanup Strategies:**\n",
    "\n",
    "```python\n",
    "# 1. Watermark-based (autom\u00e1tico)\n",
    "df.withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\")) \\\n",
    "    .count()\n",
    "# Estado limpiado cuando ventana < watermark\n",
    "\n",
    "# 2. TTL (Time-to-Live) con RocksDB\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.stateInfo.ttl\", \"2 hours\")\n",
    "# Estado no accedido por >2h eliminado\n",
    "\n",
    "# 3. Manual cleanup en mapGroupsWithState\n",
    "def cleanup_old_state(key, events, state):\n",
    "    if state.hasTimedOut:\n",
    "        state.remove()  # Cleanup expl\u00edcito\n",
    "        return iter([])\n",
    "    # ... process events\n",
    "\n",
    "# 4. State re-partitioning para balancear\n",
    "df.repartition(200, \"user_id\")  # Distribuir estado uniformemente\n",
    "```\n",
    "\n",
    "**Checkpoint Management:**\n",
    "\n",
    "```python\n",
    "# \u274c NEVER cambiar checkpoint location en producci\u00f3n\n",
    "# Cambiar checkpoint = perder estado + reprocessar desde inicio\n",
    "\n",
    "# \u2705 Migration process:\n",
    "# 1. Stop query gracefully\n",
    "query.stop()\n",
    "\n",
    "# 2. Backup checkpoint\n",
    "# hdfs dfs -cp /checkpoints/old /checkpoints/backup\n",
    "\n",
    "# 3. Start new query con nuevo checkpoint\n",
    "query_new = df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/new\") \\\n",
    "    .start()\n",
    "\n",
    "# 4. Validar outputs son correctos\n",
    "\n",
    "# 5. Cleanup old checkpoint (despu\u00e9s de d\u00edas/semanas)\n",
    "# hdfs dfs -rm -r /checkpoints/old\n",
    "```\n",
    "\n",
    "**Caso Real: Fraud Detection con Estado**\n",
    "\n",
    "```python\n",
    "# Detectar m\u00faltiples transacciones sospechosas del mismo usuario\n",
    "\n",
    "fraud_detection = transactions \\\n",
    "    .withWatermark(\"transaction_time\", \"15 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        detect_fraud_pattern,\n",
    "        outputStructType=fraud_alert_schema,\n",
    "        stateStructType=user_fraud_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "\n",
    "def detect_fraud_pattern(key, events, state):\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Restaurar estado (historial de transacciones)\n",
    "    if state.exists:\n",
    "        history = state.get()\n",
    "    else:\n",
    "        history = {'transactions': [], 'risk_score': 0}\n",
    "    \n",
    "    # Analizar nuevas transacciones\n",
    "    for event_batch in events:\n",
    "        for _, txn in event_batch.iterrows():\n",
    "            history['transactions'].append(txn)\n",
    "            \n",
    "            # Patrones sospechosos:\n",
    "            # 1. >5 transacciones en 10 minutos\n",
    "            recent_txns = [t for t in history['transactions'] \n",
    "                          if (txn['timestamp'] - t['timestamp']).seconds < 600]\n",
    "            \n",
    "            # 2. Monto total >$10,000 en 10 minutos\n",
    "            recent_total = sum(t['amount'] for t in recent_txns)\n",
    "            \n",
    "            # 3. M\u00faltiples pa\u00edses en corto tiempo\n",
    "            recent_countries = set(t['country'] for t in recent_txns)\n",
    "            \n",
    "            if len(recent_txns) > 5 or recent_total > 10000 or len(recent_countries) > 2:\n",
    "                # Emitir alerta\n",
    "                return iter([pd.DataFrame([{\n",
    "                    'user_id': user_id,\n",
    "                    'alert_type': 'FRAUD_SUSPECTED',\n",
    "                    'risk_score': calculate_risk(recent_txns),\n",
    "                    'timestamp': txn['timestamp']\n",
    "                }])])\n",
    "    \n",
    "    # Actualizar estado con TTL 24h\n",
    "    state.update(history)\n",
    "    state.setTimeoutDuration(\"24 hours\")\n",
    "    return iter([])\n",
    "\n",
    "# Escribir alertas a Kafka para acci\u00f3n inmediata\n",
    "fraud_alerts = fraud_detection.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"fraud-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/fraud-detection\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. \u2705 **Siempre configurar checkpoint** para queries stateful\n",
    "2. \u2705 **RocksDB en producci\u00f3n** (memory solo para dev)\n",
    "3. \u2705 **Monitorear state size** con m\u00e9tricas\n",
    "4. \u2705 **Watermark** para cleanup autom\u00e1tico\n",
    "5. \u2705 **Backup checkpoints** antes de upgrades\n",
    "6. \u26a0\ufe0f **State TTL** para evitar crecimiento infinito\n",
    "7. \u274c **Nunca cambiar checkpoint location** sin migration plan\n",
    "8. \u26a1 **Repartition** por key para balancear estado\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08569a8",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Integraci\u00f3n con Delta Lake y Optimizaci\u00f3n de Performance**\n",
    "\n",
    "**Spark Streaming + Delta Lake = Streaming Lakehouse**\n",
    "\n",
    "```\n",
    "Ventajas de Delta como Sink:\n",
    "\u2705 ACID Transactions: Garant\u00eda exactly-once sin duplicados\n",
    "\u2705 Schema Evolution: Agregar columnas sin interrumpir stream\n",
    "\u2705 Time Travel: Rollback si procesamiento incorrecto\n",
    "\u2705 Upserts/Merges: CDC (Change Data Capture) en streaming\n",
    "\u2705 Performance: Z-ordering, compaction autom\u00e1tica\n",
    "```\n",
    "\n",
    "**Escribir Stream a Delta:**\n",
    "\n",
    "```python\n",
    "# Configuraci\u00f3n b\u00e1sica\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"path\", \"/delta/events\") \\\n",
    "    .start()\n",
    "\n",
    "# Configuraci\u00f3n avanzada\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\              # Auto schema evolution\n",
    "    .option(\"optimizeWrite\", \"true\") \\            # Bin-packing\n",
    "    .option(\"autoCompact\", \"true\") \\              # Auto compactaci\u00f3n\n",
    "    .partitionBy(\"date\", \"hour\") \\                # Particionamiento\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/events\")\n",
    "\n",
    "# Trigger AvailableNow (Spark 3.3+): Procesar backlog completo\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .trigger(availableNow=True) \\                 # Procesar todo y terminar\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/backfill\") \\\n",
    "    .start(\"/delta/events\")\n",
    "```\n",
    "\n",
    "**Streaming Upserts (Merge):**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Funci\u00f3n para merge en cada micro-batch\n",
    "def upsert_to_delta(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Merge batch into Delta table (UPSERT)\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/user_profiles\")\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.user_id = source.user_id\"\n",
    "    ).whenMatchedUpdateAll() \\    # Update si existe\n",
    "     .whenNotMatchedInsertAll() \\ # Insert si no existe\n",
    "     .execute()\n",
    "    \n",
    "    print(f\"Batch {batch_id} merged successfully\")\n",
    "\n",
    "# Aplicar a cada batch\n",
    "query = user_updates.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-upserts\") \\\n",
    "    .start()\n",
    "\n",
    "# Ejemplo: Actualizaci\u00f3n de perfiles de usuario\n",
    "# Batch 1: user_123 {name: \"John\", purchases: 5}\n",
    "# Batch 2: user_123 {purchases: 6}  \u2192 MERGE actualiza purchases\n",
    "# Batch 3: user_456 {name: \"Jane\"} \u2192 INSERT nuevo usuario\n",
    "```\n",
    "\n",
    "**Change Data Capture (CDC) Streaming:**\n",
    "\n",
    "```python\n",
    "# Leer CDC desde Kafka (Debezium format)\n",
    "cdc_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"mysql.prod.users\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), cdc_schema).alias(\"cdc\")\n",
    "    ).select(\"cdc.*\")\n",
    "\n",
    "# Aplicar cambios a Delta\n",
    "def apply_cdc_changes(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Procesar CDC events: INSERT, UPDATE, DELETE\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "    \n",
    "    # Separar por operaci\u00f3n\n",
    "    inserts = batch_df.filter(col(\"op\") == \"c\")  # Create\n",
    "    updates = batch_df.filter(col(\"op\") == \"u\")  # Update\n",
    "    deletes = batch_df.filter(col(\"op\") == \"d\")  # Delete\n",
    "    \n",
    "    # Aplicar deletes\n",
    "    if deletes.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            deletes.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedDelete().execute()\n",
    "    \n",
    "    # Aplicar upserts (inserts + updates)\n",
    "    upserts = inserts.union(updates)\n",
    "    if upserts.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            upserts.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "query = cdc_stream.writeStream \\\n",
    "    .foreachBatch(apply_cdc_changes) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdc\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Leer Stream desde Delta (Change Data Feed):**\n",
    "\n",
    "```python\n",
    "# Habilitar CDF en tabla Delta\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE user_profiles\n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "# Leer cambios como stream\n",
    "changes_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 10) \\  # Desde versi\u00f3n espec\u00edfica\n",
    "    .table(\"user_profiles\")\n",
    "\n",
    "# Columnas adicionales en CDF:\n",
    "# _change_type: insert, update_preimage, update_postimage, delete\n",
    "# _commit_version: Delta version\n",
    "# _commit_timestamp: When change happened\n",
    "\n",
    "# Procesar solo updates\n",
    "updates_only = changes_stream.filter(col(\"_change_type\") == \"update_postimage\")\n",
    "\n",
    "# Materializar a otra tabla\n",
    "query = updates_only.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdf-consumer\") \\\n",
    "    .start(\"/delta/user_updates_log\")\n",
    "```\n",
    "\n",
    "**Performance Optimization: Kafka Source**\n",
    "\n",
    "```python\n",
    "# Configuraci\u00f3n \u00f3ptima para Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9092,kafka3:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\        # latest, earliest, {\"topic\":{\"0\":23,\"1\":-1}}\n",
    "    .option(\"maxOffsetsPerTrigger\", 50000) \\      # Limitar ingesta por batch\n",
    "    .option(\"minPartitions\", 10) \\                # Paralelismo m\u00ednimo\n",
    "    .option(\"kafka.max.poll.records\", 500) \\      # Records por poll\n",
    "    .option(\"kafka.session.timeout.ms\", 30000) \\\n",
    "    .option(\"kafka.request.timeout.ms\", 40000) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\          # Tolerar data loss en dev\n",
    "    .load()\n",
    "\n",
    "# Consumer group por query\n",
    "# - Checkpoint location determina consumer group\n",
    "# - Cambiar checkpoint = nuevo consumer group = reprocessar todo\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# \u274c Anti-pattern: Sobre-particionamiento\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"user_id\") \\  # Millones de particiones!\n",
    "    .start()\n",
    "\n",
    "# \u2705 Particionamiento balanceado\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"date\") \\  # ~30 particiones/mes\n",
    "    .start()\n",
    "\n",
    "# \u26a1 Dynamic partition overwrite (batch mode)\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"/delta/events\")\n",
    "\n",
    "# Z-ordering para columnas de filtrado frecuente\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE events\n",
    "    ZORDER BY (user_id, product_id)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Auto Compaction:**\n",
    "\n",
    "```python\n",
    "# Problema: Small files generados por streaming\n",
    "# Cada micro-batch escribe archivos peque\u00f1os (10-100 MB)\n",
    "# Resultado: Millones de archivos despu\u00e9s de d\u00edas/semanas\n",
    "\n",
    "# Soluci\u00f3n 1: Auto-compaction (Delta 1.2+)\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\n",
    "\n",
    "# Soluci\u00f3n 2: Scheduled compaction job\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def compact_table(table_path):\n",
    "    \"\"\"\n",
    "    Compactar archivos peque\u00f1os\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, table_path)\n",
    "    \n",
    "    # OPTIMIZE: Compactar archivos en particiones\n",
    "    delta_table.optimize() \\\n",
    "        .where(\"date >= current_date() - interval 7 days\") \\  # Solo \u00faltimos 7 d\u00edas\n",
    "        .executeCompaction()\n",
    "    \n",
    "    # VACUUM: Limpiar archivos antiguos (despu\u00e9s de retention period)\n",
    "    delta_table.vacuum(retentionHours=168)  # 7 d\u00edas\n",
    "\n",
    "# Ejecutar como Airflow DAG diario\n",
    "optimize_dag = DAG('delta_optimize', schedule_interval='@daily')\n",
    "\n",
    "# Soluci\u00f3n 3: Bin-packing en escritura\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\  # Bin-pack antes de escribir\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Monitoring y Alertas:**\n",
    "\n",
    "```python\n",
    "# M\u00e9tricas clave para monitoreo\n",
    "def extract_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer m\u00e9tricas para Prometheus/Datadog\n",
    "    \"\"\"\n",
    "    progress = query.lastProgress\n",
    "    \n",
    "    if progress is None:\n",
    "        return {}\n",
    "    \n",
    "    metrics = {\n",
    "        # Throughput\n",
    "        \"input_rows_per_second\": progress.get(\"inputRowsPerSecond\", 0),\n",
    "        \"processed_rows_per_second\": progress.get(\"processedRowsPerSecond\", 0),\n",
    "        \n",
    "        # Latency\n",
    "        \"batch_duration_ms\": progress.get(\"durationMs\", {}).get(\"triggerExecution\", 0),\n",
    "        \"batch_id\": progress.get(\"batchId\", 0),\n",
    "        \n",
    "        # State\n",
    "        \"num_state_rows\": 0,\n",
    "        \"state_memory_mb\": 0,\n",
    "        \n",
    "        # Lag\n",
    "        \"input_rows\": progress.get(\"numInputRows\", 0),\n",
    "    }\n",
    "    \n",
    "    # State metrics si hay operadores stateful\n",
    "    if \"stateOperators\" in progress and len(progress[\"stateOperators\"]) > 0:\n",
    "        state_op = progress[\"stateOperators\"][0]\n",
    "        metrics[\"num_state_rows\"] = state_op.get(\"numRowsTotal\", 0)\n",
    "        metrics[\"state_memory_mb\"] = state_op.get(\"memoryUsedBytes\", 0) / 1024 / 1024\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Alertas\n",
    "def check_alerts(metrics):\n",
    "    \"\"\"\n",
    "    Generar alertas si m\u00e9tricas anormales\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    # Alerta 1: Falling behind\n",
    "    if metrics[\"input_rows_per_second\"] > metrics[\"processed_rows_per_second\"] * 1.2:\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Processing falling behind: {metrics['input_rows_per_second']:.0f} in/s vs {metrics['processed_rows_per_second']:.0f} out/s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 2: High latency\n",
    "    if metrics[\"batch_duration_ms\"] > 60000:  # >1 minuto\n",
    "        alerts.append({\n",
    "            \"severity\": \"ERROR\",\n",
    "            \"message\": f\"High batch latency: {metrics['batch_duration_ms']/1000:.1f}s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 3: State growing unbounded\n",
    "    if metrics[\"num_state_rows\"] > 50_000_000:  # >50M registros\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Large state size: {metrics['num_state_rows']:,} rows, {metrics['state_memory_mb']:.1f} MB\"\n",
    "        })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Integraci\u00f3n con monitoring\n",
    "import time\n",
    "while query.isActive:\n",
    "    time.sleep(60)  # Check cada minuto\n",
    "    metrics = extract_stream_metrics(query)\n",
    "    alerts = check_alerts(metrics)\n",
    "    \n",
    "    # Enviar a Prometheus\n",
    "    push_to_prometheus(metrics)\n",
    "    \n",
    "    # Enviar alertas a Slack/PagerDuty\n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            send_alert(alert)\n",
    "```\n",
    "\n",
    "**Caso Real: Real-time Analytics Dashboard**\n",
    "\n",
    "```python\n",
    "# Pipeline completo: Kafka \u2192 Spark Streaming \u2192 Delta \u2192 BI Tool\n",
    "\n",
    "# 1. Leer eventos de Kafka\n",
    "events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"e\")) \\\n",
    "    .select(\"e.*\")\n",
    "\n",
    "# 2. Transformaciones\n",
    "events_enriched = events \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .join(\n",
    "        users_dim.alias(\"u\"),\n",
    "        events.user_id == col(\"u.user_id\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"event_id\"),\n",
    "        col(\"event_timestamp\"),\n",
    "        col(\"e.user_id\"),\n",
    "        col(\"u.user_segment\"),  # Enriquecimiento\n",
    "        col(\"event_type\"),\n",
    "        col(\"revenue\")\n",
    "    )\n",
    "\n",
    "# 3. Agregaciones por ventanas\n",
    "dashboard_metrics = events_enriched \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_segment\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\")\n",
    "    )\n",
    "\n",
    "# 4. Escribir a Delta con optimizaciones\n",
    "query = dashboard_metrics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/dashboard\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\\n",
    "    .option(\"autoCompact\", \"true\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/dashboard_metrics\")\n",
    "\n",
    "# 5. BI Tool lee desde Delta (Tableau, Power BI, Looker)\n",
    "# SELECT * FROM delta.`/delta/dashboard_metrics`\n",
    "# WHERE window.start >= current_timestamp() - interval 1 hour\n",
    "\n",
    "# 6. Z-order para performance\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta.`/delta/dashboard_metrics`\n",
    "    ZORDER BY (window, user_segment)\n",
    "\"\"\")\n",
    "\n",
    "# 7. Vacuum archivos antiguos (semanal)\n",
    "spark.sql(\"\"\"\n",
    "    VACUUM delta.`/delta/dashboard_metrics`\n",
    "    RETAIN 168 HOURS\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. \u2705 **Delta como sink primario** en producci\u00f3n (ACID + performance)\n",
    "2. \u2705 **foreachBatch para l\u00f3gica custom** (upserts, alerts, etc.)\n",
    "3. \u2705 **Auto-compaction** o scheduled OPTIMIZE\n",
    "4. \u2705 **Z-ordering** en columnas de filtrado frecuente\n",
    "5. \u2705 **Particionamiento por fecha** (balance entre granularidad y cantidad)\n",
    "6. \u2705 **maxOffsetsPerTrigger** para controlar ingesta\n",
    "7. \u2705 **Monitor m\u00e9tricas** continuamente (Prometheus, Datadog)\n",
    "8. \u26a0\ufe0f **Change Data Feed** para downstream consumers\n",
    "9. \u26a0\ufe0f **failOnDataLoss=false** solo en dev (strict en prod)\n",
    "10. \u274c **No sobre-particionar** (evitar millones de particiones)\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "\n",
    "```\n",
    "Scenario: 1M events/s, 10 KB/event, 100 partitions\n",
    "\n",
    "Kafka \u2192 Spark Streaming \u2192 Parquet:\n",
    "- Latency: ~30s (trigger interval + processing)\n",
    "- Small files: 1000+ archivos/hora\n",
    "- OPTIMIZE needed: Daily\n",
    "- Cost: $$\n",
    "\n",
    "Kafka \u2192 Spark Streaming \u2192 Delta (optimized):\n",
    "- Latency: ~30s\n",
    "- Small files: Auto-compacted\n",
    "- OPTIMIZE needed: Weekly\n",
    "- Cost: $$ (slightly higher for optimization)\n",
    "- Benefit: ACID, time travel, upserts\n",
    "\n",
    "Kafka \u2192 Flink \u2192 Delta:\n",
    "- Latency: ~5s\n",
    "- More complex setup\n",
    "- Cost: $$$\n",
    "- Benefit: Lower latency, event-time watermarks native\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c904ba",
   "metadata": {},
   "source": [
    "## 1. Inicializar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Spark Session con configuraci\u00f3n para Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkStreamingAdvanced\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar nivel de logging\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528ec21",
   "metadata": {},
   "source": [
    "## 2. Definir Esquemas para Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema para eventos de e-commerce\n",
    "ecommerce_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Esquema para logs de aplicaci\u00f3n\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"level\", StringType(), False),\n",
    "    StructField(\"service\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"error_code\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Esquemas definidos\")\n",
    "print(\"\\nEsquema E-commerce:\")\n",
    "print(ecommerce_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9bb26",
   "metadata": {},
   "source": [
    "## 3. Simulaci\u00f3n de Fuente de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3139b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo para simular streaming\n",
    "def generate_sample_data(n_records=1000):\n",
    "    \"\"\"\n",
    "    Generar datos de muestra para streaming\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_records):\n",
    "        event = {\n",
    "            'event_id': f'evt_{i:06d}',\n",
    "            'timestamp': base_time + timedelta(seconds=i),\n",
    "            'user_id': f'user_{np.random.randint(1, 101)}',\n",
    "            'event_type': np.random.choice(['view', 'add_to_cart', 'purchase', 'remove'], p=[0.5, 0.25, 0.15, 0.1]),\n",
    "            'product_id': f'prod_{np.random.randint(1, 51)}',\n",
    "            'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "            'category': np.random.choice(['Electronics', 'Accessories', 'Computers']),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "        data.append(event)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Generar datos\n",
    "sample_data = generate_sample_data(1000)\n",
    "df_sample = spark.createDataFrame(sample_data, schema=ecommerce_schema)\n",
    "\n",
    "print(f\"Generados {df_sample.count()} registros de muestra\")\n",
    "df_sample.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbdd91",
   "metadata": {},
   "source": [
    "## 4. Streaming con Rate Source (Simulaci\u00f3n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear stream simulado con rate source\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()\n",
    "\n",
    "# Enriquecer con datos simulados\n",
    "from pyspark.sql.functions import rand, when, lit\n",
    "\n",
    "enriched_stream = rate_stream \\\n",
    "    .withColumn(\"user_id\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"event_type\", \n",
    "        when(rand() < 0.5, \"view\")\n",
    "        .when(rand() < 0.75, \"add_to_cart\")\n",
    "        .when(rand() < 0.9, \"purchase\")\n",
    "        .otherwise(\"remove\")\n",
    "    ) \\\n",
    "    .withColumn(\"product_id\", (rand() * 50).cast(\"int\")) \\\n",
    "    .withColumn(\"price\", (rand() * 1990 + 10).cast(\"double\")) \\\n",
    "    .withColumn(\"quantity\", (rand() * 4 + 1).cast(\"int\"))\n",
    "\n",
    "print(\"Stream enriquecido creado\")\n",
    "print(\"Schema:\")\n",
    "enriched_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa2ae8",
   "metadata": {},
   "source": [
    "## 5. Agregaciones por Ventanas de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones por ventana de tiempo\n",
    "windowed_counts = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\"),\n",
    "        col(\"event_type\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "print(\"Query de agregaci\u00f3n por ventanas configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ba0d3",
   "metadata": {},
   "source": [
    "## 6. Procesamiento Stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c181a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantener estado acumulado por usuario\n",
    "user_aggregations = enriched_stream \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"total_spent\"),\n",
    "        spark_max(\"timestamp\").alias(\"last_activity\")\n",
    "    )\n",
    "\n",
    "print(\"Query de agregaci\u00f3n por usuario configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177bf9",
   "metadata": {},
   "source": [
    "## 7. Detecci\u00f3n de Patrones en Tiempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfe837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar usuarios con comportamiento an\u00f3malo\n",
    "anomaly_detection = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\"),\n",
    "        col(\"user_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"events_per_minute\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"spend_per_minute\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"events_per_minute\") > 50) |  # M\u00e1s de 50 eventos por minuto\n",
    "        (col(\"spend_per_minute\") > 10000)   # M\u00e1s de $10,000 por minuto\n",
    "    )\n",
    "\n",
    "print(\"Query de detecci\u00f3n de anomal\u00edas configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2880b",
   "metadata": {},
   "source": [
    "## 8. Ejemplo de Query con Output Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados en consola (modo batch para demo)\n",
    "# NOTA: En producci\u00f3n usar\u00edas .writeStream() en lugar de show()\n",
    "\n",
    "print(\"\\n=== AN\u00c1LISIS BATCH DE DATOS DE MUESTRA ===\")\n",
    "print(\"\\n1. Eventos por tipo:\")\n",
    "df_sample.groupBy(\"event_type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\n2. Revenue por categor\u00eda:\")\n",
    "df_sample.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_purchases\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_revenue\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n3. Top 10 usuarios m\u00e1s activos:\")\n",
    "df_sample.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_events\", ascending=False) \\\n",
    "    .limit(10) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n4. Tasa de conversi\u00f3n por producto:\")\n",
    "conversion_rate = df_sample.groupBy(\"product_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    )\n",
    "\n",
    "conversion_rate.withColumn(\n",
    "    \"conversion_rate\",\n",
    "    (col(\"purchases\") / col(\"views\") * 100)\n",
    ").orderBy(\"conversion_rate\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac2a3f",
   "metadata": {},
   "source": [
    "## 9. Escribir Stream a Diferentes Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de configuraci\u00f3n de escritura (comentado para evitar ejecuci\u00f3n)\n",
    "\n",
    "# 1. Escribir a consola (desarrollo/debug)\n",
    "console_query_config = {\n",
    "    'outputMode': 'complete',  # complete, append, update\n",
    "    'format': 'console',\n",
    "    'trigger': {'processingTime': '10 seconds'},\n",
    "    'options': {\n",
    "        'truncate': False,\n",
    "        'numRows': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Escribir a Parquet (data lake)\n",
    "parquet_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'parquet',\n",
    "    'path': '/path/to/output',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'trigger': {'processingTime': '1 minute'},\n",
    "    'options': {\n",
    "        'compression': 'snappy',\n",
    "        'partitionBy': 'date'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Escribir a Kafka\n",
    "kafka_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'kafka',\n",
    "    'options': {\n",
    "        'kafka.bootstrap.servers': 'localhost:9092',\n",
    "        'topic': 'processed-events',\n",
    "        'checkpointLocation': '/path/to/checkpoint'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Escribir a Delta Lake\n",
    "delta_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'delta',\n",
    "    'path': '/path/to/delta',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'options': {\n",
    "        'mergeSchema': True,\n",
    "        'optimizeWrite': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuraciones de sink definidas (ver c\u00f3digo para detalles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e869",
   "metadata": {},
   "source": [
    "## 10. M\u00e9tricas y Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci\u00f3n para monitorear estado del stream\n",
    "def monitor_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer m\u00e9tricas del streaming query\n",
    "    \"\"\"\n",
    "    status = query.status\n",
    "    \n",
    "    metrics = {\n",
    "        'isDataAvailable': status['isDataAvailable'],\n",
    "        'isTriggerActive': status['isTriggerActive'],\n",
    "        'message': status['message']\n",
    "    }\n",
    "    \n",
    "    if 'inputRowsPerSecond' in status:\n",
    "        metrics['inputRowsPerSecond'] = status['inputRowsPerSecond']\n",
    "    \n",
    "    if 'processedRowsPerSecond' in status:\n",
    "        metrics['processedRowsPerSecond'] = status['processedRowsPerSecond']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Ejemplo de m\u00e9tricas a monitorear\n",
    "print(\"\\n=== M\u00c9TRICAS CLAVE PARA MONITOREO ===\")\n",
    "print(\"\"\"\n",
    "1. Input Rate: Eventos por segundo recibidos\n",
    "2. Processing Rate: Eventos por segundo procesados\n",
    "3. Batch Duration: Tiempo de procesamiento por batch\n",
    "4. Trigger Interval: Intervalo entre ejecuciones\n",
    "5. Watermark: Retraso m\u00e1ximo aceptado\n",
    "6. Estado del Query: Activo, inactivo, error\n",
    "7. Checkpoint Location: Para recuperaci\u00f3n de fallos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a91d85",
   "metadata": {},
   "source": [
    "## 11. Optimizaci\u00f3n de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de optimizaci\u00f3n\n",
    "optimization_configs = {\n",
    "    # Particionamiento\n",
    "    'spark.sql.shuffle.partitions': '200',  # N\u00famero de particiones para shuffles\n",
    "    'spark.default.parallelism': '200',     # Paralelismo por defecto\n",
    "    \n",
    "    # Memoria\n",
    "    'spark.executor.memory': '4g',\n",
    "    'spark.driver.memory': '2g',\n",
    "    'spark.memory.fraction': '0.8',\n",
    "    \n",
    "    # Streaming espec\u00edfico\n",
    "    'spark.sql.streaming.minBatchesToRetain': '100',\n",
    "    'spark.sql.streaming.stateStore.providerClass': 'org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider',\n",
    "    \n",
    "    # Optimizaci\u00f3n de escritura\n",
    "    'spark.sql.adaptive.enabled': 'true',\n",
    "    'spark.sql.adaptive.coalescePartitions.enabled': 'true',\n",
    "}\n",
    "\n",
    "print(\"\\n=== MEJORES PR\u00c1CTICAS DE OPTIMIZACI\u00d3N ===\")\n",
    "print(\"\"\"\n",
    "1. Usar watermarks para limpiar estado antiguo\n",
    "2. Particionar datos por columnas clave\n",
    "3. Configurar apropiadamente spark.sql.shuffle.partitions\n",
    "4. Usar triggers basados en tiempo para controlar frecuencia\n",
    "5. Implementar checkpointing para recuperaci\u00f3n\n",
    "6. Monitorear m\u00e9tricas constantemente\n",
    "7. Usar Delta Lake para ACID transactions\n",
    "8. Implementar compactaci\u00f3n de archivos peque\u00f1os\n",
    "9. Optimizar esquemas y evitar tipos gen\u00e9ricos\n",
    "10. Considerar micro-batching vs continuous processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5288b",
   "metadata": {},
   "source": [
    "## Resumen y Arquitectura Enterprise\n",
    "\n",
    "### Arquitectura T\u00edpica de Streaming:\n",
    "```\n",
    "Fuentes de Datos       Ingesta           Procesamiento        Almacenamiento        Consumo\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Kafka/Kinesis    \u2192    Spark       \u2192     Transformaciones  \u2192   Delta Lake      \u2192    BI Tools\n",
    "IoT Devices      \u2192    Streaming   \u2192     Agregaciones      \u2192   Data Lake       \u2192    ML Models\n",
    "APIs             \u2192                \u2192     Joins             \u2192   Warehouse       \u2192    Dashboards\n",
    "Logs             \u2192                \u2192     Windows           \u2192   Cache (Redis)   \u2192    Alertas\n",
    "```\n",
    "\n",
    "### Patrones Avanzados:\n",
    "\n",
    "#### 1. Lambda Architecture\n",
    "- **Batch Layer**: Procesamiento hist\u00f3rico completo\n",
    "- **Speed Layer**: Procesamiento en tiempo real\n",
    "- **Serving Layer**: Combina ambas vistas\n",
    "\n",
    "#### 2. Kappa Architecture\n",
    "- Solo capa de streaming\n",
    "- Todo procesamiento en tiempo real\n",
    "- Reprocesamiento desde el inicio del stream\n",
    "\n",
    "#### 3. Delta Architecture\n",
    "- Basada en Delta Lake\n",
    "- ACID transactions\n",
    "- Time travel\n",
    "- Schema evolution\n",
    "\n",
    "### Casos de Uso Enterprise:\n",
    "\n",
    "1. **Detecci\u00f3n de Fraude en Tiempo Real**\n",
    "   - An\u00e1lisis de patrones sospechosos\n",
    "   - Machine Learning en streaming\n",
    "   - Alertas autom\u00e1ticas\n",
    "\n",
    "2. **Recomendaciones Personalizadas**\n",
    "   - Seguimiento de comportamiento en tiempo real\n",
    "   - Actualizaci\u00f3n de perfiles de usuario\n",
    "   - A/B testing din\u00e1mico\n",
    "\n",
    "3. **Monitoreo de Infraestructura**\n",
    "   - Logs y m\u00e9tricas en tiempo real\n",
    "   - Detecci\u00f3n de anomal\u00edas\n",
    "   - Auto-scaling basado en carga\n",
    "\n",
    "4. **IoT y Telemetr\u00eda**\n",
    "   - Procesamiento de sensores\n",
    "   - Mantenimiento predictivo\n",
    "   - Optimizaci\u00f3n de operaciones\n",
    "\n",
    "### Consideraciones de Producci\u00f3n:\n",
    "\n",
    "- **Alta Disponibilidad**: Cluster mode, m\u00faltiples workers\n",
    "- **Fault Tolerance**: Checkpointing, Write-Ahead Logs\n",
    "- **Escalabilidad**: Auto-scaling, dynamic allocation\n",
    "- **Seguridad**: Kerberos, SSL/TLS, encryption at rest\n",
    "- **Monitoreo**: Prometheus, Grafana, CloudWatch\n",
    "- **Testing**: Unit tests, integration tests, chaos engineering\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)\n",
    "- [Databricks Streaming Best Practices](https://docs.databricks.com/structured-streaming/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar recursos\n",
    "spark.stop()\n",
    "print(\"Spark Session cerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh \u2192](04_arquitecturas_modernas.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
