{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc763fbf",
   "metadata": {},
   "source": [
    "# Apache Spark Streaming: Procesamiento en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar Spark Structured Streaming\n",
    "- Implementar procesamiento de ventanas y agregaciones\n",
    "- Integrar con Kafka y otras fuentes de streaming\n",
    "- Manejar estado y checkpointing\n",
    "- Optimizar rendimiento en streaming\n",
    "\n",
    "## Requisitos\n",
    "- PySpark 3.x\n",
    "- Python 3.8+\n",
    "- Kafka (opcional)\n",
    "- Delta Lake (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619b7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todas las dependencias ya est√°n instaladas\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n e instalaci√≥n de dependencias\n",
    "try:\n",
    "    import pyspark\n",
    "    import pandas\n",
    "    import numpy\n",
    "    print(\"‚úÖ Todas las dependencias ya est√°n instaladas\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Falta instalar: {e.name}\")\n",
    "    print(\"üì¶ Instalando dependencias necesarias...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"pyspark\", \"pandas\", \"numpy\", \"--quiet\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300  # 5 minutos m√°ximo\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Dependencias instaladas correctamente\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d944bedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, count, sum as spark_sum, avg, max as spark_max,\n",
    "    current_timestamp, to_json, from_json, struct, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425e406",
   "metadata": {},
   "source": [
    "### üåä **Spark Structured Streaming: Arquitectura y Micro-batches**\n",
    "\n",
    "**Evoluci√≥n del Streaming en Spark:**\n",
    "\n",
    "```\n",
    "Gen 1: Spark Streaming (DStreams) - 2013\n",
    "‚îú‚îÄ‚îÄ RDDs en micro-batches\n",
    "‚îú‚îÄ‚îÄ API de bajo nivel\n",
    "‚îî‚îÄ‚îÄ ‚ùå Complejo, no unificado con batch\n",
    "\n",
    "Gen 2: Structured Streaming - 2016\n",
    "‚îú‚îÄ‚îÄ DataFrames/Datasets unificados\n",
    "‚îú‚îÄ‚îÄ Event-time processing\n",
    "‚îú‚îÄ‚îÄ Exactly-once semantics\n",
    "‚îî‚îÄ‚îÄ ‚úÖ API declarativa, tolerancia a fallos\n",
    "```\n",
    "\n",
    "**Micro-batch vs Continuous Processing:**\n",
    "\n",
    "| Aspecto | Micro-batch (default) | Continuous (experimental) |\n",
    "|---------|----------------------|---------------------------|\n",
    "| **Latencia** | ~100ms - 1s | ~1ms |\n",
    "| **Throughput** | ‚úÖ Alto (10K+ events/s) | ‚ö†Ô∏è Medio |\n",
    "| **Garant√≠as** | ‚úÖ Exactly-once | ‚ö†Ô∏è At-least-once |\n",
    "| **Estado** | ‚úÖ Full support | ‚ö†Ô∏è Limitado |\n",
    "| **Uso** | Analytics, aggregations | Ultra-low latency alerts |\n",
    "\n",
    "**Arquitectura de Micro-batch:**\n",
    "\n",
    "```python\n",
    "# Streaming Query = Infinite Table\n",
    "# Cada batch procesa un \"snapshot\" incremental\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Input Source (Kafka, Kinesis, Files)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ Batch 0: Records [1-100]\n",
    "               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Incremental Query Engine                ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Batch 0: Transform + Aggregate     ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ Write Batch 0 Results\n",
    "               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Output Sink (Delta, Kafka, Console)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ\n",
    "               ‚îÇ Trigger interval (e.g., 10s)\n",
    "               ‚ñº\n",
    "         (Repeat Batch 1, 2, 3...)\n",
    "```\n",
    "\n",
    "**Triggers (Frecuencia de Ejecuci√≥n):**\n",
    "\n",
    "```python\n",
    "# 1. ProcessingTime: Ejecutar cada N segundos\n",
    "query = df.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# 2. Once: Ejecutar una sola vez (√∫til para testing/backfill)\n",
    "query = df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# 3. Continuous: Latencia ultra-baja (~1ms)\n",
    "query = df.writeStream \\\n",
    "    .trigger(continuous='1 second') \\\n",
    "    .start()\n",
    "\n",
    "# 4. AvailableNow: Procesar todos los datos disponibles (Spark 3.3+)\n",
    "query = df.writeStream \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "```python\n",
    "# 1. Append: Solo nuevos registros (default)\n",
    "# ‚úÖ Uso: Raw logs, eventos sin agregaciones\n",
    "# ‚ùå Limitaci√≥n: No updates/deletes\n",
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .start(\"/data/events\")\n",
    "\n",
    "# 2. Complete: Toda la tabla resultado (re-escribir completo)\n",
    "# ‚úÖ Uso: Agregaciones peque√±as, dashboards\n",
    "# ‚ùå Limitaci√≥n: No escala, solo con aggregations\n",
    "df.groupBy(\"category\").count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"category_counts\") \\\n",
    "    .start()\n",
    "\n",
    "# 3. Update: Solo registros modificados\n",
    "# ‚úÖ Uso: Agregaciones con watermark, upserts\n",
    "# ‚ö° Best practice: Balanceo append vs complete\n",
    "df.groupBy(window(\"timestamp\", \"1 hour\"), \"user_id\") \\\n",
    "    .agg(count(\"*\").alias(\"events\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .start(\"/data/user_hourly_stats\")\n",
    "```\n",
    "\n",
    "**Event-time vs Processing-time:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# ‚ùå Processing-time: Cuando Spark procesa el evento\n",
    "df_processing_time = df.withColumn(\"processed_at\", current_timestamp())\n",
    "# Problema: Late data processed in wrong time window\n",
    "\n",
    "# ‚úÖ Event-time: Timestamp del evento original\n",
    "df_event_time = df.select(\"event_timestamp\", \"user_id\", \"action\")\n",
    "# Benefit: Correcta agregaci√≥n temporal incluso con retrasos\n",
    "\n",
    "# Ejemplo: Click en app a las 10:00 AM\n",
    "# - Dispositivo offline hasta 10:30 AM\n",
    "# - Processing-time: 10:30 AM ‚ùå (wrong window)\n",
    "# - Event-time: 10:00 AM ‚úÖ (correct window)\n",
    "```\n",
    "\n",
    "**Backpressure y Rate Limiting:**\n",
    "\n",
    "```python\n",
    "# Limitar ingesta para evitar overwhelm\n",
    "spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\  # Max 10K records/batch\n",
    "    .option(\"minPartitions\", 4) \\              # Paralelismo m√≠nimo\n",
    "    .load()\n",
    "\n",
    "# Backpressure autom√°tico (Spark 3.2+)\n",
    "spark.conf.set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.backpressure.initialRate\", 1000)\n",
    "```\n",
    "\n",
    "**Ejemplo Real: E-commerce Click Stream**\n",
    "\n",
    "```python\n",
    "# Fuente: Kafka con eventos de clicks\n",
    "clicks = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-clicks\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON payload\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "clicks_parsed = clicks.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Agregaci√≥n por ventanas de 5 minutos\n",
    "clicks_per_5min = clicks_parsed \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"clicks\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    )\n",
    "\n",
    "# Escribir a Delta Lake con ACID guarantees\n",
    "query = clicks_per_5min.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/clicks-5min\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/clicks_aggregated\")\n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(f\"Last Progress: {query.lastProgress}\")\n",
    "```\n",
    "\n",
    "**M√©tricas de Performance:**\n",
    "\n",
    "```python\n",
    "# Acceder a m√©tricas del stream\n",
    "progress = query.lastProgress\n",
    "\n",
    "key_metrics = {\n",
    "    \"batchId\": progress[\"batchId\"],\n",
    "    \"inputRowsPerSecond\": progress[\"inputRowsPerSecond\"],\n",
    "    \"processedRowsPerSecond\": progress[\"processedRowsPerSecond\"],\n",
    "    \"batchDuration\": progress[\"batchDuration\"],  # ms\n",
    "    \"numInputRows\": progress[\"numInputRows\"],\n",
    "    \"stateOperators\": progress[\"stateOperators\"]  # Estado acumulado\n",
    "}\n",
    "\n",
    "# Alertas si procesamiento es m√°s lento que ingesta\n",
    "if progress[\"inputRowsPerSecond\"] > progress[\"processedRowsPerSecond\"]:\n",
    "    print(\"‚ö†Ô∏è WARNING: Falling behind! Increase parallelism\")\n",
    "```\n",
    "\n",
    "**Comparaci√≥n con Flink:**\n",
    "\n",
    "| Caracter√≠stica | Spark Streaming | Apache Flink |\n",
    "|----------------|-----------------|--------------|\n",
    "| **Modelo** | Micro-batch | True streaming (record-at-a-time) |\n",
    "| **Latencia** | 100ms - 1s | 10ms - 100ms |\n",
    "| **Throughput** | ‚úÖ Muy alto | ‚úÖ Alto |\n",
    "| **Estado** | RocksDB, memory | RocksDB native |\n",
    "| **SQL Support** | ‚úÖ Excellent | ‚úÖ Good |\n",
    "| **Ecosystem** | ‚úÖ Spark ML, Delta | ‚ö†Ô∏è Limitado |\n",
    "| **Learning Curve** | ‚ö° F√°cil (si conoces Spark) | ‚ö†Ô∏è Steeper |\n",
    "| **Best For** | Analytics, ML, unified batch+stream | Ultra-low latency, complex CEP |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5369d8",
   "metadata": {},
   "source": [
    "### ‚è∞ **Watermarking: Manejo de Late Data y Event-time Windows**\n",
    "\n",
    "**Problema: Late Arriving Data**\n",
    "\n",
    "```\n",
    "Timeline:\n",
    "10:00 AM: User clicks product (event_timestamp = 10:00)\n",
    "10:15 AM: Network issues, event buffered\n",
    "10:30 AM: Event arrives at Spark (processing_time = 10:30)\n",
    "\n",
    "Sin watermark:\n",
    "- Evento procesado en ventana 10:30-10:35 ‚ùå (wrong)\n",
    "- Estado crece infinitamente (memory leak)\n",
    "\n",
    "Con watermark:\n",
    "- Evento procesado en ventana 10:00-10:05 ‚úÖ (correct)\n",
    "- Estado antiguo limpiado autom√°ticamente\n",
    "```\n",
    "\n",
    "**Watermark = \"Cu√°nto retraso tolerar\"**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "# Watermark de 10 minutos: Eventos con >10 min retraso se descartan\n",
    "df_with_watermark = df \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# C√≥mo funciona:\n",
    "# 1. Spark trackea max(event_timestamp) visto hasta ahora\n",
    "# 2. Watermark = max(event_timestamp) - threshold (10 min)\n",
    "# 3. Eventos con timestamp < watermark se descartan\n",
    "# 4. Estado de ventanas < watermark se elimina\n",
    "\n",
    "# Ejemplo num√©rico:\n",
    "# Batch 1: max_event_time = 10:15, watermark = 10:05\n",
    "#   ‚Üí Mantiene ventanas [10:00-10:05, 10:05-10:10, 10:10-10:15]\n",
    "# Batch 2: max_event_time = 10:25, watermark = 10:15\n",
    "#   ‚Üí Elimina ventana [10:00-10:05], mantiene [10:05-10:25]\n",
    "```\n",
    "\n",
    "**Tipos de Ventanas (Windows):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, session_window\n",
    "\n",
    "# 1. TUMBLING WINDOW (no solapamiento)\n",
    "# Uso: M√©tricas cada N minutos sin duplicar\n",
    "tumbling = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\")  # [00:00-00:10), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 2. SLIDING WINDOW (solapamiento)\n",
    "# Uso: Promedios m√≥viles, tendencias\n",
    "sliding = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\", \"5 minutes\")\n",
    "    # [00:00-00:10), [00:05-00:15), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 3. SESSION WINDOW (gap-based)\n",
    "# Uso: Sesiones de usuario, actividad continua\n",
    "session = df.groupBy(\n",
    "    \"user_id\",\n",
    "    session_window(\"timestamp\", \"30 minutes\")  # Gap de inactividad\n",
    ").count()\n",
    "# Si user activo 00:00, 00:05, 00:40 ‚Üí 2 sesiones:\n",
    "#   Session 1: [00:00-00:35] (last activity 00:05 + 30min gap)\n",
    "#   Session 2: [00:40-...]\n",
    "```\n",
    "\n",
    "**Configuraci√≥n de Watermark √ìptima:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Watermark muy corto (1 minuto)\n",
    "df.withWatermark(\"timestamp\", \"1 minute\")\n",
    "# Problema: Late data descartado agresivamente\n",
    "# Uso: Solo si latencia de red <1 min garantizada\n",
    "\n",
    "# ‚úÖ Watermark balanceado (10-30 minutos)\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "# Beneficio: 99% eventos capturados, estado razonable\n",
    "\n",
    "# ‚ö†Ô∏è Watermark muy largo (2 horas)\n",
    "df.withWatermark(\"timestamp\", \"2 hours\")\n",
    "# Problema: Estado crece demasiado (OutOfMemory)\n",
    "# Uso: Solo si retrasos son realmente >1 hora\n",
    "\n",
    "# üéØ Regla de oro:\n",
    "# Watermark = P99 latency de tus datos + buffer\n",
    "# Ejemplo: Si 99% eventos llegan en <5 min ‚Üí watermark 10 min\n",
    "```\n",
    "\n",
    "**Manejo de Late Data con Output Modes:**\n",
    "\n",
    "```python\n",
    "# Output Mode + Watermark interacci√≥n:\n",
    "\n",
    "# 1. APPEND + Watermark\n",
    "# ‚úÖ Escribe ventanas finalizadas (despu√©s de watermark)\n",
    "# ‚ö° Best practice: Inmutable, no updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] escrita cuando watermark > 10:05\n",
    "\n",
    "# 2. UPDATE + Watermark\n",
    "# ‚úÖ Escribe ventanas activas + finalizadas\n",
    "# ‚ö° Uso: Dashboards que necesitan updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] puede actualizarse hasta watermark\n",
    "\n",
    "# 3. COMPLETE (no necesita watermark)\n",
    "# ‚ùå Re-escribe toda la tabla cada batch\n",
    "# Uso: Agregaciones peque√±as (<1M registros)\n",
    "```\n",
    "\n",
    "**Caso Real: M√©tricas de IoT Devices**\n",
    "\n",
    "```python\n",
    "# Devices env√≠an telemetr√≠a cada 1 minuto\n",
    "# Red celular puede tener latencia variable (1s - 5 min)\n",
    "\n",
    "iot_metrics = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"iot-telemetry\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), iot_schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "# Agregaci√≥n por device + ventana de 5 min\n",
    "device_stats = iot_metrics \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\  # Tolerar 10 min retraso\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"device_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        max(\"temperature\").alias(\"max_temp\"),\n",
    "        count(\"*\").alias(\"num_readings\")\n",
    "    ) \\\n",
    "    .filter(col(\"max_temp\") > 80)  # Alertas de sobrecalentamiento\n",
    "\n",
    "# Escribir alertas a Kafka para acci√≥n inmediata\n",
    "alerts_query = device_stats.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"device-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/iot-alerts\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Debugging Late Data:**\n",
    "\n",
    "```python\n",
    "# Agregar columnas de diagn√≥stico\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df_debug = df \\\n",
    "    .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "    .withColumn(\"latency_seconds\", \n",
    "        unix_timestamp(\"processing_time\") - unix_timestamp(\"event_timestamp\")\n",
    "    )\n",
    "\n",
    "# Analizar distribuci√≥n de latencias\n",
    "latency_stats = df_debug.groupBy(\n",
    "    window(\"processing_time\", \"1 minute\")\n",
    ").agg(\n",
    "    avg(\"latency_seconds\").alias(\"avg_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.95)\").alias(\"p95_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.99)\").alias(\"p99_latency\"),\n",
    "    max(\"latency_seconds\").alias(\"max_latency\")\n",
    ")\n",
    "\n",
    "# Si p99_latency = 300s (5 min) ‚Üí watermark debe ser ‚â•10 min\n",
    "```\n",
    "\n",
    "**Watermark con Joins:**\n",
    "\n",
    "```python\n",
    "# Join entre streams requiere watermark en AMBOS\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "impressions = impressions_stream.withWatermark(\"impression_time\", \"10 minutes\")\n",
    "\n",
    "# Inner join con time constraint\n",
    "joined = clicks.join(\n",
    "    impressions,\n",
    "    expr(\"\"\"\n",
    "        click_user_id = impression_user_id AND\n",
    "        click_time >= impression_time AND\n",
    "        click_time <= impression_time + interval 1 hour\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Watermark resultante = min(5 min, 10 min) = 5 min\n",
    "# Estado mantenido: 1 hora (time constraint) + 5 min (watermark)\n",
    "```\n",
    "\n",
    "**Monitoreo de Watermark:**\n",
    "\n",
    "```python\n",
    "# Ver watermark actual en query progress\n",
    "progress = query.lastProgress\n",
    "print(f\"Current watermark: {progress['watermark']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "# {\n",
    "#   \"eventTime\": {\n",
    "#     \"avg\": \"2025-10-30T14:25:00.000Z\",\n",
    "#     \"max\": \"2025-10-30T14:30:00.000Z\",\n",
    "#     \"min\": \"2025-10-30T14:20:00.000Z\",\n",
    "#     \"watermark\": \"2025-10-30T14:20:00.000Z\"  # max - 10 min\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Alertas si watermark se retrasa mucho\n",
    "if progress['watermark'] < (current_time - timedelta(hours=1)):\n",
    "    send_alert(\"Watermark lagging! Check data source\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ‚úÖ **Siempre usar watermark** con agregaciones event-time\n",
    "2. ‚úÖ **Medir P99 latency** de tus datos antes de configurar\n",
    "3. ‚úÖ **Append mode** con watermark para inmutabilidad\n",
    "4. ‚úÖ **Session windows** para an√°lisis de comportamiento\n",
    "5. ‚ö†Ô∏è **Sliding windows** consumen m√°s estado (overlap)\n",
    "6. ‚ùå **No watermark infinito** (sin cleanup de estado)\n",
    "7. ‚ö° **Buffer 2-3x P99** para evitar drops\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06a65",
   "metadata": {},
   "source": [
    "### üíæ **State Management y Checkpointing: Fault Tolerance**\n",
    "\n",
    "**¬øQu√© es el Estado (State)?**\n",
    "\n",
    "```\n",
    "Stateless Processing (sin estado):\n",
    "Input: {\"user\": \"A\", \"action\": \"click\"}\n",
    "Output: {\"user\": \"A\", \"action\": \"click\", \"processed\": true}\n",
    "‚úÖ Cada evento independiente\n",
    "\n",
    "Stateful Processing (con estado):\n",
    "Input Batch 1: {\"user\": \"A\", \"action\": \"click\"}\n",
    "State: {A: 1 click}\n",
    "Input Batch 2: {\"user\": \"A\", \"action\": \"purchase\"}\n",
    "State: {A: 1 click, 1 purchase}\n",
    "Output: {A: total_events=2, conversion_rate=1.0}\n",
    "‚úÖ Memoria entre batches\n",
    "```\n",
    "\n",
    "**Operaciones Stateful en Spark:**\n",
    "\n",
    "```python\n",
    "# 1. Aggregations (groupBy)\n",
    "# Estado: Valores acumulados por key\n",
    "user_stats = df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\")\n",
    "    )\n",
    "# Estado almacenado: {user_123: {count: 45, revenue: 1200.50}}\n",
    "\n",
    "# 2. Windowed Aggregations\n",
    "# Estado: Valores por ventana + key\n",
    "window_stats = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "# Estado: {(window_10:00-10:05, product_42): 150}\n",
    "\n",
    "# 3. Stream-Stream Joins\n",
    "# Estado: Buffered events de ambos streams\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "views = views_stream.withWatermark(\"view_time\", \"5 minutes\")\n",
    "joined = clicks.join(views, \"session_id\")\n",
    "# Estado: Eventos no matcheados dentro de watermark\n",
    "\n",
    "# 4. Deduplication\n",
    "# Estado: Claves √∫nicas vistas\n",
    "deduplicated = df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "# Estado: {event_456, event_789, ...}\n",
    "\n",
    "# 5. mapGroupsWithState / flatMapGroupsWithState\n",
    "# Estado: Customizado por usuario\n",
    "# Ejemplo: Sesiones complejas, m√°quinas de estado\n",
    "```\n",
    "\n",
    "**State Store Backends:**\n",
    "\n",
    "```python\n",
    "# 1. Memory (default para testing)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"\n",
    ")\n",
    "# ‚ö†Ô∏è Limitado por executor memory\n",
    "# Uso: Testing local, datasets peque√±os\n",
    "\n",
    "# 2. RocksDB (producci√≥n)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "# ‚úÖ Disk-backed, escala a TB de estado\n",
    "# Uso: Producci√≥n, estado grande\n",
    "\n",
    "# 3. Custom (empresarial)\n",
    "# Ejemplo: Redis, Cassandra para estado compartido\n",
    "```\n",
    "\n",
    "**Checkpointing: Write-Ahead Log (WAL)**\n",
    "\n",
    "```\n",
    "Checkpoint Directory Structure:\n",
    "/checkpoints/my-query/\n",
    "‚îú‚îÄ‚îÄ commits/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 0                    # Batch 0 metadata\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2\n",
    "‚îú‚îÄ‚îÄ offsets/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 0                    # Kafka offsets batch 0\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2\n",
    "‚îú‚îÄ‚îÄ sources/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 0/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ 0               # Source info\n",
    "‚îî‚îÄ‚îÄ state/\n",
    "    ‚îú‚îÄ‚îÄ 0/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ 0/\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1.delta     # State snapshots\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 1.snapshot\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ 1/\n",
    "    ‚îî‚îÄ‚îÄ 1/\n",
    "```\n",
    "\n",
    "**Configuraci√≥n de Checkpointing:**\n",
    "\n",
    "```python\n",
    "# Checkpoint obligatorio para stateful queries\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-stats\") \\  # REQUERIDO\n",
    "    .start(\"/delta/user_stats\")\n",
    "\n",
    "# Sin checkpoint ‚Üí Error:\n",
    "# \"checkpointLocation must be specified either through option(\"checkpointLocation\", ...)\n",
    "# or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...)\"\n",
    "\n",
    "# Configuraci√≥n global (no recomendado)\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/checkpoints/default\")\n",
    "```\n",
    "\n",
    "**Recovery Process:**\n",
    "\n",
    "```python\n",
    "# Escenario: Executor falla durante Batch 5\n",
    "\n",
    "# 1. Query detecta falla\n",
    "# 2. Lee √∫ltimo checkpoint exitoso (Batch 4)\n",
    "#    - Offsets: Kafka partition 0 offset 1000, partition 1 offset 850\n",
    "#    - State: Usuario A ‚Üí 45 eventos, Usuario B ‚Üí 23 eventos\n",
    "# 3. Replay desde Batch 5\n",
    "#    - Lee desde offset 1000, 850 en Kafka\n",
    "#    - Restaura estado de Batch 4\n",
    "#    - Procesa Batch 5 con estado correcto\n",
    "# 4. Contin√∫a normalmente\n",
    "\n",
    "# Garant√≠a: Exactly-once processing\n",
    "# Kafka offsets + Estado + Output escritos at√≥micamente\n",
    "```\n",
    "\n",
    "**Ejemplo: Session Analytics con Estado Custom**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.streaming import GroupState, GroupStateTimeout\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "# Modelo de sesi√≥n\n",
    "@dataclass\n",
    "class SessionData:\n",
    "    user_id: str\n",
    "    start_time: datetime\n",
    "    last_activity: datetime\n",
    "    events: List[str]\n",
    "    total_revenue: float\n",
    "\n",
    "def update_session_state(\n",
    "    key: Tuple[str],\n",
    "    events: Iterator[pd.DataFrame],\n",
    "    state: GroupState\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Custom state management para sesiones\n",
    "    \"\"\"\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Leer estado anterior\n",
    "    if state.exists:\n",
    "        session = state.get()\n",
    "    else:\n",
    "        session = SessionData(\n",
    "            user_id=user_id,\n",
    "            start_time=None,\n",
    "            last_activity=None,\n",
    "            events=[],\n",
    "            total_revenue=0.0\n",
    "        )\n",
    "    \n",
    "    # Procesar eventos del batch\n",
    "    for event_batch in events:\n",
    "        for _, event in event_batch.iterrows():\n",
    "            if session.start_time is None:\n",
    "                session.start_time = event['timestamp']\n",
    "            \n",
    "            session.last_activity = event['timestamp']\n",
    "            session.events.append(event['event_type'])\n",
    "            session.total_revenue += event.get('revenue', 0.0)\n",
    "    \n",
    "    # Timeout si inactividad >30 min\n",
    "    if (datetime.now() - session.last_activity).seconds > 1800:\n",
    "        state.remove()  # Limpiar estado\n",
    "        # Emitir sesi√≥n finalizada\n",
    "        return iter([pd.DataFrame([{\n",
    "            'user_id': user_id,\n",
    "            'session_duration': (session.last_activity - session.start_time).seconds,\n",
    "            'num_events': len(session.events),\n",
    "            'total_revenue': session.total_revenue\n",
    "        }])])\n",
    "    else:\n",
    "        state.update(session)  # Guardar estado actualizado\n",
    "        state.setTimeoutDuration(\"30 minutes\")\n",
    "        return iter([])  # No output a√∫n\n",
    "\n",
    "# Aplicar state management\n",
    "sessions = df.groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        update_session_state,\n",
    "        outputStructType=session_output_schema,\n",
    "        stateStructType=session_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "```\n",
    "\n",
    "**State Size Monitoring:**\n",
    "\n",
    "```python\n",
    "# M√©tricas de estado en lastProgress\n",
    "progress = query.lastProgress\n",
    "\n",
    "state_info = progress[\"stateOperators\"][0]  # First stateful operator\n",
    "print(f\"Num state rows: {state_info['numRowsTotal']}\")\n",
    "print(f\"State memory (MB): {state_info['memoryUsedBytes'] / 1024 / 1024}\")\n",
    "print(f\"Custom metrics: {state_info['customMetrics']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "{\n",
    "  \"numRowsTotal\": 1234567,\n",
    "  \"numRowsUpdated\": 5678,\n",
    "  \"memoryUsedBytes\": 524288000,  # ~500 MB\n",
    "  \"customMetrics\": {\n",
    "    \"loadedMapCacheHitCount\": 1000,\n",
    "    \"loadedMapCacheMissCount\": 50,\n",
    "    \"stateOnCurrentVersionSizeBytes\": 450000000\n",
    "  }\n",
    "}\n",
    "\n",
    "# Alertas si estado crece sin control\n",
    "if state_info['numRowsTotal'] > 10_000_000:\n",
    "    print(\"‚ö†Ô∏è WARNING: State size >10M rows, consider:\")\n",
    "    print(\"  1. Reduce watermark threshold\")\n",
    "    print(\"  2. Increase parallelism (more partitions)\")\n",
    "    print(\"  3. Use state TTL (time-to-live)\")\n",
    "```\n",
    "\n",
    "**State Cleanup Strategies:**\n",
    "\n",
    "```python\n",
    "# 1. Watermark-based (autom√°tico)\n",
    "df.withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\")) \\\n",
    "    .count()\n",
    "# Estado limpiado cuando ventana < watermark\n",
    "\n",
    "# 2. TTL (Time-to-Live) con RocksDB\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.stateInfo.ttl\", \"2 hours\")\n",
    "# Estado no accedido por >2h eliminado\n",
    "\n",
    "# 3. Manual cleanup en mapGroupsWithState\n",
    "def cleanup_old_state(key, events, state):\n",
    "    if state.hasTimedOut:\n",
    "        state.remove()  # Cleanup expl√≠cito\n",
    "        return iter([])\n",
    "    # ... process events\n",
    "\n",
    "# 4. State re-partitioning para balancear\n",
    "df.repartition(200, \"user_id\")  # Distribuir estado uniformemente\n",
    "```\n",
    "\n",
    "**Checkpoint Management:**\n",
    "\n",
    "```python\n",
    "# ‚ùå NEVER cambiar checkpoint location en producci√≥n\n",
    "# Cambiar checkpoint = perder estado + reprocessar desde inicio\n",
    "\n",
    "# ‚úÖ Migration process:\n",
    "# 1. Stop query gracefully\n",
    "query.stop()\n",
    "\n",
    "# 2. Backup checkpoint\n",
    "# hdfs dfs -cp /checkpoints/old /checkpoints/backup\n",
    "\n",
    "# 3. Start new query con nuevo checkpoint\n",
    "query_new = df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/new\") \\\n",
    "    .start()\n",
    "\n",
    "# 4. Validar outputs son correctos\n",
    "\n",
    "# 5. Cleanup old checkpoint (despu√©s de d√≠as/semanas)\n",
    "# hdfs dfs -rm -r /checkpoints/old\n",
    "```\n",
    "\n",
    "**Caso Real: Fraud Detection con Estado**\n",
    "\n",
    "```python\n",
    "# Detectar m√∫ltiples transacciones sospechosas del mismo usuario\n",
    "\n",
    "fraud_detection = transactions \\\n",
    "    .withWatermark(\"transaction_time\", \"15 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        detect_fraud_pattern,\n",
    "        outputStructType=fraud_alert_schema,\n",
    "        stateStructType=user_fraud_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "\n",
    "def detect_fraud_pattern(key, events, state):\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Restaurar estado (historial de transacciones)\n",
    "    if state.exists:\n",
    "        history = state.get()\n",
    "    else:\n",
    "        history = {'transactions': [], 'risk_score': 0}\n",
    "    \n",
    "    # Analizar nuevas transacciones\n",
    "    for event_batch in events:\n",
    "        for _, txn in event_batch.iterrows():\n",
    "            history['transactions'].append(txn)\n",
    "            \n",
    "            # Patrones sospechosos:\n",
    "            # 1. >5 transacciones en 10 minutos\n",
    "            recent_txns = [t for t in history['transactions'] \n",
    "                          if (txn['timestamp'] - t['timestamp']).seconds < 600]\n",
    "            \n",
    "            # 2. Monto total >$10,000 en 10 minutos\n",
    "            recent_total = sum(t['amount'] for t in recent_txns)\n",
    "            \n",
    "            # 3. M√∫ltiples pa√≠ses en corto tiempo\n",
    "            recent_countries = set(t['country'] for t in recent_txns)\n",
    "            \n",
    "            if len(recent_txns) > 5 or recent_total > 10000 or len(recent_countries) > 2:\n",
    "                # Emitir alerta\n",
    "                return iter([pd.DataFrame([{\n",
    "                    'user_id': user_id,\n",
    "                    'alert_type': 'FRAUD_SUSPECTED',\n",
    "                    'risk_score': calculate_risk(recent_txns),\n",
    "                    'timestamp': txn['timestamp']\n",
    "                }])])\n",
    "    \n",
    "    # Actualizar estado con TTL 24h\n",
    "    state.update(history)\n",
    "    state.setTimeoutDuration(\"24 hours\")\n",
    "    return iter([])\n",
    "\n",
    "# Escribir alertas a Kafka para acci√≥n inmediata\n",
    "fraud_alerts = fraud_detection.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"fraud-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/fraud-detection\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ‚úÖ **Siempre configurar checkpoint** para queries stateful\n",
    "2. ‚úÖ **RocksDB en producci√≥n** (memory solo para dev)\n",
    "3. ‚úÖ **Monitorear state size** con m√©tricas\n",
    "4. ‚úÖ **Watermark** para cleanup autom√°tico\n",
    "5. ‚úÖ **Backup checkpoints** antes de upgrades\n",
    "6. ‚ö†Ô∏è **State TTL** para evitar crecimiento infinito\n",
    "7. ‚ùå **Nunca cambiar checkpoint location** sin migration plan\n",
    "8. ‚ö° **Repartition** por key para balancear estado\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08569a8",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Integraci√≥n con Delta Lake y Optimizaci√≥n de Performance**\n",
    "\n",
    "**Spark Streaming + Delta Lake = Streaming Lakehouse**\n",
    "\n",
    "```\n",
    "Ventajas de Delta como Sink:\n",
    "‚úÖ ACID Transactions: Garant√≠a exactly-once sin duplicados\n",
    "‚úÖ Schema Evolution: Agregar columnas sin interrumpir stream\n",
    "‚úÖ Time Travel: Rollback si procesamiento incorrecto\n",
    "‚úÖ Upserts/Merges: CDC (Change Data Capture) en streaming\n",
    "‚úÖ Performance: Z-ordering, compaction autom√°tica\n",
    "```\n",
    "\n",
    "**Escribir Stream a Delta:**\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n b√°sica\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"path\", \"/delta/events\") \\\n",
    "    .start()\n",
    "\n",
    "# Configuraci√≥n avanzada\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\              # Auto schema evolution\n",
    "    .option(\"optimizeWrite\", \"true\") \\            # Bin-packing\n",
    "    .option(\"autoCompact\", \"true\") \\              # Auto compactaci√≥n\n",
    "    .partitionBy(\"date\", \"hour\") \\                # Particionamiento\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/events\")\n",
    "\n",
    "# Trigger AvailableNow (Spark 3.3+): Procesar backlog completo\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .trigger(availableNow=True) \\                 # Procesar todo y terminar\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/backfill\") \\\n",
    "    .start(\"/delta/events\")\n",
    "```\n",
    "\n",
    "**Streaming Upserts (Merge):**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Funci√≥n para merge en cada micro-batch\n",
    "def upsert_to_delta(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Merge batch into Delta table (UPSERT)\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/user_profiles\")\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.user_id = source.user_id\"\n",
    "    ).whenMatchedUpdateAll() \\    # Update si existe\n",
    "     .whenNotMatchedInsertAll() \\ # Insert si no existe\n",
    "     .execute()\n",
    "    \n",
    "    print(f\"Batch {batch_id} merged successfully\")\n",
    "\n",
    "# Aplicar a cada batch\n",
    "query = user_updates.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-upserts\") \\\n",
    "    .start()\n",
    "\n",
    "# Ejemplo: Actualizaci√≥n de perfiles de usuario\n",
    "# Batch 1: user_123 {name: \"John\", purchases: 5}\n",
    "# Batch 2: user_123 {purchases: 6}  ‚Üí MERGE actualiza purchases\n",
    "# Batch 3: user_456 {name: \"Jane\"} ‚Üí INSERT nuevo usuario\n",
    "```\n",
    "\n",
    "**Change Data Capture (CDC) Streaming:**\n",
    "\n",
    "```python\n",
    "# Leer CDC desde Kafka (Debezium format)\n",
    "cdc_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"mysql.prod.users\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), cdc_schema).alias(\"cdc\")\n",
    "    ).select(\"cdc.*\")\n",
    "\n",
    "# Aplicar cambios a Delta\n",
    "def apply_cdc_changes(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Procesar CDC events: INSERT, UPDATE, DELETE\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "    \n",
    "    # Separar por operaci√≥n\n",
    "    inserts = batch_df.filter(col(\"op\") == \"c\")  # Create\n",
    "    updates = batch_df.filter(col(\"op\") == \"u\")  # Update\n",
    "    deletes = batch_df.filter(col(\"op\") == \"d\")  # Delete\n",
    "    \n",
    "    # Aplicar deletes\n",
    "    if deletes.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            deletes.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedDelete().execute()\n",
    "    \n",
    "    # Aplicar upserts (inserts + updates)\n",
    "    upserts = inserts.union(updates)\n",
    "    if upserts.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            upserts.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "query = cdc_stream.writeStream \\\n",
    "    .foreachBatch(apply_cdc_changes) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdc\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Leer Stream desde Delta (Change Data Feed):**\n",
    "\n",
    "```python\n",
    "# Habilitar CDF en tabla Delta\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE user_profiles\n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "# Leer cambios como stream\n",
    "changes_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 10) \\  # Desde versi√≥n espec√≠fica\n",
    "    .table(\"user_profiles\")\n",
    "\n",
    "# Columnas adicionales en CDF:\n",
    "# _change_type: insert, update_preimage, update_postimage, delete\n",
    "# _commit_version: Delta version\n",
    "# _commit_timestamp: When change happened\n",
    "\n",
    "# Procesar solo updates\n",
    "updates_only = changes_stream.filter(col(\"_change_type\") == \"update_postimage\")\n",
    "\n",
    "# Materializar a otra tabla\n",
    "query = updates_only.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdf-consumer\") \\\n",
    "    .start(\"/delta/user_updates_log\")\n",
    "```\n",
    "\n",
    "**Performance Optimization: Kafka Source**\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n √≥ptima para Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9092,kafka3:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\        # latest, earliest, {\"topic\":{\"0\":23,\"1\":-1}}\n",
    "    .option(\"maxOffsetsPerTrigger\", 50000) \\      # Limitar ingesta por batch\n",
    "    .option(\"minPartitions\", 10) \\                # Paralelismo m√≠nimo\n",
    "    .option(\"kafka.max.poll.records\", 500) \\      # Records por poll\n",
    "    .option(\"kafka.session.timeout.ms\", 30000) \\\n",
    "    .option(\"kafka.request.timeout.ms\", 40000) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\          # Tolerar data loss en dev\n",
    "    .load()\n",
    "\n",
    "# Consumer group por query\n",
    "# - Checkpoint location determina consumer group\n",
    "# - Cambiar checkpoint = nuevo consumer group = reprocessar todo\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Anti-pattern: Sobre-particionamiento\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"user_id\") \\  # Millones de particiones!\n",
    "    .start()\n",
    "\n",
    "# ‚úÖ Particionamiento balanceado\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"date\") \\  # ~30 particiones/mes\n",
    "    .start()\n",
    "\n",
    "# ‚ö° Dynamic partition overwrite (batch mode)\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"/delta/events\")\n",
    "\n",
    "# Z-ordering para columnas de filtrado frecuente\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE events\n",
    "    ZORDER BY (user_id, product_id)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Auto Compaction:**\n",
    "\n",
    "```python\n",
    "# Problema: Small files generados por streaming\n",
    "# Cada micro-batch escribe archivos peque√±os (10-100 MB)\n",
    "# Resultado: Millones de archivos despu√©s de d√≠as/semanas\n",
    "\n",
    "# Soluci√≥n 1: Auto-compaction (Delta 1.2+)\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\n",
    "\n",
    "# Soluci√≥n 2: Scheduled compaction job\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def compact_table(table_path):\n",
    "    \"\"\"\n",
    "    Compactar archivos peque√±os\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, table_path)\n",
    "    \n",
    "    # OPTIMIZE: Compactar archivos en particiones\n",
    "    delta_table.optimize() \\\n",
    "        .where(\"date >= current_date() - interval 7 days\") \\  # Solo √∫ltimos 7 d√≠as\n",
    "        .executeCompaction()\n",
    "    \n",
    "    # VACUUM: Limpiar archivos antiguos (despu√©s de retention period)\n",
    "    delta_table.vacuum(retentionHours=168)  # 7 d√≠as\n",
    "\n",
    "# Ejecutar como Airflow DAG diario\n",
    "optimize_dag = DAG('delta_optimize', schedule_interval='@daily')\n",
    "\n",
    "# Soluci√≥n 3: Bin-packing en escritura\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\  # Bin-pack antes de escribir\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Monitoring y Alertas:**\n",
    "\n",
    "```python\n",
    "# M√©tricas clave para monitoreo\n",
    "def extract_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer m√©tricas para Prometheus/Datadog\n",
    "    \"\"\"\n",
    "    progress = query.lastProgress\n",
    "    \n",
    "    if progress is None:\n",
    "        return {}\n",
    "    \n",
    "    metrics = {\n",
    "        # Throughput\n",
    "        \"input_rows_per_second\": progress.get(\"inputRowsPerSecond\", 0),\n",
    "        \"processed_rows_per_second\": progress.get(\"processedRowsPerSecond\", 0),\n",
    "        \n",
    "        # Latency\n",
    "        \"batch_duration_ms\": progress.get(\"durationMs\", {}).get(\"triggerExecution\", 0),\n",
    "        \"batch_id\": progress.get(\"batchId\", 0),\n",
    "        \n",
    "        # State\n",
    "        \"num_state_rows\": 0,\n",
    "        \"state_memory_mb\": 0,\n",
    "        \n",
    "        # Lag\n",
    "        \"input_rows\": progress.get(\"numInputRows\", 0),\n",
    "    }\n",
    "    \n",
    "    # State metrics si hay operadores stateful\n",
    "    if \"stateOperators\" in progress and len(progress[\"stateOperators\"]) > 0:\n",
    "        state_op = progress[\"stateOperators\"][0]\n",
    "        metrics[\"num_state_rows\"] = state_op.get(\"numRowsTotal\", 0)\n",
    "        metrics[\"state_memory_mb\"] = state_op.get(\"memoryUsedBytes\", 0) / 1024 / 1024\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Alertas\n",
    "def check_alerts(metrics):\n",
    "    \"\"\"\n",
    "    Generar alertas si m√©tricas anormales\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    # Alerta 1: Falling behind\n",
    "    if metrics[\"input_rows_per_second\"] > metrics[\"processed_rows_per_second\"] * 1.2:\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Processing falling behind: {metrics['input_rows_per_second']:.0f} in/s vs {metrics['processed_rows_per_second']:.0f} out/s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 2: High latency\n",
    "    if metrics[\"batch_duration_ms\"] > 60000:  # >1 minuto\n",
    "        alerts.append({\n",
    "            \"severity\": \"ERROR\",\n",
    "            \"message\": f\"High batch latency: {metrics['batch_duration_ms']/1000:.1f}s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 3: State growing unbounded\n",
    "    if metrics[\"num_state_rows\"] > 50_000_000:  # >50M registros\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Large state size: {metrics['num_state_rows']:,} rows, {metrics['state_memory_mb']:.1f} MB\"\n",
    "        })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Integraci√≥n con monitoring\n",
    "import time\n",
    "while query.isActive:\n",
    "    time.sleep(60)  # Check cada minuto\n",
    "    metrics = extract_stream_metrics(query)\n",
    "    alerts = check_alerts(metrics)\n",
    "    \n",
    "    # Enviar a Prometheus\n",
    "    push_to_prometheus(metrics)\n",
    "    \n",
    "    # Enviar alertas a Slack/PagerDuty\n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            send_alert(alert)\n",
    "```\n",
    "\n",
    "**Caso Real: Real-time Analytics Dashboard**\n",
    "\n",
    "```python\n",
    "# Pipeline completo: Kafka ‚Üí Spark Streaming ‚Üí Delta ‚Üí BI Tool\n",
    "\n",
    "# 1. Leer eventos de Kafka\n",
    "events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"e\")) \\\n",
    "    .select(\"e.*\")\n",
    "\n",
    "# 2. Transformaciones\n",
    "events_enriched = events \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .join(\n",
    "        users_dim.alias(\"u\"),\n",
    "        events.user_id == col(\"u.user_id\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"event_id\"),\n",
    "        col(\"event_timestamp\"),\n",
    "        col(\"e.user_id\"),\n",
    "        col(\"u.user_segment\"),  # Enriquecimiento\n",
    "        col(\"event_type\"),\n",
    "        col(\"revenue\")\n",
    "    )\n",
    "\n",
    "# 3. Agregaciones por ventanas\n",
    "dashboard_metrics = events_enriched \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_segment\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\")\n",
    "    )\n",
    "\n",
    "# 4. Escribir a Delta con optimizaciones\n",
    "query = dashboard_metrics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/dashboard\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\\n",
    "    .option(\"autoCompact\", \"true\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/dashboard_metrics\")\n",
    "\n",
    "# 5. BI Tool lee desde Delta (Tableau, Power BI, Looker)\n",
    "# SELECT * FROM delta.`/delta/dashboard_metrics`\n",
    "# WHERE window.start >= current_timestamp() - interval 1 hour\n",
    "\n",
    "# 6. Z-order para performance\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta.`/delta/dashboard_metrics`\n",
    "    ZORDER BY (window, user_segment)\n",
    "\"\"\")\n",
    "\n",
    "# 7. Vacuum archivos antiguos (semanal)\n",
    "spark.sql(\"\"\"\n",
    "    VACUUM delta.`/delta/dashboard_metrics`\n",
    "    RETAIN 168 HOURS\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ‚úÖ **Delta como sink primario** en producci√≥n (ACID + performance)\n",
    "2. ‚úÖ **foreachBatch para l√≥gica custom** (upserts, alerts, etc.)\n",
    "3. ‚úÖ **Auto-compaction** o scheduled OPTIMIZE\n",
    "4. ‚úÖ **Z-ordering** en columnas de filtrado frecuente\n",
    "5. ‚úÖ **Particionamiento por fecha** (balance entre granularidad y cantidad)\n",
    "6. ‚úÖ **maxOffsetsPerTrigger** para controlar ingesta\n",
    "7. ‚úÖ **Monitor m√©tricas** continuamente (Prometheus, Datadog)\n",
    "8. ‚ö†Ô∏è **Change Data Feed** para downstream consumers\n",
    "9. ‚ö†Ô∏è **failOnDataLoss=false** solo en dev (strict en prod)\n",
    "10. ‚ùå **No sobre-particionar** (evitar millones de particiones)\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "\n",
    "```\n",
    "Scenario: 1M events/s, 10 KB/event, 100 partitions\n",
    "\n",
    "Kafka ‚Üí Spark Streaming ‚Üí Parquet:\n",
    "- Latency: ~30s (trigger interval + processing)\n",
    "- Small files: 1000+ archivos/hora\n",
    "- OPTIMIZE needed: Daily\n",
    "- Cost: $$\n",
    "\n",
    "Kafka ‚Üí Spark Streaming ‚Üí Delta (optimized):\n",
    "- Latency: ~30s\n",
    "- Small files: Auto-compacted\n",
    "- OPTIMIZE needed: Weekly\n",
    "- Cost: $$ (slightly higher for optimization)\n",
    "- Benefit: ACID, time travel, upserts\n",
    "\n",
    "Kafka ‚Üí Flink ‚Üí Delta:\n",
    "- Latency: ~5s\n",
    "- More complex setup\n",
    "- Cost: $$$\n",
    "- Benefit: Lower latency, event-time watermarks native\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c904ba",
   "metadata": {},
   "source": [
    "## 1. Inicializar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac13d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è NOTA: PySpark requiere Java JDK 8/11/17 instalado.\n",
      "üìö Este notebook muestra c√≥digo de ejemplo y simulaciones con pandas\n",
      "üîó Para usar Spark real: https://spark.apache.org/downloads.html\n",
      "\n",
      "üìù C√≥digo de ejemplo Spark Session:\n",
      "\n",
      "spark = SparkSession.builder \\\n",
      "    .appName(\"SparkStreamingAdvanced\") \\\n",
      "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
      "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
      "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
      "    .getOrCreate()\n",
      "\n",
      "spark.sparkContext.setLogLevel(\"WARN\")\n",
      "print(f\"Spark Version: {spark.version}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nota: PySpark requiere Java instalado. Para este ejemplo educativo,\n",
    "# mostraremos el c√≥digo y simularemos los resultados con pandas\n",
    "\n",
    "print(\"‚ö†Ô∏è NOTA: PySpark requiere Java JDK 8/11/17 instalado.\")\n",
    "print(\"üìö Este notebook muestra c√≥digo de ejemplo y simulaciones con pandas\")\n",
    "print(\"üîó Para usar Spark real: https://spark.apache.org/downloads.html\\n\")\n",
    "\n",
    "# C√≥digo de ejemplo para crear Spark Session (requiere Java)\n",
    "spark_code = '''\n",
    "spark = SparkSession.builder \\\\\n",
    "    .appName(\"SparkStreamingAdvanced\") \\\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "'''\n",
    "print(\"üìù C√≥digo de ejemplo Spark Session:\")\n",
    "print(spark_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528ec21",
   "metadata": {},
   "source": [
    "## 2. Definir Esquemas para Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efbfbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquemas definidos\n",
      "\n",
      "Esquema E-commerce:\n",
      "StructType([StructField('event_id', StringType(), False), StructField('timestamp', TimestampType(), False), StructField('user_id', StringType(), False), StructField('event_type', StringType(), False), StructField('product_id', StringType(), False), StructField('product_name', StringType(), True), StructField('category', StringType(), True), StructField('price', DoubleType(), True), StructField('quantity', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Esquema para eventos de e-commerce\n",
    "ecommerce_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Esquema para logs de aplicaci√≥n\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"level\", StringType(), False),\n",
    "    StructField(\"service\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"error_code\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Esquemas definidos\")\n",
    "print(\"\\nEsquema E-commerce:\")\n",
    "print(ecommerce_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9bb26",
   "metadata": {},
   "source": [
    "## 3. Simulaci√≥n de Fuente de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3139b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generados 1000 registros de muestra\n",
      "\n",
      "üìä Primeros 5 registros:\n",
      "     event_id                  timestamp  user_id event_type product_id  \\\n",
      "0  evt_000000 2025-12-09 13:08:01.282407  user_52     remove    prod_43   \n",
      "1  evt_000001 2025-12-09 13:08:02.282407  user_87       view    prod_24   \n",
      "2  evt_000002 2025-12-09 13:08:03.282407  user_88   purchase    prod_38   \n",
      "3  evt_000003 2025-12-09 13:08:04.282407  user_22       view    prod_25   \n",
      "4  evt_000004 2025-12-09 13:08:05.282407  user_92     remove    prod_15   \n",
      "\n",
      "  product_name     category    price  quantity  \n",
      "0   Headphones  Electronics   320.48         3  \n",
      "1   Headphones    Computers    50.96         2  \n",
      "2        Mouse  Electronics  1238.79         2  \n",
      "3       Laptop    Computers  1227.59         2  \n",
      "4     Keyboard    Computers  1966.63         1  \n"
     ]
    }
   ],
   "source": [
    "# Crear datos de ejemplo para simular streaming\n",
    "def generate_sample_data(n_records=1000):\n",
    "    \"\"\"\n",
    "    Generar datos de muestra para streaming\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_records):\n",
    "        event = {\n",
    "            'event_id': f'evt_{i:06d}',\n",
    "            'timestamp': base_time + timedelta(seconds=i),\n",
    "            'user_id': f'user_{np.random.randint(1, 101)}',\n",
    "            'event_type': np.random.choice(['view', 'add_to_cart', 'purchase', 'remove'], p=[0.5, 0.25, 0.15, 0.1]),\n",
    "            'product_id': f'prod_{np.random.randint(1, 51)}',\n",
    "            'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "            'category': np.random.choice(['Electronics', 'Accessories', 'Computers']),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "        data.append(event)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Generar datos con pandas\n",
    "sample_data = generate_sample_data(1000)\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"‚úÖ Generados {len(df_sample)} registros de muestra\")\n",
    "print(\"\\nüìä Primeros 5 registros:\")\n",
    "print(df_sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbdd91",
   "metadata": {},
   "source": [
    "## 4. Streaming con Rate Source (Simulaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be1bd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù C√≥digo Spark para rate source (requiere Java):\n",
      "\n",
      "rate_stream = spark.readStream \\\n",
      "    .format(\"rate\") \\\n",
      "    .option(\"rowsPerSecond\", 10) \\\n",
      "    .option(\"numPartitions\", 2) \\\n",
      "    .load()\n",
      "\n",
      "\n",
      "üîÑ Simulaci√≥n de micro-batch con pandas:\n",
      "‚úÖ Micro-batch generado: 100 registros\n",
      "üìä Tipos de eventos: {np.str_('view'): 54, np.str_('add_to_cart'): 18, np.str_('remove'): 15, np.str_('purchase'): 13}\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de streaming con pandas\n",
    "print(\"üìù C√≥digo Spark para rate source (requiere Java):\")\n",
    "print('''\n",
    "rate_stream = spark.readStream \\\\\n",
    "    .format(\"rate\") \\\\\n",
    "    .option(\"rowsPerSecond\", 10) \\\\\n",
    "    .option(\"numPartitions\", 2) \\\\\n",
    "    .load()\n",
    "''')\n",
    "\n",
    "# Simulaci√≥n con pandas: generar micro-batch\n",
    "print(\"\\nüîÑ Simulaci√≥n de micro-batch con pandas:\")\n",
    "micro_batch = generate_sample_data(100)\n",
    "df_micro_batch = pd.DataFrame(micro_batch)\n",
    "\n",
    "print(f\"‚úÖ Micro-batch generado: {len(df_micro_batch)} registros\")\n",
    "print(f\"üìä Tipos de eventos: {df_micro_batch['event_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa2ae8",
   "metadata": {},
   "source": [
    "## 5. Agregaciones por Ventanas de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a6d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù C√≥digo Spark para windowed aggregations:\n",
      "\n",
      "windowed_counts = stream \\\n",
      "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
      "    .groupBy(window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\"), col(\"event_type\")) \\\n",
      "    .agg(count(\"*\").alias(\"event_count\"), sum(expr(\"price * quantity\")).alias(\"total_revenue\"))\n",
      "\n",
      "\n",
      "üîÑ Simulaci√≥n con pandas:\n",
      "‚úÖ Agregaciones por ventana calculadas\n",
      "                                 event_count    avg_price\n",
      "timestamp           event_type                           \n",
      "2025-12-09 13:08:30 add_to_cart            2  1926.795000\n",
      "                    purchase               2   689.120000\n",
      "                    remove                 3   776.293333\n",
      "                    view                   5   919.804000\n",
      "2025-12-09 13:09:00 add_to_cart            3  1399.356667\n",
      "                    purchase               5  1062.180000\n",
      "                    remove                 4   761.950000\n",
      "                    view                  18  1066.811111\n",
      "2025-12-09 13:09:30 add_to_cart           10   681.440000\n",
      "                    purchase               3  1539.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis\\AppData\\Local\\Temp\\ipykernel_5752\\530390406.py:16: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  windowed = df_micro_batch.groupby([pd.Grouper(freq='30S'), 'event_type']).agg({\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de agregaciones por ventanas de tiempo con pandas\n",
    "print(\"üìù C√≥digo Spark para windowed aggregations:\")\n",
    "print('''\n",
    "windowed_counts = stream \\\\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\\\n",
    "    .groupBy(window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\"), col(\"event_type\")) \\\\\n",
    "    .agg(count(\"*\").alias(\"event_count\"), sum(expr(\"price * quantity\")).alias(\"total_revenue\"))\n",
    "''')\n",
    "\n",
    "# Simulaci√≥n con pandas usando resample\n",
    "print(\"\\nüîÑ Simulaci√≥n con pandas:\")\n",
    "df_micro_batch['timestamp'] = pd.to_datetime(df_micro_batch['timestamp'])\n",
    "df_micro_batch = df_micro_batch.set_index('timestamp')\n",
    "\n",
    "# Agregaci√≥n por ventanas de 30 segundos\n",
    "windowed = df_micro_batch.groupby([pd.Grouper(freq='30S'), 'event_type']).agg({\n",
    "    'event_id': 'count',\n",
    "    'price': 'mean'\n",
    "}).rename(columns={'event_id': 'event_count', 'price': 'avg_price'})\n",
    "\n",
    "print(\"‚úÖ Agregaciones por ventana calculadas\")\n",
    "print(windowed.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ba0d3",
   "metadata": {},
   "source": [
    "## 6. Procesamiento Stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c181a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù C√≥digo Spark para stateful aggregations:\n",
      "\n",
      "user_aggregations = stream \\\n",
      "    .groupBy(\"user_id\") \\\n",
      "    .agg(\n",
      "        count(\"*\").alias(\"total_events\"),\n",
      "        sum(when(col(\"event_type\") == \"purchase\", 1)).alias(\"purchases\")\n",
      "    )\n",
      "\n",
      "\n",
      "üîÑ Simulaci√≥n con pandas:\n",
      "‚úÖ Estad√≠sticas por usuario:\n",
      "          total_events  purchases              last_activity\n",
      "user_id                                                     \n",
      "user_1               9          2 2025-12-09 13:23:57.282407\n",
      "user_10             10          2 2025-12-09 13:24:01.282407\n",
      "user_100             7          2 2025-12-09 13:24:22.282407\n",
      "user_11             12          3 2025-12-09 13:21:27.282407\n",
      "user_12              7          0 2025-12-09 13:24:00.282407\n",
      "user_13             13          2 2025-12-09 13:24:20.282407\n",
      "user_14             17          3 2025-12-09 13:24:02.282407\n",
      "user_15             13          2 2025-12-09 13:23:37.282407\n",
      "user_16             15          0 2025-12-09 13:23:43.282407\n",
      "user_17             13          4 2025-12-09 13:24:05.282407\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de agregaciones stateful por usuario con pandas\n",
    "print(\"üìù C√≥digo Spark para stateful aggregations:\")\n",
    "print('''\n",
    "user_aggregations = stream \\\\\n",
    "    .groupBy(\"user_id\") \\\\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", 1)).alias(\"purchases\")\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Simulaci√≥n con pandas\n",
    "print(\"\\nüîÑ Simulaci√≥n con pandas:\")\n",
    "user_stats = df_sample.groupby('user_id').agg({\n",
    "    'event_id': 'count',\n",
    "    'price': lambda x: (df_sample.loc[x.index, 'event_type'] == 'purchase').sum(),\n",
    "    'timestamp': 'max'\n",
    "}).rename(columns={'event_id': 'total_events', 'price': 'purchases', 'timestamp': 'last_activity'})\n",
    "\n",
    "print(\"‚úÖ Estad√≠sticas por usuario:\")\n",
    "print(user_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177bf9",
   "metadata": {},
   "source": [
    "## 7. Detecci√≥n de Patrones en Tiempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68bfe837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù C√≥digo Spark para anomaly detection:\n",
      "\n",
      "anomaly_detection = stream \\\n",
      "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
      "    .groupBy(window(col(\"timestamp\"), \"1 minute\"), col(\"user_id\")) \\\n",
      "    .agg(count(\"*\").alias(\"events_per_minute\")) \\\n",
      "    .filter(col(\"events_per_minute\") > 50)\n",
      "\n",
      "\n",
      "üîÑ Simulaci√≥n con pandas:\n",
      "‚úÖ Usuarios con comportamiento an√≥malo detectados: 3\n",
      "user_id\n",
      "user_63    4\n",
      "user_89    4\n",
      "user_92    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de detecci√≥n de anomal√≠as con pandas\n",
    "print(\"üìù C√≥digo Spark para anomaly detection:\")\n",
    "print('''\n",
    "anomaly_detection = stream \\\\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\\\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 minute\"), col(\"user_id\")) \\\\\n",
    "    .agg(count(\"*\").alias(\"events_per_minute\")) \\\\\n",
    "    .filter(col(\"events_per_minute\") > 50)\n",
    "''')\n",
    "\n",
    "# Simulaci√≥n con pandas\n",
    "print(\"\\nüîÑ Simulaci√≥n con pandas:\")\n",
    "df_micro_batch_reset = df_micro_batch.reset_index()\n",
    "anomalies = df_micro_batch_reset.groupby('user_id').size()\n",
    "anomalies = anomalies[anomalies > 3]  # Usuarios con m√°s de 3 eventos\n",
    "\n",
    "print(f\"‚úÖ Usuarios con comportamiento an√≥malo detectados: {len(anomalies)}\")\n",
    "if len(anomalies) > 0:\n",
    "    print(anomalies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2880b",
   "metadata": {},
   "source": [
    "## 8. Ejemplo de Query con Output Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a5d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AN√ÅLISIS BATCH DE DATOS DE MUESTRA ===\n",
      "\n",
      "1. Eventos por tipo:\n",
      "event_type\n",
      "view           481\n",
      "add_to_cart    257\n",
      "purchase       161\n",
      "remove         101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Revenue por categor√≠a (solo compras):\n",
      "             num_purchases  total_revenue    avg_price\n",
      "category                                              \n",
      "Electronics             61      158917.62  1010.036885\n",
      "Computers               54      117024.44   997.727593\n",
      "Accessories             46       89701.40   874.156522\n",
      "\n",
      "3. Top 10 usuarios m√°s activos:\n",
      "         total_events  purchases\n",
      "user_id                         \n",
      "user_60            23          5\n",
      "user_98            18          3\n",
      "user_14            17          3\n",
      "user_80            16          4\n",
      "user_57            16          2\n",
      "user_61            15          0\n",
      "user_82            15          4\n",
      "user_58            15          2\n",
      "user_16            15          0\n",
      "user_50            14          2\n",
      "\n",
      "4. Tasa de conversi√≥n por producto:\n",
      "              total_events  views  purchases  conversion_rate\n",
      "product_name                                                 \n",
      "Headphones             206     89         40        44.943820\n",
      "Monitor                189     90         33        36.666667\n",
      "Mouse                  191     86         30        34.883721\n",
      "Keyboard               194     98         31        31.632653\n",
      "Laptop                 220    118         27        22.881356\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis batch con pandas (simulaci√≥n de resultados Spark)\n",
    "print(\"\\n=== AN√ÅLISIS BATCH DE DATOS DE MUESTRA ===\")\n",
    "\n",
    "print(\"\\n1. Eventos por tipo:\")\n",
    "print(df_sample['event_type'].value_counts())\n",
    "\n",
    "print(\"\\n2. Revenue por categor√≠a (solo compras):\")\n",
    "purchases = df_sample[df_sample['event_type'] == 'purchase'].copy()\n",
    "purchases['revenue'] = purchases['price'] * purchases['quantity']\n",
    "revenue_by_cat = purchases.groupby('category').agg({\n",
    "    'event_id': 'count',\n",
    "    'revenue': 'sum',\n",
    "    'price': 'mean'\n",
    "}).rename(columns={'event_id': 'num_purchases', 'revenue': 'total_revenue', 'price': 'avg_price'})\n",
    "print(revenue_by_cat.sort_values('total_revenue', ascending=False))\n",
    "\n",
    "print(\"\\n3. Top 10 usuarios m√°s activos:\")\n",
    "user_activity = df_sample.groupby('user_id').agg({\n",
    "    'event_id': 'count',\n",
    "    'event_type': lambda x: (x == 'purchase').sum()\n",
    "}).rename(columns={'event_id': 'total_events', 'event_type': 'purchases'})\n",
    "print(user_activity.sort_values('total_events', ascending=False).head(10))\n",
    "\n",
    "print(\"\\n4. Tasa de conversi√≥n por producto:\")\n",
    "product_conv = df_sample.groupby('product_name').agg({\n",
    "    'event_type': ['count', lambda x: (x == 'view').sum(), lambda x: (x == 'purchase').sum()]\n",
    "})\n",
    "product_conv.columns = ['total_events', 'views', 'purchases']\n",
    "product_conv['conversion_rate'] = (product_conv['purchases'] / product_conv['views'] * 100).fillna(0)\n",
    "print(product_conv.sort_values('conversion_rate', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac2a3f",
   "metadata": {},
   "source": [
    "## 9. Escribir Stream a Diferentes Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a3fc241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraciones de sink definidas (ver c√≥digo para detalles)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de configuraci√≥n de escritura (comentado para evitar ejecuci√≥n)\n",
    "\n",
    "# 1. Escribir a consola (desarrollo/debug)\n",
    "console_query_config = {\n",
    "    'outputMode': 'complete',  # complete, append, update\n",
    "    'format': 'console',\n",
    "    'trigger': {'processingTime': '10 seconds'},\n",
    "    'options': {\n",
    "        'truncate': False,\n",
    "        'numRows': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Escribir a Parquet (data lake)\n",
    "parquet_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'parquet',\n",
    "    'path': '/path/to/output',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'trigger': {'processingTime': '1 minute'},\n",
    "    'options': {\n",
    "        'compression': 'snappy',\n",
    "        'partitionBy': 'date'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Escribir a Kafka\n",
    "kafka_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'kafka',\n",
    "    'options': {\n",
    "        'kafka.bootstrap.servers': 'localhost:9092',\n",
    "        'topic': 'processed-events',\n",
    "        'checkpointLocation': '/path/to/checkpoint'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Escribir a Delta Lake\n",
    "delta_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'delta',\n",
    "    'path': '/path/to/delta',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'options': {\n",
    "        'mergeSchema': True,\n",
    "        'optimizeWrite': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuraciones de sink definidas (ver c√≥digo para detalles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e869",
   "metadata": {},
   "source": [
    "## 10. M√©tricas y Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43c0bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√âTRICAS CLAVE PARA MONITOREO ===\n",
      "\n",
      "1. Input Rate: Eventos por segundo recibidos\n",
      "2. Processing Rate: Eventos por segundo procesados\n",
      "3. Batch Duration: Tiempo de procesamiento por batch\n",
      "4. Trigger Interval: Intervalo entre ejecuciones\n",
      "5. Watermark: Retraso m√°ximo aceptado\n",
      "6. Estado del Query: Activo, inactivo, error\n",
      "7. Checkpoint Location: Para recuperaci√≥n de fallos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para monitorear estado del stream\n",
    "def monitor_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer m√©tricas del streaming query\n",
    "    \"\"\"\n",
    "    status = query.status\n",
    "    \n",
    "    metrics = {\n",
    "        'isDataAvailable': status['isDataAvailable'],\n",
    "        'isTriggerActive': status['isTriggerActive'],\n",
    "        'message': status['message']\n",
    "    }\n",
    "    \n",
    "    if 'inputRowsPerSecond' in status:\n",
    "        metrics['inputRowsPerSecond'] = status['inputRowsPerSecond']\n",
    "    \n",
    "    if 'processedRowsPerSecond' in status:\n",
    "        metrics['processedRowsPerSecond'] = status['processedRowsPerSecond']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Ejemplo de m√©tricas a monitorear\n",
    "print(\"\\n=== M√âTRICAS CLAVE PARA MONITOREO ===\")\n",
    "print(\"\"\"\n",
    "1. Input Rate: Eventos por segundo recibidos\n",
    "2. Processing Rate: Eventos por segundo procesados\n",
    "3. Batch Duration: Tiempo de procesamiento por batch\n",
    "4. Trigger Interval: Intervalo entre ejecuciones\n",
    "5. Watermark: Retraso m√°ximo aceptado\n",
    "6. Estado del Query: Activo, inactivo, error\n",
    "7. Checkpoint Location: Para recuperaci√≥n de fallos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a91d85",
   "metadata": {},
   "source": [
    "## 11. Optimizaci√≥n de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a806e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEJORES PR√ÅCTICAS DE OPTIMIZACI√ìN ===\n",
      "\n",
      "1. Usar watermarks para limpiar estado antiguo\n",
      "2. Particionar datos por columnas clave\n",
      "3. Configurar apropiadamente spark.sql.shuffle.partitions\n",
      "4. Usar triggers basados en tiempo para controlar frecuencia\n",
      "5. Implementar checkpointing para recuperaci√≥n\n",
      "6. Monitorear m√©tricas constantemente\n",
      "7. Usar Delta Lake para ACID transactions\n",
      "8. Implementar compactaci√≥n de archivos peque√±os\n",
      "9. Optimizar esquemas y evitar tipos gen√©ricos\n",
      "10. Considerar micro-batching vs continuous processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuraciones de optimizaci√≥n\n",
    "optimization_configs = {\n",
    "    # Particionamiento\n",
    "    'spark.sql.shuffle.partitions': '200',  # N√∫mero de particiones para shuffles\n",
    "    'spark.default.parallelism': '200',     # Paralelismo por defecto\n",
    "    \n",
    "    # Memoria\n",
    "    'spark.executor.memory': '4g',\n",
    "    'spark.driver.memory': '2g',\n",
    "    'spark.memory.fraction': '0.8',\n",
    "    \n",
    "    # Streaming espec√≠fico\n",
    "    'spark.sql.streaming.minBatchesToRetain': '100',\n",
    "    'spark.sql.streaming.stateStore.providerClass': 'org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider',\n",
    "    \n",
    "    # Optimizaci√≥n de escritura\n",
    "    'spark.sql.adaptive.enabled': 'true',\n",
    "    'spark.sql.adaptive.coalescePartitions.enabled': 'true',\n",
    "}\n",
    "\n",
    "print(\"\\n=== MEJORES PR√ÅCTICAS DE OPTIMIZACI√ìN ===\")\n",
    "print(\"\"\"\n",
    "1. Usar watermarks para limpiar estado antiguo\n",
    "2. Particionar datos por columnas clave\n",
    "3. Configurar apropiadamente spark.sql.shuffle.partitions\n",
    "4. Usar triggers basados en tiempo para controlar frecuencia\n",
    "5. Implementar checkpointing para recuperaci√≥n\n",
    "6. Monitorear m√©tricas constantemente\n",
    "7. Usar Delta Lake para ACID transactions\n",
    "8. Implementar compactaci√≥n de archivos peque√±os\n",
    "9. Optimizar esquemas y evitar tipos gen√©ricos\n",
    "10. Considerar micro-batching vs continuous processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5288b",
   "metadata": {},
   "source": [
    "## Resumen y Arquitectura Enterprise\n",
    "\n",
    "### Arquitectura T√≠pica de Streaming:\n",
    "```\n",
    "Fuentes de Datos       Ingesta           Procesamiento        Almacenamiento        Consumo\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Kafka/Kinesis    ‚Üí    Spark       ‚Üí     Transformaciones  ‚Üí   Delta Lake      ‚Üí    BI Tools\n",
    "IoT Devices      ‚Üí    Streaming   ‚Üí     Agregaciones      ‚Üí   Data Lake       ‚Üí    ML Models\n",
    "APIs             ‚Üí                ‚Üí     Joins             ‚Üí   Warehouse       ‚Üí    Dashboards\n",
    "Logs             ‚Üí                ‚Üí     Windows           ‚Üí   Cache (Redis)   ‚Üí    Alertas\n",
    "```\n",
    "\n",
    "### Patrones Avanzados:\n",
    "\n",
    "#### 1. Lambda Architecture\n",
    "- **Batch Layer**: Procesamiento hist√≥rico completo\n",
    "- **Speed Layer**: Procesamiento en tiempo real\n",
    "- **Serving Layer**: Combina ambas vistas\n",
    "\n",
    "#### 2. Kappa Architecture\n",
    "- Solo capa de streaming\n",
    "- Todo procesamiento en tiempo real\n",
    "- Reprocesamiento desde el inicio del stream\n",
    "\n",
    "#### 3. Delta Architecture\n",
    "- Basada en Delta Lake\n",
    "- ACID transactions\n",
    "- Time travel\n",
    "- Schema evolution\n",
    "\n",
    "### Casos de Uso Enterprise:\n",
    "\n",
    "1. **Detecci√≥n de Fraude en Tiempo Real**\n",
    "   - An√°lisis de patrones sospechosos\n",
    "   - Machine Learning en streaming\n",
    "   - Alertas autom√°ticas\n",
    "\n",
    "2. **Recomendaciones Personalizadas**\n",
    "   - Seguimiento de comportamiento en tiempo real\n",
    "   - Actualizaci√≥n de perfiles de usuario\n",
    "   - A/B testing din√°mico\n",
    "\n",
    "3. **Monitoreo de Infraestructura**\n",
    "   - Logs y m√©tricas en tiempo real\n",
    "   - Detecci√≥n de anomal√≠as\n",
    "   - Auto-scaling basado en carga\n",
    "\n",
    "4. **IoT y Telemetr√≠a**\n",
    "   - Procesamiento de sensores\n",
    "   - Mantenimiento predictivo\n",
    "   - Optimizaci√≥n de operaciones\n",
    "\n",
    "### Consideraciones de Producci√≥n:\n",
    "\n",
    "- **Alta Disponibilidad**: Cluster mode, m√∫ltiples workers\n",
    "- **Fault Tolerance**: Checkpointing, Write-Ahead Logs\n",
    "- **Escalabilidad**: Auto-scaling, dynamic allocation\n",
    "- **Seguridad**: Kerberos, SSL/TLS, encryption at rest\n",
    "- **Monitoreo**: Prometheus, Grafana, CloudWatch\n",
    "- **Testing**: Unit tests, integration tests, chaos engineering\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)\n",
    "- [Databricks Streaming Best Practices](https://docs.databricks.com/structured-streaming/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef7785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notebook completado exitosamente\n",
      "üìö Has aprendido los conceptos de Spark Streaming con ejemplos pr√°cticos\n",
      "üîó Para implementar con Spark real, instala Java JDK y configura PySpark\n"
     ]
    }
   ],
   "source": [
    "# Limpiar recursos\n",
    "# Nota: En un entorno Spark real, ejecutar√≠as: spark.stop()\n",
    "print(\"‚úÖ Notebook completado exitosamente\")\n",
    "print(\"üìö Has aprendido los conceptos de Spark Streaming con ejemplos pr√°cticos\")\n",
    "print(\"üîó Para implementar con Spark real, instala Java JDK y configura PySpark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß≠ Navegaci√≥n\n",
    "\n",
    "**‚Üê Anterior:** [üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "\n",
    "**Siguiente ‚Üí:** [üèõÔ∏è Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh ‚Üí](04_arquitecturas_modernas.ipynb)\n",
    "\n",
    "**üìö √çndice de Nivel Senior:**\n",
    "- [üèõÔ∏è Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb) ‚Üê üîµ Est√°s aqu√≠\n",
    "- [üèõÔ∏è Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ü§ñ ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [üí∞ Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [üîê Seguridad, Compliance y Auditor√≠a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [üìä Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [üèÜ Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [üåê Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**üéì Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
