{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc763fbf",
   "metadata": {},
   "source": [
    "# Apache Spark Streaming: Procesamiento en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar Spark Structured Streaming\n",
    "- Implementar procesamiento de ventanas y agregaciones\n",
    "- Integrar con Kafka y otras fuentes de streaming\n",
    "- Manejar estado y checkpointing\n",
    "- Optimizar rendimiento en streaming\n",
    "\n",
    "## Requisitos\n",
    "- PySpark 3.x\n",
    "- Python 3.8+\n",
    "- Kafka (opcional)\n",
    "- Delta Lake (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, count, sum as spark_sum, avg, max as spark_max,\n",
    "    current_timestamp, to_json, from_json, struct, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425e406",
   "metadata": {},
   "source": [
    "### 🌊 **Spark Structured Streaming: Arquitectura y Micro-batches**\n",
    "\n",
    "**Evolución del Streaming en Spark:**\n",
    "\n",
    "```\n",
    "Gen 1: Spark Streaming (DStreams) - 2013\n",
    "├── RDDs en micro-batches\n",
    "├── API de bajo nivel\n",
    "└── ❌ Complejo, no unificado con batch\n",
    "\n",
    "Gen 2: Structured Streaming - 2016\n",
    "├── DataFrames/Datasets unificados\n",
    "├── Event-time processing\n",
    "├── Exactly-once semantics\n",
    "└── ✅ API declarativa, tolerancia a fallos\n",
    "```\n",
    "\n",
    "**Micro-batch vs Continuous Processing:**\n",
    "\n",
    "| Aspecto | Micro-batch (default) | Continuous (experimental) |\n",
    "|---------|----------------------|---------------------------|\n",
    "| **Latencia** | ~100ms - 1s | ~1ms |\n",
    "| **Throughput** | ✅ Alto (10K+ events/s) | ⚠️ Medio |\n",
    "| **Garantías** | ✅ Exactly-once | ⚠️ At-least-once |\n",
    "| **Estado** | ✅ Full support | ⚠️ Limitado |\n",
    "| **Uso** | Analytics, aggregations | Ultra-low latency alerts |\n",
    "\n",
    "**Arquitectura de Micro-batch:**\n",
    "\n",
    "```python\n",
    "# Streaming Query = Infinite Table\n",
    "# Cada batch procesa un \"snapshot\" incremental\n",
    "\n",
    "┌──────────────────────────────────────────┐\n",
    "│  Input Source (Kafka, Kinesis, Files)   │\n",
    "└──────────────┬───────────────────────────┘\n",
    "               │ Batch 0: Records [1-100]\n",
    "               ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│  Incremental Query Engine                │\n",
    "│  ┌────────────────────────────────────┐  │\n",
    "│  │ Batch 0: Transform + Aggregate     │  │\n",
    "│  └────────────────────────────────────┘  │\n",
    "└──────────────┬───────────────────────────┘\n",
    "               │ Write Batch 0 Results\n",
    "               ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│  Output Sink (Delta, Kafka, Console)    │\n",
    "└──────────────────────────────────────────┘\n",
    "               │\n",
    "               │ Trigger interval (e.g., 10s)\n",
    "               ▼\n",
    "         (Repeat Batch 1, 2, 3...)\n",
    "```\n",
    "\n",
    "**Triggers (Frecuencia de Ejecución):**\n",
    "\n",
    "```python\n",
    "# 1. ProcessingTime: Ejecutar cada N segundos\n",
    "query = df.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# 2. Once: Ejecutar una sola vez (útil para testing/backfill)\n",
    "query = df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# 3. Continuous: Latencia ultra-baja (~1ms)\n",
    "query = df.writeStream \\\n",
    "    .trigger(continuous='1 second') \\\n",
    "    .start()\n",
    "\n",
    "# 4. AvailableNow: Procesar todos los datos disponibles (Spark 3.3+)\n",
    "query = df.writeStream \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "```python\n",
    "# 1. Append: Solo nuevos registros (default)\n",
    "# ✅ Uso: Raw logs, eventos sin agregaciones\n",
    "# ❌ Limitación: No updates/deletes\n",
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .start(\"/data/events\")\n",
    "\n",
    "# 2. Complete: Toda la tabla resultado (re-escribir completo)\n",
    "# ✅ Uso: Agregaciones pequeñas, dashboards\n",
    "# ❌ Limitación: No escala, solo con aggregations\n",
    "df.groupBy(\"category\").count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"category_counts\") \\\n",
    "    .start()\n",
    "\n",
    "# 3. Update: Solo registros modificados\n",
    "# ✅ Uso: Agregaciones con watermark, upserts\n",
    "# ⚡ Best practice: Balanceo append vs complete\n",
    "df.groupBy(window(\"timestamp\", \"1 hour\"), \"user_id\") \\\n",
    "    .agg(count(\"*\").alias(\"events\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .start(\"/data/user_hourly_stats\")\n",
    "```\n",
    "\n",
    "**Event-time vs Processing-time:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# ❌ Processing-time: Cuando Spark procesa el evento\n",
    "df_processing_time = df.withColumn(\"processed_at\", current_timestamp())\n",
    "# Problema: Late data processed in wrong time window\n",
    "\n",
    "# ✅ Event-time: Timestamp del evento original\n",
    "df_event_time = df.select(\"event_timestamp\", \"user_id\", \"action\")\n",
    "# Benefit: Correcta agregación temporal incluso con retrasos\n",
    "\n",
    "# Ejemplo: Click en app a las 10:00 AM\n",
    "# - Dispositivo offline hasta 10:30 AM\n",
    "# - Processing-time: 10:30 AM ❌ (wrong window)\n",
    "# - Event-time: 10:00 AM ✅ (correct window)\n",
    "```\n",
    "\n",
    "**Backpressure y Rate Limiting:**\n",
    "\n",
    "```python\n",
    "# Limitar ingesta para evitar overwhelm\n",
    "spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\  # Max 10K records/batch\n",
    "    .option(\"minPartitions\", 4) \\              # Paralelismo mínimo\n",
    "    .load()\n",
    "\n",
    "# Backpressure automático (Spark 3.2+)\n",
    "spark.conf.set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.backpressure.initialRate\", 1000)\n",
    "```\n",
    "\n",
    "**Ejemplo Real: E-commerce Click Stream**\n",
    "\n",
    "```python\n",
    "# Fuente: Kafka con eventos de clicks\n",
    "clicks = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-clicks\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON payload\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "clicks_parsed = clicks.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Agregación por ventanas de 5 minutos\n",
    "clicks_per_5min = clicks_parsed \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"clicks\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    )\n",
    "\n",
    "# Escribir a Delta Lake con ACID guarantees\n",
    "query = clicks_per_5min.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/clicks-5min\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/clicks_aggregated\")\n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(f\"Last Progress: {query.lastProgress}\")\n",
    "```\n",
    "\n",
    "**Métricas de Performance:**\n",
    "\n",
    "```python\n",
    "# Acceder a métricas del stream\n",
    "progress = query.lastProgress\n",
    "\n",
    "key_metrics = {\n",
    "    \"batchId\": progress[\"batchId\"],\n",
    "    \"inputRowsPerSecond\": progress[\"inputRowsPerSecond\"],\n",
    "    \"processedRowsPerSecond\": progress[\"processedRowsPerSecond\"],\n",
    "    \"batchDuration\": progress[\"batchDuration\"],  # ms\n",
    "    \"numInputRows\": progress[\"numInputRows\"],\n",
    "    \"stateOperators\": progress[\"stateOperators\"]  # Estado acumulado\n",
    "}\n",
    "\n",
    "# Alertas si procesamiento es más lento que ingesta\n",
    "if progress[\"inputRowsPerSecond\"] > progress[\"processedRowsPerSecond\"]:\n",
    "    print(\"⚠️ WARNING: Falling behind! Increase parallelism\")\n",
    "```\n",
    "\n",
    "**Comparación con Flink:**\n",
    "\n",
    "| Característica | Spark Streaming | Apache Flink |\n",
    "|----------------|-----------------|--------------|\n",
    "| **Modelo** | Micro-batch | True streaming (record-at-a-time) |\n",
    "| **Latencia** | 100ms - 1s | 10ms - 100ms |\n",
    "| **Throughput** | ✅ Muy alto | ✅ Alto |\n",
    "| **Estado** | RocksDB, memory | RocksDB native |\n",
    "| **SQL Support** | ✅ Excellent | ✅ Good |\n",
    "| **Ecosystem** | ✅ Spark ML, Delta | ⚠️ Limitado |\n",
    "| **Learning Curve** | ⚡ Fácil (si conoces Spark) | ⚠️ Steeper |\n",
    "| **Best For** | Analytics, ML, unified batch+stream | Ultra-low latency, complex CEP |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5369d8",
   "metadata": {},
   "source": [
    "### ⏰ **Watermarking: Manejo de Late Data y Event-time Windows**\n",
    "\n",
    "**Problema: Late Arriving Data**\n",
    "\n",
    "```\n",
    "Timeline:\n",
    "10:00 AM: User clicks product (event_timestamp = 10:00)\n",
    "10:15 AM: Network issues, event buffered\n",
    "10:30 AM: Event arrives at Spark (processing_time = 10:30)\n",
    "\n",
    "Sin watermark:\n",
    "- Evento procesado en ventana 10:30-10:35 ❌ (wrong)\n",
    "- Estado crece infinitamente (memory leak)\n",
    "\n",
    "Con watermark:\n",
    "- Evento procesado en ventana 10:00-10:05 ✅ (correct)\n",
    "- Estado antiguo limpiado automáticamente\n",
    "```\n",
    "\n",
    "**Watermark = \"Cuánto retraso tolerar\"**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "# Watermark de 10 minutos: Eventos con >10 min retraso se descartan\n",
    "df_with_watermark = df \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Cómo funciona:\n",
    "# 1. Spark trackea max(event_timestamp) visto hasta ahora\n",
    "# 2. Watermark = max(event_timestamp) - threshold (10 min)\n",
    "# 3. Eventos con timestamp < watermark se descartan\n",
    "# 4. Estado de ventanas < watermark se elimina\n",
    "\n",
    "# Ejemplo numérico:\n",
    "# Batch 1: max_event_time = 10:15, watermark = 10:05\n",
    "#   → Mantiene ventanas [10:00-10:05, 10:05-10:10, 10:10-10:15]\n",
    "# Batch 2: max_event_time = 10:25, watermark = 10:15\n",
    "#   → Elimina ventana [10:00-10:05], mantiene [10:05-10:25]\n",
    "```\n",
    "\n",
    "**Tipos de Ventanas (Windows):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, session_window\n",
    "\n",
    "# 1. TUMBLING WINDOW (no solapamiento)\n",
    "# Uso: Métricas cada N minutos sin duplicar\n",
    "tumbling = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\")  # [00:00-00:10), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 2. SLIDING WINDOW (solapamiento)\n",
    "# Uso: Promedios móviles, tendencias\n",
    "sliding = df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\", \"5 minutes\")\n",
    "    # [00:00-00:10), [00:05-00:15), [00:10-00:20)\n",
    ").count()\n",
    "\n",
    "# 3. SESSION WINDOW (gap-based)\n",
    "# Uso: Sesiones de usuario, actividad continua\n",
    "session = df.groupBy(\n",
    "    \"user_id\",\n",
    "    session_window(\"timestamp\", \"30 minutes\")  # Gap de inactividad\n",
    ").count()\n",
    "# Si user activo 00:00, 00:05, 00:40 → 2 sesiones:\n",
    "#   Session 1: [00:00-00:35] (last activity 00:05 + 30min gap)\n",
    "#   Session 2: [00:40-...]\n",
    "```\n",
    "\n",
    "**Configuración de Watermark Óptima:**\n",
    "\n",
    "```python\n",
    "# ❌ Watermark muy corto (1 minuto)\n",
    "df.withWatermark(\"timestamp\", \"1 minute\")\n",
    "# Problema: Late data descartado agresivamente\n",
    "# Uso: Solo si latencia de red <1 min garantizada\n",
    "\n",
    "# ✅ Watermark balanceado (10-30 minutos)\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "# Beneficio: 99% eventos capturados, estado razonable\n",
    "\n",
    "# ⚠️ Watermark muy largo (2 horas)\n",
    "df.withWatermark(\"timestamp\", \"2 hours\")\n",
    "# Problema: Estado crece demasiado (OutOfMemory)\n",
    "# Uso: Solo si retrasos son realmente >1 hora\n",
    "\n",
    "# 🎯 Regla de oro:\n",
    "# Watermark = P99 latency de tus datos + buffer\n",
    "# Ejemplo: Si 99% eventos llegan en <5 min → watermark 10 min\n",
    "```\n",
    "\n",
    "**Manejo de Late Data con Output Modes:**\n",
    "\n",
    "```python\n",
    "# Output Mode + Watermark interacción:\n",
    "\n",
    "# 1. APPEND + Watermark\n",
    "# ✅ Escribe ventanas finalizadas (después de watermark)\n",
    "# ⚡ Best practice: Inmutable, no updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] escrita cuando watermark > 10:05\n",
    "\n",
    "# 2. UPDATE + Watermark\n",
    "# ✅ Escribe ventanas activas + finalizadas\n",
    "# ⚡ Uso: Dashboards que necesitan updates\n",
    "df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "# Ventana [10:00-10:05] puede actualizarse hasta watermark\n",
    "\n",
    "# 3. COMPLETE (no necesita watermark)\n",
    "# ❌ Re-escribe toda la tabla cada batch\n",
    "# Uso: Agregaciones pequeñas (<1M registros)\n",
    "```\n",
    "\n",
    "**Caso Real: Métricas de IoT Devices**\n",
    "\n",
    "```python\n",
    "# Devices envían telemetría cada 1 minuto\n",
    "# Red celular puede tener latencia variable (1s - 5 min)\n",
    "\n",
    "iot_metrics = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"iot-telemetry\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), iot_schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "# Agregación por device + ventana de 5 min\n",
    "device_stats = iot_metrics \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\  # Tolerar 10 min retraso\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"device_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        max(\"temperature\").alias(\"max_temp\"),\n",
    "        count(\"*\").alias(\"num_readings\")\n",
    "    ) \\\n",
    "    .filter(col(\"max_temp\") > 80)  # Alertas de sobrecalentamiento\n",
    "\n",
    "# Escribir alertas a Kafka para acción inmediata\n",
    "alerts_query = device_stats.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"device-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/iot-alerts\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Debugging Late Data:**\n",
    "\n",
    "```python\n",
    "# Agregar columnas de diagnóstico\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df_debug = df \\\n",
    "    .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "    .withColumn(\"latency_seconds\", \n",
    "        unix_timestamp(\"processing_time\") - unix_timestamp(\"event_timestamp\")\n",
    "    )\n",
    "\n",
    "# Analizar distribución de latencias\n",
    "latency_stats = df_debug.groupBy(\n",
    "    window(\"processing_time\", \"1 minute\")\n",
    ").agg(\n",
    "    avg(\"latency_seconds\").alias(\"avg_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.95)\").alias(\"p95_latency\"),\n",
    "    expr(\"percentile_approx(latency_seconds, 0.99)\").alias(\"p99_latency\"),\n",
    "    max(\"latency_seconds\").alias(\"max_latency\")\n",
    ")\n",
    "\n",
    "# Si p99_latency = 300s (5 min) → watermark debe ser ≥10 min\n",
    "```\n",
    "\n",
    "**Watermark con Joins:**\n",
    "\n",
    "```python\n",
    "# Join entre streams requiere watermark en AMBOS\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "impressions = impressions_stream.withWatermark(\"impression_time\", \"10 minutes\")\n",
    "\n",
    "# Inner join con time constraint\n",
    "joined = clicks.join(\n",
    "    impressions,\n",
    "    expr(\"\"\"\n",
    "        click_user_id = impression_user_id AND\n",
    "        click_time >= impression_time AND\n",
    "        click_time <= impression_time + interval 1 hour\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Watermark resultante = min(5 min, 10 min) = 5 min\n",
    "# Estado mantenido: 1 hora (time constraint) + 5 min (watermark)\n",
    "```\n",
    "\n",
    "**Monitoreo de Watermark:**\n",
    "\n",
    "```python\n",
    "# Ver watermark actual en query progress\n",
    "progress = query.lastProgress\n",
    "print(f\"Current watermark: {progress['watermark']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "# {\n",
    "#   \"eventTime\": {\n",
    "#     \"avg\": \"2025-10-30T14:25:00.000Z\",\n",
    "#     \"max\": \"2025-10-30T14:30:00.000Z\",\n",
    "#     \"min\": \"2025-10-30T14:20:00.000Z\",\n",
    "#     \"watermark\": \"2025-10-30T14:20:00.000Z\"  # max - 10 min\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Alertas si watermark se retrasa mucho\n",
    "if progress['watermark'] < (current_time - timedelta(hours=1)):\n",
    "    send_alert(\"Watermark lagging! Check data source\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ✅ **Siempre usar watermark** con agregaciones event-time\n",
    "2. ✅ **Medir P99 latency** de tus datos antes de configurar\n",
    "3. ✅ **Append mode** con watermark para inmutabilidad\n",
    "4. ✅ **Session windows** para análisis de comportamiento\n",
    "5. ⚠️ **Sliding windows** consumen más estado (overlap)\n",
    "6. ❌ **No watermark infinito** (sin cleanup de estado)\n",
    "7. ⚡ **Buffer 2-3x P99** para evitar drops\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06a65",
   "metadata": {},
   "source": [
    "### 💾 **State Management y Checkpointing: Fault Tolerance**\n",
    "\n",
    "**¿Qué es el Estado (State)?**\n",
    "\n",
    "```\n",
    "Stateless Processing (sin estado):\n",
    "Input: {\"user\": \"A\", \"action\": \"click\"}\n",
    "Output: {\"user\": \"A\", \"action\": \"click\", \"processed\": true}\n",
    "✅ Cada evento independiente\n",
    "\n",
    "Stateful Processing (con estado):\n",
    "Input Batch 1: {\"user\": \"A\", \"action\": \"click\"}\n",
    "State: {A: 1 click}\n",
    "Input Batch 2: {\"user\": \"A\", \"action\": \"purchase\"}\n",
    "State: {A: 1 click, 1 purchase}\n",
    "Output: {A: total_events=2, conversion_rate=1.0}\n",
    "✅ Memoria entre batches\n",
    "```\n",
    "\n",
    "**Operaciones Stateful en Spark:**\n",
    "\n",
    "```python\n",
    "# 1. Aggregations (groupBy)\n",
    "# Estado: Valores acumulados por key\n",
    "user_stats = df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\")\n",
    "    )\n",
    "# Estado almacenado: {user_123: {count: 45, revenue: 1200.50}}\n",
    "\n",
    "# 2. Windowed Aggregations\n",
    "# Estado: Valores por ventana + key\n",
    "window_stats = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        \"product_id\"\n",
    "    ) \\\n",
    "    .count()\n",
    "# Estado: {(window_10:00-10:05, product_42): 150}\n",
    "\n",
    "# 3. Stream-Stream Joins\n",
    "# Estado: Buffered events de ambos streams\n",
    "clicks = clicks_stream.withWatermark(\"click_time\", \"5 minutes\")\n",
    "views = views_stream.withWatermark(\"view_time\", \"5 minutes\")\n",
    "joined = clicks.join(views, \"session_id\")\n",
    "# Estado: Eventos no matcheados dentro de watermark\n",
    "\n",
    "# 4. Deduplication\n",
    "# Estado: Claves únicas vistas\n",
    "deduplicated = df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "# Estado: {event_456, event_789, ...}\n",
    "\n",
    "# 5. mapGroupsWithState / flatMapGroupsWithState\n",
    "# Estado: Customizado por usuario\n",
    "# Ejemplo: Sesiones complejas, máquinas de estado\n",
    "```\n",
    "\n",
    "**State Store Backends:**\n",
    "\n",
    "```python\n",
    "# 1. Memory (default para testing)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"\n",
    ")\n",
    "# ⚠️ Limitado por executor memory\n",
    "# Uso: Testing local, datasets pequeños\n",
    "\n",
    "# 2. RocksDB (producción)\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "# ✅ Disk-backed, escala a TB de estado\n",
    "# Uso: Producción, estado grande\n",
    "\n",
    "# 3. Custom (empresarial)\n",
    "# Ejemplo: Redis, Cassandra para estado compartido\n",
    "```\n",
    "\n",
    "**Checkpointing: Write-Ahead Log (WAL)**\n",
    "\n",
    "```\n",
    "Checkpoint Directory Structure:\n",
    "/checkpoints/my-query/\n",
    "├── commits/\n",
    "│   ├── 0                    # Batch 0 metadata\n",
    "│   ├── 1\n",
    "│   └── 2\n",
    "├── offsets/\n",
    "│   ├── 0                    # Kafka offsets batch 0\n",
    "│   ├── 1\n",
    "│   └── 2\n",
    "├── sources/\n",
    "│   └── 0/\n",
    "│       └── 0               # Source info\n",
    "└── state/\n",
    "    ├── 0/\n",
    "    │   ├── 0/\n",
    "    │   │   ├── 1.delta     # State snapshots\n",
    "    │   │   └── 1.snapshot\n",
    "    │   └── 1/\n",
    "    └── 1/\n",
    "```\n",
    "\n",
    "**Configuración de Checkpointing:**\n",
    "\n",
    "```python\n",
    "# Checkpoint obligatorio para stateful queries\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-stats\") \\  # REQUERIDO\n",
    "    .start(\"/delta/user_stats\")\n",
    "\n",
    "# Sin checkpoint → Error:\n",
    "# \"checkpointLocation must be specified either through option(\"checkpointLocation\", ...)\n",
    "# or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...)\"\n",
    "\n",
    "# Configuración global (no recomendado)\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/checkpoints/default\")\n",
    "```\n",
    "\n",
    "**Recovery Process:**\n",
    "\n",
    "```python\n",
    "# Escenario: Executor falla durante Batch 5\n",
    "\n",
    "# 1. Query detecta falla\n",
    "# 2. Lee último checkpoint exitoso (Batch 4)\n",
    "#    - Offsets: Kafka partition 0 offset 1000, partition 1 offset 850\n",
    "#    - State: Usuario A → 45 eventos, Usuario B → 23 eventos\n",
    "# 3. Replay desde Batch 5\n",
    "#    - Lee desde offset 1000, 850 en Kafka\n",
    "#    - Restaura estado de Batch 4\n",
    "#    - Procesa Batch 5 con estado correcto\n",
    "# 4. Continúa normalmente\n",
    "\n",
    "# Garantía: Exactly-once processing\n",
    "# Kafka offsets + Estado + Output escritos atómicamente\n",
    "```\n",
    "\n",
    "**Ejemplo: Session Analytics con Estado Custom**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.streaming import GroupState, GroupStateTimeout\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "# Modelo de sesión\n",
    "@dataclass\n",
    "class SessionData:\n",
    "    user_id: str\n",
    "    start_time: datetime\n",
    "    last_activity: datetime\n",
    "    events: List[str]\n",
    "    total_revenue: float\n",
    "\n",
    "def update_session_state(\n",
    "    key: Tuple[str],\n",
    "    events: Iterator[pd.DataFrame],\n",
    "    state: GroupState\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Custom state management para sesiones\n",
    "    \"\"\"\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Leer estado anterior\n",
    "    if state.exists:\n",
    "        session = state.get()\n",
    "    else:\n",
    "        session = SessionData(\n",
    "            user_id=user_id,\n",
    "            start_time=None,\n",
    "            last_activity=None,\n",
    "            events=[],\n",
    "            total_revenue=0.0\n",
    "        )\n",
    "    \n",
    "    # Procesar eventos del batch\n",
    "    for event_batch in events:\n",
    "        for _, event in event_batch.iterrows():\n",
    "            if session.start_time is None:\n",
    "                session.start_time = event['timestamp']\n",
    "            \n",
    "            session.last_activity = event['timestamp']\n",
    "            session.events.append(event['event_type'])\n",
    "            session.total_revenue += event.get('revenue', 0.0)\n",
    "    \n",
    "    # Timeout si inactividad >30 min\n",
    "    if (datetime.now() - session.last_activity).seconds > 1800:\n",
    "        state.remove()  # Limpiar estado\n",
    "        # Emitir sesión finalizada\n",
    "        return iter([pd.DataFrame([{\n",
    "            'user_id': user_id,\n",
    "            'session_duration': (session.last_activity - session.start_time).seconds,\n",
    "            'num_events': len(session.events),\n",
    "            'total_revenue': session.total_revenue\n",
    "        }])])\n",
    "    else:\n",
    "        state.update(session)  # Guardar estado actualizado\n",
    "        state.setTimeoutDuration(\"30 minutes\")\n",
    "        return iter([])  # No output aún\n",
    "\n",
    "# Aplicar state management\n",
    "sessions = df.groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        update_session_state,\n",
    "        outputStructType=session_output_schema,\n",
    "        stateStructType=session_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "```\n",
    "\n",
    "**State Size Monitoring:**\n",
    "\n",
    "```python\n",
    "# Métricas de estado en lastProgress\n",
    "progress = query.lastProgress\n",
    "\n",
    "state_info = progress[\"stateOperators\"][0]  # First stateful operator\n",
    "print(f\"Num state rows: {state_info['numRowsTotal']}\")\n",
    "print(f\"State memory (MB): {state_info['memoryUsedBytes'] / 1024 / 1024}\")\n",
    "print(f\"Custom metrics: {state_info['customMetrics']}\")\n",
    "\n",
    "# Ejemplo output:\n",
    "{\n",
    "  \"numRowsTotal\": 1234567,\n",
    "  \"numRowsUpdated\": 5678,\n",
    "  \"memoryUsedBytes\": 524288000,  # ~500 MB\n",
    "  \"customMetrics\": {\n",
    "    \"loadedMapCacheHitCount\": 1000,\n",
    "    \"loadedMapCacheMissCount\": 50,\n",
    "    \"stateOnCurrentVersionSizeBytes\": 450000000\n",
    "  }\n",
    "}\n",
    "\n",
    "# Alertas si estado crece sin control\n",
    "if state_info['numRowsTotal'] > 10_000_000:\n",
    "    print(\"⚠️ WARNING: State size >10M rows, consider:\")\n",
    "    print(\"  1. Reduce watermark threshold\")\n",
    "    print(\"  2. Increase parallelism (more partitions)\")\n",
    "    print(\"  3. Use state TTL (time-to-live)\")\n",
    "```\n",
    "\n",
    "**State Cleanup Strategies:**\n",
    "\n",
    "```python\n",
    "# 1. Watermark-based (automático)\n",
    "df.withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\")) \\\n",
    "    .count()\n",
    "# Estado limpiado cuando ventana < watermark\n",
    "\n",
    "# 2. TTL (Time-to-Live) con RocksDB\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.stateInfo.ttl\", \"2 hours\")\n",
    "# Estado no accedido por >2h eliminado\n",
    "\n",
    "# 3. Manual cleanup en mapGroupsWithState\n",
    "def cleanup_old_state(key, events, state):\n",
    "    if state.hasTimedOut:\n",
    "        state.remove()  # Cleanup explícito\n",
    "        return iter([])\n",
    "    # ... process events\n",
    "\n",
    "# 4. State re-partitioning para balancear\n",
    "df.repartition(200, \"user_id\")  # Distribuir estado uniformemente\n",
    "```\n",
    "\n",
    "**Checkpoint Management:**\n",
    "\n",
    "```python\n",
    "# ❌ NEVER cambiar checkpoint location en producción\n",
    "# Cambiar checkpoint = perder estado + reprocessar desde inicio\n",
    "\n",
    "# ✅ Migration process:\n",
    "# 1. Stop query gracefully\n",
    "query.stop()\n",
    "\n",
    "# 2. Backup checkpoint\n",
    "# hdfs dfs -cp /checkpoints/old /checkpoints/backup\n",
    "\n",
    "# 3. Start new query con nuevo checkpoint\n",
    "query_new = df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/new\") \\\n",
    "    .start()\n",
    "\n",
    "# 4. Validar outputs son correctos\n",
    "\n",
    "# 5. Cleanup old checkpoint (después de días/semanas)\n",
    "# hdfs dfs -rm -r /checkpoints/old\n",
    "```\n",
    "\n",
    "**Caso Real: Fraud Detection con Estado**\n",
    "\n",
    "```python\n",
    "# Detectar múltiples transacciones sospechosas del mismo usuario\n",
    "\n",
    "fraud_detection = transactions \\\n",
    "    .withWatermark(\"transaction_time\", \"15 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .applyInPandasWithState(\n",
    "        detect_fraud_pattern,\n",
    "        outputStructType=fraud_alert_schema,\n",
    "        stateStructType=user_fraud_state_schema,\n",
    "        outputMode=\"update\",\n",
    "        timeoutConf=GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )\n",
    "\n",
    "def detect_fraud_pattern(key, events, state):\n",
    "    user_id = key[0]\n",
    "    \n",
    "    # Restaurar estado (historial de transacciones)\n",
    "    if state.exists:\n",
    "        history = state.get()\n",
    "    else:\n",
    "        history = {'transactions': [], 'risk_score': 0}\n",
    "    \n",
    "    # Analizar nuevas transacciones\n",
    "    for event_batch in events:\n",
    "        for _, txn in event_batch.iterrows():\n",
    "            history['transactions'].append(txn)\n",
    "            \n",
    "            # Patrones sospechosos:\n",
    "            # 1. >5 transacciones en 10 minutos\n",
    "            recent_txns = [t for t in history['transactions'] \n",
    "                          if (txn['timestamp'] - t['timestamp']).seconds < 600]\n",
    "            \n",
    "            # 2. Monto total >$10,000 en 10 minutos\n",
    "            recent_total = sum(t['amount'] for t in recent_txns)\n",
    "            \n",
    "            # 3. Múltiples países en corto tiempo\n",
    "            recent_countries = set(t['country'] for t in recent_txns)\n",
    "            \n",
    "            if len(recent_txns) > 5 or recent_total > 10000 or len(recent_countries) > 2:\n",
    "                # Emitir alerta\n",
    "                return iter([pd.DataFrame([{\n",
    "                    'user_id': user_id,\n",
    "                    'alert_type': 'FRAUD_SUSPECTED',\n",
    "                    'risk_score': calculate_risk(recent_txns),\n",
    "                    'timestamp': txn['timestamp']\n",
    "                }])])\n",
    "    \n",
    "    # Actualizar estado con TTL 24h\n",
    "    state.update(history)\n",
    "    state.setTimeoutDuration(\"24 hours\")\n",
    "    return iter([])\n",
    "\n",
    "# Escribir alertas a Kafka para acción inmediata\n",
    "fraud_alerts = fraud_detection.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"fraud-alerts\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/fraud-detection\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ✅ **Siempre configurar checkpoint** para queries stateful\n",
    "2. ✅ **RocksDB en producción** (memory solo para dev)\n",
    "3. ✅ **Monitorear state size** con métricas\n",
    "4. ✅ **Watermark** para cleanup automático\n",
    "5. ✅ **Backup checkpoints** antes de upgrades\n",
    "6. ⚠️ **State TTL** para evitar crecimiento infinito\n",
    "7. ❌ **Nunca cambiar checkpoint location** sin migration plan\n",
    "8. ⚡ **Repartition** por key para balancear estado\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08569a8",
   "metadata": {},
   "source": [
    "### 🏗️ **Integración con Delta Lake y Optimización de Performance**\n",
    "\n",
    "**Spark Streaming + Delta Lake = Streaming Lakehouse**\n",
    "\n",
    "```\n",
    "Ventajas de Delta como Sink:\n",
    "✅ ACID Transactions: Garantía exactly-once sin duplicados\n",
    "✅ Schema Evolution: Agregar columnas sin interrumpir stream\n",
    "✅ Time Travel: Rollback si procesamiento incorrecto\n",
    "✅ Upserts/Merges: CDC (Change Data Capture) en streaming\n",
    "✅ Performance: Z-ordering, compaction automática\n",
    "```\n",
    "\n",
    "**Escribir Stream a Delta:**\n",
    "\n",
    "```python\n",
    "# Configuración básica\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"path\", \"/delta/events\") \\\n",
    "    .start()\n",
    "\n",
    "# Configuración avanzada\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/events-delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\              # Auto schema evolution\n",
    "    .option(\"optimizeWrite\", \"true\") \\            # Bin-packing\n",
    "    .option(\"autoCompact\", \"true\") \\              # Auto compactación\n",
    "    .partitionBy(\"date\", \"hour\") \\                # Particionamiento\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/events\")\n",
    "\n",
    "# Trigger AvailableNow (Spark 3.3+): Procesar backlog completo\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .trigger(availableNow=True) \\                 # Procesar todo y terminar\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/backfill\") \\\n",
    "    .start(\"/delta/events\")\n",
    "```\n",
    "\n",
    "**Streaming Upserts (Merge):**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Función para merge en cada micro-batch\n",
    "def upsert_to_delta(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Merge batch into Delta table (UPSERT)\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/user_profiles\")\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.user_id = source.user_id\"\n",
    "    ).whenMatchedUpdateAll() \\    # Update si existe\n",
    "     .whenNotMatchedInsertAll() \\ # Insert si no existe\n",
    "     .execute()\n",
    "    \n",
    "    print(f\"Batch {batch_id} merged successfully\")\n",
    "\n",
    "# Aplicar a cada batch\n",
    "query = user_updates.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/user-upserts\") \\\n",
    "    .start()\n",
    "\n",
    "# Ejemplo: Actualización de perfiles de usuario\n",
    "# Batch 1: user_123 {name: \"John\", purchases: 5}\n",
    "# Batch 2: user_123 {purchases: 6}  → MERGE actualiza purchases\n",
    "# Batch 3: user_456 {name: \"Jane\"} → INSERT nuevo usuario\n",
    "```\n",
    "\n",
    "**Change Data Capture (CDC) Streaming:**\n",
    "\n",
    "```python\n",
    "# Leer CDC desde Kafka (Debezium format)\n",
    "cdc_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"mysql.prod.users\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), cdc_schema).alias(\"cdc\")\n",
    "    ).select(\"cdc.*\")\n",
    "\n",
    "# Aplicar cambios a Delta\n",
    "def apply_cdc_changes(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Procesar CDC events: INSERT, UPDATE, DELETE\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "    \n",
    "    # Separar por operación\n",
    "    inserts = batch_df.filter(col(\"op\") == \"c\")  # Create\n",
    "    updates = batch_df.filter(col(\"op\") == \"u\")  # Update\n",
    "    deletes = batch_df.filter(col(\"op\") == \"d\")  # Delete\n",
    "    \n",
    "    # Aplicar deletes\n",
    "    if deletes.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            deletes.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedDelete().execute()\n",
    "    \n",
    "    # Aplicar upserts (inserts + updates)\n",
    "    upserts = inserts.union(updates)\n",
    "    if upserts.count() > 0:\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            upserts.alias(\"s\"),\n",
    "            \"t.user_id = s.user_id\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "query = cdc_stream.writeStream \\\n",
    "    .foreachBatch(apply_cdc_changes) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdc\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Leer Stream desde Delta (Change Data Feed):**\n",
    "\n",
    "```python\n",
    "# Habilitar CDF en tabla Delta\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE user_profiles\n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "# Leer cambios como stream\n",
    "changes_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 10) \\  # Desde versión específica\n",
    "    .table(\"user_profiles\")\n",
    "\n",
    "# Columnas adicionales en CDF:\n",
    "# _change_type: insert, update_preimage, update_postimage, delete\n",
    "# _commit_version: Delta version\n",
    "# _commit_timestamp: When change happened\n",
    "\n",
    "# Procesar solo updates\n",
    "updates_only = changes_stream.filter(col(\"_change_type\") == \"update_postimage\")\n",
    "\n",
    "# Materializar a otra tabla\n",
    "query = updates_only.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/cdf-consumer\") \\\n",
    "    .start(\"/delta/user_updates_log\")\n",
    "```\n",
    "\n",
    "**Performance Optimization: Kafka Source**\n",
    "\n",
    "```python\n",
    "# Configuración óptima para Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9092,kafka3:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\        # latest, earliest, {\"topic\":{\"0\":23,\"1\":-1}}\n",
    "    .option(\"maxOffsetsPerTrigger\", 50000) \\      # Limitar ingesta por batch\n",
    "    .option(\"minPartitions\", 10) \\                # Paralelismo mínimo\n",
    "    .option(\"kafka.max.poll.records\", 500) \\      # Records por poll\n",
    "    .option(\"kafka.session.timeout.ms\", 30000) \\\n",
    "    .option(\"kafka.request.timeout.ms\", 40000) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\          # Tolerar data loss en dev\n",
    "    .load()\n",
    "\n",
    "# Consumer group por query\n",
    "# - Checkpoint location determina consumer group\n",
    "# - Cambiar checkpoint = nuevo consumer group = reprocessar todo\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# ❌ Anti-pattern: Sobre-particionamiento\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"user_id\") \\  # Millones de particiones!\n",
    "    .start()\n",
    "\n",
    "# ✅ Particionamiento balanceado\n",
    "df.writeStream \\\n",
    "    .partitionBy(\"date\") \\  # ~30 particiones/mes\n",
    "    .start()\n",
    "\n",
    "# ⚡ Dynamic partition overwrite (batch mode)\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"/delta/events\")\n",
    "\n",
    "# Z-ordering para columnas de filtrado frecuente\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE events\n",
    "    ZORDER BY (user_id, product_id)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Auto Compaction:**\n",
    "\n",
    "```python\n",
    "# Problema: Small files generados por streaming\n",
    "# Cada micro-batch escribe archivos pequeños (10-100 MB)\n",
    "# Resultado: Millones de archivos después de días/semanas\n",
    "\n",
    "# Solución 1: Auto-compaction (Delta 1.2+)\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\n",
    "\n",
    "# Solución 2: Scheduled compaction job\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def compact_table(table_path):\n",
    "    \"\"\"\n",
    "    Compactar archivos pequeños\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, table_path)\n",
    "    \n",
    "    # OPTIMIZE: Compactar archivos en particiones\n",
    "    delta_table.optimize() \\\n",
    "        .where(\"date >= current_date() - interval 7 days\") \\  # Solo últimos 7 días\n",
    "        .executeCompaction()\n",
    "    \n",
    "    # VACUUM: Limpiar archivos antiguos (después de retention period)\n",
    "    delta_table.vacuum(retentionHours=168)  # 7 días\n",
    "\n",
    "# Ejecutar como Airflow DAG diario\n",
    "optimize_dag = DAG('delta_optimize', schedule_interval='@daily')\n",
    "\n",
    "# Solución 3: Bin-packing en escritura\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\  # Bin-pack antes de escribir\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Monitoring y Alertas:**\n",
    "\n",
    "```python\n",
    "# Métricas clave para monitoreo\n",
    "def extract_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer métricas para Prometheus/Datadog\n",
    "    \"\"\"\n",
    "    progress = query.lastProgress\n",
    "    \n",
    "    if progress is None:\n",
    "        return {}\n",
    "    \n",
    "    metrics = {\n",
    "        # Throughput\n",
    "        \"input_rows_per_second\": progress.get(\"inputRowsPerSecond\", 0),\n",
    "        \"processed_rows_per_second\": progress.get(\"processedRowsPerSecond\", 0),\n",
    "        \n",
    "        # Latency\n",
    "        \"batch_duration_ms\": progress.get(\"durationMs\", {}).get(\"triggerExecution\", 0),\n",
    "        \"batch_id\": progress.get(\"batchId\", 0),\n",
    "        \n",
    "        # State\n",
    "        \"num_state_rows\": 0,\n",
    "        \"state_memory_mb\": 0,\n",
    "        \n",
    "        # Lag\n",
    "        \"input_rows\": progress.get(\"numInputRows\", 0),\n",
    "    }\n",
    "    \n",
    "    # State metrics si hay operadores stateful\n",
    "    if \"stateOperators\" in progress and len(progress[\"stateOperators\"]) > 0:\n",
    "        state_op = progress[\"stateOperators\"][0]\n",
    "        metrics[\"num_state_rows\"] = state_op.get(\"numRowsTotal\", 0)\n",
    "        metrics[\"state_memory_mb\"] = state_op.get(\"memoryUsedBytes\", 0) / 1024 / 1024\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Alertas\n",
    "def check_alerts(metrics):\n",
    "    \"\"\"\n",
    "    Generar alertas si métricas anormales\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    # Alerta 1: Falling behind\n",
    "    if metrics[\"input_rows_per_second\"] > metrics[\"processed_rows_per_second\"] * 1.2:\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Processing falling behind: {metrics['input_rows_per_second']:.0f} in/s vs {metrics['processed_rows_per_second']:.0f} out/s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 2: High latency\n",
    "    if metrics[\"batch_duration_ms\"] > 60000:  # >1 minuto\n",
    "        alerts.append({\n",
    "            \"severity\": \"ERROR\",\n",
    "            \"message\": f\"High batch latency: {metrics['batch_duration_ms']/1000:.1f}s\"\n",
    "        })\n",
    "    \n",
    "    # Alerta 3: State growing unbounded\n",
    "    if metrics[\"num_state_rows\"] > 50_000_000:  # >50M registros\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Large state size: {metrics['num_state_rows']:,} rows, {metrics['state_memory_mb']:.1f} MB\"\n",
    "        })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Integración con monitoring\n",
    "import time\n",
    "while query.isActive:\n",
    "    time.sleep(60)  # Check cada minuto\n",
    "    metrics = extract_stream_metrics(query)\n",
    "    alerts = check_alerts(metrics)\n",
    "    \n",
    "    # Enviar a Prometheus\n",
    "    push_to_prometheus(metrics)\n",
    "    \n",
    "    # Enviar alertas a Slack/PagerDuty\n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            send_alert(alert)\n",
    "```\n",
    "\n",
    "**Caso Real: Real-time Analytics Dashboard**\n",
    "\n",
    "```python\n",
    "# Pipeline completo: Kafka → Spark Streaming → Delta → BI Tool\n",
    "\n",
    "# 1. Leer eventos de Kafka\n",
    "events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"user-events\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"e\")) \\\n",
    "    .select(\"e.*\")\n",
    "\n",
    "# 2. Transformaciones\n",
    "events_enriched = events \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .join(\n",
    "        users_dim.alias(\"u\"),\n",
    "        events.user_id == col(\"u.user_id\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"event_id\"),\n",
    "        col(\"event_timestamp\"),\n",
    "        col(\"e.user_id\"),\n",
    "        col(\"u.user_segment\"),  # Enriquecimiento\n",
    "        col(\"event_type\"),\n",
    "        col(\"revenue\")\n",
    "    )\n",
    "\n",
    "# 3. Agregaciones por ventanas\n",
    "dashboard_metrics = events_enriched \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"5 minutes\"),\n",
    "        \"user_segment\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\")\n",
    "    )\n",
    "\n",
    "# 4. Escribir a Delta con optimizaciones\n",
    "query = dashboard_metrics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/dashboard\") \\\n",
    "    .option(\"optimizeWrite\", \"true\") \\\n",
    "    .option(\"autoCompact\", \"true\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start(\"/delta/dashboard_metrics\")\n",
    "\n",
    "# 5. BI Tool lee desde Delta (Tableau, Power BI, Looker)\n",
    "# SELECT * FROM delta.`/delta/dashboard_metrics`\n",
    "# WHERE window.start >= current_timestamp() - interval 1 hour\n",
    "\n",
    "# 6. Z-order para performance\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta.`/delta/dashboard_metrics`\n",
    "    ZORDER BY (window, user_segment)\n",
    "\"\"\")\n",
    "\n",
    "# 7. Vacuum archivos antiguos (semanal)\n",
    "spark.sql(\"\"\"\n",
    "    VACUUM delta.`/delta/dashboard_metrics`\n",
    "    RETAIN 168 HOURS\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. ✅ **Delta como sink primario** en producción (ACID + performance)\n",
    "2. ✅ **foreachBatch para lógica custom** (upserts, alerts, etc.)\n",
    "3. ✅ **Auto-compaction** o scheduled OPTIMIZE\n",
    "4. ✅ **Z-ordering** en columnas de filtrado frecuente\n",
    "5. ✅ **Particionamiento por fecha** (balance entre granularidad y cantidad)\n",
    "6. ✅ **maxOffsetsPerTrigger** para controlar ingesta\n",
    "7. ✅ **Monitor métricas** continuamente (Prometheus, Datadog)\n",
    "8. ⚠️ **Change Data Feed** para downstream consumers\n",
    "9. ⚠️ **failOnDataLoss=false** solo en dev (strict en prod)\n",
    "10. ❌ **No sobre-particionar** (evitar millones de particiones)\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "\n",
    "```\n",
    "Scenario: 1M events/s, 10 KB/event, 100 partitions\n",
    "\n",
    "Kafka → Spark Streaming → Parquet:\n",
    "- Latency: ~30s (trigger interval + processing)\n",
    "- Small files: 1000+ archivos/hora\n",
    "- OPTIMIZE needed: Daily\n",
    "- Cost: $$\n",
    "\n",
    "Kafka → Spark Streaming → Delta (optimized):\n",
    "- Latency: ~30s\n",
    "- Small files: Auto-compacted\n",
    "- OPTIMIZE needed: Weekly\n",
    "- Cost: $$ (slightly higher for optimization)\n",
    "- Benefit: ACID, time travel, upserts\n",
    "\n",
    "Kafka → Flink → Delta:\n",
    "- Latency: ~5s\n",
    "- More complex setup\n",
    "- Cost: $$$\n",
    "- Benefit: Lower latency, event-time watermarks native\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c904ba",
   "metadata": {},
   "source": [
    "## 1. Inicializar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Spark Session con configuración para Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkStreamingAdvanced\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar nivel de logging\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528ec21",
   "metadata": {},
   "source": [
    "## 2. Definir Esquemas para Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema para eventos de e-commerce\n",
    "ecommerce_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Esquema para logs de aplicación\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"level\", StringType(), False),\n",
    "    StructField(\"service\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"error_code\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Esquemas definidos\")\n",
    "print(\"\\nEsquema E-commerce:\")\n",
    "print(ecommerce_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9bb26",
   "metadata": {},
   "source": [
    "## 3. Simulación de Fuente de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3139b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo para simular streaming\n",
    "def generate_sample_data(n_records=1000):\n",
    "    \"\"\"\n",
    "    Generar datos de muestra para streaming\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_records):\n",
    "        event = {\n",
    "            'event_id': f'evt_{i:06d}',\n",
    "            'timestamp': base_time + timedelta(seconds=i),\n",
    "            'user_id': f'user_{np.random.randint(1, 101)}',\n",
    "            'event_type': np.random.choice(['view', 'add_to_cart', 'purchase', 'remove'], p=[0.5, 0.25, 0.15, 0.1]),\n",
    "            'product_id': f'prod_{np.random.randint(1, 51)}',\n",
    "            'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "            'category': np.random.choice(['Electronics', 'Accessories', 'Computers']),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "        data.append(event)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Generar datos\n",
    "sample_data = generate_sample_data(1000)\n",
    "df_sample = spark.createDataFrame(sample_data, schema=ecommerce_schema)\n",
    "\n",
    "print(f\"Generados {df_sample.count()} registros de muestra\")\n",
    "df_sample.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbdd91",
   "metadata": {},
   "source": [
    "## 4. Streaming con Rate Source (Simulación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear stream simulado con rate source\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()\n",
    "\n",
    "# Enriquecer con datos simulados\n",
    "from pyspark.sql.functions import rand, when, lit\n",
    "\n",
    "enriched_stream = rate_stream \\\n",
    "    .withColumn(\"user_id\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"event_type\", \n",
    "        when(rand() < 0.5, \"view\")\n",
    "        .when(rand() < 0.75, \"add_to_cart\")\n",
    "        .when(rand() < 0.9, \"purchase\")\n",
    "        .otherwise(\"remove\")\n",
    "    ) \\\n",
    "    .withColumn(\"product_id\", (rand() * 50).cast(\"int\")) \\\n",
    "    .withColumn(\"price\", (rand() * 1990 + 10).cast(\"double\")) \\\n",
    "    .withColumn(\"quantity\", (rand() * 4 + 1).cast(\"int\"))\n",
    "\n",
    "print(\"Stream enriquecido creado\")\n",
    "print(\"Schema:\")\n",
    "enriched_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa2ae8",
   "metadata": {},
   "source": [
    "## 5. Agregaciones por Ventanas de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones por ventana de tiempo\n",
    "windowed_counts = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\"),\n",
    "        col(\"event_type\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "print(\"Query de agregación por ventanas configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ba0d3",
   "metadata": {},
   "source": [
    "## 6. Procesamiento Stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c181a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantener estado acumulado por usuario\n",
    "user_aggregations = enriched_stream \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"total_spent\"),\n",
    "        spark_max(\"timestamp\").alias(\"last_activity\")\n",
    "    )\n",
    "\n",
    "print(\"Query de agregación por usuario configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177bf9",
   "metadata": {},
   "source": [
    "## 7. Detección de Patrones en Tiempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfe837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar usuarios con comportamiento anómalo\n",
    "anomaly_detection = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\"),\n",
    "        col(\"user_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"events_per_minute\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"spend_per_minute\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"events_per_minute\") > 50) |  # Más de 50 eventos por minuto\n",
    "        (col(\"spend_per_minute\") > 10000)   # Más de $10,000 por minuto\n",
    "    )\n",
    "\n",
    "print(\"Query de detección de anomalías configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2880b",
   "metadata": {},
   "source": [
    "## 8. Ejemplo de Query con Output Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados en consola (modo batch para demo)\n",
    "# NOTA: En producción usarías .writeStream() en lugar de show()\n",
    "\n",
    "print(\"\\n=== ANÁLISIS BATCH DE DATOS DE MUESTRA ===\")\n",
    "print(\"\\n1. Eventos por tipo:\")\n",
    "df_sample.groupBy(\"event_type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\n2. Revenue por categoría:\")\n",
    "df_sample.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_purchases\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_revenue\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n3. Top 10 usuarios más activos:\")\n",
    "df_sample.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_events\", ascending=False) \\\n",
    "    .limit(10) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n4. Tasa de conversión por producto:\")\n",
    "conversion_rate = df_sample.groupBy(\"product_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    )\n",
    "\n",
    "conversion_rate.withColumn(\n",
    "    \"conversion_rate\",\n",
    "    (col(\"purchases\") / col(\"views\") * 100)\n",
    ").orderBy(\"conversion_rate\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac2a3f",
   "metadata": {},
   "source": [
    "## 9. Escribir Stream a Diferentes Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de configuración de escritura (comentado para evitar ejecución)\n",
    "\n",
    "# 1. Escribir a consola (desarrollo/debug)\n",
    "console_query_config = {\n",
    "    'outputMode': 'complete',  # complete, append, update\n",
    "    'format': 'console',\n",
    "    'trigger': {'processingTime': '10 seconds'},\n",
    "    'options': {\n",
    "        'truncate': False,\n",
    "        'numRows': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Escribir a Parquet (data lake)\n",
    "parquet_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'parquet',\n",
    "    'path': '/path/to/output',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'trigger': {'processingTime': '1 minute'},\n",
    "    'options': {\n",
    "        'compression': 'snappy',\n",
    "        'partitionBy': 'date'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Escribir a Kafka\n",
    "kafka_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'kafka',\n",
    "    'options': {\n",
    "        'kafka.bootstrap.servers': 'localhost:9092',\n",
    "        'topic': 'processed-events',\n",
    "        'checkpointLocation': '/path/to/checkpoint'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Escribir a Delta Lake\n",
    "delta_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'delta',\n",
    "    'path': '/path/to/delta',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'options': {\n",
    "        'mergeSchema': True,\n",
    "        'optimizeWrite': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuraciones de sink definidas (ver código para detalles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e869",
   "metadata": {},
   "source": [
    "## 10. Métricas y Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para monitorear estado del stream\n",
    "def monitor_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer métricas del streaming query\n",
    "    \"\"\"\n",
    "    status = query.status\n",
    "    \n",
    "    metrics = {\n",
    "        'isDataAvailable': status['isDataAvailable'],\n",
    "        'isTriggerActive': status['isTriggerActive'],\n",
    "        'message': status['message']\n",
    "    }\n",
    "    \n",
    "    if 'inputRowsPerSecond' in status:\n",
    "        metrics['inputRowsPerSecond'] = status['inputRowsPerSecond']\n",
    "    \n",
    "    if 'processedRowsPerSecond' in status:\n",
    "        metrics['processedRowsPerSecond'] = status['processedRowsPerSecond']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Ejemplo de métricas a monitorear\n",
    "print(\"\\n=== MÉTRICAS CLAVE PARA MONITOREO ===\")\n",
    "print(\"\"\"\n",
    "1. Input Rate: Eventos por segundo recibidos\n",
    "2. Processing Rate: Eventos por segundo procesados\n",
    "3. Batch Duration: Tiempo de procesamiento por batch\n",
    "4. Trigger Interval: Intervalo entre ejecuciones\n",
    "5. Watermark: Retraso máximo aceptado\n",
    "6. Estado del Query: Activo, inactivo, error\n",
    "7. Checkpoint Location: Para recuperación de fallos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a91d85",
   "metadata": {},
   "source": [
    "## 11. Optimización de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de optimización\n",
    "optimization_configs = {\n",
    "    # Particionamiento\n",
    "    'spark.sql.shuffle.partitions': '200',  # Número de particiones para shuffles\n",
    "    'spark.default.parallelism': '200',     # Paralelismo por defecto\n",
    "    \n",
    "    # Memoria\n",
    "    'spark.executor.memory': '4g',\n",
    "    'spark.driver.memory': '2g',\n",
    "    'spark.memory.fraction': '0.8',\n",
    "    \n",
    "    # Streaming específico\n",
    "    'spark.sql.streaming.minBatchesToRetain': '100',\n",
    "    'spark.sql.streaming.stateStore.providerClass': 'org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider',\n",
    "    \n",
    "    # Optimización de escritura\n",
    "    'spark.sql.adaptive.enabled': 'true',\n",
    "    'spark.sql.adaptive.coalescePartitions.enabled': 'true',\n",
    "}\n",
    "\n",
    "print(\"\\n=== MEJORES PRÁCTICAS DE OPTIMIZACIÓN ===\")\n",
    "print(\"\"\"\n",
    "1. Usar watermarks para limpiar estado antiguo\n",
    "2. Particionar datos por columnas clave\n",
    "3. Configurar apropiadamente spark.sql.shuffle.partitions\n",
    "4. Usar triggers basados en tiempo para controlar frecuencia\n",
    "5. Implementar checkpointing para recuperación\n",
    "6. Monitorear métricas constantemente\n",
    "7. Usar Delta Lake para ACID transactions\n",
    "8. Implementar compactación de archivos pequeños\n",
    "9. Optimizar esquemas y evitar tipos genéricos\n",
    "10. Considerar micro-batching vs continuous processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5288b",
   "metadata": {},
   "source": [
    "## Resumen y Arquitectura Enterprise\n",
    "\n",
    "### Arquitectura Típica de Streaming:\n",
    "```\n",
    "Fuentes de Datos       Ingesta           Procesamiento        Almacenamiento        Consumo\n",
    "────────────────       ───────           ──────────────       ──────────────        ───────\n",
    "Kafka/Kinesis    →    Spark       →     Transformaciones  →   Delta Lake      →    BI Tools\n",
    "IoT Devices      →    Streaming   →     Agregaciones      →   Data Lake       →    ML Models\n",
    "APIs             →                →     Joins             →   Warehouse       →    Dashboards\n",
    "Logs             →                →     Windows           →   Cache (Redis)   →    Alertas\n",
    "```\n",
    "\n",
    "### Patrones Avanzados:\n",
    "\n",
    "#### 1. Lambda Architecture\n",
    "- **Batch Layer**: Procesamiento histórico completo\n",
    "- **Speed Layer**: Procesamiento en tiempo real\n",
    "- **Serving Layer**: Combina ambas vistas\n",
    "\n",
    "#### 2. Kappa Architecture\n",
    "- Solo capa de streaming\n",
    "- Todo procesamiento en tiempo real\n",
    "- Reprocesamiento desde el inicio del stream\n",
    "\n",
    "#### 3. Delta Architecture\n",
    "- Basada en Delta Lake\n",
    "- ACID transactions\n",
    "- Time travel\n",
    "- Schema evolution\n",
    "\n",
    "### Casos de Uso Enterprise:\n",
    "\n",
    "1. **Detección de Fraude en Tiempo Real**\n",
    "   - Análisis de patrones sospechosos\n",
    "   - Machine Learning en streaming\n",
    "   - Alertas automáticas\n",
    "\n",
    "2. **Recomendaciones Personalizadas**\n",
    "   - Seguimiento de comportamiento en tiempo real\n",
    "   - Actualización de perfiles de usuario\n",
    "   - A/B testing dinámico\n",
    "\n",
    "3. **Monitoreo de Infraestructura**\n",
    "   - Logs y métricas en tiempo real\n",
    "   - Detección de anomalías\n",
    "   - Auto-scaling basado en carga\n",
    "\n",
    "4. **IoT y Telemetría**\n",
    "   - Procesamiento de sensores\n",
    "   - Mantenimiento predictivo\n",
    "   - Optimización de operaciones\n",
    "\n",
    "### Consideraciones de Producción:\n",
    "\n",
    "- **Alta Disponibilidad**: Cluster mode, múltiples workers\n",
    "- **Fault Tolerance**: Checkpointing, Write-Ahead Logs\n",
    "- **Escalabilidad**: Auto-scaling, dynamic allocation\n",
    "- **Seguridad**: Kerberos, SSL/TLS, encryption at rest\n",
    "- **Monitoreo**: Prometheus, Grafana, CloudWatch\n",
    "- **Testing**: Unit tests, integration tests, chaos engineering\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)\n",
    "- [Databricks Streaming Best Practices](https://docs.databricks.com/structured-streaming/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar recursos\n",
    "spark.stop()\n",
    "print(\"Spark Session cerrada\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
