{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc763fbf",
   "metadata": {},
   "source": [
    "# Apache Spark Streaming: Procesamiento en Tiempo Real\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar Spark Structured Streaming\n",
    "- Implementar procesamiento de ventanas y agregaciones\n",
    "- Integrar con Kafka y otras fuentes de streaming\n",
    "- Manejar estado y checkpointing\n",
    "- Optimizar rendimiento en streaming\n",
    "\n",
    "## Requisitos\n",
    "- PySpark 3.x\n",
    "- Python 3.8+\n",
    "- Kafka (opcional)\n",
    "- Delta Lake (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, count, sum as spark_sum, avg, max as spark_max,\n",
    "    current_timestamp, to_json, from_json, struct, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c904ba",
   "metadata": {},
   "source": [
    "## 1. Inicializar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Spark Session con configuración para Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkStreamingAdvanced\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar nivel de logging\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528ec21",
   "metadata": {},
   "source": [
    "## 2. Definir Esquemas para Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema para eventos de e-commerce\n",
    "ecommerce_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Esquema para logs de aplicación\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"level\", StringType(), False),\n",
    "    StructField(\"service\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"error_code\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Esquemas definidos\")\n",
    "print(\"\\nEsquema E-commerce:\")\n",
    "print(ecommerce_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9bb26",
   "metadata": {},
   "source": [
    "## 3. Simulación de Fuente de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3139b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo para simular streaming\n",
    "def generate_sample_data(n_records=1000):\n",
    "    \"\"\"\n",
    "    Generar datos de muestra para streaming\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_records):\n",
    "        event = {\n",
    "            'event_id': f'evt_{i:06d}',\n",
    "            'timestamp': base_time + timedelta(seconds=i),\n",
    "            'user_id': f'user_{np.random.randint(1, 101)}',\n",
    "            'event_type': np.random.choice(['view', 'add_to_cart', 'purchase', 'remove'], p=[0.5, 0.25, 0.15, 0.1]),\n",
    "            'product_id': f'prod_{np.random.randint(1, 51)}',\n",
    "            'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "            'category': np.random.choice(['Electronics', 'Accessories', 'Computers']),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "        data.append(event)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Generar datos\n",
    "sample_data = generate_sample_data(1000)\n",
    "df_sample = spark.createDataFrame(sample_data, schema=ecommerce_schema)\n",
    "\n",
    "print(f\"Generados {df_sample.count()} registros de muestra\")\n",
    "df_sample.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbdd91",
   "metadata": {},
   "source": [
    "## 4. Streaming con Rate Source (Simulación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear stream simulado con rate source\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()\n",
    "\n",
    "# Enriquecer con datos simulados\n",
    "from pyspark.sql.functions import rand, when, lit\n",
    "\n",
    "enriched_stream = rate_stream \\\n",
    "    .withColumn(\"user_id\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"event_type\", \n",
    "        when(rand() < 0.5, \"view\")\n",
    "        .when(rand() < 0.75, \"add_to_cart\")\n",
    "        .when(rand() < 0.9, \"purchase\")\n",
    "        .otherwise(\"remove\")\n",
    "    ) \\\n",
    "    .withColumn(\"product_id\", (rand() * 50).cast(\"int\")) \\\n",
    "    .withColumn(\"price\", (rand() * 1990 + 10).cast(\"double\")) \\\n",
    "    .withColumn(\"quantity\", (rand() * 4 + 1).cast(\"int\"))\n",
    "\n",
    "print(\"Stream enriquecido creado\")\n",
    "print(\"Schema:\")\n",
    "enriched_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa2ae8",
   "metadata": {},
   "source": [
    "## 5. Agregaciones por Ventanas de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones por ventana de tiempo\n",
    "windowed_counts = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\"),\n",
    "        col(\"event_type\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "print(\"Query de agregación por ventanas configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ba0d3",
   "metadata": {},
   "source": [
    "## 6. Procesamiento Stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c181a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantener estado acumulado por usuario\n",
    "user_aggregations = enriched_stream \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"total_spent\"),\n",
    "        spark_max(\"timestamp\").alias(\"last_activity\")\n",
    "    )\n",
    "\n",
    "print(\"Query de agregación por usuario configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177bf9",
   "metadata": {},
   "source": [
    "## 7. Detección de Patrones en Tiempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfe837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar usuarios con comportamiento anómalo\n",
    "anomaly_detection = enriched_stream \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\"),\n",
    "        col(\"user_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"events_per_minute\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", \n",
    "                      expr(\"price * quantity\")).otherwise(0)).alias(\"spend_per_minute\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"events_per_minute\") > 50) |  # Más de 50 eventos por minuto\n",
    "        (col(\"spend_per_minute\") > 10000)   # Más de $10,000 por minuto\n",
    "    )\n",
    "\n",
    "print(\"Query de detección de anomalías configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2880b",
   "metadata": {},
   "source": [
    "## 8. Ejemplo de Query con Output Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados en consola (modo batch para demo)\n",
    "# NOTA: En producción usarías .writeStream() en lugar de show()\n",
    "\n",
    "print(\"\\n=== ANÁLISIS BATCH DE DATOS DE MUESTRA ===\")\n",
    "print(\"\\n1. Eventos por tipo:\")\n",
    "df_sample.groupBy(\"event_type\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\n2. Revenue por categoría:\")\n",
    "df_sample.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_purchases\"),\n",
    "        spark_sum(expr(\"price * quantity\")).alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_revenue\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n3. Top 10 usuarios más activos:\")\n",
    "df_sample.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_events\", ascending=False) \\\n",
    "    .limit(10) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n4. Tasa de conversión por producto:\")\n",
    "conversion_rate = df_sample.groupBy(\"product_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    )\n",
    "\n",
    "conversion_rate.withColumn(\n",
    "    \"conversion_rate\",\n",
    "    (col(\"purchases\") / col(\"views\") * 100)\n",
    ").orderBy(\"conversion_rate\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac2a3f",
   "metadata": {},
   "source": [
    "## 9. Escribir Stream a Diferentes Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de configuración de escritura (comentado para evitar ejecución)\n",
    "\n",
    "# 1. Escribir a consola (desarrollo/debug)\n",
    "console_query_config = {\n",
    "    'outputMode': 'complete',  # complete, append, update\n",
    "    'format': 'console',\n",
    "    'trigger': {'processingTime': '10 seconds'},\n",
    "    'options': {\n",
    "        'truncate': False,\n",
    "        'numRows': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Escribir a Parquet (data lake)\n",
    "parquet_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'parquet',\n",
    "    'path': '/path/to/output',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'trigger': {'processingTime': '1 minute'},\n",
    "    'options': {\n",
    "        'compression': 'snappy',\n",
    "        'partitionBy': 'date'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Escribir a Kafka\n",
    "kafka_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'kafka',\n",
    "    'options': {\n",
    "        'kafka.bootstrap.servers': 'localhost:9092',\n",
    "        'topic': 'processed-events',\n",
    "        'checkpointLocation': '/path/to/checkpoint'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Escribir a Delta Lake\n",
    "delta_query_config = {\n",
    "    'outputMode': 'append',\n",
    "    'format': 'delta',\n",
    "    'path': '/path/to/delta',\n",
    "    'checkpointLocation': '/path/to/checkpoint',\n",
    "    'options': {\n",
    "        'mergeSchema': True,\n",
    "        'optimizeWrite': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuraciones de sink definidas (ver código para detalles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e869",
   "metadata": {},
   "source": [
    "## 10. Métricas y Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para monitorear estado del stream\n",
    "def monitor_stream_metrics(query):\n",
    "    \"\"\"\n",
    "    Extraer métricas del streaming query\n",
    "    \"\"\"\n",
    "    status = query.status\n",
    "    \n",
    "    metrics = {\n",
    "        'isDataAvailable': status['isDataAvailable'],\n",
    "        'isTriggerActive': status['isTriggerActive'],\n",
    "        'message': status['message']\n",
    "    }\n",
    "    \n",
    "    if 'inputRowsPerSecond' in status:\n",
    "        metrics['inputRowsPerSecond'] = status['inputRowsPerSecond']\n",
    "    \n",
    "    if 'processedRowsPerSecond' in status:\n",
    "        metrics['processedRowsPerSecond'] = status['processedRowsPerSecond']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Ejemplo de métricas a monitorear\n",
    "print(\"\\n=== MÉTRICAS CLAVE PARA MONITOREO ===\")\n",
    "print(\"\"\"\n",
    "1. Input Rate: Eventos por segundo recibidos\n",
    "2. Processing Rate: Eventos por segundo procesados\n",
    "3. Batch Duration: Tiempo de procesamiento por batch\n",
    "4. Trigger Interval: Intervalo entre ejecuciones\n",
    "5. Watermark: Retraso máximo aceptado\n",
    "6. Estado del Query: Activo, inactivo, error\n",
    "7. Checkpoint Location: Para recuperación de fallos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a91d85",
   "metadata": {},
   "source": [
    "## 11. Optimización de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de optimización\n",
    "optimization_configs = {\n",
    "    # Particionamiento\n",
    "    'spark.sql.shuffle.partitions': '200',  # Número de particiones para shuffles\n",
    "    'spark.default.parallelism': '200',     # Paralelismo por defecto\n",
    "    \n",
    "    # Memoria\n",
    "    'spark.executor.memory': '4g',\n",
    "    'spark.driver.memory': '2g',\n",
    "    'spark.memory.fraction': '0.8',\n",
    "    \n",
    "    # Streaming específico\n",
    "    'spark.sql.streaming.minBatchesToRetain': '100',\n",
    "    'spark.sql.streaming.stateStore.providerClass': 'org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider',\n",
    "    \n",
    "    # Optimización de escritura\n",
    "    'spark.sql.adaptive.enabled': 'true',\n",
    "    'spark.sql.adaptive.coalescePartitions.enabled': 'true',\n",
    "}\n",
    "\n",
    "print(\"\\n=== MEJORES PRÁCTICAS DE OPTIMIZACIÓN ===\")\n",
    "print(\"\"\"\n",
    "1. Usar watermarks para limpiar estado antiguo\n",
    "2. Particionar datos por columnas clave\n",
    "3. Configurar apropiadamente spark.sql.shuffle.partitions\n",
    "4. Usar triggers basados en tiempo para controlar frecuencia\n",
    "5. Implementar checkpointing para recuperación\n",
    "6. Monitorear métricas constantemente\n",
    "7. Usar Delta Lake para ACID transactions\n",
    "8. Implementar compactación de archivos pequeños\n",
    "9. Optimizar esquemas y evitar tipos genéricos\n",
    "10. Considerar micro-batching vs continuous processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5288b",
   "metadata": {},
   "source": [
    "## Resumen y Arquitectura Enterprise\n",
    "\n",
    "### Arquitectura Típica de Streaming:\n",
    "```\n",
    "Fuentes de Datos       Ingesta           Procesamiento        Almacenamiento        Consumo\n",
    "────────────────       ───────           ──────────────       ──────────────        ───────\n",
    "Kafka/Kinesis    →    Spark       →     Transformaciones  →   Delta Lake      →    BI Tools\n",
    "IoT Devices      →    Streaming   →     Agregaciones      →   Data Lake       →    ML Models\n",
    "APIs             →                →     Joins             →   Warehouse       →    Dashboards\n",
    "Logs             →                →     Windows           →   Cache (Redis)   →    Alertas\n",
    "```\n",
    "\n",
    "### Patrones Avanzados:\n",
    "\n",
    "#### 1. Lambda Architecture\n",
    "- **Batch Layer**: Procesamiento histórico completo\n",
    "- **Speed Layer**: Procesamiento en tiempo real\n",
    "- **Serving Layer**: Combina ambas vistas\n",
    "\n",
    "#### 2. Kappa Architecture\n",
    "- Solo capa de streaming\n",
    "- Todo procesamiento en tiempo real\n",
    "- Reprocesamiento desde el inicio del stream\n",
    "\n",
    "#### 3. Delta Architecture\n",
    "- Basada en Delta Lake\n",
    "- ACID transactions\n",
    "- Time travel\n",
    "- Schema evolution\n",
    "\n",
    "### Casos de Uso Enterprise:\n",
    "\n",
    "1. **Detección de Fraude en Tiempo Real**\n",
    "   - Análisis de patrones sospechosos\n",
    "   - Machine Learning en streaming\n",
    "   - Alertas automáticas\n",
    "\n",
    "2. **Recomendaciones Personalizadas**\n",
    "   - Seguimiento de comportamiento en tiempo real\n",
    "   - Actualización de perfiles de usuario\n",
    "   - A/B testing dinámico\n",
    "\n",
    "3. **Monitoreo de Infraestructura**\n",
    "   - Logs y métricas en tiempo real\n",
    "   - Detección de anomalías\n",
    "   - Auto-scaling basado en carga\n",
    "\n",
    "4. **IoT y Telemetría**\n",
    "   - Procesamiento de sensores\n",
    "   - Mantenimiento predictivo\n",
    "   - Optimización de operaciones\n",
    "\n",
    "### Consideraciones de Producción:\n",
    "\n",
    "- **Alta Disponibilidad**: Cluster mode, múltiples workers\n",
    "- **Fault Tolerance**: Checkpointing, Write-Ahead Logs\n",
    "- **Escalabilidad**: Auto-scaling, dynamic allocation\n",
    "- **Seguridad**: Kerberos, SSL/TLS, encryption at rest\n",
    "- **Monitoreo**: Prometheus, Grafana, CloudWatch\n",
    "- **Testing**: Unit tests, integration tests, chaos engineering\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)\n",
    "- [Databricks Streaming Best Practices](https://docs.databricks.com/structured-streaming/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar recursos\n",
    "spark.stop()\n",
    "print(\"Spark Session cerrada\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
