{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f7966c",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los fundamentos de Data Governance\n",
    "- [ ] Implementar frameworks de calidad de datos\n",
    "- [ ] Establecer data lineage y catalogaciÃ³n\n",
    "- [ ] Aplicar principios de DAMA-DMBOK\n",
    "- [ ] Implementar validaciones automatizadas con Great Expectations\n",
    "- [ ] DiseÃ±ar estrategias de metadata management\n",
    "\n",
    "**DuraciÃ³n Estimada:** 150 minutos  \n",
    "**Nivel de Dificultad:** Avanzado  \n",
    "**Prerrequisitos:** Notebooks nivel Junior y Mid completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da27686",
   "metadata": {},
   "source": [
    "## âš ï¸ NIVEL SENIOR - PRÃCTICAS DE PRODUCCIÃ“N\n",
    "\n",
    "### ğŸ¯ A este nivel, ya sabes que:\n",
    "\n",
    "**âŒ Notebooks NO son para producciÃ³n** (son solo para aprendizaje aquÃ­)\n",
    "\n",
    "**âœ… En tu rol Senior, implementas:**\n",
    "- Arquitecturas enterprise (Data Mesh, Lakehouse)\n",
    "- Governance frameworks (DAMA-DMBOK)\n",
    "- Pipelines de producciÃ³n con Airflow/Prefect\n",
    "- Infrastructure as Code (Terraform, CloudFormation)\n",
    "- Observabilidad completa (OpenTelemetry, Datadog)\n",
    "- Security & Compliance (GDPR, SOC2)\n",
    "\n",
    "**ğŸ“– Referencia obligatoria:** `notebooks/âš ï¸_IMPORTANTE_LEER_PRIMERO.md`\n",
    "\n",
    "**Este nivel te enseÃ±a:** DiseÃ±o de arquitecturas y governanza, luego lo implementas en cÃ³digo de producciÃ³n.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | Â© 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60596fe",
   "metadata": {},
   "source": [
    "### ğŸ›ï¸ **Data Governance: El Fundamento de Datos Confiables**\n",
    "\n",
    "**Â¿Por quÃ© Data Governance es crÃ­tico en Senior-level?**\n",
    "\n",
    "A nivel Senior, ya no solo implementas pipelines tÃ©cnicos â†’ **diseÃ±as la estrategia organizacional** de cÃ³mo los datos se gestionan, gobiernan y monetizan.\n",
    "\n",
    "**EvoluciÃ³n del Data Engineer:**\n",
    "\n",
    "```\n",
    "Junior:  \"Â¿CÃ³mo muevo datos de A a B?\"\n",
    "Mid:     \"Â¿CÃ³mo lo hago escalable, confiable y monitoreado?\"\n",
    "Senior:  \"Â¿CÃ³mo aseguro que TODOS los datos de la org sean confiables, \n",
    "          cumplan regulaciones y generen valor de negocio?\"\n",
    "```\n",
    "\n",
    "**Impacto de Mala Calidad de Datos:**\n",
    "\n",
    "| Sector | Costo Anual (Industria) | Ejemplo Real |\n",
    "|--------|-------------------------|--------------|\n",
    "| **Retail** | $3.1 trillones USD | Target: PromociÃ³n a clientes equivocados â†’ $1M perdido |\n",
    "| **Healthcare** | $74B USD | Registros duplicados â†’ medicaciÃ³n incorrecta |\n",
    "| **Finance** | $15M/empresa | Knight Capital: Bug en prod â†’ $440M en 45 min |\n",
    "| **Manufacturing** | $5M/empresa | Datos de inventario desactualizados â†’ Stockouts |\n",
    "\n",
    "**Data Governance Framework (Componentes):**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    GOVERNANCE LAYER                     â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚  Policy  â”‚  â”‚  Roles   â”‚  â”‚Standards â”‚            â”‚\n",
    "â”‚  â”‚ PolÃ­ticasâ”‚  â”‚ Stewards â”‚  â”‚EstÃ¡ndaresâ”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â–¼               â–¼               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Data    â”‚    â”‚   Data   â”‚    â”‚   Data   â”‚\n",
    "â”‚ Quality  â”‚    â”‚ Security â”‚    â”‚ Lineage  â”‚\n",
    "â”‚          â”‚    â”‚          â”‚    â”‚          â”‚\n",
    "â”‚ â€¢ 6 dim. â”‚    â”‚ â€¢ RBAC   â”‚    â”‚ â€¢ Origen â”‚\n",
    "â”‚ â€¢ SLAs   â”‚    â”‚ â€¢ Encryptâ”‚    â”‚ â€¢ Impactoâ”‚\n",
    "â”‚ â€¢ Monitorâ”‚    â”‚ â€¢ Audit  â”‚    â”‚ â€¢ Deps   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚               â”‚               â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "                       â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚  Data Catalog    â”‚\n",
    "            â”‚  (Metadata Hub)  â”‚\n",
    "            â”‚                  â”‚\n",
    "            â”‚  â€¢ Assets        â”‚\n",
    "            â”‚  â€¢ Ownership     â”‚\n",
    "            â”‚  â€¢ Glossary      â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**DAMA-DMBOK (Data Management Body of Knowledge):**\n",
    "\n",
    "Framework estÃ¡ndar de la industria con 11 Ã¡reas:\n",
    "\n",
    "1. **Data Governance** (Core): Marco organizacional\n",
    "2. **Data Architecture**: Blueprints de sistemas de datos\n",
    "3. **Data Modeling**: DiseÃ±o de estructuras\n",
    "4. **Data Storage & Operations**: Almacenamiento\n",
    "5. **Data Security**: ProtecciÃ³n y privacidad\n",
    "6. **Data Integration & Interoperability**: ETL/ELT\n",
    "7. **Document & Content Management**: Docs no estructurados\n",
    "8. **Reference & Master Data**: Golden records (MDM)\n",
    "9. **Data Warehousing & BI**: Analytics\n",
    "10. **Metadata Management**: \"Data about data\"\n",
    "11. **Data Quality**: 6 dimensiones crÃ­ticas\n",
    "\n",
    "**Roles en Data Governance:**\n",
    "\n",
    "```python\n",
    "class DataGovernanceTeam:\n",
    "    \"\"\"\n",
    "    Chief Data Officer (CDO)\n",
    "      â”œâ”€â”€ Data Governance Manager\n",
    "      â”‚     â”œâ”€â”€ Data Stewards (por dominio: Ventas, Finanzas, HR)\n",
    "      â”‚     â””â”€â”€ Data Quality Analysts\n",
    "      â”œâ”€â”€ Data Architect\n",
    "      â”œâ”€â”€ Data Security Officer\n",
    "      â””â”€â”€ Compliance Officer\n",
    "    \"\"\"\n",
    "    \n",
    "    roles = {\n",
    "        'Data Steward': 'Define reglas de negocio, resuelve issues de calidad',\n",
    "        'Data Owner': 'Responsable final de un dominio de datos',\n",
    "        'Data Custodian': 'Implementa controles tÃ©cnicos (Data Engineer)',\n",
    "        'Data Consumer': 'Usa datos para anÃ¡lisis/decisiones'\n",
    "    }\n",
    "```\n",
    "\n",
    "**Herramientas del Ecosistema:**\n",
    "\n",
    "| CategorÃ­a | Open Source | Comercial |\n",
    "|-----------|-------------|-----------|\n",
    "| **Data Catalog** | Apache Atlas, Amundsen, DataHub | Alation, Collibra, Informatica |\n",
    "| **Data Quality** | Great Expectations, Soda, dbt tests | Talend DQ, Informatica DQ |\n",
    "| **Data Lineage** | OpenLineage, Marquez | Manta, Collibra Lineage |\n",
    "| **MDM (Master Data)** | - | Informatica MDM, SAP MDM |\n",
    "| **Metadata Mgmt** | Apache Atlas | Collibra, Alation |\n",
    "\n",
    "**MÃ©tricas de Data Governance (KPIs):**\n",
    "\n",
    "```python\n",
    "governance_metrics = {\n",
    "    'Data Quality Score': '% de datasets que pasan quality gates',\n",
    "    'Metadata Coverage': '% de tablas con descripciones/owners',\n",
    "    'Policy Compliance': '% de datasets que cumplen polÃ­ticas',\n",
    "    'Mean Time to Remediation': 'Tiempo promedio para arreglar issues',\n",
    "    'Data Lineage Coverage': '% de pipelines con lineage documentado',\n",
    "    'Security Incidents': '# de breaches/accesos no autorizados'\n",
    "}\n",
    "\n",
    "# Ejemplo de SLA\n",
    "quality_sla = {\n",
    "    'Critical datasets': {\n",
    "        'completeness': '> 99%',\n",
    "        'freshness': '< 1 hour',\n",
    "        'accuracy': '> 99.5%'\n",
    "    },\n",
    "    'Standard datasets': {\n",
    "        'completeness': '> 95%',\n",
    "        'freshness': '< 24 hours',\n",
    "        'accuracy': '> 95%'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Regulaciones de Compliance:**\n",
    "\n",
    "- **GDPR** (Europa): Derecho al olvido, portabilidad, consentimiento\n",
    "- **CCPA** (California): Privacidad de consumidores\n",
    "- **HIPAA** (USA): Datos de salud\n",
    "- **SOX** (USA): Datos financieros\n",
    "- **LGPD** (Brasil): ProtecciÃ³n de datos personales\n",
    "\n",
    "**ImplementaciÃ³n PrÃ¡ctica en Pipelines:**\n",
    "\n",
    "```python\n",
    "# Pipeline con governance embebido\n",
    "@data_quality_check(min_score=95)\n",
    "@pii_detector(action='mask')\n",
    "@lineage_tracking(catalog='atlas')\n",
    "def ingest_customer_data(source, dest):\n",
    "    df = extract(source)\n",
    "    df = validate_schema(df)          # Quality\n",
    "    df = mask_pii_columns(df)         # Security\n",
    "    df = enrich_metadata(df)          # Catalog\n",
    "    load(df, dest)\n",
    "    track_lineage(source, dest)       # Lineage\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322de2a",
   "metadata": {},
   "source": [
    "## ğŸ¯ Â¿QuÃ© es Data Governance?\n",
    "\n",
    "**Data Governance** es el conjunto de procesos, polÃ­ticas, estÃ¡ndares y mÃ©tricas que aseguran el uso efectivo y eficiente de la informaciÃ³n, habilitando a una organizaciÃ³n a alcanzar sus objetivos.\n",
    "\n",
    "### ğŸ—ï¸ Pilares de Data Governance:\n",
    "\n",
    "1. **Data Quality** (Calidad de Datos)\n",
    "   - PrecisiÃ³n, completitud, consistencia\n",
    "   - Validaciones y monitoreo\n",
    "   - RemediaciÃ³n de problemas\n",
    "\n",
    "2. **Data Security** (Seguridad)\n",
    "   - Control de accesos\n",
    "   - EncriptaciÃ³n\n",
    "   - AuditorÃ­a\n",
    "\n",
    "3. **Data Lineage** (Linaje)\n",
    "   - Trazabilidad origen-destino\n",
    "   - Impacto de cambios\n",
    "   - DocumentaciÃ³n automÃ¡tica\n",
    "\n",
    "4. **Data Catalog** (CatÃ¡logo)\n",
    "   - Inventario de activos de datos\n",
    "   - Metadata management\n",
    "   - Descubrimiento y bÃºsqueda\n",
    "\n",
    "5. **Compliance** (Cumplimiento)\n",
    "   - GDPR, CCPA, HIPAA\n",
    "   - PolÃ­ticas de retenciÃ³n\n",
    "   - AuditorÃ­as regulatorias\n",
    "\n",
    "### ğŸŒŸ Beneficios:\n",
    "\n",
    "```\n",
    "âœ… Confianza en los datos\n",
    "âœ… ReducciÃ³n de riesgos\n",
    "âœ… Cumplimiento regulatorio\n",
    "âœ… Mejor toma de decisiones\n",
    "âœ… Eficiencia operacional\n",
    "âœ… ColaboraciÃ³n mejorada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c17614",
   "metadata": {},
   "source": [
    "### ğŸ“Š **Data Quality: Las 6 Dimensiones CrÃ­ticas (ISO 8000)**\n",
    "\n",
    "**Framework de Calidad basado en estÃ¡ndar ISO 8000:**\n",
    "\n",
    "**1. COMPLETITUD (Completeness):**\n",
    "\n",
    "```python\n",
    "# Â¿Todos los datos esperados estÃ¡n presentes?\n",
    "\n",
    "# Ejemplo: Dataset de clientes\n",
    "df_clientes = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'nombre': ['Ana', 'Bob', None, 'Diana', 'Eva'],  # 1 null\n",
    "    'email': ['a@x.com', None, 'c@x.com', 'd@x.com', None],  # 2 nulls\n",
    "    'telefono': ['111', '222', '333', None, '555']  # 1 null\n",
    "})\n",
    "\n",
    "# MÃ©trica: % de valores no-null\n",
    "completitud = (1 - df_clientes.isnull().sum().sum() / df_clientes.size) * 100\n",
    "# = (1 - 4 / 20) * 100 = 80%\n",
    "\n",
    "# SLA tÃ­pico: > 95% para campos crÃ­ticos, > 85% para opcionales\n",
    "```\n",
    "\n",
    "**Causas de incompletitud:**\n",
    "- Campos opcionales en formularios\n",
    "- Errores en integraciÃ³n de sistemas\n",
    "- Cambios de schema sin backfill\n",
    "\n",
    "**RemediaciÃ³n:**\n",
    "```python\n",
    "# ImputaciÃ³n inteligente\n",
    "df['telefono'] = df['telefono'].fillna('UNKNOWN')\n",
    "\n",
    "# Alertas automÃ¡ticas\n",
    "if completitud < 95:\n",
    "    send_slack_alert(f\"Completitud baja: {completitud}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. UNICIDAD (Uniqueness):**\n",
    "\n",
    "```python\n",
    "# Â¿Los registros son Ãºnicos segÃºn business key?\n",
    "\n",
    "# Ejemplo: Duplicados por error de ETL\n",
    "df_ventas = pd.DataFrame({\n",
    "    'venta_id': [1, 2, 2, 3],  # venta_id=2 duplicado\n",
    "    'cliente': ['A', 'B', 'B', 'C'],\n",
    "    'total': [100, 200, 200, 150]\n",
    "})\n",
    "\n",
    "# MÃ©trica\n",
    "duplicados = df_ventas.duplicated(subset=['venta_id']).sum()  # 1\n",
    "unicidad = (1 - duplicados / len(df_ventas)) * 100  # 75%\n",
    "\n",
    "# Identificar duplicados\n",
    "df_ventas[df_ventas.duplicated(subset=['venta_id'], keep=False)]\n",
    "```\n",
    "\n",
    "**Causas de duplicados:**\n",
    "- Retry logic sin idempotencia\n",
    "- Merge de mÃºltiples fuentes sin deduplicaciÃ³n\n",
    "- Race conditions en escrituras concurrentes\n",
    "\n",
    "**RemediaciÃ³n:**\n",
    "```python\n",
    "# DeduplicaciÃ³n con estrategia\n",
    "df_clean = df_ventas.drop_duplicates(subset=['venta_id'], keep='last')\n",
    "\n",
    "# O upsert en DB\n",
    "INSERT INTO ventas (...) VALUES (...)\n",
    "ON CONFLICT (venta_id) DO UPDATE SET ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. VALIDEZ (Validity):**\n",
    "\n",
    "```python\n",
    "# Â¿Los valores cumplen reglas de negocio?\n",
    "\n",
    "# Reglas de negocio\n",
    "reglas = {\n",
    "    'cantidad_positiva': 'cantidad > 0',\n",
    "    'precio_razonable': 'precio_unitario >= 0 and precio_unitario <= 10000',\n",
    "    'descuento_valido': 'descuento_pct >= 0 and descuento_pct <= 100',\n",
    "    'total_coherente': 'total == cantidad * precio_unitario * (1 - descuento_pct/100)'\n",
    "}\n",
    "\n",
    "# ValidaciÃ³n\n",
    "for regla, condicion in reglas.items():\n",
    "    violaciones = (~df.eval(condicion)).sum()\n",
    "    if violaciones > 0:\n",
    "        print(f\"âš ï¸ {regla}: {violaciones} violaciones\")\n",
    "\n",
    "# Ejemplo de violaciones\n",
    "# cantidad: -5 (negativa) â†’ InvÃ¡lida\n",
    "# descuento_pct: 150% â†’ InvÃ¡lida\n",
    "```\n",
    "\n",
    "**Tipos de validaciones:**\n",
    "- **Tipo de dato**: `isinstance(valor, int)`\n",
    "- **Rango**: `0 <= edad <= 120`\n",
    "- **Formato**: `email.match(r'^\\S+@\\S+\\.\\S+$')`\n",
    "- **Referencial**: `cliente_id in clientes.id`\n",
    "- **LÃ³gica de negocio**: `fecha_entrega > fecha_pedido`\n",
    "\n",
    "---\n",
    "\n",
    "**4. CONSISTENCIA (Consistency):**\n",
    "\n",
    "```python\n",
    "# Â¿Los datos son coherentes entre sÃ­ y a lo largo del tiempo?\n",
    "\n",
    "# Inconsistencia cross-field\n",
    "df['total_calculado'] = df['cantidad'] * df['precio_unitario']\n",
    "df['inconsistencia'] = abs(df['total'] - df['total_calculado']) > 0.01\n",
    "\n",
    "# Inconsistencia temporal\n",
    "df['mes_factura'] = pd.to_datetime(df['fecha_factura']).dt.month\n",
    "df['mes_pago'] = pd.to_datetime(df['fecha_pago']).dt.month\n",
    "inconsistentes = df[df['mes_pago'] < df['mes_factura']]  # Pago antes de factura\n",
    "\n",
    "# Inconsistencia entre sistemas\n",
    "df_erp['stock'] != df_warehouse['stock']  # DesincronizaciÃ³n\n",
    "```\n",
    "\n",
    "**Causas:**\n",
    "- Falta de transacciones ACID\n",
    "- SincronizaciÃ³n eventual entre sistemas\n",
    "- Cambios de lÃ³gica de negocio\n",
    "\n",
    "---\n",
    "\n",
    "**5. EXACTITUD (Accuracy):**\n",
    "\n",
    "```python\n",
    "# Â¿Los valores reflejan la realidad?\n",
    "\n",
    "# Ejemplo: Comparar con fuente de verdad (ground truth)\n",
    "df_measured = pd.DataFrame({'temp': [20.1, 25.3, 30.5]})\n",
    "df_ground_truth = pd.DataFrame({'temp': [20.0, 25.0, 30.0]})\n",
    "\n",
    "# RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(((df_measured['temp'] - df_ground_truth['temp'])**2).mean())\n",
    "# RMSE = 0.36Â°C\n",
    "\n",
    "# Exactitud\n",
    "exactitud = 100 - (rmse / df_ground_truth['temp'].mean()) * 100\n",
    "```\n",
    "\n",
    "**DifÃ­cil de medir sin \"ground truth\":**\n",
    "- Requiere auditorÃ­as manuales\n",
    "- Sampling aleatorio + verificaciÃ³n humana\n",
    "- ComparaciÃ³n con fuentes externas confiables\n",
    "\n",
    "---\n",
    "\n",
    "**6. ACTUALIDAD (Timeliness/Freshness):**\n",
    "\n",
    "```python\n",
    "# Â¿Los datos son suficientemente recientes?\n",
    "\n",
    "# Ejemplo\n",
    "hoy = pd.Timestamp.now()\n",
    "df['lag'] = (hoy - pd.to_datetime(df['fecha_ingestion'])).dt.total_seconds() / 3600\n",
    "\n",
    "# SLA: Datos crÃ­ticos < 1 hora\n",
    "df_criticos = df[df['categoria'] == 'critical']\n",
    "sla_breach = df_criticos[df_criticos['lag'] > 1]\n",
    "\n",
    "print(f\"SLA breaches: {len(sla_breach)} datasets\")\n",
    "```\n",
    "\n",
    "**SLAs tÃ­picos:**\n",
    "- **Real-time**: < 1 minuto (fraud detection)\n",
    "- **Near real-time**: < 15 minutos (operational dashboards)\n",
    "- **Batch**: < 24 horas (reporting)\n",
    "- **Archive**: > 7 dÃ­as (historical analysis)\n",
    "\n",
    "---\n",
    "\n",
    "**Data Quality Score Agregado:**\n",
    "\n",
    "```python\n",
    "def calculate_dq_score(df, rules):\n",
    "    \"\"\"Score compuesto 0-100\"\"\"\n",
    "    \n",
    "    scores = {\n",
    "        'completitud': (1 - df.isnull().sum().sum() / df.size) * 100,\n",
    "        'unicidad': (1 - df.duplicated().sum() / len(df)) * 100,\n",
    "        'validez': calcular_validez(df, rules),\n",
    "        'consistencia': calcular_consistencia(df),\n",
    "        'exactitud': 95,  # Requiere ground truth\n",
    "        'actualidad': calcular_freshness(df, 'fecha')\n",
    "    }\n",
    "    \n",
    "    # Weighted average (ponderado segÃºn criticidad)\n",
    "    weights = {\n",
    "        'completitud': 0.25,\n",
    "        'unicidad': 0.20,\n",
    "        'validez': 0.25,\n",
    "        'consistencia': 0.15,\n",
    "        'exactitud': 0.10,\n",
    "        'actualidad': 0.05\n",
    "    }\n",
    "    \n",
    "    dq_score = sum(scores[k] * weights[k] for k in scores)\n",
    "    \n",
    "    return dq_score, scores\n",
    "```\n",
    "\n",
    "**Niveles de Calidad:**\n",
    "- **â­â­â­ Excelente**: 95-100% â†’ Prod-ready\n",
    "- **â­â­ Bueno**: 85-94% â†’ Requiere atenciÃ³n\n",
    "- **â­ Aceptable**: 70-84% â†’ Plan de remediaciÃ³n\n",
    "- **âŒ CrÃ­tico**: < 70% â†’ Bloquear uso\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f59a07",
   "metadata": {},
   "source": [
    "## ğŸ“Š Framework DAMA-DMBOK\n",
    "\n",
    "### Las 11 Ãreas de Conocimiento:\n",
    "\n",
    "1. **Data Governance** - Marco general\n",
    "2. **Data Architecture** - Estructura y diseÃ±o\n",
    "3. **Data Modeling & Design** - Modelado de datos\n",
    "4. **Data Storage & Operations** - Almacenamiento y operaciones\n",
    "5. **Data Security** - Seguridad\n",
    "6. **Data Integration & Interoperability** - IntegraciÃ³n\n",
    "7. **Document & Content Management** - GestiÃ³n documental\n",
    "8. **Reference & Master Data** - Datos maestros\n",
    "9. **Data Warehousing & BI** - Almacenes y BI\n",
    "10. **Metadata Management** - GestiÃ³n de metadatos\n",
    "11. **Data Quality** - Calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas\")\n",
    "print(f\"ğŸ“… SesiÃ³n iniciada: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12529fc",
   "metadata": {},
   "source": [
    "## ğŸ” Dimensiones de Calidad de Datos\n",
    "\n",
    "### Las 6 Dimensiones CrÃ­ticas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c1603",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ **Data Quality Framework: ImplementaciÃ³n Productiva**\n",
    "\n",
    "**Â¿Por quÃ© un Framework y no validaciones ad-hoc?**\n",
    "\n",
    "```python\n",
    "# âŒ Enfoque ad-hoc (no escalable)\n",
    "if df['precio'].isnull().sum() > 0:\n",
    "    print(\"Hay nulls en precio\")\n",
    "if len(df[df['cantidad'] < 0]) > 0:\n",
    "    print(\"Cantidades negativas\")\n",
    "# ... 50 validaciones mÃ¡s dispersas en el cÃ³digo\n",
    "\n",
    "# âœ… Enfoque Framework (escalable, reusable)\n",
    "dq_framework = DataQualityFramework(df, \"ventas\")\n",
    "dq_framework.evaluar_completitud()\n",
    "dq_framework.evaluar_validez(business_rules)\n",
    "dq_framework.generar_reporte_completo()\n",
    "```\n",
    "\n",
    "**Componentes del Framework:**\n",
    "\n",
    "```\n",
    "DataQualityFramework\n",
    "â”œâ”€â”€ __init__()              â†’ InicializaciÃ³n con dataset\n",
    "â”œâ”€â”€ evaluar_completitud()   â†’ DimensiÃ³n 1\n",
    "â”œâ”€â”€ evaluar_unicidad()      â†’ DimensiÃ³n 2\n",
    "â”œâ”€â”€ evaluar_validez()       â†’ DimensiÃ³n 3\n",
    "â”œâ”€â”€ evaluar_consistencia()  â†’ DimensiÃ³n 4\n",
    "â”œâ”€â”€ evaluar_exactitud()     â†’ DimensiÃ³n 5\n",
    "â”œâ”€â”€ evaluar_actualidad()    â†’ DimensiÃ³n 6\n",
    "â”œâ”€â”€ generar_reporte()       â†’ JSON output\n",
    "â””â”€â”€ _clasificar_calidad()   â†’ Score â†’ Rating\n",
    "```\n",
    "\n",
    "**Arquitectura de Calidad en Pipelines:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         SOURCE SYSTEM (API/DB)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚   RAW LAYER      â”‚  â† Landing zone (sin validar)\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚  DATA QUALITY CHECKS     â”‚\n",
    "       â”‚  (DataQualityFramework)  â”‚\n",
    "       â”‚                          â”‚\n",
    "       â”‚  IF score < threshold:   â”‚\n",
    "       â”‚    â†’ DLQ (Dead Letter)   â”‚\n",
    "       â”‚    â†’ Alert team          â”‚\n",
    "       â”‚    â†’ Block downstream    â”‚\n",
    "       â”‚  ELSE:                   â”‚\n",
    "       â”‚    â†’ Continue pipeline   â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚  TRUSTED LAYER   â”‚  â† Datos validados\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ REFINED LAYER    â”‚  â† Datos refinados\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Integration con Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "\n",
    "def check_data_quality(**context):\n",
    "    df = context['ti'].xcom_pull(task_ids='extract_data')\n",
    "    \n",
    "    dq = DataQualityFramework(df, \"ventas_daily\")\n",
    "    dq.evaluar_completitud()\n",
    "    dq.evaluar_unicidad(['venta_id'])\n",
    "    dq.evaluar_validez(business_rules)\n",
    "    \n",
    "    report = dq.generar_reporte_completo()\n",
    "    context['ti'].xcom_push(key='dq_score', value=report['score_general'])\n",
    "    \n",
    "    return 'pass_quality' if report['score_general'] >= 90 else 'fail_quality'\n",
    "\n",
    "with DAG('pipeline_with_dq', ...) as dag:\n",
    "    extract = PythonOperator(task_id='extract_data', ...)\n",
    "    \n",
    "    quality_check = BranchPythonOperator(\n",
    "        task_id='quality_check',\n",
    "        python_callable=check_data_quality\n",
    "    )\n",
    "    \n",
    "    pass_quality = PythonOperator(task_id='pass_quality', ...)\n",
    "    fail_quality = PythonOperator(task_id='fail_quality', ...)  # Send alert\n",
    "    \n",
    "    extract >> quality_check >> [pass_quality, fail_quality]\n",
    "```\n",
    "\n",
    "**Great Expectations Integration (Alternative):**\n",
    "\n",
    "```python\n",
    "import great_expectations as gx\n",
    "\n",
    "# Setup\n",
    "context = gx.get_context()\n",
    "suite = context.create_expectation_suite(\"ventas_suite\")\n",
    "\n",
    "# Expectations (equivalente a nuestras dimensiones)\n",
    "validator = context.sources.pandas_default.read_dataframe(df)\n",
    "\n",
    "# Completitud\n",
    "validator.expect_column_values_to_not_be_null(\"precio\")\n",
    "\n",
    "# Unicidad\n",
    "validator.expect_column_values_to_be_unique(\"venta_id\")\n",
    "\n",
    "# Validez (rango)\n",
    "validator.expect_column_values_to_be_between(\"cantidad\", min_value=0, max_value=1000)\n",
    "\n",
    "# Consistencia\n",
    "validator.expect_column_pair_values_A_to_be_greater_than_B(\n",
    "    column_A=\"fecha_entrega\", \n",
    "    column_B=\"fecha_pedido\"\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "results = validator.validate()\n",
    "print(results.success)  # True/False\n",
    "```\n",
    "\n",
    "**ComparaciÃ³n de Herramientas:**\n",
    "\n",
    "| Feature | Custom Framework | Great Expectations | dbt tests |\n",
    "|---------|------------------|-------------------|-----------|\n",
    "| **Setup** | Simple (1 clase) | Complejo (contexto, suite) | Medio (SQL) |\n",
    "| **Flexibilidad** | Alta | Media (extensible) | Media |\n",
    "| **VisualizaciÃ³n** | DIY | Data Docs automÃ¡tico | dbt Cloud |\n",
    "| **IntegraciÃ³n** | Manual | Airflow/Prefect | dbt native |\n",
    "| **Learning Curve** | Bajo | Alto | Medio |\n",
    "| **Best For** | Custom rules | Enterprise governance | SQL-centric teams |\n",
    "\n",
    "**Logging y Observability:**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "# Structured logging\n",
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def evaluar_completitud(self):\n",
    "    logger.info(\"DQ check started\", extra={\n",
    "        'check_type': 'completitud',\n",
    "        'dataset': self.nombre_dataset,\n",
    "        'total_records': len(self.df)\n",
    "    })\n",
    "    \n",
    "    # ... evaluaciÃ³n ...\n",
    "    \n",
    "    logger.info(\"DQ check completed\", extra={\n",
    "        'check_type': 'completitud',\n",
    "        'score': completitud_pct,\n",
    "        'passed': completitud_pct >= 95\n",
    "    })\n",
    "    \n",
    "    return completitud_pct\n",
    "```\n",
    "\n",
    "**MÃ©tricas para Prometheus:**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "dq_checks_total = Counter('dq_checks_total', 'Total DQ checks', ['dataset', 'dimension'])\n",
    "dq_score = Gauge('dq_score', 'DQ score 0-100', ['dataset'])\n",
    "dq_violations = Counter('dq_violations_total', 'Total violations', ['dataset', 'rule'])\n",
    "\n",
    "def evaluar_completitud(self):\n",
    "    dq_checks_total.labels(dataset=self.nombre_dataset, dimension='completitud').inc()\n",
    "    \n",
    "    # ... evaluaciÃ³n ...\n",
    "    \n",
    "    dq_score.labels(dataset=self.nombre_dataset).set(completitud_pct)\n",
    "    \n",
    "    if completitud_pct < 95:\n",
    "        dq_violations.labels(dataset=self.nombre_dataset, rule='completitud_threshold').inc()\n",
    "```\n",
    "\n",
    "**Testing del Framework:**\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_completitud_100_percent():\n",
    "    df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_completitud()\n",
    "    assert score == 100.0\n",
    "\n",
    "def test_unicidad_detecta_duplicados():\n",
    "    df = pd.DataFrame({'id': [1, 2, 2, 3]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_unicidad(['id'])\n",
    "    assert score == 75.0  # 3/4 Ãºnicos\n",
    "\n",
    "@pytest.mark.parametrize(\"cantidad,expected_valid\", [\n",
    "    (5, True),\n",
    "    (-1, False),\n",
    "    (0, False)\n",
    "])\n",
    "def test_validez_cantidad_positiva(cantidad, expected_valid):\n",
    "    df = pd.DataFrame({'cantidad': [cantidad]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_validez({'cantidad_positiva': 'cantidad > 0'})\n",
    "    assert (score == 100.0) == expected_valid\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5620f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityFramework:\n",
    "    \"\"\"\n",
    "    Framework para evaluar calidad de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, nombre_dataset: str):\n",
    "        self.df = df\n",
    "        self.nombre_dataset = nombre_dataset\n",
    "        self.reporte = {\n",
    "            'dataset': nombre_dataset,\n",
    "            'fecha_evaluacion': datetime.now().isoformat(),\n",
    "            'total_registros': len(df),\n",
    "            'total_columnas': len(df.columns),\n",
    "            'dimensiones': {}\n",
    "        }\n",
    "    \n",
    "    def evaluar_completitud(self):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 1: Completitud\n",
    "        Â¿Los datos estÃ¡n completos?\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š Evaluando COMPLETITUD...\")\n",
    "        \n",
    "        total_valores = self.df.size\n",
    "        valores_nulos = self.df.isnull().sum().sum()\n",
    "        completitud_pct = ((total_valores - valores_nulos) / total_valores) * 100\n",
    "        \n",
    "        # Por columna\n",
    "        completitud_cols = {}\n",
    "        for col in self.df.columns:\n",
    "            pct = (1 - self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            completitud_cols[col] = round(pct, 2)\n",
    "        \n",
    "        self.reporte['dimensiones']['completitud'] = {\n",
    "            'score': round(completitud_pct, 2),\n",
    "            'valores_nulos': int(valores_nulos),\n",
    "            'por_columna': completitud_cols,\n",
    "            'aprobado': completitud_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {completitud_pct:.2f}%\")\n",
    "        print(f\"   âœ“ Valores nulos: {valores_nulos:,}\")\n",
    "        return completitud_pct\n",
    "    \n",
    "    def evaluar_unicidad(self, columnas_clave: list):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 2: Unicidad\n",
    "        Â¿Los registros son Ãºnicos?\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”‘ Evaluando UNICIDAD...\")\n",
    "        \n",
    "        duplicados = self.df.duplicated(subset=columnas_clave).sum()\n",
    "        unicidad_pct = ((len(self.df) - duplicados) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['unicidad'] = {\n",
    "            'score': round(unicidad_pct, 2),\n",
    "            'duplicados': int(duplicados),\n",
    "            'columnas_evaluadas': columnas_clave,\n",
    "            'aprobado': duplicados == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {unicidad_pct:.2f}%\")\n",
    "        print(f\"   âœ“ Duplicados: {duplicados}\")\n",
    "        return unicidad_pct\n",
    "    \n",
    "    def evaluar_validez(self, reglas: dict):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 3: Validez\n",
    "        Â¿Los datos cumplen reglas de negocio?\n",
    "        \"\"\"\n",
    "        print(\"\\nâœ… Evaluando VALIDEZ...\")\n",
    "        \n",
    "        violaciones = {}\n",
    "        total_registros = len(self.df)\n",
    "        registros_validos = total_registros\n",
    "        \n",
    "        for regla_nombre, regla_condicion in reglas.items():\n",
    "            try:\n",
    "                registros_invalidos = (~self.df.eval(regla_condicion)).sum()\n",
    "                violaciones[regla_nombre] = int(registros_invalidos)\n",
    "                registros_validos -= registros_invalidos\n",
    "                print(f\"   â€¢ {regla_nombre}: {registros_invalidos} violaciones\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Error en regla '{regla_nombre}': {e}\")\n",
    "        \n",
    "        validez_pct = (registros_validos / total_registros) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['validez'] = {\n",
    "            'score': round(validez_pct, 2),\n",
    "            'violaciones': violaciones,\n",
    "            'reglas_evaluadas': list(reglas.keys()),\n",
    "            'aprobado': validez_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {validez_pct:.2f}%\")\n",
    "        return validez_pct\n",
    "    \n",
    "    def evaluar_consistencia(self, columnas_relacionadas: dict):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 4: Consistencia\n",
    "        Â¿Los datos son consistentes entre sÃ­?\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”„ Evaluando CONSISTENCIA...\")\n",
    "        \n",
    "        inconsistencias = {}\n",
    "        \n",
    "        for nombre, condicion in columnas_relacionadas.items():\n",
    "            try:\n",
    "                count = (~self.df.eval(condicion)).sum()\n",
    "                inconsistencias[nombre] = int(count)\n",
    "                print(f\"   â€¢ {nombre}: {count} inconsistencias\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Error en validaciÃ³n '{nombre}': {e}\")\n",
    "        \n",
    "        total_inconsistencias = sum(inconsistencias.values())\n",
    "        consistencia_pct = ((len(self.df) - total_inconsistencias) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['consistencia'] = {\n",
    "            'score': round(consistencia_pct, 2),\n",
    "            'inconsistencias': inconsistencias,\n",
    "            'aprobado': consistencia_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {consistencia_pct:.2f}%\")\n",
    "        return consistencia_pct\n",
    "    \n",
    "    def evaluar_exactitud(self, columna_numerica: str, rango_esperado: tuple):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 5: Exactitud\n",
    "        Â¿Los valores son correctos?\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ¯ Evaluando EXACTITUD...\")\n",
    "        \n",
    "        min_val, max_val = rango_esperado\n",
    "        fuera_rango = ((self.df[columna_numerica] < min_val) | \n",
    "                       (self.df[columna_numerica] > max_val)).sum()\n",
    "        \n",
    "        exactitud_pct = ((len(self.df) - fuera_rango) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['exactitud'] = {\n",
    "            'score': round(exactitud_pct, 2),\n",
    "            'fuera_de_rango': int(fuera_rango),\n",
    "            'columna_evaluada': columna_numerica,\n",
    "            'rango_esperado': rango_esperado,\n",
    "            'aprobado': fuera_rango == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {exactitud_pct:.2f}%\")\n",
    "        print(f\"   âœ“ Fuera de rango: {fuera_rango}\")\n",
    "        return exactitud_pct\n",
    "    \n",
    "    def evaluar_actualidad(self, columna_fecha: str, dias_maximos: int):\n",
    "        \"\"\"\n",
    "        DimensiÃ³n 6: Actualidad\n",
    "        Â¿Los datos son recientes?\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“… Evaluando ACTUALIDAD...\")\n",
    "        \n",
    "        hoy = pd.Timestamp.now()\n",
    "        self.df[columna_fecha] = pd.to_datetime(self.df[columna_fecha])\n",
    "        antiguos = (hoy - self.df[columna_fecha]) > timedelta(days=dias_maximos)\n",
    "        registros_antiguos = antiguos.sum()\n",
    "        \n",
    "        actualidad_pct = ((len(self.df) - registros_antiguos) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['actualidad'] = {\n",
    "            'score': round(actualidad_pct, 2),\n",
    "            'registros_antiguos': int(registros_antiguos),\n",
    "            'dias_maximos': dias_maximos,\n",
    "            'aprobado': actualidad_pct >= 80\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Score: {actualidad_pct:.2f}%\")\n",
    "        print(f\"   âœ“ Registros antiguos: {registros_antiguos}\")\n",
    "        return actualidad_pct\n",
    "    \n",
    "    def generar_reporte_completo(self):\n",
    "        \"\"\"\n",
    "        Genera reporte ejecutivo de calidad\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ğŸ“Š REPORTE DE CALIDAD DE DATOS: {self.nombre_dataset}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calcular score general\n",
    "        scores = [d['score'] for d in self.reporte['dimensiones'].values() if 'score' in d]\n",
    "        score_general = sum(scores) / len(scores) if scores else 0\n",
    "        \n",
    "        self.reporte['score_general'] = round(score_general, 2)\n",
    "        self.reporte['nivel_calidad'] = self._clasificar_calidad(score_general)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Score General: {score_general:.2f}%\")\n",
    "        print(f\"ğŸ“ˆ Nivel de Calidad: {self.reporte['nivel_calidad']}\")\n",
    "        print(f\"\\nğŸ“‹ Detalles por DimensiÃ³n:\")\n",
    "        \n",
    "        for dimension, datos in self.reporte['dimensiones'].items():\n",
    "            status = \"âœ…\" if datos.get('aprobado', False) else \"âŒ\"\n",
    "            print(f\"   {status} {dimension.upper()}: {datos.get('score', 'N/A')}%\")\n",
    "        \n",
    "        # Guardar reporte\n",
    "        output_path = f\"../datasets/processed/reporte_calidad_{self.nombre_dataset}.json\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.reporte, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Reporte guardado en: {output_path}\")\n",
    "        return self.reporte\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clasificar_calidad(score):\n",
    "        \"\"\"Clasifica el nivel de calidad segÃºn el score\"\"\"\n",
    "        if score >= 95:\n",
    "            return \"â­â­â­ EXCELENTE\"\n",
    "        elif score >= 85:\n",
    "            return \"â­â­ BUENO\"\n",
    "        elif score >= 70:\n",
    "            return \"â­ ACEPTABLE\"\n",
    "        else:\n",
    "            return \"âŒ CRÃTICO\"\n",
    "\n",
    "print(\"âœ… DataQualityFramework definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213b42",
   "metadata": {},
   "source": [
    "## ğŸ§ª Caso PrÃ¡ctico: EvaluaciÃ³n de Calidad\n",
    "\n",
    "Vamos a evaluar un dataset de ventas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5b060",
   "metadata": {},
   "source": [
    "### ğŸ§ª **Caso PrÃ¡ctico: From Bad Data to Gold Standard**\n",
    "\n",
    "**Escenario Real:**\n",
    "\n",
    "Empresa de e-commerce con 1M+ ventas diarias. El equipo de Analytics reporta:\n",
    "- âŒ Dashboards muestran ventas negativas\n",
    "- âŒ Duplicados causan sobreconteo de revenue\n",
    "- âŒ Missing data en ~20% de registros\n",
    "- âŒ Descuentos > 100% generan alertas falsas\n",
    "\n",
    "**Impacto:** CFO no confÃ­a en reportes â†’ Decisiones basadas en \"feeling\"\n",
    "\n",
    "**SoluciÃ³n: Implementar DQ Framework**\n",
    "\n",
    "**Step 1: Assessment (Baseline)**\n",
    "\n",
    "```python\n",
    "# Snapshot inicial\n",
    "dq_initial = DataQualityFramework(df_ventas_raw, \"ventas_oct_2025\")\n",
    "\n",
    "# Run all dimensions\n",
    "dq_initial.evaluar_completitud()        # 80%  âŒ\n",
    "dq_initial.evaluar_unicidad(['venta_id'])  # 99%  âš ï¸ (10 duplicados)\n",
    "dq_initial.evaluar_validez(business_rules)  # 75%  âŒ\n",
    "dq_initial.evaluar_consistencia({...})   # 85%  âš ï¸\n",
    "\n",
    "report_initial = dq_initial.generar_reporte_completo()\n",
    "# Score general: 82% â†’ â­ ACEPTABLE (pero inaceptable para prod)\n",
    "```\n",
    "\n",
    "**Step 2: Data Profiling (Root Cause Analysis)**\n",
    "\n",
    "```python\n",
    "# Â¿QuÃ© columnas tienen mÃ¡s problemas?\n",
    "completitud_por_columna = report_initial['dimensiones']['completitud']['por_columna']\n",
    "worst_columns = sorted(completitud_por_columna.items(), key=lambda x: x[1])[:5]\n",
    "\n",
    "print(\"Top 5 columnas con mÃ¡s nulls:\")\n",
    "for col, pct in worst_columns:\n",
    "    print(f\"  - {col}: {100-pct:.1f}% nulls\")\n",
    "    \n",
    "# Output:\n",
    "#   - producto: 15% nulls         â†’ API devuelve null cuando SKU invÃ¡lido\n",
    "#   - descuento_pct: 10% nulls    â†’ Campo opcional en checkout\n",
    "#   - email_cliente: 8% nulls     â†’ Guests no proveen email\n",
    "#   - fecha_entrega: 20% nulls    â†’ Solo para Ã³rdenes enviadas\n",
    "#   - cupon_id: 60% nulls         â†’ MayorÃ­a de ventas sin cupÃ³n\n",
    "```\n",
    "\n",
    "**Step 3: Remediation Plan**\n",
    "\n",
    "```python\n",
    "# Plan de remediaciÃ³n priorizado\n",
    "remediation_plan = {\n",
    "    'P0_Critical': [\n",
    "        {\n",
    "            'issue': 'Cantidades negativas',\n",
    "            'rule': 'cantidad > 0',\n",
    "            'fix': 'Rechazar en validaciÃ³n pre-insert',\n",
    "            'owner': 'Backend team'\n",
    "        },\n",
    "        {\n",
    "            'issue': 'Duplicados por retry',\n",
    "            'rule': 'venta_id unique',\n",
    "            'fix': 'Implement idempotency key',\n",
    "            'owner': 'Platform team'\n",
    "        }\n",
    "    ],\n",
    "    'P1_High': [\n",
    "        {\n",
    "            'issue': 'Missing producto',\n",
    "            'rule': 'producto not null',\n",
    "            'fix': 'Lookup SKU en master data antes de insert',\n",
    "            'owner': 'Data team'\n",
    "        },\n",
    "        {\n",
    "            'issue': 'Descuentos > 100%',\n",
    "            'rule': 'descuento_pct between 0 and 100',\n",
    "            'fix': 'ValidaciÃ³n en frontend + backend',\n",
    "            'owner': 'Frontend team'\n",
    "        }\n",
    "    ],\n",
    "    'P2_Medium': [\n",
    "        {\n",
    "            'issue': 'Email null',\n",
    "            'rule': 'email not null for registered users',\n",
    "            'fix': 'Default to \"guest@example.com\" for guests',\n",
    "            'owner': 'Data team'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 4: Implementation (Data Cleaning)**\n",
    "\n",
    "```python\n",
    "def clean_ventas_data(df_raw):\n",
    "    \"\"\"Pipeline de limpieza aplicando remediaciones\"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Fix 1: Remover cantidades negativas (P0)\n",
    "    before = len(df)\n",
    "    df = df[df['cantidad'] > 0]\n",
    "    logger.info(f\"Removed {before - len(df)} records with cantidad <= 0\")\n",
    "    \n",
    "    # Fix 2: Deduplicar por venta_id (P0)\n",
    "    df = df.drop_duplicates(subset=['venta_id'], keep='last')\n",
    "    logger.info(f\"Removed {before - len(df)} duplicates\")\n",
    "    \n",
    "    # Fix 3: Imputar producto missing (P1)\n",
    "    df.loc[df['producto'].isnull(), 'producto'] = 'UNKNOWN'\n",
    "    \n",
    "    # Fix 4: Caps descuentos (P1)\n",
    "    df['descuento_pct'] = df['descuento_pct'].clip(lower=0, upper=100)\n",
    "    \n",
    "    # Fix 5: Imputar email (P2)\n",
    "    df.loc[df['email_cliente'].isnull() & df['tipo_cliente'] == 'guest', \n",
    "           'email_cliente'] = 'guest@example.com'\n",
    "    \n",
    "    # Fix 6: Recalcular total para consistencia\n",
    "    df['total'] = (df['cantidad'] * df['precio_unitario'] * \n",
    "                   (1 - df['descuento_pct'] / 100))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_ventas_clean = clean_ventas_data(df_ventas_raw)\n",
    "```\n",
    "\n",
    "**Step 5: Re-assessment (Validation)**\n",
    "\n",
    "```python\n",
    "# Re-evaluar post-limpieza\n",
    "dq_final = DataQualityFramework(df_ventas_clean, \"ventas_oct_2025_clean\")\n",
    "\n",
    "dq_final.evaluar_completitud()        # 98%  âœ…\n",
    "dq_final.evaluar_unicidad(['venta_id'])  # 100%  âœ…\n",
    "dq_final.evaluar_validez(business_rules)  # 99%  âœ…\n",
    "dq_final.evaluar_consistencia({...})   # 99%  âœ…\n",
    "\n",
    "report_final = dq_final.generar_reporte_completo()\n",
    "# Score general: 97.5% â†’ â­â­â­ EXCELENTE\n",
    "```\n",
    "\n",
    "**Step 6: Monitoring (Continuous)**\n",
    "\n",
    "```python\n",
    "# Airflow DAG diario\n",
    "@dag(schedule_interval='@daily')\n",
    "def ventas_dq_monitoring():\n",
    "    \n",
    "    @task\n",
    "    def extract_ventas():\n",
    "        return query_db(\"SELECT * FROM ventas WHERE date = CURRENT_DATE\")\n",
    "    \n",
    "    @task\n",
    "    def run_dq_checks(df):\n",
    "        dq = DataQualityFramework(df, f\"ventas_{datetime.now().date()}\")\n",
    "        report = dq.generar_reporte_completo()\n",
    "        \n",
    "        # Store metrics in TimeSeries DB\n",
    "        store_metrics(report)\n",
    "        \n",
    "        # Alert if regression\n",
    "        if report['score_general'] < 95:\n",
    "            send_slack_alert(\n",
    "                channel=\"#data-quality\",\n",
    "                message=f\"âš ï¸ DQ score dropped to {report['score_general']}%\",\n",
    "                report_url=f\"/reports/{report['dataset']}\"\n",
    "            )\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    df = extract_ventas()\n",
    "    run_dq_checks(df)\n",
    "\n",
    "dag = ventas_dq_monitoring()\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "\n",
    "| Metric | Before | After | Improvement |\n",
    "|--------|--------|-------|-------------|\n",
    "| **DQ Score** | 82% | 97.5% | +15.5% |\n",
    "| **Duplicates** | 10/day | 0 | 100% |\n",
    "| **Completitud** | 80% | 98% | +18% |\n",
    "| **Validez** | 75% | 99% | +24% |\n",
    "| **CFO Confidence** | âŒ | âœ… | Priceless |\n",
    "\n",
    "**Business Impact:**\n",
    "- âœ… Revenue reporting accuracy: +$500K/month (false negatives eliminados)\n",
    "- âœ… Compliance audit: Pass (antes Fail)\n",
    "- âœ… Data analyst productivity: +30% (menos tiempo limpiando)\n",
    "- âœ… Executive trust: \"Now I can make data-driven decisions\"\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7273b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset de ejemplo con problemas de calidad intencionales\n",
    "np.random.seed(42)\n",
    "\n",
    "n_registros = 1000\n",
    "\n",
    "df_ventas = pd.DataFrame({\n",
    "    'venta_id': range(1, n_registros + 1),\n",
    "    'cliente_id': np.random.randint(1, 200, n_registros),\n",
    "    'producto': np.random.choice(['Laptop', 'Mouse', 'Teclado', 'Monitor', None], n_registros),\n",
    "    'cantidad': np.random.randint(-5, 50, n_registros),  # Algunas cantidades negativas\n",
    "    'precio_unitario': np.random.uniform(10, 2000, n_registros),\n",
    "    'descuento_pct': np.random.uniform(0, 100, n_registros),  # Algunos > 100%\n",
    "    'total': np.random.uniform(0, 5000, n_registros),\n",
    "    'fecha_venta': pd.date_range(start='2023-01-01', periods=n_registros, freq='H'),\n",
    "    'estado': np.random.choice(['Completada', 'Pendiente', 'Cancelada', None], n_registros)\n",
    "})\n",
    "\n",
    "# Introducir duplicados intencionales\n",
    "df_ventas = pd.concat([df_ventas, df_ventas.iloc[:10]], ignore_index=True)\n",
    "\n",
    "# Introducir algunos nulls adicionales\n",
    "df_ventas.loc[np.random.choice(df_ventas.index, 50), 'precio_unitario'] = np.nan\n",
    "\n",
    "print(f\"ğŸ“Š Dataset creado: {len(df_ventas)} registros, {len(df_ventas.columns)} columnas\")\n",
    "print(f\"\\nğŸ” Primeras filas:\")\n",
    "print(df_ventas.head())\n",
    "print(f\"\\nğŸ“ˆ Info del dataset:\")\n",
    "print(df_ventas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar framework\n",
    "dq = DataQualityFramework(df_ventas, 'ventas_ecommerce')\n",
    "\n",
    "# Evaluar todas las dimensiones\n",
    "print(\"ğŸš€ Iniciando evaluaciÃ³n de calidad...\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "dq.evaluar_completitud()\n",
    "\n",
    "# 2. Unicidad\n",
    "dq.evaluar_unicidad(columnas_clave=['venta_id'])\n",
    "\n",
    "# 3. Validez\n",
    "reglas_negocio = {\n",
    "    'cantidad_positiva': 'cantidad > 0',\n",
    "    'descuento_valido': 'descuento_pct >= 0 and descuento_pct <= 100',\n",
    "    'total_positivo': 'total > 0'\n",
    "}\n",
    "dq.evaluar_validez(reglas_negocio)\n",
    "\n",
    "# 4. Consistencia\n",
    "relaciones = {\n",
    "    'total_coherente': 'total >= (precio_unitario * cantidad * (1 - descuento_pct/100)) * 0.95',\n",
    "}\n",
    "dq.evaluar_consistencia(relaciones)\n",
    "\n",
    "# 5. Exactitud\n",
    "dq.evaluar_exactitud('precio_unitario', (5, 3000))\n",
    "\n",
    "# 6. Actualidad\n",
    "dq.evaluar_actualidad('fecha_venta', dias_maximos=365)\n",
    "\n",
    "# Generar reporte completo\n",
    "reporte = dq.generar_reporte_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e327a",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Data Lineage - Trazabilidad de Datos\n",
    "\n",
    "ImplementaciÃ³n de un sistema bÃ¡sico de lineage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35031a",
   "metadata": {},
   "source": [
    "### ğŸ”¬ **Great Expectations: Enterprise Data Quality**\n",
    "\n",
    "**Â¿CuÃ¡ndo usar Great Expectations vs Custom Framework?**\n",
    "\n",
    "| Escenario | RecomendaciÃ³n |\n",
    "|-----------|---------------|\n",
    "| **Startup/SMB** | Custom Framework (simple, rÃ¡pido) |\n",
    "| **Enterprise** | Great Expectations (governance completo) |\n",
    "| **Team < 5** | Custom |\n",
    "| **Team > 10** | Great Expectations |\n",
    "| **SQL-first team** | dbt tests |\n",
    "| **Python-first team** | Great Expectations o Custom |\n",
    "\n",
    "**Great Expectations Architecture:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              DATA CONTEXT (GX Project)                â”‚\n",
    "â”‚                                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  EXPECTATIONS   â”‚  â”‚   DATASOURCES   â”‚          â”‚\n",
    "â”‚  â”‚  (Rules/Tests)  â”‚  â”‚   (Connections) â”‚          â”‚\n",
    "â”‚  â”‚                 â”‚  â”‚                 â”‚          â”‚\n",
    "â”‚  â”‚  â€¢ Suite 1      â”‚  â”‚  â€¢ Postgres     â”‚          â”‚\n",
    "â”‚  â”‚  â€¢ Suite 2      â”‚  â”‚  â€¢ S3           â”‚          â”‚\n",
    "â”‚  â”‚  â€¢ Suite 3      â”‚  â”‚  â€¢ Pandas DF    â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  CHECKPOINTS    â”‚  â”‚   DATA DOCS     â”‚          â”‚\n",
    "â”‚  â”‚  (Validation    â”‚  â”‚   (Reports)     â”‚          â”‚\n",
    "â”‚  â”‚   Workflows)    â”‚  â”‚                 â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "```python\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "\n",
    "# Initialize context\n",
    "context = gx.get_context()\n",
    "\n",
    "# Add datasource\n",
    "datasource = context.sources.add_pandas(\"my_datasource\")\n",
    "data_asset = datasource.add_dataframe_asset(name=\"ventas_df\")\n",
    "\n",
    "# Create expectation suite\n",
    "suite = context.add_expectation_suite(\"ventas_quality_suite\")\n",
    "\n",
    "# Get validator\n",
    "batch_request = data_asset.build_batch_request(dataframe=df_ventas)\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"ventas_quality_suite\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Expectations (200+ built-in):**\n",
    "\n",
    "```python\n",
    "# COMPLETITUD\n",
    "validator.expect_column_values_to_not_be_null(\n",
    "    column=\"venta_id\",\n",
    "    mostly=1.0  # 100% not null\n",
    ")\n",
    "\n",
    "validator.expect_column_values_to_not_be_null(\n",
    "    column=\"email\",\n",
    "    mostly=0.8  # Al menos 80% not null\n",
    ")\n",
    "\n",
    "# UNICIDAD\n",
    "validator.expect_column_values_to_be_unique(\n",
    "    column=\"venta_id\"\n",
    ")\n",
    "\n",
    "validator.expect_compound_columns_to_be_unique(\n",
    "    column_list=[\"cliente_id\", \"fecha_venta\", \"producto_id\"]\n",
    ")\n",
    "\n",
    "# VALIDEZ (Rango)\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column=\"cantidad\",\n",
    "    min_value=1,\n",
    "    max_value=1000\n",
    ")\n",
    "\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column=\"descuento_pct\",\n",
    "    min_value=0,\n",
    "    max_value=100,\n",
    "    mostly=0.99  # 99% dentro del rango\n",
    ")\n",
    "\n",
    "# VALIDEZ (Valores permitidos)\n",
    "validator.expect_column_values_to_be_in_set(\n",
    "    column=\"estado\",\n",
    "    value_set=[\"Completada\", \"Pendiente\", \"Cancelada\"]\n",
    ")\n",
    "\n",
    "# VALIDEZ (Regex)\n",
    "validator.expect_column_values_to_match_regex(\n",
    "    column=\"email\",\n",
    "    regex=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    ")\n",
    "\n",
    "# VALIDEZ (Tipo de dato)\n",
    "validator.expect_column_values_to_be_of_type(\n",
    "    column=\"total\",\n",
    "    type_=\"float\"\n",
    ")\n",
    "\n",
    "# CONSISTENCIA (Cross-column)\n",
    "validator.expect_column_pair_values_A_to_be_greater_than_B(\n",
    "    column_A=\"fecha_entrega\",\n",
    "    column_B=\"fecha_pedido\"\n",
    ")\n",
    "\n",
    "# EXACTITUD (EstadÃ­sticas)\n",
    "validator.expect_column_mean_to_be_between(\n",
    "    column=\"total\",\n",
    "    min_value=50,\n",
    "    max_value=500\n",
    ")\n",
    "\n",
    "validator.expect_column_stdev_to_be_between(\n",
    "    column=\"total\",\n",
    "    min_value=10,\n",
    "    max_value=200\n",
    ")\n",
    "\n",
    "# ACTUALIDAD\n",
    "validator.expect_column_max_to_be_between(\n",
    "    column=\"fecha_venta\",\n",
    "    min_value=datetime.now() - timedelta(days=1),\n",
    "    max_value=datetime.now()\n",
    ")\n",
    "\n",
    "# Save suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "```\n",
    "\n",
    "**Checkpoint (Automated Validation):**\n",
    "\n",
    "```python\n",
    "# Create checkpoint\n",
    "checkpoint_config = {\n",
    "    \"name\": \"ventas_daily_checkpoint\",\n",
    "    \"config_version\": 1.0,\n",
    "    \"class_name\": \"Checkpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": {\n",
    "                \"datasource_name\": \"my_datasource\",\n",
    "                \"data_asset_name\": \"ventas_df\"\n",
    "            },\n",
    "            \"expectation_suite_name\": \"ventas_quality_suite\"\n",
    "        }\n",
    "    ],\n",
    "    \"action_list\": [\n",
    "        {\n",
    "            \"name\": \"store_validation_result\",\n",
    "            \"action\": {\"class_name\": \"StoreValidationResultAction\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"update_data_docs\",\n",
    "            \"action\": {\"class_name\": \"UpdateDataDocsAction\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"send_slack_notification\",\n",
    "            \"action\": {\n",
    "                \"class_name\": \"SlackNotificationAction\",\n",
    "                \"slack_webhook\": \"https://hooks.slack.com/...\",\n",
    "                \"notify_on\": \"failure\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "checkpoint = context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "# Run checkpoint\n",
    "result = checkpoint.run()\n",
    "\n",
    "# Check result\n",
    "if not result[\"success\"]:\n",
    "    print(\"âŒ Validation failed!\")\n",
    "    for validation in result[\"run_results\"].values():\n",
    "        for expectation in validation[\"validation_result\"][\"results\"]:\n",
    "            if not expectation[\"success\"]:\n",
    "                print(f\"  - {expectation['expectation_config']['expectation_type']}\")\n",
    "                print(f\"    Observed: {expectation['result']['observed_value']}\")\n",
    "else:\n",
    "    print(\"âœ… All validations passed!\")\n",
    "```\n",
    "\n",
    "**Data Docs (Auto-generated Reports):**\n",
    "\n",
    "```python\n",
    "# Build Data Docs\n",
    "context.build_data_docs()\n",
    "\n",
    "# Open in browser\n",
    "context.open_data_docs()\n",
    "\n",
    "# Output: Beautiful HTML report with:\n",
    "# - Expectation suite overview\n",
    "# - Validation results (pass/fail)\n",
    "# - Data profiling statistics\n",
    "# - Historical trends\n",
    "```\n",
    "\n",
    "**Integration con Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "import great_expectations as gx\n",
    "\n",
    "def validate_with_ge(**context):\n",
    "    gx_context = gx.get_context()\n",
    "    \n",
    "    # Get data\n",
    "    df = context['ti'].xcom_pull(task_ids='extract_data')\n",
    "    \n",
    "    # Run checkpoint\n",
    "    checkpoint = gx_context.get_checkpoint(\"ventas_daily_checkpoint\")\n",
    "    result = checkpoint.run(batch_request={\"dataframe\": df})\n",
    "    \n",
    "    # Fail task if validation fails\n",
    "    if not result[\"success\"]:\n",
    "        raise AirflowException(\"Great Expectations validation failed\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "validate_task = PythonOperator(\n",
    "    task_id='validate_data_quality',\n",
    "    python_callable=validate_with_ge\n",
    ")\n",
    "```\n",
    "\n",
    "**Custom Expectations (Extensible):**\n",
    "\n",
    "```python\n",
    "from great_expectations.expectations.expectation import ColumnMapExpectation\n",
    "\n",
    "class ExpectColumnValuesToBePrime(ColumnMapExpectation):\n",
    "    \"\"\"Custom: Validar nÃºmeros primos\"\"\"\n",
    "    \n",
    "    map_metric = \"column_values.prime\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_prime(n):\n",
    "        if n < 2:\n",
    "            return False\n",
    "        for i in range(2, int(n**0.5) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _validate(self, metrics, runtime_configuration, execution_engine):\n",
    "        return self.is_prime(metrics.get(\"column_values\"))\n",
    "\n",
    "# Use\n",
    "validator.expect_column_values_to_be_prime(column=\"id\")\n",
    "```\n",
    "\n",
    "**Profiling (Auto-discovery):**\n",
    "\n",
    "```python\n",
    "# GX can auto-generate expectations\n",
    "from great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler\n",
    "\n",
    "profiler = UserConfigurableProfiler(\n",
    "    profile_dataset=validator,\n",
    "    excluded_expectations=[\n",
    "        \"expect_column_values_to_be_unique\"  # Exclude specific\n",
    "    ]\n",
    ")\n",
    "\n",
    "suite = profiler.build_suite()\n",
    "# Auto-generates ~20-30 expectations based on data profile\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLineageTracker:\n",
    "    \"\"\"\n",
    "    Sistema para rastrear el linaje de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.linaje = []\n",
    "        self.grafo = {}\n",
    "    \n",
    "    def registrar_transformacion(self, \n",
    "                                 origen: str, \n",
    "                                 destino: str, \n",
    "                                 transformacion: str,\n",
    "                                 metadata: dict = None):\n",
    "        \"\"\"\n",
    "        Registra una transformaciÃ³n de datos\n",
    "        \"\"\"\n",
    "        evento = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'origen': origen,\n",
    "            'destino': destino,\n",
    "            'transformacion': transformacion,\n",
    "            'metadata': metadata or {},\n",
    "            'id': hashlib.md5(f\"{origen}{destino}{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        }\n",
    "        \n",
    "        self.linaje.append(evento)\n",
    "        \n",
    "        # Construir grafo\n",
    "        if origen not in self.grafo:\n",
    "            self.grafo[origen] = []\n",
    "        self.grafo[origen].append(destino)\n",
    "        \n",
    "        print(f\"ğŸ“ Registrado: {origen} â†’ {destino} ({transformacion})\")\n",
    "        return evento['id']\n",
    "    \n",
    "    def obtener_linaje_completo(self, entidad: str, direccion='downstream'):\n",
    "        \"\"\"\n",
    "        Obtiene el linaje completo de una entidad\n",
    "        \"\"\"\n",
    "        if direccion == 'downstream':\n",
    "            # Hacia adelante (downstream)\n",
    "            return self._traverse_downstream(entidad)\n",
    "        else:\n",
    "            # Hacia atrÃ¡s (upstream)\n",
    "            return self._traverse_upstream(entidad)\n",
    "    \n",
    "    def _traverse_downstream(self, nodo, visitados=None):\n",
    "        if visitados is None:\n",
    "            visitados = set()\n",
    "        \n",
    "        if nodo in visitados:\n",
    "            return []\n",
    "        \n",
    "        visitados.add(nodo)\n",
    "        camino = [nodo]\n",
    "        \n",
    "        if nodo in self.grafo:\n",
    "            for hijo in self.grafo[nodo]:\n",
    "                camino.extend(self._traverse_downstream(hijo, visitados))\n",
    "        \n",
    "        return camino\n",
    "    \n",
    "    def _traverse_upstream(self, nodo):\n",
    "        # Invertir el grafo para upstream\n",
    "        grafo_invertido = {}\n",
    "        for origen, destinos in self.grafo.items():\n",
    "            for destino in destinos:\n",
    "                if destino not in grafo_invertido:\n",
    "                    grafo_invertido[destino] = []\n",
    "                grafo_invertido[destino].append(origen)\n",
    "        \n",
    "        return self._traverse_downstream(nodo, set())\n",
    "    \n",
    "    def visualizar_linaje(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaciÃ³n simple del linaje\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“Š MAPA DE LINAJE DE DATOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for origen, destinos in self.grafo.items():\n",
    "            print(f\"\\nğŸ”¹ {origen}\")\n",
    "            for destino in destinos:\n",
    "                # Encontrar transformaciÃ³n\n",
    "                trans = next((e for e in self.linaje \n",
    "                            if e['origen'] == origen and e['destino'] == destino), {})\n",
    "                print(f\"   â””â”€â†’ {destino} [{trans.get('transformacion', 'N/A')}]\")\n",
    "    \n",
    "    def exportar_linaje(self, ruta_salida: str):\n",
    "        \"\"\"\n",
    "        Exporta linaje a archivo JSON\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)\n",
    "        \n",
    "        with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'eventos': self.linaje,\n",
    "                'grafo': self.grafo,\n",
    "                'generado': datetime.now().isoformat()\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Linaje exportado a: {ruta_salida}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "tracker = DataLineageTracker()\n",
    "\n",
    "print(\"ğŸ” Simulando pipeline con lineage tracking:\\n\")\n",
    "\n",
    "# Registrar transformaciones\n",
    "tracker.registrar_transformacion(\n",
    "    'raw_ventas_csv',\n",
    "    'stage_ventas',\n",
    "    'extraccion',\n",
    "    {'filas': 1000, 'columnas': 9}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'stage_ventas',\n",
    "    'clean_ventas',\n",
    "    'limpieza',\n",
    "    {'nulos_removidos': 50, 'duplicados_removidos': 10}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'agg_ventas_diarias',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'fecha', 'metricas': ['sum', 'count', 'avg']}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'ventas_por_producto',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'producto'}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'agg_ventas_diarias',\n",
    "    'dashboard_ventas',\n",
    "    'visualizacion',\n",
    "    {'tipo': 'time_series'}\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "tracker.visualizar_linaje()\n",
    "\n",
    "# Exportar\n",
    "tracker.exportar_linaje('../datasets/processed/data_lineage.json')\n",
    "\n",
    "# Consultar linaje\n",
    "print(\"\\nğŸ” Linaje downstream de 'clean_ventas':\")\n",
    "downstream = tracker.obtener_linaje_completo('clean_ventas', 'downstream')\n",
    "print(f\"   {' â†’ '.join(downstream)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c090f1",
   "metadata": {},
   "source": [
    "## ğŸ“ Resumen y Mejores PrÃ¡cticas\n",
    "\n",
    "### âœ… Principios Clave de Data Governance:\n",
    "\n",
    "1. **Proactivo, no Reactivo**\n",
    "   - Validaciones automÃ¡ticas en cada etapa\n",
    "   - PrevenciÃ³n vs correcciÃ³n\n",
    "\n",
    "2. **Cultura de Calidad**\n",
    "   - Responsabilidad compartida\n",
    "   - Data stewards claramente definidos\n",
    "\n",
    "3. **AutomatizaciÃ³n**\n",
    "   - Tests automatizados\n",
    "   - Monitoreo continuo\n",
    "   - Alertas proactivas\n",
    "\n",
    "4. **DocumentaciÃ³n**\n",
    "   - Lineage completo\n",
    "   - Metadata actualizado\n",
    "   - PolÃ­ticas claras\n",
    "\n",
    "5. **Balance**\n",
    "   - Gobernanza sin burocracia\n",
    "   - Flexibilidad con control\n",
    "\n",
    "### ğŸ”œ PrÃ³ximos Temas:\n",
    "\n",
    "- ImplementaciÃ³n de Data Catalogs\n",
    "- Master Data Management (MDM)\n",
    "- Privacy by Design y PII handling\n",
    "- Compliance automation (GDPR, CCPA)\n",
    "- Cost optimization y FinOps\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Has dominado los fundamentos de Data Governance!** ğŸ†"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
