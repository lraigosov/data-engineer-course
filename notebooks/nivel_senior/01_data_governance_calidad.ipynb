{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f7966c",
   "metadata": {},
   "source": [
    "# 🏛️ Senior - 01. Data Governance y Calidad de Datos\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los fundamentos de Data Governance\n",
    "- [ ] Implementar frameworks de calidad de datos\n",
    "- [ ] Establecer data lineage y catalogación\n",
    "- [ ] Aplicar principios de DAMA-DMBOK\n",
    "- [ ] Implementar validaciones automatizadas con Great Expectations\n",
    "- [ ] Diseñar estrategias de metadata management\n",
    "\n",
    "**Duración Estimada:** 150 minutos  \n",
    "**Nivel de Dificultad:** Avanzado  \n",
    "**Prerrequisitos:** Notebooks nivel Junior y Mid completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322de2a",
   "metadata": {},
   "source": [
    "## 🎯 ¿Qué es Data Governance?\n",
    "\n",
    "**Data Governance** es el conjunto de procesos, políticas, estándares y métricas que aseguran el uso efectivo y eficiente de la información, habilitando a una organización a alcanzar sus objetivos.\n",
    "\n",
    "### 🏗️ Pilares de Data Governance:\n",
    "\n",
    "1. **Data Quality** (Calidad de Datos)\n",
    "   - Precisión, completitud, consistencia\n",
    "   - Validaciones y monitoreo\n",
    "   - Remediación de problemas\n",
    "\n",
    "2. **Data Security** (Seguridad)\n",
    "   - Control de accesos\n",
    "   - Encriptación\n",
    "   - Auditoría\n",
    "\n",
    "3. **Data Lineage** (Linaje)\n",
    "   - Trazabilidad origen-destino\n",
    "   - Impacto de cambios\n",
    "   - Documentación automática\n",
    "\n",
    "4. **Data Catalog** (Catálogo)\n",
    "   - Inventario de activos de datos\n",
    "   - Metadata management\n",
    "   - Descubrimiento y búsqueda\n",
    "\n",
    "5. **Compliance** (Cumplimiento)\n",
    "   - GDPR, CCPA, HIPAA\n",
    "   - Políticas de retención\n",
    "   - Auditorías regulatorias\n",
    "\n",
    "### 🌟 Beneficios:\n",
    "\n",
    "```\n",
    "✅ Confianza en los datos\n",
    "✅ Reducción de riesgos\n",
    "✅ Cumplimiento regulatorio\n",
    "✅ Mejor toma de decisiones\n",
    "✅ Eficiencia operacional\n",
    "✅ Colaboración mejorada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f59a07",
   "metadata": {},
   "source": [
    "## 📊 Framework DAMA-DMBOK\n",
    "\n",
    "### Las 11 Áreas de Conocimiento:\n",
    "\n",
    "1. **Data Governance** - Marco general\n",
    "2. **Data Architecture** - Estructura y diseño\n",
    "3. **Data Modeling & Design** - Modelado de datos\n",
    "4. **Data Storage & Operations** - Almacenamiento y operaciones\n",
    "5. **Data Security** - Seguridad\n",
    "6. **Data Integration & Interoperability** - Integración\n",
    "7. **Document & Content Management** - Gestión documental\n",
    "8. **Reference & Master Data** - Datos maestros\n",
    "9. **Data Warehousing & BI** - Almacenes y BI\n",
    "10. **Metadata Management** - Gestión de metadatos\n",
    "11. **Data Quality** - Calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Librerías importadas\")\n",
    "print(f\"📅 Sesión iniciada: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12529fc",
   "metadata": {},
   "source": [
    "## 🔍 Dimensiones de Calidad de Datos\n",
    "\n",
    "### Las 6 Dimensiones Críticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5620f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityFramework:\n",
    "    \"\"\"\n",
    "    Framework para evaluar calidad de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, nombre_dataset: str):\n",
    "        self.df = df\n",
    "        self.nombre_dataset = nombre_dataset\n",
    "        self.reporte = {\n",
    "            'dataset': nombre_dataset,\n",
    "            'fecha_evaluacion': datetime.now().isoformat(),\n",
    "            'total_registros': len(df),\n",
    "            'total_columnas': len(df.columns),\n",
    "            'dimensiones': {}\n",
    "        }\n",
    "    \n",
    "    def evaluar_completitud(self):\n",
    "        \"\"\"\n",
    "        Dimensión 1: Completitud\n",
    "        ¿Los datos están completos?\n",
    "        \"\"\"\n",
    "        print(\"📊 Evaluando COMPLETITUD...\")\n",
    "        \n",
    "        total_valores = self.df.size\n",
    "        valores_nulos = self.df.isnull().sum().sum()\n",
    "        completitud_pct = ((total_valores - valores_nulos) / total_valores) * 100\n",
    "        \n",
    "        # Por columna\n",
    "        completitud_cols = {}\n",
    "        for col in self.df.columns:\n",
    "            pct = (1 - self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            completitud_cols[col] = round(pct, 2)\n",
    "        \n",
    "        self.reporte['dimensiones']['completitud'] = {\n",
    "            'score': round(completitud_pct, 2),\n",
    "            'valores_nulos': int(valores_nulos),\n",
    "            'por_columna': completitud_cols,\n",
    "            'aprobado': completitud_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {completitud_pct:.2f}%\")\n",
    "        print(f\"   ✓ Valores nulos: {valores_nulos:,}\")\n",
    "        return completitud_pct\n",
    "    \n",
    "    def evaluar_unicidad(self, columnas_clave: list):\n",
    "        \"\"\"\n",
    "        Dimensión 2: Unicidad\n",
    "        ¿Los registros son únicos?\n",
    "        \"\"\"\n",
    "        print(\"\\n🔑 Evaluando UNICIDAD...\")\n",
    "        \n",
    "        duplicados = self.df.duplicated(subset=columnas_clave).sum()\n",
    "        unicidad_pct = ((len(self.df) - duplicados) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['unicidad'] = {\n",
    "            'score': round(unicidad_pct, 2),\n",
    "            'duplicados': int(duplicados),\n",
    "            'columnas_evaluadas': columnas_clave,\n",
    "            'aprobado': duplicados == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {unicidad_pct:.2f}%\")\n",
    "        print(f\"   ✓ Duplicados: {duplicados}\")\n",
    "        return unicidad_pct\n",
    "    \n",
    "    def evaluar_validez(self, reglas: dict):\n",
    "        \"\"\"\n",
    "        Dimensión 3: Validez\n",
    "        ¿Los datos cumplen reglas de negocio?\n",
    "        \"\"\"\n",
    "        print(\"\\n✅ Evaluando VALIDEZ...\")\n",
    "        \n",
    "        violaciones = {}\n",
    "        total_registros = len(self.df)\n",
    "        registros_validos = total_registros\n",
    "        \n",
    "        for regla_nombre, regla_condicion in reglas.items():\n",
    "            try:\n",
    "                registros_invalidos = (~self.df.eval(regla_condicion)).sum()\n",
    "                violaciones[regla_nombre] = int(registros_invalidos)\n",
    "                registros_validos -= registros_invalidos\n",
    "                print(f\"   • {regla_nombre}: {registros_invalidos} violaciones\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error en regla '{regla_nombre}': {e}\")\n",
    "        \n",
    "        validez_pct = (registros_validos / total_registros) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['validez'] = {\n",
    "            'score': round(validez_pct, 2),\n",
    "            'violaciones': violaciones,\n",
    "            'reglas_evaluadas': list(reglas.keys()),\n",
    "            'aprobado': validez_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {validez_pct:.2f}%\")\n",
    "        return validez_pct\n",
    "    \n",
    "    def evaluar_consistencia(self, columnas_relacionadas: dict):\n",
    "        \"\"\"\n",
    "        Dimensión 4: Consistencia\n",
    "        ¿Los datos son consistentes entre sí?\n",
    "        \"\"\"\n",
    "        print(\"\\n🔄 Evaluando CONSISTENCIA...\")\n",
    "        \n",
    "        inconsistencias = {}\n",
    "        \n",
    "        for nombre, condicion in columnas_relacionadas.items():\n",
    "            try:\n",
    "                count = (~self.df.eval(condicion)).sum()\n",
    "                inconsistencias[nombre] = int(count)\n",
    "                print(f\"   • {nombre}: {count} inconsistencias\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error en validación '{nombre}': {e}\")\n",
    "        \n",
    "        total_inconsistencias = sum(inconsistencias.values())\n",
    "        consistencia_pct = ((len(self.df) - total_inconsistencias) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['consistencia'] = {\n",
    "            'score': round(consistencia_pct, 2),\n",
    "            'inconsistencias': inconsistencias,\n",
    "            'aprobado': consistencia_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {consistencia_pct:.2f}%\")\n",
    "        return consistencia_pct\n",
    "    \n",
    "    def evaluar_exactitud(self, columna_numerica: str, rango_esperado: tuple):\n",
    "        \"\"\"\n",
    "        Dimensión 5: Exactitud\n",
    "        ¿Los valores son correctos?\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 Evaluando EXACTITUD...\")\n",
    "        \n",
    "        min_val, max_val = rango_esperado\n",
    "        fuera_rango = ((self.df[columna_numerica] < min_val) | \n",
    "                       (self.df[columna_numerica] > max_val)).sum()\n",
    "        \n",
    "        exactitud_pct = ((len(self.df) - fuera_rango) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['exactitud'] = {\n",
    "            'score': round(exactitud_pct, 2),\n",
    "            'fuera_de_rango': int(fuera_rango),\n",
    "            'columna_evaluada': columna_numerica,\n",
    "            'rango_esperado': rango_esperado,\n",
    "            'aprobado': fuera_rango == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {exactitud_pct:.2f}%\")\n",
    "        print(f\"   ✓ Fuera de rango: {fuera_rango}\")\n",
    "        return exactitud_pct\n",
    "    \n",
    "    def evaluar_actualidad(self, columna_fecha: str, dias_maximos: int):\n",
    "        \"\"\"\n",
    "        Dimensión 6: Actualidad\n",
    "        ¿Los datos son recientes?\n",
    "        \"\"\"\n",
    "        print(\"\\n📅 Evaluando ACTUALIDAD...\")\n",
    "        \n",
    "        hoy = pd.Timestamp.now()\n",
    "        self.df[columna_fecha] = pd.to_datetime(self.df[columna_fecha])\n",
    "        antiguos = (hoy - self.df[columna_fecha]) > timedelta(days=dias_maximos)\n",
    "        registros_antiguos = antiguos.sum()\n",
    "        \n",
    "        actualidad_pct = ((len(self.df) - registros_antiguos) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['actualidad'] = {\n",
    "            'score': round(actualidad_pct, 2),\n",
    "            'registros_antiguos': int(registros_antiguos),\n",
    "            'dias_maximos': dias_maximos,\n",
    "            'aprobado': actualidad_pct >= 80\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Score: {actualidad_pct:.2f}%\")\n",
    "        print(f\"   ✓ Registros antiguos: {registros_antiguos}\")\n",
    "        return actualidad_pct\n",
    "    \n",
    "    def generar_reporte_completo(self):\n",
    "        \"\"\"\n",
    "        Genera reporte ejecutivo de calidad\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"📊 REPORTE DE CALIDAD DE DATOS: {self.nombre_dataset}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calcular score general\n",
    "        scores = [d['score'] for d in self.reporte['dimensiones'].values() if 'score' in d]\n",
    "        score_general = sum(scores) / len(scores) if scores else 0\n",
    "        \n",
    "        self.reporte['score_general'] = round(score_general, 2)\n",
    "        self.reporte['nivel_calidad'] = self._clasificar_calidad(score_general)\n",
    "        \n",
    "        print(f\"\\n🎯 Score General: {score_general:.2f}%\")\n",
    "        print(f\"📈 Nivel de Calidad: {self.reporte['nivel_calidad']}\")\n",
    "        print(f\"\\n📋 Detalles por Dimensión:\")\n",
    "        \n",
    "        for dimension, datos in self.reporte['dimensiones'].items():\n",
    "            status = \"✅\" if datos.get('aprobado', False) else \"❌\"\n",
    "            print(f\"   {status} {dimension.upper()}: {datos.get('score', 'N/A')}%\")\n",
    "        \n",
    "        # Guardar reporte\n",
    "        output_path = f\"../datasets/processed/reporte_calidad_{self.nombre_dataset}.json\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.reporte, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n💾 Reporte guardado en: {output_path}\")\n",
    "        return self.reporte\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clasificar_calidad(score):\n",
    "        \"\"\"Clasifica el nivel de calidad según el score\"\"\"\n",
    "        if score >= 95:\n",
    "            return \"⭐⭐⭐ EXCELENTE\"\n",
    "        elif score >= 85:\n",
    "            return \"⭐⭐ BUENO\"\n",
    "        elif score >= 70:\n",
    "            return \"⭐ ACEPTABLE\"\n",
    "        else:\n",
    "            return \"❌ CRÍTICO\"\n",
    "\n",
    "print(\"✅ DataQualityFramework definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213b42",
   "metadata": {},
   "source": [
    "## 🧪 Caso Práctico: Evaluación de Calidad\n",
    "\n",
    "Vamos a evaluar un dataset de ventas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7273b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset de ejemplo con problemas de calidad intencionales\n",
    "np.random.seed(42)\n",
    "\n",
    "n_registros = 1000\n",
    "\n",
    "df_ventas = pd.DataFrame({\n",
    "    'venta_id': range(1, n_registros + 1),\n",
    "    'cliente_id': np.random.randint(1, 200, n_registros),\n",
    "    'producto': np.random.choice(['Laptop', 'Mouse', 'Teclado', 'Monitor', None], n_registros),\n",
    "    'cantidad': np.random.randint(-5, 50, n_registros),  # Algunas cantidades negativas\n",
    "    'precio_unitario': np.random.uniform(10, 2000, n_registros),\n",
    "    'descuento_pct': np.random.uniform(0, 100, n_registros),  # Algunos > 100%\n",
    "    'total': np.random.uniform(0, 5000, n_registros),\n",
    "    'fecha_venta': pd.date_range(start='2023-01-01', periods=n_registros, freq='H'),\n",
    "    'estado': np.random.choice(['Completada', 'Pendiente', 'Cancelada', None], n_registros)\n",
    "})\n",
    "\n",
    "# Introducir duplicados intencionales\n",
    "df_ventas = pd.concat([df_ventas, df_ventas.iloc[:10]], ignore_index=True)\n",
    "\n",
    "# Introducir algunos nulls adicionales\n",
    "df_ventas.loc[np.random.choice(df_ventas.index, 50), 'precio_unitario'] = np.nan\n",
    "\n",
    "print(f\"📊 Dataset creado: {len(df_ventas)} registros, {len(df_ventas.columns)} columnas\")\n",
    "print(f\"\\n🔍 Primeras filas:\")\n",
    "print(df_ventas.head())\n",
    "print(f\"\\n📈 Info del dataset:\")\n",
    "print(df_ventas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar framework\n",
    "dq = DataQualityFramework(df_ventas, 'ventas_ecommerce')\n",
    "\n",
    "# Evaluar todas las dimensiones\n",
    "print(\"🚀 Iniciando evaluación de calidad...\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "dq.evaluar_completitud()\n",
    "\n",
    "# 2. Unicidad\n",
    "dq.evaluar_unicidad(columnas_clave=['venta_id'])\n",
    "\n",
    "# 3. Validez\n",
    "reglas_negocio = {\n",
    "    'cantidad_positiva': 'cantidad > 0',\n",
    "    'descuento_valido': 'descuento_pct >= 0 and descuento_pct <= 100',\n",
    "    'total_positivo': 'total > 0'\n",
    "}\n",
    "dq.evaluar_validez(reglas_negocio)\n",
    "\n",
    "# 4. Consistencia\n",
    "relaciones = {\n",
    "    'total_coherente': 'total >= (precio_unitario * cantidad * (1 - descuento_pct/100)) * 0.95',\n",
    "}\n",
    "dq.evaluar_consistencia(relaciones)\n",
    "\n",
    "# 5. Exactitud\n",
    "dq.evaluar_exactitud('precio_unitario', (5, 3000))\n",
    "\n",
    "# 6. Actualidad\n",
    "dq.evaluar_actualidad('fecha_venta', dias_maximos=365)\n",
    "\n",
    "# Generar reporte completo\n",
    "reporte = dq.generar_reporte_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e327a",
   "metadata": {},
   "source": [
    "## 📋 Data Lineage - Trazabilidad de Datos\n",
    "\n",
    "Implementación de un sistema básico de lineage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLineageTracker:\n",
    "    \"\"\"\n",
    "    Sistema para rastrear el linaje de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.linaje = []\n",
    "        self.grafo = {}\n",
    "    \n",
    "    def registrar_transformacion(self, \n",
    "                                 origen: str, \n",
    "                                 destino: str, \n",
    "                                 transformacion: str,\n",
    "                                 metadata: dict = None):\n",
    "        \"\"\"\n",
    "        Registra una transformación de datos\n",
    "        \"\"\"\n",
    "        evento = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'origen': origen,\n",
    "            'destino': destino,\n",
    "            'transformacion': transformacion,\n",
    "            'metadata': metadata or {},\n",
    "            'id': hashlib.md5(f\"{origen}{destino}{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        }\n",
    "        \n",
    "        self.linaje.append(evento)\n",
    "        \n",
    "        # Construir grafo\n",
    "        if origen not in self.grafo:\n",
    "            self.grafo[origen] = []\n",
    "        self.grafo[origen].append(destino)\n",
    "        \n",
    "        print(f\"📝 Registrado: {origen} → {destino} ({transformacion})\")\n",
    "        return evento['id']\n",
    "    \n",
    "    def obtener_linaje_completo(self, entidad: str, direccion='downstream'):\n",
    "        \"\"\"\n",
    "        Obtiene el linaje completo de una entidad\n",
    "        \"\"\"\n",
    "        if direccion == 'downstream':\n",
    "            # Hacia adelante (downstream)\n",
    "            return self._traverse_downstream(entidad)\n",
    "        else:\n",
    "            # Hacia atrás (upstream)\n",
    "            return self._traverse_upstream(entidad)\n",
    "    \n",
    "    def _traverse_downstream(self, nodo, visitados=None):\n",
    "        if visitados is None:\n",
    "            visitados = set()\n",
    "        \n",
    "        if nodo in visitados:\n",
    "            return []\n",
    "        \n",
    "        visitados.add(nodo)\n",
    "        camino = [nodo]\n",
    "        \n",
    "        if nodo in self.grafo:\n",
    "            for hijo in self.grafo[nodo]:\n",
    "                camino.extend(self._traverse_downstream(hijo, visitados))\n",
    "        \n",
    "        return camino\n",
    "    \n",
    "    def _traverse_upstream(self, nodo):\n",
    "        # Invertir el grafo para upstream\n",
    "        grafo_invertido = {}\n",
    "        for origen, destinos in self.grafo.items():\n",
    "            for destino in destinos:\n",
    "                if destino not in grafo_invertido:\n",
    "                    grafo_invertido[destino] = []\n",
    "                grafo_invertido[destino].append(origen)\n",
    "        \n",
    "        return self._traverse_downstream(nodo, set())\n",
    "    \n",
    "    def visualizar_linaje(self):\n",
    "        \"\"\"\n",
    "        Genera visualización simple del linaje\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 MAPA DE LINAJE DE DATOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for origen, destinos in self.grafo.items():\n",
    "            print(f\"\\n🔹 {origen}\")\n",
    "            for destino in destinos:\n",
    "                # Encontrar transformación\n",
    "                trans = next((e for e in self.linaje \n",
    "                            if e['origen'] == origen and e['destino'] == destino), {})\n",
    "                print(f\"   └─→ {destino} [{trans.get('transformacion', 'N/A')}]\")\n",
    "    \n",
    "    def exportar_linaje(self, ruta_salida: str):\n",
    "        \"\"\"\n",
    "        Exporta linaje a archivo JSON\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)\n",
    "        \n",
    "        with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'eventos': self.linaje,\n",
    "                'grafo': self.grafo,\n",
    "                'generado': datetime.now().isoformat()\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n💾 Linaje exportado a: {ruta_salida}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "tracker = DataLineageTracker()\n",
    "\n",
    "print(\"🔍 Simulando pipeline con lineage tracking:\\n\")\n",
    "\n",
    "# Registrar transformaciones\n",
    "tracker.registrar_transformacion(\n",
    "    'raw_ventas_csv',\n",
    "    'stage_ventas',\n",
    "    'extraccion',\n",
    "    {'filas': 1000, 'columnas': 9}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'stage_ventas',\n",
    "    'clean_ventas',\n",
    "    'limpieza',\n",
    "    {'nulos_removidos': 50, 'duplicados_removidos': 10}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'agg_ventas_diarias',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'fecha', 'metricas': ['sum', 'count', 'avg']}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'ventas_por_producto',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'producto'}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'agg_ventas_diarias',\n",
    "    'dashboard_ventas',\n",
    "    'visualizacion',\n",
    "    {'tipo': 'time_series'}\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "tracker.visualizar_linaje()\n",
    "\n",
    "# Exportar\n",
    "tracker.exportar_linaje('../datasets/processed/data_lineage.json')\n",
    "\n",
    "# Consultar linaje\n",
    "print(\"\\n🔎 Linaje downstream de 'clean_ventas':\")\n",
    "downstream = tracker.obtener_linaje_completo('clean_ventas', 'downstream')\n",
    "print(f\"   {' → '.join(downstream)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c090f1",
   "metadata": {},
   "source": [
    "## 🎓 Resumen y Mejores Prácticas\n",
    "\n",
    "### ✅ Principios Clave de Data Governance:\n",
    "\n",
    "1. **Proactivo, no Reactivo**\n",
    "   - Validaciones automáticas en cada etapa\n",
    "   - Prevención vs corrección\n",
    "\n",
    "2. **Cultura de Calidad**\n",
    "   - Responsabilidad compartida\n",
    "   - Data stewards claramente definidos\n",
    "\n",
    "3. **Automatización**\n",
    "   - Tests automatizados\n",
    "   - Monitoreo continuo\n",
    "   - Alertas proactivas\n",
    "\n",
    "4. **Documentación**\n",
    "   - Lineage completo\n",
    "   - Metadata actualizado\n",
    "   - Políticas claras\n",
    "\n",
    "5. **Balance**\n",
    "   - Gobernanza sin burocracia\n",
    "   - Flexibilidad con control\n",
    "\n",
    "### 🔜 Próximos Temas:\n",
    "\n",
    "- Implementación de Data Catalogs\n",
    "- Master Data Management (MDM)\n",
    "- Privacy by Design y PII handling\n",
    "- Compliance automation (GDPR, CCPA)\n",
    "- Cost optimization y FinOps\n",
    "\n",
    "---\n",
    "\n",
    "**¡Has dominado los fundamentos de Data Governance!** 🏆"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
