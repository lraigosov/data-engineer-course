{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f7966c",
   "metadata": {},
   "source": [
    "# \ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los fundamentos de Data Governance\n",
    "- [ ] Implementar frameworks de calidad de datos\n",
    "- [ ] Establecer data lineage y catalogaci\u00f3n\n",
    "- [ ] Aplicar principios de DAMA-DMBOK\n",
    "- [ ] Implementar validaciones automatizadas con Great Expectations\n",
    "- [ ] Dise\u00f1ar estrategias de metadata management\n",
    "\n",
    "**Duraci\u00f3n Estimada:** 150 minutos  \n",
    "**Nivel de Dificultad:** Avanzado  \n",
    "**Prerrequisitos:** Notebooks nivel Junior y Mid completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da27686",
   "metadata": {},
   "source": [
    "## \u26a0\ufe0f NIVEL SENIOR - PR\u00c1CTICAS DE PRODUCCI\u00d3N\n",
    "\n",
    "### \ud83c\udfaf A este nivel, ya sabes que:\n",
    "\n",
    "**\u274c Notebooks NO son para producci\u00f3n** (son solo para aprendizaje aqu\u00ed)\n",
    "\n",
    "**\u2705 En tu rol Senior, implementas:**\n",
    "- Arquitecturas enterprise (Data Mesh, Lakehouse)\n",
    "- Governance frameworks (DAMA-DMBOK)\n",
    "- Pipelines de producci\u00f3n con Airflow/Prefect\n",
    "- Infrastructure as Code (Terraform, CloudFormation)\n",
    "- Observabilidad completa (OpenTelemetry, Datadog)\n",
    "- Security & Compliance (GDPR, SOC2)\n",
    "\n",
    "**\ud83d\udcd6 Referencia obligatoria:** `notebooks/\u26a0\ufe0f_IMPORTANTE_LEER_PRIMERO.md`\n",
    "\n",
    "**Este nivel te ense\u00f1a:** Dise\u00f1o de arquitecturas y governanza, luego lo implementas en c\u00f3digo de producci\u00f3n.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | \u00a9 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60596fe",
   "metadata": {},
   "source": [
    "### \ud83c\udfdb\ufe0f **Data Governance: El Fundamento de Datos Confiables**\n",
    "\n",
    "**\u00bfPor qu\u00e9 Data Governance es cr\u00edtico en Senior-level?**\n",
    "\n",
    "A nivel Senior, ya no solo implementas pipelines t\u00e9cnicos \u2192 **dise\u00f1as la estrategia organizacional** de c\u00f3mo los datos se gestionan, gobiernan y monetizan.\n",
    "\n",
    "**Evoluci\u00f3n del Data Engineer:**\n",
    "\n",
    "```\n",
    "Junior:  \"\u00bfC\u00f3mo muevo datos de A a B?\"\n",
    "Mid:     \"\u00bfC\u00f3mo lo hago escalable, confiable y monitoreado?\"\n",
    "Senior:  \"\u00bfC\u00f3mo aseguro que TODOS los datos de la org sean confiables, \n",
    "          cumplan regulaciones y generen valor de negocio?\"\n",
    "```\n",
    "\n",
    "**Impacto de Mala Calidad de Datos:**\n",
    "\n",
    "| Sector | Costo Anual (Industria) | Ejemplo Real |\n",
    "|--------|-------------------------|--------------|\n",
    "| **Retail** | $3.1 trillones USD | Target: Promoci\u00f3n a clientes equivocados \u2192 $1M perdido |\n",
    "| **Healthcare** | $74B USD | Registros duplicados \u2192 medicaci\u00f3n incorrecta |\n",
    "| **Finance** | $15M/empresa | Knight Capital: Bug en prod \u2192 $440M en 45 min |\n",
    "| **Manufacturing** | $5M/empresa | Datos de inventario desactualizados \u2192 Stockouts |\n",
    "\n",
    "**Data Governance Framework (Componentes):**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    GOVERNANCE LAYER                     \u2502\n",
    "\u2502                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502  \u2502  Policy  \u2502  \u2502  Roles   \u2502  \u2502Standards \u2502            \u2502\n",
    "\u2502  \u2502 Pol\u00edticas\u2502  \u2502 Stewards \u2502  \u2502Est\u00e1ndares\u2502            \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                       \u2502\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u25bc               \u25bc               \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Data    \u2502    \u2502   Data   \u2502    \u2502   Data   \u2502\n",
    "\u2502 Quality  \u2502    \u2502 Security \u2502    \u2502 Lineage  \u2502\n",
    "\u2502          \u2502    \u2502          \u2502    \u2502          \u2502\n",
    "\u2502 \u2022 6 dim. \u2502    \u2502 \u2022 RBAC   \u2502    \u2502 \u2022 Origen \u2502\n",
    "\u2502 \u2022 SLAs   \u2502    \u2502 \u2022 Encrypt\u2502    \u2502 \u2022 Impacto\u2502\n",
    "\u2502 \u2022 Monitor\u2502    \u2502 \u2022 Audit  \u2502    \u2502 \u2022 Deps   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502               \u2502               \u2502\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                       \u2502\n",
    "                       \u25bc\n",
    "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "            \u2502  Data Catalog    \u2502\n",
    "            \u2502  (Metadata Hub)  \u2502\n",
    "            \u2502                  \u2502\n",
    "            \u2502  \u2022 Assets        \u2502\n",
    "            \u2502  \u2022 Ownership     \u2502\n",
    "            \u2502  \u2022 Glossary      \u2502\n",
    "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**DAMA-DMBOK (Data Management Body of Knowledge):**\n",
    "\n",
    "Framework est\u00e1ndar de la industria con 11 \u00e1reas:\n",
    "\n",
    "1. **Data Governance** (Core): Marco organizacional\n",
    "2. **Data Architecture**: Blueprints de sistemas de datos\n",
    "3. **Data Modeling**: Dise\u00f1o de estructuras\n",
    "4. **Data Storage & Operations**: Almacenamiento\n",
    "5. **Data Security**: Protecci\u00f3n y privacidad\n",
    "6. **Data Integration & Interoperability**: ETL/ELT\n",
    "7. **Document & Content Management**: Docs no estructurados\n",
    "8. **Reference & Master Data**: Golden records (MDM)\n",
    "9. **Data Warehousing & BI**: Analytics\n",
    "10. **Metadata Management**: \"Data about data\"\n",
    "11. **Data Quality**: 6 dimensiones cr\u00edticas\n",
    "\n",
    "**Roles en Data Governance:**\n",
    "\n",
    "```python\n",
    "class DataGovernanceTeam:\n",
    "    \"\"\"\n",
    "    Chief Data Officer (CDO)\n",
    "      \u251c\u2500\u2500 Data Governance Manager\n",
    "      \u2502     \u251c\u2500\u2500 Data Stewards (por dominio: Ventas, Finanzas, HR)\n",
    "      \u2502     \u2514\u2500\u2500 Data Quality Analysts\n",
    "      \u251c\u2500\u2500 Data Architect\n",
    "      \u251c\u2500\u2500 Data Security Officer\n",
    "      \u2514\u2500\u2500 Compliance Officer\n",
    "    \"\"\"\n",
    "    \n",
    "    roles = {\n",
    "        'Data Steward': 'Define reglas de negocio, resuelve issues de calidad',\n",
    "        'Data Owner': 'Responsable final de un dominio de datos',\n",
    "        'Data Custodian': 'Implementa controles t\u00e9cnicos (Data Engineer)',\n",
    "        'Data Consumer': 'Usa datos para an\u00e1lisis/decisiones'\n",
    "    }\n",
    "```\n",
    "\n",
    "**Herramientas del Ecosistema:**\n",
    "\n",
    "| Categor\u00eda | Open Source | Comercial |\n",
    "|-----------|-------------|-----------|\n",
    "| **Data Catalog** | Apache Atlas, Amundsen, DataHub | Alation, Collibra, Informatica |\n",
    "| **Data Quality** | Great Expectations, Soda, dbt tests | Talend DQ, Informatica DQ |\n",
    "| **Data Lineage** | OpenLineage, Marquez | Manta, Collibra Lineage |\n",
    "| **MDM (Master Data)** | - | Informatica MDM, SAP MDM |\n",
    "| **Metadata Mgmt** | Apache Atlas | Collibra, Alation |\n",
    "\n",
    "**M\u00e9tricas de Data Governance (KPIs):**\n",
    "\n",
    "```python\n",
    "governance_metrics = {\n",
    "    'Data Quality Score': '% de datasets que pasan quality gates',\n",
    "    'Metadata Coverage': '% de tablas con descripciones/owners',\n",
    "    'Policy Compliance': '% de datasets que cumplen pol\u00edticas',\n",
    "    'Mean Time to Remediation': 'Tiempo promedio para arreglar issues',\n",
    "    'Data Lineage Coverage': '% de pipelines con lineage documentado',\n",
    "    'Security Incidents': '# de breaches/accesos no autorizados'\n",
    "}\n",
    "\n",
    "# Ejemplo de SLA\n",
    "quality_sla = {\n",
    "    'Critical datasets': {\n",
    "        'completeness': '> 99%',\n",
    "        'freshness': '< 1 hour',\n",
    "        'accuracy': '> 99.5%'\n",
    "    },\n",
    "    'Standard datasets': {\n",
    "        'completeness': '> 95%',\n",
    "        'freshness': '< 24 hours',\n",
    "        'accuracy': '> 95%'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Regulaciones de Compliance:**\n",
    "\n",
    "- **GDPR** (Europa): Derecho al olvido, portabilidad, consentimiento\n",
    "- **CCPA** (California): Privacidad de consumidores\n",
    "- **HIPAA** (USA): Datos de salud\n",
    "- **SOX** (USA): Datos financieros\n",
    "- **LGPD** (Brasil): Protecci\u00f3n de datos personales\n",
    "\n",
    "**Implementaci\u00f3n Pr\u00e1ctica en Pipelines:**\n",
    "\n",
    "```python\n",
    "# Pipeline con governance embebido\n",
    "@data_quality_check(min_score=95)\n",
    "@pii_detector(action='mask')\n",
    "@lineage_tracking(catalog='atlas')\n",
    "def ingest_customer_data(source, dest):\n",
    "    df = extract(source)\n",
    "    df = validate_schema(df)          # Quality\n",
    "    df = mask_pii_columns(df)         # Security\n",
    "    df = enrich_metadata(df)          # Catalog\n",
    "    load(df, dest)\n",
    "    track_lineage(source, dest)       # Lineage\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322de2a",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf \u00bfQu\u00e9 es Data Governance?\n",
    "\n",
    "**Data Governance** es el conjunto de procesos, pol\u00edticas, est\u00e1ndares y m\u00e9tricas que aseguran el uso efectivo y eficiente de la informaci\u00f3n, habilitando a una organizaci\u00f3n a alcanzar sus objetivos.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Pilares de Data Governance:\n",
    "\n",
    "1. **Data Quality** (Calidad de Datos)\n",
    "   - Precisi\u00f3n, completitud, consistencia\n",
    "   - Validaciones y monitoreo\n",
    "   - Remediaci\u00f3n de problemas\n",
    "\n",
    "2. **Data Security** (Seguridad)\n",
    "   - Control de accesos\n",
    "   - Encriptaci\u00f3n\n",
    "   - Auditor\u00eda\n",
    "\n",
    "3. **Data Lineage** (Linaje)\n",
    "   - Trazabilidad origen-destino\n",
    "   - Impacto de cambios\n",
    "   - Documentaci\u00f3n autom\u00e1tica\n",
    "\n",
    "4. **Data Catalog** (Cat\u00e1logo)\n",
    "   - Inventario de activos de datos\n",
    "   - Metadata management\n",
    "   - Descubrimiento y b\u00fasqueda\n",
    "\n",
    "5. **Compliance** (Cumplimiento)\n",
    "   - GDPR, CCPA, HIPAA\n",
    "   - Pol\u00edticas de retenci\u00f3n\n",
    "   - Auditor\u00edas regulatorias\n",
    "\n",
    "### \ud83c\udf1f Beneficios:\n",
    "\n",
    "```\n",
    "\u2705 Confianza en los datos\n",
    "\u2705 Reducci\u00f3n de riesgos\n",
    "\u2705 Cumplimiento regulatorio\n",
    "\u2705 Mejor toma de decisiones\n",
    "\u2705 Eficiencia operacional\n",
    "\u2705 Colaboraci\u00f3n mejorada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c17614",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **Data Quality: Las 6 Dimensiones Cr\u00edticas (ISO 8000)**\n",
    "\n",
    "**Framework de Calidad basado en est\u00e1ndar ISO 8000:**\n",
    "\n",
    "**1. COMPLETITUD (Completeness):**\n",
    "\n",
    "```python\n",
    "# \u00bfTodos los datos esperados est\u00e1n presentes?\n",
    "\n",
    "# Ejemplo: Dataset de clientes\n",
    "df_clientes = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'nombre': ['Ana', 'Bob', None, 'Diana', 'Eva'],  # 1 null\n",
    "    'email': ['a@x.com', None, 'c@x.com', 'd@x.com', None],  # 2 nulls\n",
    "    'telefono': ['111', '222', '333', None, '555']  # 1 null\n",
    "})\n",
    "\n",
    "# M\u00e9trica: % de valores no-null\n",
    "completitud = (1 - df_clientes.isnull().sum().sum() / df_clientes.size) * 100\n",
    "# = (1 - 4 / 20) * 100 = 80%\n",
    "\n",
    "# SLA t\u00edpico: > 95% para campos cr\u00edticos, > 85% para opcionales\n",
    "```\n",
    "\n",
    "**Causas de incompletitud:**\n",
    "- Campos opcionales en formularios\n",
    "- Errores en integraci\u00f3n de sistemas\n",
    "- Cambios de schema sin backfill\n",
    "\n",
    "**Remediaci\u00f3n:**\n",
    "```python\n",
    "# Imputaci\u00f3n inteligente\n",
    "df['telefono'] = df['telefono'].fillna('UNKNOWN')\n",
    "\n",
    "# Alertas autom\u00e1ticas\n",
    "if completitud < 95:\n",
    "    send_slack_alert(f\"Completitud baja: {completitud}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. UNICIDAD (Uniqueness):**\n",
    "\n",
    "```python\n",
    "# \u00bfLos registros son \u00fanicos seg\u00fan business key?\n",
    "\n",
    "# Ejemplo: Duplicados por error de ETL\n",
    "df_ventas = pd.DataFrame({\n",
    "    'venta_id': [1, 2, 2, 3],  # venta_id=2 duplicado\n",
    "    'cliente': ['A', 'B', 'B', 'C'],\n",
    "    'total': [100, 200, 200, 150]\n",
    "})\n",
    "\n",
    "# M\u00e9trica\n",
    "duplicados = df_ventas.duplicated(subset=['venta_id']).sum()  # 1\n",
    "unicidad = (1 - duplicados / len(df_ventas)) * 100  # 75%\n",
    "\n",
    "# Identificar duplicados\n",
    "df_ventas[df_ventas.duplicated(subset=['venta_id'], keep=False)]\n",
    "```\n",
    "\n",
    "**Causas de duplicados:**\n",
    "- Retry logic sin idempotencia\n",
    "- Merge de m\u00faltiples fuentes sin deduplicaci\u00f3n\n",
    "- Race conditions en escrituras concurrentes\n",
    "\n",
    "**Remediaci\u00f3n:**\n",
    "```python\n",
    "# Deduplicaci\u00f3n con estrategia\n",
    "df_clean = df_ventas.drop_duplicates(subset=['venta_id'], keep='last')\n",
    "\n",
    "# O upsert en DB\n",
    "INSERT INTO ventas (...) VALUES (...)\n",
    "ON CONFLICT (venta_id) DO UPDATE SET ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. VALIDEZ (Validity):**\n",
    "\n",
    "```python\n",
    "# \u00bfLos valores cumplen reglas de negocio?\n",
    "\n",
    "# Reglas de negocio\n",
    "reglas = {\n",
    "    'cantidad_positiva': 'cantidad > 0',\n",
    "    'precio_razonable': 'precio_unitario >= 0 and precio_unitario <= 10000',\n",
    "    'descuento_valido': 'descuento_pct >= 0 and descuento_pct <= 100',\n",
    "    'total_coherente': 'total == cantidad * precio_unitario * (1 - descuento_pct/100)'\n",
    "}\n",
    "\n",
    "# Validaci\u00f3n\n",
    "for regla, condicion in reglas.items():\n",
    "    violaciones = (~df.eval(condicion)).sum()\n",
    "    if violaciones > 0:\n",
    "        print(f\"\u26a0\ufe0f {regla}: {violaciones} violaciones\")\n",
    "\n",
    "# Ejemplo de violaciones\n",
    "# cantidad: -5 (negativa) \u2192 Inv\u00e1lida\n",
    "# descuento_pct: 150% \u2192 Inv\u00e1lida\n",
    "```\n",
    "\n",
    "**Tipos de validaciones:**\n",
    "- **Tipo de dato**: `isinstance(valor, int)`\n",
    "- **Rango**: `0 <= edad <= 120`\n",
    "- **Formato**: `email.match(r'^\\S+@\\S+\\.\\S+$')`\n",
    "- **Referencial**: `cliente_id in clientes.id`\n",
    "- **L\u00f3gica de negocio**: `fecha_entrega > fecha_pedido`\n",
    "\n",
    "---\n",
    "\n",
    "**4. CONSISTENCIA (Consistency):**\n",
    "\n",
    "```python\n",
    "# \u00bfLos datos son coherentes entre s\u00ed y a lo largo del tiempo?\n",
    "\n",
    "# Inconsistencia cross-field\n",
    "df['total_calculado'] = df['cantidad'] * df['precio_unitario']\n",
    "df['inconsistencia'] = abs(df['total'] - df['total_calculado']) > 0.01\n",
    "\n",
    "# Inconsistencia temporal\n",
    "df['mes_factura'] = pd.to_datetime(df['fecha_factura']).dt.month\n",
    "df['mes_pago'] = pd.to_datetime(df['fecha_pago']).dt.month\n",
    "inconsistentes = df[df['mes_pago'] < df['mes_factura']]  # Pago antes de factura\n",
    "\n",
    "# Inconsistencia entre sistemas\n",
    "df_erp['stock'] != df_warehouse['stock']  # Desincronizaci\u00f3n\n",
    "```\n",
    "\n",
    "**Causas:**\n",
    "- Falta de transacciones ACID\n",
    "- Sincronizaci\u00f3n eventual entre sistemas\n",
    "- Cambios de l\u00f3gica de negocio\n",
    "\n",
    "---\n",
    "\n",
    "**5. EXACTITUD (Accuracy):**\n",
    "\n",
    "```python\n",
    "# \u00bfLos valores reflejan la realidad?\n",
    "\n",
    "# Ejemplo: Comparar con fuente de verdad (ground truth)\n",
    "df_measured = pd.DataFrame({'temp': [20.1, 25.3, 30.5]})\n",
    "df_ground_truth = pd.DataFrame({'temp': [20.0, 25.0, 30.0]})\n",
    "\n",
    "# RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(((df_measured['temp'] - df_ground_truth['temp'])**2).mean())\n",
    "# RMSE = 0.36\u00b0C\n",
    "\n",
    "# Exactitud\n",
    "exactitud = 100 - (rmse / df_ground_truth['temp'].mean()) * 100\n",
    "```\n",
    "\n",
    "**Dif\u00edcil de medir sin \"ground truth\":**\n",
    "- Requiere auditor\u00edas manuales\n",
    "- Sampling aleatorio + verificaci\u00f3n humana\n",
    "- Comparaci\u00f3n con fuentes externas confiables\n",
    "\n",
    "---\n",
    "\n",
    "**6. ACTUALIDAD (Timeliness/Freshness):**\n",
    "\n",
    "```python\n",
    "# \u00bfLos datos son suficientemente recientes?\n",
    "\n",
    "# Ejemplo\n",
    "hoy = pd.Timestamp.now()\n",
    "df['lag'] = (hoy - pd.to_datetime(df['fecha_ingestion'])).dt.total_seconds() / 3600\n",
    "\n",
    "# SLA: Datos cr\u00edticos < 1 hora\n",
    "df_criticos = df[df['categoria'] == 'critical']\n",
    "sla_breach = df_criticos[df_criticos['lag'] > 1]\n",
    "\n",
    "print(f\"SLA breaches: {len(sla_breach)} datasets\")\n",
    "```\n",
    "\n",
    "**SLAs t\u00edpicos:**\n",
    "- **Real-time**: < 1 minuto (fraud detection)\n",
    "- **Near real-time**: < 15 minutos (operational dashboards)\n",
    "- **Batch**: < 24 horas (reporting)\n",
    "- **Archive**: > 7 d\u00edas (historical analysis)\n",
    "\n",
    "---\n",
    "\n",
    "**Data Quality Score Agregado:**\n",
    "\n",
    "```python\n",
    "def calculate_dq_score(df, rules):\n",
    "    \"\"\"Score compuesto 0-100\"\"\"\n",
    "    \n",
    "    scores = {\n",
    "        'completitud': (1 - df.isnull().sum().sum() / df.size) * 100,\n",
    "        'unicidad': (1 - df.duplicated().sum() / len(df)) * 100,\n",
    "        'validez': calcular_validez(df, rules),\n",
    "        'consistencia': calcular_consistencia(df),\n",
    "        'exactitud': 95,  # Requiere ground truth\n",
    "        'actualidad': calcular_freshness(df, 'fecha')\n",
    "    }\n",
    "    \n",
    "    # Weighted average (ponderado seg\u00fan criticidad)\n",
    "    weights = {\n",
    "        'completitud': 0.25,\n",
    "        'unicidad': 0.20,\n",
    "        'validez': 0.25,\n",
    "        'consistencia': 0.15,\n",
    "        'exactitud': 0.10,\n",
    "        'actualidad': 0.05\n",
    "    }\n",
    "    \n",
    "    dq_score = sum(scores[k] * weights[k] for k in scores)\n",
    "    \n",
    "    return dq_score, scores\n",
    "```\n",
    "\n",
    "**Niveles de Calidad:**\n",
    "- **\u2b50\u2b50\u2b50 Excelente**: 95-100% \u2192 Prod-ready\n",
    "- **\u2b50\u2b50 Bueno**: 85-94% \u2192 Requiere atenci\u00f3n\n",
    "- **\u2b50 Aceptable**: 70-84% \u2192 Plan de remediaci\u00f3n\n",
    "- **\u274c Cr\u00edtico**: < 70% \u2192 Bloquear uso\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f59a07",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Framework DAMA-DMBOK\n",
    "\n",
    "### Las 11 \u00c1reas de Conocimiento:\n",
    "\n",
    "1. **Data Governance** - Marco general\n",
    "2. **Data Architecture** - Estructura y dise\u00f1o\n",
    "3. **Data Modeling & Design** - Modelado de datos\n",
    "4. **Data Storage & Operations** - Almacenamiento y operaciones\n",
    "5. **Data Security** - Seguridad\n",
    "6. **Data Integration & Interoperability** - Integraci\u00f3n\n",
    "7. **Document & Content Management** - Gesti\u00f3n documental\n",
    "8. **Reference & Master Data** - Datos maestros\n",
    "9. **Data Warehousing & BI** - Almacenes y BI\n",
    "10. **Metadata Management** - Gesti\u00f3n de metadatos\n",
    "11. **Data Quality** - Calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\u2705 Librer\u00edas importadas\")\n",
    "print(f\"\ud83d\udcc5 Sesi\u00f3n iniciada: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12529fc",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Dimensiones de Calidad de Datos\n",
    "\n",
    "### Las 6 Dimensiones Cr\u00edticas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c1603",
   "metadata": {},
   "source": [
    "### \ud83d\udee0\ufe0f **Data Quality Framework: Implementaci\u00f3n Productiva**\n",
    "\n",
    "**\u00bfPor qu\u00e9 un Framework y no validaciones ad-hoc?**\n",
    "\n",
    "```python\n",
    "# \u274c Enfoque ad-hoc (no escalable)\n",
    "if df['precio'].isnull().sum() > 0:\n",
    "    print(\"Hay nulls en precio\")\n",
    "if len(df[df['cantidad'] < 0]) > 0:\n",
    "    print(\"Cantidades negativas\")\n",
    "# ... 50 validaciones m\u00e1s dispersas en el c\u00f3digo\n",
    "\n",
    "# \u2705 Enfoque Framework (escalable, reusable)\n",
    "dq_framework = DataQualityFramework(df, \"ventas\")\n",
    "dq_framework.evaluar_completitud()\n",
    "dq_framework.evaluar_validez(business_rules)\n",
    "dq_framework.generar_reporte_completo()\n",
    "```\n",
    "\n",
    "**Componentes del Framework:**\n",
    "\n",
    "```\n",
    "DataQualityFramework\n",
    "\u251c\u2500\u2500 __init__()              \u2192 Inicializaci\u00f3n con dataset\n",
    "\u251c\u2500\u2500 evaluar_completitud()   \u2192 Dimensi\u00f3n 1\n",
    "\u251c\u2500\u2500 evaluar_unicidad()      \u2192 Dimensi\u00f3n 2\n",
    "\u251c\u2500\u2500 evaluar_validez()       \u2192 Dimensi\u00f3n 3\n",
    "\u251c\u2500\u2500 evaluar_consistencia()  \u2192 Dimensi\u00f3n 4\n",
    "\u251c\u2500\u2500 evaluar_exactitud()     \u2192 Dimensi\u00f3n 5\n",
    "\u251c\u2500\u2500 evaluar_actualidad()    \u2192 Dimensi\u00f3n 6\n",
    "\u251c\u2500\u2500 generar_reporte()       \u2192 JSON output\n",
    "\u2514\u2500\u2500 _clasificar_calidad()   \u2192 Score \u2192 Rating\n",
    "```\n",
    "\n",
    "**Arquitectura de Calidad en Pipelines:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         SOURCE SYSTEM (API/DB)           \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                  \u2502\n",
    "                  \u25bc\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u2502   RAW LAYER      \u2502  \u2190 Landing zone (sin validar)\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2502\n",
    "                \u25bc\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u2502  DATA QUALITY CHECKS     \u2502\n",
    "       \u2502  (DataQualityFramework)  \u2502\n",
    "       \u2502                          \u2502\n",
    "       \u2502  IF score < threshold:   \u2502\n",
    "       \u2502    \u2192 DLQ (Dead Letter)   \u2502\n",
    "       \u2502    \u2192 Alert team          \u2502\n",
    "       \u2502    \u2192 Block downstream    \u2502\n",
    "       \u2502  ELSE:                   \u2502\n",
    "       \u2502    \u2192 Continue pipeline   \u2502\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2502\n",
    "                \u25bc\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u2502  TRUSTED LAYER   \u2502  \u2190 Datos validados\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2502\n",
    "                \u25bc\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u2502 REFINED LAYER    \u2502  \u2190 Datos refinados\n",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Integration con Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "\n",
    "def check_data_quality(**context):\n",
    "    df = context['ti'].xcom_pull(task_ids='extract_data')\n",
    "    \n",
    "    dq = DataQualityFramework(df, \"ventas_daily\")\n",
    "    dq.evaluar_completitud()\n",
    "    dq.evaluar_unicidad(['venta_id'])\n",
    "    dq.evaluar_validez(business_rules)\n",
    "    \n",
    "    report = dq.generar_reporte_completo()\n",
    "    context['ti'].xcom_push(key='dq_score', value=report['score_general'])\n",
    "    \n",
    "    return 'pass_quality' if report['score_general'] >= 90 else 'fail_quality'\n",
    "\n",
    "with DAG('pipeline_with_dq', ...) as dag:\n",
    "    extract = PythonOperator(task_id='extract_data', ...)\n",
    "    \n",
    "    quality_check = BranchPythonOperator(\n",
    "        task_id='quality_check',\n",
    "        python_callable=check_data_quality\n",
    "    )\n",
    "    \n",
    "    pass_quality = PythonOperator(task_id='pass_quality', ...)\n",
    "    fail_quality = PythonOperator(task_id='fail_quality', ...)  # Send alert\n",
    "    \n",
    "    extract >> quality_check >> [pass_quality, fail_quality]\n",
    "```\n",
    "\n",
    "**Great Expectations Integration (Alternative):**\n",
    "\n",
    "```python\n",
    "import great_expectations as gx\n",
    "\n",
    "# Setup\n",
    "context = gx.get_context()\n",
    "suite = context.create_expectation_suite(\"ventas_suite\")\n",
    "\n",
    "# Expectations (equivalente a nuestras dimensiones)\n",
    "validator = context.sources.pandas_default.read_dataframe(df)\n",
    "\n",
    "# Completitud\n",
    "validator.expect_column_values_to_not_be_null(\"precio\")\n",
    "\n",
    "# Unicidad\n",
    "validator.expect_column_values_to_be_unique(\"venta_id\")\n",
    "\n",
    "# Validez (rango)\n",
    "validator.expect_column_values_to_be_between(\"cantidad\", min_value=0, max_value=1000)\n",
    "\n",
    "# Consistencia\n",
    "validator.expect_column_pair_values_A_to_be_greater_than_B(\n",
    "    column_A=\"fecha_entrega\", \n",
    "    column_B=\"fecha_pedido\"\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "results = validator.validate()\n",
    "print(results.success)  # True/False\n",
    "```\n",
    "\n",
    "**Comparaci\u00f3n de Herramientas:**\n",
    "\n",
    "| Feature | Custom Framework | Great Expectations | dbt tests |\n",
    "|---------|------------------|-------------------|-----------|\n",
    "| **Setup** | Simple (1 clase) | Complejo (contexto, suite) | Medio (SQL) |\n",
    "| **Flexibilidad** | Alta | Media (extensible) | Media |\n",
    "| **Visualizaci\u00f3n** | DIY | Data Docs autom\u00e1tico | dbt Cloud |\n",
    "| **Integraci\u00f3n** | Manual | Airflow/Prefect | dbt native |\n",
    "| **Learning Curve** | Bajo | Alto | Medio |\n",
    "| **Best For** | Custom rules | Enterprise governance | SQL-centric teams |\n",
    "\n",
    "**Logging y Observability:**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "# Structured logging\n",
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def evaluar_completitud(self):\n",
    "    logger.info(\"DQ check started\", extra={\n",
    "        'check_type': 'completitud',\n",
    "        'dataset': self.nombre_dataset,\n",
    "        'total_records': len(self.df)\n",
    "    })\n",
    "    \n",
    "    # ... evaluaci\u00f3n ...\n",
    "    \n",
    "    logger.info(\"DQ check completed\", extra={\n",
    "        'check_type': 'completitud',\n",
    "        'score': completitud_pct,\n",
    "        'passed': completitud_pct >= 95\n",
    "    })\n",
    "    \n",
    "    return completitud_pct\n",
    "```\n",
    "\n",
    "**M\u00e9tricas para Prometheus:**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "dq_checks_total = Counter('dq_checks_total', 'Total DQ checks', ['dataset', 'dimension'])\n",
    "dq_score = Gauge('dq_score', 'DQ score 0-100', ['dataset'])\n",
    "dq_violations = Counter('dq_violations_total', 'Total violations', ['dataset', 'rule'])\n",
    "\n",
    "def evaluar_completitud(self):\n",
    "    dq_checks_total.labels(dataset=self.nombre_dataset, dimension='completitud').inc()\n",
    "    \n",
    "    # ... evaluaci\u00f3n ...\n",
    "    \n",
    "    dq_score.labels(dataset=self.nombre_dataset).set(completitud_pct)\n",
    "    \n",
    "    if completitud_pct < 95:\n",
    "        dq_violations.labels(dataset=self.nombre_dataset, rule='completitud_threshold').inc()\n",
    "```\n",
    "\n",
    "**Testing del Framework:**\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_completitud_100_percent():\n",
    "    df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_completitud()\n",
    "    assert score == 100.0\n",
    "\n",
    "def test_unicidad_detecta_duplicados():\n",
    "    df = pd.DataFrame({'id': [1, 2, 2, 3]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_unicidad(['id'])\n",
    "    assert score == 75.0  # 3/4 \u00fanicos\n",
    "\n",
    "@pytest.mark.parametrize(\"cantidad,expected_valid\", [\n",
    "    (5, True),\n",
    "    (-1, False),\n",
    "    (0, False)\n",
    "])\n",
    "def test_validez_cantidad_positiva(cantidad, expected_valid):\n",
    "    df = pd.DataFrame({'cantidad': [cantidad]})\n",
    "    dq = DataQualityFramework(df, \"test\")\n",
    "    score = dq.evaluar_validez({'cantidad_positiva': 'cantidad > 0'})\n",
    "    assert (score == 100.0) == expected_valid\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5620f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityFramework:\n",
    "    \"\"\"\n",
    "    Framework para evaluar calidad de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, nombre_dataset: str):\n",
    "        self.df = df\n",
    "        self.nombre_dataset = nombre_dataset\n",
    "        self.reporte = {\n",
    "            'dataset': nombre_dataset,\n",
    "            'fecha_evaluacion': datetime.now().isoformat(),\n",
    "            'total_registros': len(df),\n",
    "            'total_columnas': len(df.columns),\n",
    "            'dimensiones': {}\n",
    "        }\n",
    "    \n",
    "    def evaluar_completitud(self):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 1: Completitud\n",
    "        \u00bfLos datos est\u00e1n completos?\n",
    "        \"\"\"\n",
    "        print(\"\ud83d\udcca Evaluando COMPLETITUD...\")\n",
    "        \n",
    "        total_valores = self.df.size\n",
    "        valores_nulos = self.df.isnull().sum().sum()\n",
    "        completitud_pct = ((total_valores - valores_nulos) / total_valores) * 100\n",
    "        \n",
    "        # Por columna\n",
    "        completitud_cols = {}\n",
    "        for col in self.df.columns:\n",
    "            pct = (1 - self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            completitud_cols[col] = round(pct, 2)\n",
    "        \n",
    "        self.reporte['dimensiones']['completitud'] = {\n",
    "            'score': round(completitud_pct, 2),\n",
    "            'valores_nulos': int(valores_nulos),\n",
    "            'por_columna': completitud_cols,\n",
    "            'aprobado': completitud_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {completitud_pct:.2f}%\")\n",
    "        print(f\"   \u2713 Valores nulos: {valores_nulos:,}\")\n",
    "        return completitud_pct\n",
    "    \n",
    "    def evaluar_unicidad(self, columnas_clave: list):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 2: Unicidad\n",
    "        \u00bfLos registros son \u00fanicos?\n",
    "        \"\"\"\n",
    "        print(\"\\n\ud83d\udd11 Evaluando UNICIDAD...\")\n",
    "        \n",
    "        duplicados = self.df.duplicated(subset=columnas_clave).sum()\n",
    "        unicidad_pct = ((len(self.df) - duplicados) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['unicidad'] = {\n",
    "            'score': round(unicidad_pct, 2),\n",
    "            'duplicados': int(duplicados),\n",
    "            'columnas_evaluadas': columnas_clave,\n",
    "            'aprobado': duplicados == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {unicidad_pct:.2f}%\")\n",
    "        print(f\"   \u2713 Duplicados: {duplicados}\")\n",
    "        return unicidad_pct\n",
    "    \n",
    "    def evaluar_validez(self, reglas: dict):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 3: Validez\n",
    "        \u00bfLos datos cumplen reglas de negocio?\n",
    "        \"\"\"\n",
    "        print(\"\\n\u2705 Evaluando VALIDEZ...\")\n",
    "        \n",
    "        violaciones = {}\n",
    "        total_registros = len(self.df)\n",
    "        registros_validos = total_registros\n",
    "        \n",
    "        for regla_nombre, regla_condicion in reglas.items():\n",
    "            try:\n",
    "                registros_invalidos = (~self.df.eval(regla_condicion)).sum()\n",
    "                violaciones[regla_nombre] = int(registros_invalidos)\n",
    "                registros_validos -= registros_invalidos\n",
    "                print(f\"   \u2022 {regla_nombre}: {registros_invalidos} violaciones\")\n",
    "            except Exception as e:\n",
    "                print(f\"   \u26a0\ufe0f Error en regla '{regla_nombre}': {e}\")\n",
    "        \n",
    "        validez_pct = (registros_validos / total_registros) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['validez'] = {\n",
    "            'score': round(validez_pct, 2),\n",
    "            'violaciones': violaciones,\n",
    "            'reglas_evaluadas': list(reglas.keys()),\n",
    "            'aprobado': validez_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {validez_pct:.2f}%\")\n",
    "        return validez_pct\n",
    "    \n",
    "    def evaluar_consistencia(self, columnas_relacionadas: dict):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 4: Consistencia\n",
    "        \u00bfLos datos son consistentes entre s\u00ed?\n",
    "        \"\"\"\n",
    "        print(\"\\n\ud83d\udd04 Evaluando CONSISTENCIA...\")\n",
    "        \n",
    "        inconsistencias = {}\n",
    "        \n",
    "        for nombre, condicion in columnas_relacionadas.items():\n",
    "            try:\n",
    "                count = (~self.df.eval(condicion)).sum()\n",
    "                inconsistencias[nombre] = int(count)\n",
    "                print(f\"   \u2022 {nombre}: {count} inconsistencias\")\n",
    "            except Exception as e:\n",
    "                print(f\"   \u26a0\ufe0f Error en validaci\u00f3n '{nombre}': {e}\")\n",
    "        \n",
    "        total_inconsistencias = sum(inconsistencias.values())\n",
    "        consistencia_pct = ((len(self.df) - total_inconsistencias) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['consistencia'] = {\n",
    "            'score': round(consistencia_pct, 2),\n",
    "            'inconsistencias': inconsistencias,\n",
    "            'aprobado': consistencia_pct >= 95\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {consistencia_pct:.2f}%\")\n",
    "        return consistencia_pct\n",
    "    \n",
    "    def evaluar_exactitud(self, columna_numerica: str, rango_esperado: tuple):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 5: Exactitud\n",
    "        \u00bfLos valores son correctos?\n",
    "        \"\"\"\n",
    "        print(\"\\n\ud83c\udfaf Evaluando EXACTITUD...\")\n",
    "        \n",
    "        min_val, max_val = rango_esperado\n",
    "        fuera_rango = ((self.df[columna_numerica] < min_val) | \n",
    "                       (self.df[columna_numerica] > max_val)).sum()\n",
    "        \n",
    "        exactitud_pct = ((len(self.df) - fuera_rango) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['exactitud'] = {\n",
    "            'score': round(exactitud_pct, 2),\n",
    "            'fuera_de_rango': int(fuera_rango),\n",
    "            'columna_evaluada': columna_numerica,\n",
    "            'rango_esperado': rango_esperado,\n",
    "            'aprobado': fuera_rango == 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {exactitud_pct:.2f}%\")\n",
    "        print(f\"   \u2713 Fuera de rango: {fuera_rango}\")\n",
    "        return exactitud_pct\n",
    "    \n",
    "    def evaluar_actualidad(self, columna_fecha: str, dias_maximos: int):\n",
    "        \"\"\"\n",
    "        Dimensi\u00f3n 6: Actualidad\n",
    "        \u00bfLos datos son recientes?\n",
    "        \"\"\"\n",
    "        print(\"\\n\ud83d\udcc5 Evaluando ACTUALIDAD...\")\n",
    "        \n",
    "        hoy = pd.Timestamp.now()\n",
    "        self.df[columna_fecha] = pd.to_datetime(self.df[columna_fecha])\n",
    "        antiguos = (hoy - self.df[columna_fecha]) > timedelta(days=dias_maximos)\n",
    "        registros_antiguos = antiguos.sum()\n",
    "        \n",
    "        actualidad_pct = ((len(self.df) - registros_antiguos) / len(self.df)) * 100\n",
    "        \n",
    "        self.reporte['dimensiones']['actualidad'] = {\n",
    "            'score': round(actualidad_pct, 2),\n",
    "            'registros_antiguos': int(registros_antiguos),\n",
    "            'dias_maximos': dias_maximos,\n",
    "            'aprobado': actualidad_pct >= 80\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Score: {actualidad_pct:.2f}%\")\n",
    "        print(f\"   \u2713 Registros antiguos: {registros_antiguos}\")\n",
    "        return actualidad_pct\n",
    "    \n",
    "    def generar_reporte_completo(self):\n",
    "        \"\"\"\n",
    "        Genera reporte ejecutivo de calidad\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"\ud83d\udcca REPORTE DE CALIDAD DE DATOS: {self.nombre_dataset}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calcular score general\n",
    "        scores = [d['score'] for d in self.reporte['dimensiones'].values() if 'score' in d]\n",
    "        score_general = sum(scores) / len(scores) if scores else 0\n",
    "        \n",
    "        self.reporte['score_general'] = round(score_general, 2)\n",
    "        self.reporte['nivel_calidad'] = self._clasificar_calidad(score_general)\n",
    "        \n",
    "        print(f\"\\n\ud83c\udfaf Score General: {score_general:.2f}%\")\n",
    "        print(f\"\ud83d\udcc8 Nivel de Calidad: {self.reporte['nivel_calidad']}\")\n",
    "        print(f\"\\n\ud83d\udccb Detalles por Dimensi\u00f3n:\")\n",
    "        \n",
    "        for dimension, datos in self.reporte['dimensiones'].items():\n",
    "            status = \"\u2705\" if datos.get('aprobado', False) else \"\u274c\"\n",
    "            print(f\"   {status} {dimension.upper()}: {datos.get('score', 'N/A')}%\")\n",
    "        \n",
    "        # Guardar reporte\n",
    "        output_path = f\"../datasets/processed/reporte_calidad_{self.nombre_dataset}.json\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.reporte, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcbe Reporte guardado en: {output_path}\")\n",
    "        return self.reporte\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clasificar_calidad(score):\n",
    "        \"\"\"Clasifica el nivel de calidad seg\u00fan el score\"\"\"\n",
    "        if score >= 95:\n",
    "            return \"\u2b50\u2b50\u2b50 EXCELENTE\"\n",
    "        elif score >= 85:\n",
    "            return \"\u2b50\u2b50 BUENO\"\n",
    "        elif score >= 70:\n",
    "            return \"\u2b50 ACEPTABLE\"\n",
    "        else:\n",
    "            return \"\u274c CR\u00cdTICO\"\n",
    "\n",
    "print(\"\u2705 DataQualityFramework definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213b42",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Caso Pr\u00e1ctico: Evaluaci\u00f3n de Calidad\n",
    "\n",
    "Vamos a evaluar un dataset de ventas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5b060",
   "metadata": {},
   "source": [
    "### \ud83e\uddea **Caso Pr\u00e1ctico: From Bad Data to Gold Standard**\n",
    "\n",
    "**Escenario Real:**\n",
    "\n",
    "Empresa de e-commerce con 1M+ ventas diarias. El equipo de Analytics reporta:\n",
    "- \u274c Dashboards muestran ventas negativas\n",
    "- \u274c Duplicados causan sobreconteo de revenue\n",
    "- \u274c Missing data en ~20% de registros\n",
    "- \u274c Descuentos > 100% generan alertas falsas\n",
    "\n",
    "**Impacto:** CFO no conf\u00eda en reportes \u2192 Decisiones basadas en \"feeling\"\n",
    "\n",
    "**Soluci\u00f3n: Implementar DQ Framework**\n",
    "\n",
    "**Step 1: Assessment (Baseline)**\n",
    "\n",
    "```python\n",
    "# Snapshot inicial\n",
    "dq_initial = DataQualityFramework(df_ventas_raw, \"ventas_oct_2025\")\n",
    "\n",
    "# Run all dimensions\n",
    "dq_initial.evaluar_completitud()        # 80%  \u274c\n",
    "dq_initial.evaluar_unicidad(['venta_id'])  # 99%  \u26a0\ufe0f (10 duplicados)\n",
    "dq_initial.evaluar_validez(business_rules)  # 75%  \u274c\n",
    "dq_initial.evaluar_consistencia({...})   # 85%  \u26a0\ufe0f\n",
    "\n",
    "report_initial = dq_initial.generar_reporte_completo()\n",
    "# Score general: 82% \u2192 \u2b50 ACEPTABLE (pero inaceptable para prod)\n",
    "```\n",
    "\n",
    "**Step 2: Data Profiling (Root Cause Analysis)**\n",
    "\n",
    "```python\n",
    "# \u00bfQu\u00e9 columnas tienen m\u00e1s problemas?\n",
    "completitud_por_columna = report_initial['dimensiones']['completitud']['por_columna']\n",
    "worst_columns = sorted(completitud_por_columna.items(), key=lambda x: x[1])[:5]\n",
    "\n",
    "print(\"Top 5 columnas con m\u00e1s nulls:\")\n",
    "for col, pct in worst_columns:\n",
    "    print(f\"  - {col}: {100-pct:.1f}% nulls\")\n",
    "    \n",
    "# Output:\n",
    "#   - producto: 15% nulls         \u2192 API devuelve null cuando SKU inv\u00e1lido\n",
    "#   - descuento_pct: 10% nulls    \u2192 Campo opcional en checkout\n",
    "#   - email_cliente: 8% nulls     \u2192 Guests no proveen email\n",
    "#   - fecha_entrega: 20% nulls    \u2192 Solo para \u00f3rdenes enviadas\n",
    "#   - cupon_id: 60% nulls         \u2192 Mayor\u00eda de ventas sin cup\u00f3n\n",
    "```\n",
    "\n",
    "**Step 3: Remediation Plan**\n",
    "\n",
    "```python\n",
    "# Plan de remediaci\u00f3n priorizado\n",
    "remediation_plan = {\n",
    "    'P0_Critical': [\n",
    "        {\n",
    "            'issue': 'Cantidades negativas',\n",
    "            'rule': 'cantidad > 0',\n",
    "            'fix': 'Rechazar en validaci\u00f3n pre-insert',\n",
    "            'owner': 'Backend team'\n",
    "        },\n",
    "        {\n",
    "            'issue': 'Duplicados por retry',\n",
    "            'rule': 'venta_id unique',\n",
    "            'fix': 'Implement idempotency key',\n",
    "            'owner': 'Platform team'\n",
    "        }\n",
    "    ],\n",
    "    'P1_High': [\n",
    "        {\n",
    "            'issue': 'Missing producto',\n",
    "            'rule': 'producto not null',\n",
    "            'fix': 'Lookup SKU en master data antes de insert',\n",
    "            'owner': 'Data team'\n",
    "        },\n",
    "        {\n",
    "            'issue': 'Descuentos > 100%',\n",
    "            'rule': 'descuento_pct between 0 and 100',\n",
    "            'fix': 'Validaci\u00f3n en frontend + backend',\n",
    "            'owner': 'Frontend team'\n",
    "        }\n",
    "    ],\n",
    "    'P2_Medium': [\n",
    "        {\n",
    "            'issue': 'Email null',\n",
    "            'rule': 'email not null for registered users',\n",
    "            'fix': 'Default to \"guest@example.com\" for guests',\n",
    "            'owner': 'Data team'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 4: Implementation (Data Cleaning)**\n",
    "\n",
    "```python\n",
    "def clean_ventas_data(df_raw):\n",
    "    \"\"\"Pipeline de limpieza aplicando remediaciones\"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Fix 1: Remover cantidades negativas (P0)\n",
    "    before = len(df)\n",
    "    df = df[df['cantidad'] > 0]\n",
    "    logger.info(f\"Removed {before - len(df)} records with cantidad <= 0\")\n",
    "    \n",
    "    # Fix 2: Deduplicar por venta_id (P0)\n",
    "    df = df.drop_duplicates(subset=['venta_id'], keep='last')\n",
    "    logger.info(f\"Removed {before - len(df)} duplicates\")\n",
    "    \n",
    "    # Fix 3: Imputar producto missing (P1)\n",
    "    df.loc[df['producto'].isnull(), 'producto'] = 'UNKNOWN'\n",
    "    \n",
    "    # Fix 4: Caps descuentos (P1)\n",
    "    df['descuento_pct'] = df['descuento_pct'].clip(lower=0, upper=100)\n",
    "    \n",
    "    # Fix 5: Imputar email (P2)\n",
    "    df.loc[df['email_cliente'].isnull() & df['tipo_cliente'] == 'guest', \n",
    "           'email_cliente'] = 'guest@example.com'\n",
    "    \n",
    "    # Fix 6: Recalcular total para consistencia\n",
    "    df['total'] = (df['cantidad'] * df['precio_unitario'] * \n",
    "                   (1 - df['descuento_pct'] / 100))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_ventas_clean = clean_ventas_data(df_ventas_raw)\n",
    "```\n",
    "\n",
    "**Step 5: Re-assessment (Validation)**\n",
    "\n",
    "```python\n",
    "# Re-evaluar post-limpieza\n",
    "dq_final = DataQualityFramework(df_ventas_clean, \"ventas_oct_2025_clean\")\n",
    "\n",
    "dq_final.evaluar_completitud()        # 98%  \u2705\n",
    "dq_final.evaluar_unicidad(['venta_id'])  # 100%  \u2705\n",
    "dq_final.evaluar_validez(business_rules)  # 99%  \u2705\n",
    "dq_final.evaluar_consistencia({...})   # 99%  \u2705\n",
    "\n",
    "report_final = dq_final.generar_reporte_completo()\n",
    "# Score general: 97.5% \u2192 \u2b50\u2b50\u2b50 EXCELENTE\n",
    "```\n",
    "\n",
    "**Step 6: Monitoring (Continuous)**\n",
    "\n",
    "```python\n",
    "# Airflow DAG diario\n",
    "@dag(schedule_interval='@daily')\n",
    "def ventas_dq_monitoring():\n",
    "    \n",
    "    @task\n",
    "    def extract_ventas():\n",
    "        return query_db(\"SELECT * FROM ventas WHERE date = CURRENT_DATE\")\n",
    "    \n",
    "    @task\n",
    "    def run_dq_checks(df):\n",
    "        dq = DataQualityFramework(df, f\"ventas_{datetime.now().date()}\")\n",
    "        report = dq.generar_reporte_completo()\n",
    "        \n",
    "        # Store metrics in TimeSeries DB\n",
    "        store_metrics(report)\n",
    "        \n",
    "        # Alert if regression\n",
    "        if report['score_general'] < 95:\n",
    "            send_slack_alert(\n",
    "                channel=\"#data-quality\",\n",
    "                message=f\"\u26a0\ufe0f DQ score dropped to {report['score_general']}%\",\n",
    "                report_url=f\"/reports/{report['dataset']}\"\n",
    "            )\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    df = extract_ventas()\n",
    "    run_dq_checks(df)\n",
    "\n",
    "dag = ventas_dq_monitoring()\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "\n",
    "| Metric | Before | After | Improvement |\n",
    "|--------|--------|-------|-------------|\n",
    "| **DQ Score** | 82% | 97.5% | +15.5% |\n",
    "| **Duplicates** | 10/day | 0 | 100% |\n",
    "| **Completitud** | 80% | 98% | +18% |\n",
    "| **Validez** | 75% | 99% | +24% |\n",
    "| **CFO Confidence** | \u274c | \u2705 | Priceless |\n",
    "\n",
    "**Business Impact:**\n",
    "- \u2705 Revenue reporting accuracy: +$500K/month (false negatives eliminados)\n",
    "- \u2705 Compliance audit: Pass (antes Fail)\n",
    "- \u2705 Data analyst productivity: +30% (menos tiempo limpiando)\n",
    "- \u2705 Executive trust: \"Now I can make data-driven decisions\"\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7273b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset de ejemplo con problemas de calidad intencionales\n",
    "np.random.seed(42)\n",
    "\n",
    "n_registros = 1000\n",
    "\n",
    "df_ventas = pd.DataFrame({\n",
    "    'venta_id': range(1, n_registros + 1),\n",
    "    'cliente_id': np.random.randint(1, 200, n_registros),\n",
    "    'producto': np.random.choice(['Laptop', 'Mouse', 'Teclado', 'Monitor', None], n_registros),\n",
    "    'cantidad': np.random.randint(-5, 50, n_registros),  # Algunas cantidades negativas\n",
    "    'precio_unitario': np.random.uniform(10, 2000, n_registros),\n",
    "    'descuento_pct': np.random.uniform(0, 100, n_registros),  # Algunos > 100%\n",
    "    'total': np.random.uniform(0, 5000, n_registros),\n",
    "    'fecha_venta': pd.date_range(start='2023-01-01', periods=n_registros, freq='H'),\n",
    "    'estado': np.random.choice(['Completada', 'Pendiente', 'Cancelada', None], n_registros)\n",
    "})\n",
    "\n",
    "# Introducir duplicados intencionales\n",
    "df_ventas = pd.concat([df_ventas, df_ventas.iloc[:10]], ignore_index=True)\n",
    "\n",
    "# Introducir algunos nulls adicionales\n",
    "df_ventas.loc[np.random.choice(df_ventas.index, 50), 'precio_unitario'] = np.nan\n",
    "\n",
    "print(f\"\ud83d\udcca Dataset creado: {len(df_ventas)} registros, {len(df_ventas.columns)} columnas\")\n",
    "print(f\"\\n\ud83d\udd0d Primeras filas:\")\n",
    "print(df_ventas.head())\n",
    "print(f\"\\n\ud83d\udcc8 Info del dataset:\")\n",
    "print(df_ventas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar framework\n",
    "dq = DataQualityFramework(df_ventas, 'ventas_ecommerce')\n",
    "\n",
    "# Evaluar todas las dimensiones\n",
    "print(\"\ud83d\ude80 Iniciando evaluaci\u00f3n de calidad...\\n\")\n",
    "\n",
    "# 1. Completitud\n",
    "dq.evaluar_completitud()\n",
    "\n",
    "# 2. Unicidad\n",
    "dq.evaluar_unicidad(columnas_clave=['venta_id'])\n",
    "\n",
    "# 3. Validez\n",
    "reglas_negocio = {\n",
    "    'cantidad_positiva': 'cantidad > 0',\n",
    "    'descuento_valido': 'descuento_pct >= 0 and descuento_pct <= 100',\n",
    "    'total_positivo': 'total > 0'\n",
    "}\n",
    "dq.evaluar_validez(reglas_negocio)\n",
    "\n",
    "# 4. Consistencia\n",
    "relaciones = {\n",
    "    'total_coherente': 'total >= (precio_unitario * cantidad * (1 - descuento_pct/100)) * 0.95',\n",
    "}\n",
    "dq.evaluar_consistencia(relaciones)\n",
    "\n",
    "# 5. Exactitud\n",
    "dq.evaluar_exactitud('precio_unitario', (5, 3000))\n",
    "\n",
    "# 6. Actualidad\n",
    "dq.evaluar_actualidad('fecha_venta', dias_maximos=365)\n",
    "\n",
    "# Generar reporte completo\n",
    "reporte = dq.generar_reporte_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e327a",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Data Lineage - Trazabilidad de Datos\n",
    "\n",
    "Implementaci\u00f3n de un sistema b\u00e1sico de lineage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35031a",
   "metadata": {},
   "source": [
    "### \ud83d\udd2c **Great Expectations: Enterprise Data Quality**\n",
    "\n",
    "**\u00bfCu\u00e1ndo usar Great Expectations vs Custom Framework?**\n",
    "\n",
    "| Escenario | Recomendaci\u00f3n |\n",
    "|-----------|---------------|\n",
    "| **Startup/SMB** | Custom Framework (simple, r\u00e1pido) |\n",
    "| **Enterprise** | Great Expectations (governance completo) |\n",
    "| **Team < 5** | Custom |\n",
    "| **Team > 10** | Great Expectations |\n",
    "| **SQL-first team** | dbt tests |\n",
    "| **Python-first team** | Great Expectations o Custom |\n",
    "\n",
    "**Great Expectations Architecture:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              DATA CONTEXT (GX Project)                \u2502\n",
    "\u2502                                                       \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n",
    "\u2502  \u2502  EXPECTATIONS   \u2502  \u2502   DATASOURCES   \u2502          \u2502\n",
    "\u2502  \u2502  (Rules/Tests)  \u2502  \u2502   (Connections) \u2502          \u2502\n",
    "\u2502  \u2502                 \u2502  \u2502                 \u2502          \u2502\n",
    "\u2502  \u2502  \u2022 Suite 1      \u2502  \u2502  \u2022 Postgres     \u2502          \u2502\n",
    "\u2502  \u2502  \u2022 Suite 2      \u2502  \u2502  \u2022 S3           \u2502          \u2502\n",
    "\u2502  \u2502  \u2022 Suite 3      \u2502  \u2502  \u2022 Pandas DF    \u2502          \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n",
    "\u2502                                                       \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n",
    "\u2502  \u2502  CHECKPOINTS    \u2502  \u2502   DATA DOCS     \u2502          \u2502\n",
    "\u2502  \u2502  (Validation    \u2502  \u2502   (Reports)     \u2502          \u2502\n",
    "\u2502  \u2502   Workflows)    \u2502  \u2502                 \u2502          \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "```python\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "\n",
    "# Initialize context\n",
    "context = gx.get_context()\n",
    "\n",
    "# Add datasource\n",
    "datasource = context.sources.add_pandas(\"my_datasource\")\n",
    "data_asset = datasource.add_dataframe_asset(name=\"ventas_df\")\n",
    "\n",
    "# Create expectation suite\n",
    "suite = context.add_expectation_suite(\"ventas_quality_suite\")\n",
    "\n",
    "# Get validator\n",
    "batch_request = data_asset.build_batch_request(dataframe=df_ventas)\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"ventas_quality_suite\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Expectations (200+ built-in):**\n",
    "\n",
    "```python\n",
    "# COMPLETITUD\n",
    "validator.expect_column_values_to_not_be_null(\n",
    "    column=\"venta_id\",\n",
    "    mostly=1.0  # 100% not null\n",
    ")\n",
    "\n",
    "validator.expect_column_values_to_not_be_null(\n",
    "    column=\"email\",\n",
    "    mostly=0.8  # Al menos 80% not null\n",
    ")\n",
    "\n",
    "# UNICIDAD\n",
    "validator.expect_column_values_to_be_unique(\n",
    "    column=\"venta_id\"\n",
    ")\n",
    "\n",
    "validator.expect_compound_columns_to_be_unique(\n",
    "    column_list=[\"cliente_id\", \"fecha_venta\", \"producto_id\"]\n",
    ")\n",
    "\n",
    "# VALIDEZ (Rango)\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column=\"cantidad\",\n",
    "    min_value=1,\n",
    "    max_value=1000\n",
    ")\n",
    "\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column=\"descuento_pct\",\n",
    "    min_value=0,\n",
    "    max_value=100,\n",
    "    mostly=0.99  # 99% dentro del rango\n",
    ")\n",
    "\n",
    "# VALIDEZ (Valores permitidos)\n",
    "validator.expect_column_values_to_be_in_set(\n",
    "    column=\"estado\",\n",
    "    value_set=[\"Completada\", \"Pendiente\", \"Cancelada\"]\n",
    ")\n",
    "\n",
    "# VALIDEZ (Regex)\n",
    "validator.expect_column_values_to_match_regex(\n",
    "    column=\"email\",\n",
    "    regex=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    ")\n",
    "\n",
    "# VALIDEZ (Tipo de dato)\n",
    "validator.expect_column_values_to_be_of_type(\n",
    "    column=\"total\",\n",
    "    type_=\"float\"\n",
    ")\n",
    "\n",
    "# CONSISTENCIA (Cross-column)\n",
    "validator.expect_column_pair_values_A_to_be_greater_than_B(\n",
    "    column_A=\"fecha_entrega\",\n",
    "    column_B=\"fecha_pedido\"\n",
    ")\n",
    "\n",
    "# EXACTITUD (Estad\u00edsticas)\n",
    "validator.expect_column_mean_to_be_between(\n",
    "    column=\"total\",\n",
    "    min_value=50,\n",
    "    max_value=500\n",
    ")\n",
    "\n",
    "validator.expect_column_stdev_to_be_between(\n",
    "    column=\"total\",\n",
    "    min_value=10,\n",
    "    max_value=200\n",
    ")\n",
    "\n",
    "# ACTUALIDAD\n",
    "validator.expect_column_max_to_be_between(\n",
    "    column=\"fecha_venta\",\n",
    "    min_value=datetime.now() - timedelta(days=1),\n",
    "    max_value=datetime.now()\n",
    ")\n",
    "\n",
    "# Save suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "```\n",
    "\n",
    "**Checkpoint (Automated Validation):**\n",
    "\n",
    "```python\n",
    "# Create checkpoint\n",
    "checkpoint_config = {\n",
    "    \"name\": \"ventas_daily_checkpoint\",\n",
    "    \"config_version\": 1.0,\n",
    "    \"class_name\": \"Checkpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": {\n",
    "                \"datasource_name\": \"my_datasource\",\n",
    "                \"data_asset_name\": \"ventas_df\"\n",
    "            },\n",
    "            \"expectation_suite_name\": \"ventas_quality_suite\"\n",
    "        }\n",
    "    ],\n",
    "    \"action_list\": [\n",
    "        {\n",
    "            \"name\": \"store_validation_result\",\n",
    "            \"action\": {\"class_name\": \"StoreValidationResultAction\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"update_data_docs\",\n",
    "            \"action\": {\"class_name\": \"UpdateDataDocsAction\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"send_slack_notification\",\n",
    "            \"action\": {\n",
    "                \"class_name\": \"SlackNotificationAction\",\n",
    "                \"slack_webhook\": \"https://hooks.slack.com/...\",\n",
    "                \"notify_on\": \"failure\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "checkpoint = context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "# Run checkpoint\n",
    "result = checkpoint.run()\n",
    "\n",
    "# Check result\n",
    "if not result[\"success\"]:\n",
    "    print(\"\u274c Validation failed!\")\n",
    "    for validation in result[\"run_results\"].values():\n",
    "        for expectation in validation[\"validation_result\"][\"results\"]:\n",
    "            if not expectation[\"success\"]:\n",
    "                print(f\"  - {expectation['expectation_config']['expectation_type']}\")\n",
    "                print(f\"    Observed: {expectation['result']['observed_value']}\")\n",
    "else:\n",
    "    print(\"\u2705 All validations passed!\")\n",
    "```\n",
    "\n",
    "**Data Docs (Auto-generated Reports):**\n",
    "\n",
    "```python\n",
    "# Build Data Docs\n",
    "context.build_data_docs()\n",
    "\n",
    "# Open in browser\n",
    "context.open_data_docs()\n",
    "\n",
    "# Output: Beautiful HTML report with:\n",
    "# - Expectation suite overview\n",
    "# - Validation results (pass/fail)\n",
    "# - Data profiling statistics\n",
    "# - Historical trends\n",
    "```\n",
    "\n",
    "**Integration con Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "import great_expectations as gx\n",
    "\n",
    "def validate_with_ge(**context):\n",
    "    gx_context = gx.get_context()\n",
    "    \n",
    "    # Get data\n",
    "    df = context['ti'].xcom_pull(task_ids='extract_data')\n",
    "    \n",
    "    # Run checkpoint\n",
    "    checkpoint = gx_context.get_checkpoint(\"ventas_daily_checkpoint\")\n",
    "    result = checkpoint.run(batch_request={\"dataframe\": df})\n",
    "    \n",
    "    # Fail task if validation fails\n",
    "    if not result[\"success\"]:\n",
    "        raise AirflowException(\"Great Expectations validation failed\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "validate_task = PythonOperator(\n",
    "    task_id='validate_data_quality',\n",
    "    python_callable=validate_with_ge\n",
    ")\n",
    "```\n",
    "\n",
    "**Custom Expectations (Extensible):**\n",
    "\n",
    "```python\n",
    "from great_expectations.expectations.expectation import ColumnMapExpectation\n",
    "\n",
    "class ExpectColumnValuesToBePrime(ColumnMapExpectation):\n",
    "    \"\"\"Custom: Validar n\u00fameros primos\"\"\"\n",
    "    \n",
    "    map_metric = \"column_values.prime\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_prime(n):\n",
    "        if n < 2:\n",
    "            return False\n",
    "        for i in range(2, int(n**0.5) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _validate(self, metrics, runtime_configuration, execution_engine):\n",
    "        return self.is_prime(metrics.get(\"column_values\"))\n",
    "\n",
    "# Use\n",
    "validator.expect_column_values_to_be_prime(column=\"id\")\n",
    "```\n",
    "\n",
    "**Profiling (Auto-discovery):**\n",
    "\n",
    "```python\n",
    "# GX can auto-generate expectations\n",
    "from great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler\n",
    "\n",
    "profiler = UserConfigurableProfiler(\n",
    "    profile_dataset=validator,\n",
    "    excluded_expectations=[\n",
    "        \"expect_column_values_to_be_unique\"  # Exclude specific\n",
    "    ]\n",
    ")\n",
    "\n",
    "suite = profiler.build_suite()\n",
    "# Auto-generates ~20-30 expectations based on data profile\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLineageTracker:\n",
    "    \"\"\"\n",
    "    Sistema para rastrear el linaje de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.linaje = []\n",
    "        self.grafo = {}\n",
    "    \n",
    "    def registrar_transformacion(self, \n",
    "                                 origen: str, \n",
    "                                 destino: str, \n",
    "                                 transformacion: str,\n",
    "                                 metadata: dict = None):\n",
    "        \"\"\"\n",
    "        Registra una transformaci\u00f3n de datos\n",
    "        \"\"\"\n",
    "        evento = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'origen': origen,\n",
    "            'destino': destino,\n",
    "            'transformacion': transformacion,\n",
    "            'metadata': metadata or {},\n",
    "            'id': hashlib.md5(f\"{origen}{destino}{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        }\n",
    "        \n",
    "        self.linaje.append(evento)\n",
    "        \n",
    "        # Construir grafo\n",
    "        if origen not in self.grafo:\n",
    "            self.grafo[origen] = []\n",
    "        self.grafo[origen].append(destino)\n",
    "        \n",
    "        print(f\"\ud83d\udcdd Registrado: {origen} \u2192 {destino} ({transformacion})\")\n",
    "        return evento['id']\n",
    "    \n",
    "    def obtener_linaje_completo(self, entidad: str, direccion='downstream'):\n",
    "        \"\"\"\n",
    "        Obtiene el linaje completo de una entidad\n",
    "        \"\"\"\n",
    "        if direccion == 'downstream':\n",
    "            # Hacia adelante (downstream)\n",
    "            return self._traverse_downstream(entidad)\n",
    "        else:\n",
    "            # Hacia atr\u00e1s (upstream)\n",
    "            return self._traverse_upstream(entidad)\n",
    "    \n",
    "    def _traverse_downstream(self, nodo, visitados=None):\n",
    "        if visitados is None:\n",
    "            visitados = set()\n",
    "        \n",
    "        if nodo in visitados:\n",
    "            return []\n",
    "        \n",
    "        visitados.add(nodo)\n",
    "        camino = [nodo]\n",
    "        \n",
    "        if nodo in self.grafo:\n",
    "            for hijo in self.grafo[nodo]:\n",
    "                camino.extend(self._traverse_downstream(hijo, visitados))\n",
    "        \n",
    "        return camino\n",
    "    \n",
    "    def _traverse_upstream(self, nodo):\n",
    "        # Invertir el grafo para upstream\n",
    "        grafo_invertido = {}\n",
    "        for origen, destinos in self.grafo.items():\n",
    "            for destino in destinos:\n",
    "                if destino not in grafo_invertido:\n",
    "                    grafo_invertido[destino] = []\n",
    "                grafo_invertido[destino].append(origen)\n",
    "        \n",
    "        return self._traverse_downstream(nodo, set())\n",
    "    \n",
    "    def visualizar_linaje(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaci\u00f3n simple del linaje\n",
    "        \"\"\"\n",
    "        print(\"\\n\ud83d\udcca MAPA DE LINAJE DE DATOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for origen, destinos in self.grafo.items():\n",
    "            print(f\"\\n\ud83d\udd39 {origen}\")\n",
    "            for destino in destinos:\n",
    "                # Encontrar transformaci\u00f3n\n",
    "                trans = next((e for e in self.linaje \n",
    "                            if e['origen'] == origen and e['destino'] == destino), {})\n",
    "                print(f\"   \u2514\u2500\u2192 {destino} [{trans.get('transformacion', 'N/A')}]\")\n",
    "    \n",
    "    def exportar_linaje(self, ruta_salida: str):\n",
    "        \"\"\"\n",
    "        Exporta linaje a archivo JSON\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)\n",
    "        \n",
    "        with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'eventos': self.linaje,\n",
    "                'grafo': self.grafo,\n",
    "                'generado': datetime.now().isoformat()\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcbe Linaje exportado a: {ruta_salida}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "tracker = DataLineageTracker()\n",
    "\n",
    "print(\"\ud83d\udd0d Simulando pipeline con lineage tracking:\\n\")\n",
    "\n",
    "# Registrar transformaciones\n",
    "tracker.registrar_transformacion(\n",
    "    'raw_ventas_csv',\n",
    "    'stage_ventas',\n",
    "    'extraccion',\n",
    "    {'filas': 1000, 'columnas': 9}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'stage_ventas',\n",
    "    'clean_ventas',\n",
    "    'limpieza',\n",
    "    {'nulos_removidos': 50, 'duplicados_removidos': 10}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'agg_ventas_diarias',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'fecha', 'metricas': ['sum', 'count', 'avg']}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'clean_ventas',\n",
    "    'ventas_por_producto',\n",
    "    'agregacion',\n",
    "    {'grupo_por': 'producto'}\n",
    ")\n",
    "\n",
    "tracker.registrar_transformacion(\n",
    "    'agg_ventas_diarias',\n",
    "    'dashboard_ventas',\n",
    "    'visualizacion',\n",
    "    {'tipo': 'time_series'}\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "tracker.visualizar_linaje()\n",
    "\n",
    "# Exportar\n",
    "tracker.exportar_linaje('../datasets/processed/data_lineage.json')\n",
    "\n",
    "# Consultar linaje\n",
    "print(\"\\n\ud83d\udd0e Linaje downstream de 'clean_ventas':\")\n",
    "downstream = tracker.obtener_linaje_completo('clean_ventas', 'downstream')\n",
    "print(f\"   {' \u2192 '.join(downstream)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c090f1",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Resumen y Mejores Pr\u00e1cticas\n",
    "\n",
    "### \u2705 Principios Clave de Data Governance:\n",
    "\n",
    "1. **Proactivo, no Reactivo**\n",
    "   - Validaciones autom\u00e1ticas en cada etapa\n",
    "   - Prevenci\u00f3n vs correcci\u00f3n\n",
    "\n",
    "2. **Cultura de Calidad**\n",
    "   - Responsabilidad compartida\n",
    "   - Data stewards claramente definidos\n",
    "\n",
    "3. **Automatizaci\u00f3n**\n",
    "   - Tests automatizados\n",
    "   - Monitoreo continuo\n",
    "   - Alertas proactivas\n",
    "\n",
    "4. **Documentaci\u00f3n**\n",
    "   - Lineage completo\n",
    "   - Metadata actualizado\n",
    "   - Pol\u00edticas claras\n",
    "\n",
    "5. **Balance**\n",
    "   - Gobernanza sin burocracia\n",
    "   - Flexibilidad con control\n",
    "\n",
    "### \ud83d\udd1c Pr\u00f3ximos Temas:\n",
    "\n",
    "- Implementaci\u00f3n de Data Catalogs\n",
    "- Master Data Management (MDM)\n",
    "- Privacy by Design y PII handling\n",
    "- Compliance automation (GDPR, CCPA)\n",
    "- Cost optimization y FinOps\n",
    "\n",
    "---\n",
    "\n",
    "**\u00a1Has dominado los fundamentos de Data Governance!** \ud83c\udfc6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\u2190 README del Curso](../../README.md)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera) \u2192](02_lakehouse_delta_iceberg.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
