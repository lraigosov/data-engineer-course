{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6b9ec7",
   "metadata": {},
   "source": [
    "# \ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store\n",
    "\n",
    "Objetivo: dise\u00f1ar una arquitectura Data Mesh con m\u00faltiples dominios aut\u00f3nomos, feature store centralizado para ML, y gobernanza federada.\n",
    "\n",
    "- Duraci\u00f3n: 180+ min (proyecto multi-d\u00eda)\n",
    "- Dificultad: Muy Alta\n",
    "- Prerrequisitos: Senior completo (01\u201308), experiencia organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb239f",
   "metadata": {},
   "source": [
    "### \ud83c\udf10 **Data Mesh: Paradigm Shift from Centralized to Federated**\n",
    "\n",
    "**1. The Problem with Centralized Data Platforms**\n",
    "\n",
    "```\n",
    "Traditional Monolithic Data Platform:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502     Central Data Platform Team             \u2502\n",
    "\u2502  (Bottleneck: 5 engineers, 50 consumers)   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 \u2022 All ETL pipelines                        \u2502\n",
    "\u2502 \u2022 All data quality checks                  \u2502\n",
    "\u2502 \u2022 All API development                      \u2502\n",
    "\u2502 \u2022 All documentation                        \u2502\n",
    "\u2502 \u2022 All incident response                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u25bc  \u25bc  \u25bc  \u25bc  \u25bc\n",
    "    Requests: 200+/month\n",
    "    Lead Time: 6-12 weeks/feature\n",
    "    Quality: One team can't know all domains\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- \ud83d\udeab **Bottleneck**: Central team overwhelmed\n",
    "- \ud83d\udeab **Lack of Domain Expertise**: Platform team doesn't understand business nuances\n",
    "- \ud83d\udeab **Slow Innovation**: 3-month wait for new features\n",
    "- \ud83d\udeab **Poor Quality**: Generic validations miss domain-specific issues\n",
    "- \ud83d\udeab **No Ownership**: \"Not my problem\" mentality\n",
    "\n",
    "---\n",
    "\n",
    "**2. Data Mesh: Four Principles (Zhamak Dehghani)**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    Data Mesh Principles                         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  1. DOMAIN OWNERSHIP                                            \u2502\n",
    "\u2502     \"You build it, you own it\"                                  \u2502\n",
    "\u2502     - Each business domain owns its data products              \u2502\n",
    "\u2502     - Domain teams are accountable for quality & SLOs          \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  2. DATA AS A PRODUCT                                           \u2502\n",
    "\u2502     \"Treat data like software products\"                         \u2502\n",
    "\u2502     - Discoverable (catalog)                                    \u2502\n",
    "\u2502     - Addressable (versioned APIs)                              \u2502\n",
    "\u2502     - Trustworthy (quality SLOs)                                \u2502\n",
    "\u2502     - Self-describing (documentation)                           \u2502\n",
    "\u2502     - Secure (access controls)                                  \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  3. SELF-SERVE DATA PLATFORM                                    \u2502\n",
    "\u2502     \"Democratize infrastructure, not data\"                      \u2502\n",
    "\u2502     - Platform provides: storage, compute, observability       \u2502\n",
    "\u2502     - Domains consume: CI/CD, monitoring, governance tools     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  4. FEDERATED COMPUTATIONAL GOVERNANCE                          \u2502\n",
    "\u2502     \"Global policies, local execution\"                          \u2502\n",
    "\u2502     - Central: Security standards, PII policies, cost budgets  \u2502\n",
    "\u2502     - Local: Implementation details, tooling choices           \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Domain-Oriented Architecture**\n",
    "\n",
    "```python\n",
    "# domains.yml - Domain Registry\n",
    "domains:\n",
    "  \n",
    "  ventas:\n",
    "    owner: sales-team@company.com\n",
    "    mission: \"Provide revenue and transaction data products\"\n",
    "    data_products:\n",
    "      - daily_revenue_api\n",
    "      - customer_transaction_history\n",
    "      - product_performance\n",
    "    slos:\n",
    "      latency_p99: \"15 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "      data_quality: \"99.5% completeness\"\n",
    "    dependencies:\n",
    "      - producto.catalog_api  # Cross-domain dependency\n",
    "    budget_monthly: \"$2,000\"\n",
    "  \n",
    "  logistica:\n",
    "    owner: fulfillment-team@company.com\n",
    "    mission: \"Enable logistics optimization and tracking\"\n",
    "    data_products:\n",
    "      - shipment_tracking_api\n",
    "      - warehouse_inventory\n",
    "      - delivery_performance\n",
    "    slos:\n",
    "      latency_p99: \"5 minutes\"\n",
    "      availability: \"99.95%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$1,500\"\n",
    "  \n",
    "  producto:\n",
    "    owner: catalog-team@company.com\n",
    "    mission: \"Provide product catalog and pricing data\"\n",
    "    data_products:\n",
    "      - catalog_api\n",
    "      - pricing_history\n",
    "      - category_taxonomy\n",
    "    slos:\n",
    "      latency_p99: \"30 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "    dependencies: []\n",
    "    budget_monthly: \"$1,000\"\n",
    "  \n",
    "  marketing:\n",
    "    owner: campaigns-team@company.com\n",
    "    mission: \"Support campaign performance and customer segmentation\"\n",
    "    data_products:\n",
    "      - campaign_metrics\n",
    "      - customer_segments\n",
    "      - attribution_model\n",
    "    slos:\n",
    "      latency_p99: \"60 minutes\"\n",
    "      availability: \"99.5%\"\n",
    "    dependencies:\n",
    "      - ventas.customer_transaction_history\n",
    "      - producto.catalog_api\n",
    "    budget_monthly: \"$2,500\"\n",
    "  \n",
    "  finanzas:\n",
    "    owner: payments-team@company.com\n",
    "    mission: \"Financial reporting and fraud detection\"\n",
    "    data_products:\n",
    "      - payment_transactions\n",
    "      - fraud_scores\n",
    "      - accounting_reports\n",
    "    slos:\n",
    "      latency_p99: \"10 minutes\"\n",
    "      availability: \"99.99%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$3,000\"\n",
    "    compliance:\n",
    "      - PCI-DSS\n",
    "      - SOX\n",
    "```\n",
    "\n",
    "**Domain Team Structure:**\n",
    "\n",
    "```\n",
    "Ventas Domain Team (6 people):\n",
    "\u251c\u2500\u2500 Data Product Owner (1)\n",
    "\u2502   - Define requirements\n",
    "\u2502   - Prioritize features\n",
    "\u2502   - Communicate with consumers\n",
    "\u2502\n",
    "\u251c\u2500\u2500 Data Engineers (3)\n",
    "\u2502   - Build pipelines\n",
    "\u2502   - Maintain data quality\n",
    "\u2502   - Optimize performance\n",
    "\u2502\n",
    "\u251c\u2500\u2500 Analytics Engineer (1)\n",
    "\u2502   - Create gold layer aggregations\n",
    "\u2502   - Build BI dashboards\n",
    "\u2502\n",
    "\u2514\u2500\u2500 SRE/DevOps (1)\n",
    "    - CI/CD pipelines\n",
    "    - Incident response\n",
    "    - Cost optimization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Data Product Canvas**\n",
    "\n",
    "```markdown\n",
    "# Data Product: Daily Revenue API\n",
    "\n",
    "## Business Context\n",
    "**Purpose**: Provide real-time revenue metrics for executive dashboards and financial reporting\n",
    "\n",
    "**Consumers**: \n",
    "- Finance team (accounting reconciliation)\n",
    "- Executive dashboards (Tableau)\n",
    "- ML models (revenue forecasting)\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "### Input Sources\n",
    "- Kafka topic: `ecommerce.ventas.transactions`\n",
    "- S3 batch: `s3://mesh/ventas/raw/refunds/`\n",
    "\n",
    "### Output Schema (v2.1)\n",
    "```json\n",
    "{\n",
    "  \"date\": \"2024-01-15\",\n",
    "  \"region\": \"LATAM\",\n",
    "  \"revenue_gross\": 125000.50,\n",
    "  \"revenue_net\": 118000.30,\n",
    "  \"transactions_count\": 1543,\n",
    "  \"refunds_count\": 23,\n",
    "  \"currency\": \"USD\",\n",
    "  \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "### SLOs\n",
    "- **Latency**: p99 < 15 minutes (from transaction to API availability)\n",
    "- **Availability**: 99.9% uptime (43 minutes downtime/month allowed)\n",
    "- **Accuracy**: \u00b10.5% vs accounting system\n",
    "- **Freshness**: Data no older than 20 minutes\n",
    "\n",
    "### API Endpoints\n",
    "- `GET /ventas/v2/daily-revenue?date={YYYY-MM-DD}&region={region}`\n",
    "- `GET /ventas/v2/revenue-trend?start_date={}&end_date={}`\n",
    "\n",
    "### Access Control\n",
    "- Public: No (internal only)\n",
    "- Authentication: API key (service accounts)\n",
    "- Authorization: Read-only for finance, marketing, executive teams\n",
    "\n",
    "### Dependencies\n",
    "- **Upstream**: producto.catalog_api (for product prices)\n",
    "- **Downstream**: ML feature store, executive dashboards\n",
    "\n",
    "### Costs\n",
    "- Storage: $50/month (S3 Standard)\n",
    "- Compute: $200/month (EMR Serverless)\n",
    "- API hosting: $100/month (ECS Fargate)\n",
    "- **Total**: $350/month\n",
    "\n",
    "### Metrics\n",
    "- Request rate: 500 req/day\n",
    "- p99 latency: 12 minutes (within SLO)\n",
    "- Availability (30d): 99.92%\n",
    "- Quality score: 99.7% completeness\n",
    "\n",
    "### Changelog\n",
    "- v2.1 (2024-01): Added `refunds_count` field\n",
    "- v2.0 (2023-12): Breaking change - renamed `total` to `revenue_gross`\n",
    "- v1.5 (2023-10): Added `currency` field\n",
    "- v1.0 (2023-08): Initial release\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Self-Service Platform Components**\n",
    "\n",
    "```python\n",
    "# platform/infrastructure.py\n",
    "\"\"\"\n",
    "Self-service platform capabilities provided to all domains\n",
    "\"\"\"\n",
    "\n",
    "class DataPlatform:\n",
    "    \"\"\"\n",
    "    Shared infrastructure that domains consume\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.components = {\n",
    "            # Storage\n",
    "            \"s3_buckets\": self._provision_s3(),\n",
    "            \"glue_catalog\": self._setup_glue_catalog(),\n",
    "            \n",
    "            # Compute\n",
    "            \"emr_serverless\": self._create_emr_apps(),\n",
    "            \"airflow\": self._deploy_mwaa(),\n",
    "            \n",
    "            # Observability\n",
    "            \"datahub\": self._setup_datahub(),\n",
    "            \"grafana\": self._deploy_grafana(),\n",
    "            \"prometheus\": self._deploy_prometheus(),\n",
    "            \n",
    "            # Governance\n",
    "            \"iam_roles\": self._create_domain_roles(),\n",
    "            \"kms_keys\": self._create_encryption_keys(),\n",
    "            \n",
    "            # CI/CD\n",
    "            \"github_actions\": self._setup_workflows(),\n",
    "            \"terraform_modules\": self._publish_modules()\n",
    "        }\n",
    "    \n",
    "    def provision_domain(self, domain_name: str):\n",
    "        \"\"\"\n",
    "        Self-service: Domain team provisions own infrastructure\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"s3_bucket\": f\"s3://mesh/{domain_name}/\",\n",
    "            \"iam_role\": f\"arn:aws:iam::123:role/{domain_name}-pipeline\",\n",
    "            \"airflow_connection\": f\"{domain_name}_aws\",\n",
    "            \"datahub_domain\": f\"urn:li:domain:{domain_name}\",\n",
    "            \"grafana_folder\": f\"Domains/{domain_name}\",\n",
    "            \"cost_center_tag\": domain_name\n",
    "        }\n",
    "    \n",
    "    def _provision_s3(self):\n",
    "        \"\"\"\n",
    "        Create S3 buckets with standard structure\n",
    "        \"\"\"\n",
    "        bucket_policy = {\n",
    "            \"lifecycle_rules\": [\n",
    "                {\"raw\": \"7 days \u2192 Glacier\"},\n",
    "                {\"curated\": \"90 days \u2192 IA\"},\n",
    "                {\"gold\": \"retain indefinitely\"}\n",
    "            ],\n",
    "            \"encryption\": \"AWS KMS\",\n",
    "            \"versioning\": True,\n",
    "            \"tags\": {\"ManagedBy\": \"platform-team\"}\n",
    "        }\n",
    "        return bucket_policy\n",
    "    \n",
    "    def _setup_glue_catalog(self):\n",
    "        \"\"\"\n",
    "        Central catalog with domain isolation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"database_naming\": \"{domain}_db\",\n",
    "            \"table_naming\": \"{domain}_{entity}\",\n",
    "            \"cross_domain_access\": \"IAM policies\"\n",
    "        }\n",
    "```\n",
    "\n",
    "**Platform Team Responsibilities:**\n",
    "\n",
    "| Area | Platform Team | Domain Team |\n",
    "|------|---------------|-------------|\n",
    "| **Infrastructure** | Provision & maintain | Consume via self-service |\n",
    "| **Standards** | Define (e.g., PII policy) | Implement |\n",
    "| **Tooling** | Provide (Airflow, DataHub) | Use & extend |\n",
    "| **Costs** | Set budgets | Optimize within budget |\n",
    "| **Security** | Global policies (IAM) | Local access controls |\n",
    "| **Monitoring** | Shared dashboards | Domain-specific alerts |\n",
    "| **Incidents** | Platform outages | Data quality issues |\n",
    "\n",
    "---\n",
    "\n",
    "**6. Benefits vs Challenges**\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "| Benefit | Impact |\n",
    "|---------|--------|\n",
    "| **Faster Innovation** | 12 weeks \u2192 2 weeks (feature delivery) |\n",
    "| **Better Quality** | Domain experts validate data (80% \u2192 95% accuracy) |\n",
    "| **Scalability** | Add domains without bottleneck |\n",
    "| **Accountability** | Clear ownership (no more \"not my problem\") |\n",
    "| **Cost Transparency** | Per-domain budgets (showback/chargeback) |\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "| Challenge | Mitigation |\n",
    "|-----------|-----------|\n",
    "| **Duplication** | Shared platform components, reusable modules |\n",
    "| **Coordination** | Data contracts, OpenAPI specs |\n",
    "| **Discoverability** | DataHub catalog with rich metadata |\n",
    "| **Governance** | Automated policy enforcement (OPA, Cedar) |\n",
    "| **Skills Gap** | Training, guilds, internal wiki |\n",
    "\n",
    "---\n",
    "\n",
    "**7. Migration Strategy: Monolith \u2192 Mesh**\n",
    "\n",
    "```\n",
    "Phase 1: Foundation (3 months)\n",
    "\u251c\u2500\u2500 Week 1-4: Define domains and owners\n",
    "\u251c\u2500\u2500 Week 5-8: Build self-service platform (Terraform modules)\n",
    "\u251c\u2500\u2500 Week 9-12: Train domain teams, pilot with 1 domain\n",
    "\u2514\u2500\u2500 Success Criteria: 1 domain fully migrated\n",
    "\n",
    "Phase 2: Scale (6 months)\n",
    "\u251c\u2500\u2500 Month 4-5: Migrate 3 high-value domains\n",
    "\u251c\u2500\u2500 Month 6-8: Implement federated governance\n",
    "\u251c\u2500\u2500 Month 9: Sunset legacy central pipelines\n",
    "\u2514\u2500\u2500 Success Criteria: 5 domains producing data products\n",
    "\n",
    "Phase 3: Maturity (ongoing)\n",
    "\u251c\u2500\u2500 Add new domains (self-service onboarding)\n",
    "\u251c\u2500\u2500 Advanced features (feature store, real-time)\n",
    "\u251c\u2500\u2500 Cross-domain analytics (federated queries)\n",
    "\u2514\u2500\u2500 Success Criteria: <1 week onboarding for new domains\n",
    "```\n",
    "\n",
    "**Pilot Domain Selection Criteria:**\n",
    "- \u2705 High business value\n",
    "- \u2705 Motivated team (early adopters)\n",
    "- \u2705 Clear boundaries (not too many dependencies)\n",
    "- \u2705 Medium complexity (not trivial, not impossible)\n",
    "\n",
    "**Example Pilot: Ventas Domain**\n",
    "- **Why**: Critical for business (revenue data)\n",
    "- **Team**: Experienced, eager to improve\n",
    "- **Scope**: Clear (transactions, revenue, customers)\n",
    "- **Timeline**: 3 months \u2192 production data product\n",
    "\n",
    "---\n",
    "\n",
    "**8. Real-World Examples**\n",
    "\n",
    "**Netflix:**\n",
    "- 50+ data domains (Content, Playback, Recommendations, etc.)\n",
    "- Each domain owns pipelines, quality, APIs\n",
    "- Central platform: Metacat (catalog), Iceberg (storage), DBT (transformations)\n",
    "- Result: 5,000+ datasets, self-serve access for 2,000+ engineers\n",
    "\n",
    "**Uber:**\n",
    "- Data domains by business unit (Rides, Eats, Freight)\n",
    "- Feature Store (Michelangelo) with domain-owned features\n",
    "- Central governance: Data Quality Portal, Lineage (DataBook)\n",
    "- Result: 10+ PB data lake, <1 week for new data products\n",
    "\n",
    "**Zalando:**\n",
    "- 200+ data products across 40 domains\n",
    "- Self-service: Nakadi (event streaming), Data Lake (S3), dbt (SQL)\n",
    "- Governance: Data Mesh Portal (discovery), Compliance Scanner\n",
    "- Result: 90% of data teams self-sufficient\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33626",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf **Feature Store: ML-Ready Data Infrastructure**\n",
    "\n",
    "**1. The Feature Engineering Problem**\n",
    "\n",
    "```\n",
    "Traditional ML Pipeline (Problems):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Data Scientist A                              \u2502\n",
    "\u2502  \u251c\u2500\u2500 Extracts: customer_total_purchases        \u2502\n",
    "\u2502  \u251c\u2500\u2500 Logic: SQL query (30 days rolling)       \u2502\n",
    "\u2502  \u251c\u2500\u2500 Storage: Local CSV                        \u2502\n",
    "\u2502  \u2514\u2500\u2500 Model: Churn prediction (prod)            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2193 (6 months later)\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Data Scientist B                              \u2502\n",
    "\u2502  \u251c\u2500\u2500 Extracts: customer_total_purchases (again)\u2502\n",
    "\u2502  \u251c\u2500\u2500 Logic: Slightly different SQL            \u2502\n",
    "\u2502  \u251c\u2500\u2500 Storage: Different S3 path               \u2502\n",
    "\u2502  \u2514\u2500\u2500 Model: Upsell prediction                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Issues:\n",
    "\u274c Duplication: Same feature, different implementations\n",
    "\u274c Inconsistency: Different logic \u2192 different values\n",
    "\u274c Training/Serving Skew: Batch SQL vs real-time Python\n",
    "\u274c No Versioning: Can't reproduce model from 6 months ago\n",
    "\u274c No Discovery: Team B doesn't know Team A computed this\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Feature Store Architecture**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                       Feature Store                             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502  \u2502  Offline Store   \u2502         \u2502  Online Store    \u2502            \u2502\n",
    "\u2502  \u2502  (S3/BigQuery)   \u2502         \u2502  (Redis/DynamoDB)\u2502            \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524            \u2502\n",
    "\u2502  \u2502 \u2022 Training data  \u2502         \u2502 \u2022 Serving (ms)   \u2502            \u2502\n",
    "\u2502  \u2502 \u2022 Batch (hours)  \u2502         \u2502 \u2022 Low latency    \u2502            \u2502\n",
    "\u2502  \u2502 \u2022 Historical     \u2502         \u2502 \u2022 Recent data    \u2502            \u2502\n",
    "\u2502  \u2502 \u2022 Petabytes      \u2502         \u2502 \u2022 Gigabytes      \u2502            \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2502           \u25b2                            \u25b2                        \u2502\n",
    "\u2502           \u2502                            \u2502                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n",
    "\u2502  \u2502         Feature Registry (Metadata)            \u2502            \u2502\n",
    "\u2502  \u2502  - Feature definitions                         \u2502            \u2502\n",
    "\u2502  \u2502  - Schema & types                              \u2502            \u2502\n",
    "\u2502  \u2502  - Owners & documentation                      \u2502            \u2502\n",
    "\u2502  \u2502  - Lineage & dependencies                      \u2502            \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u25b2                                     \u25b2\n",
    "         \u2502                                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502  Training \u2502                      \u2502  Serving   \u2502\n",
    "    \u2502  (batch)  \u2502                      \u2502 (real-time)\u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Feast Implementation (Open Source)**\n",
    "\n",
    "**Installation & Setup:**\n",
    "\n",
    "```python\n",
    "# Install Feast\n",
    "!pip install feast[aws,redis]\n",
    "\n",
    "# Initialize repository\n",
    "!feast init feature_repo\n",
    "cd feature_repo/\n",
    "```\n",
    "\n",
    "**Feature Definitions:**\n",
    "\n",
    "```python\n",
    "# feature_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, RedisSource\n",
    "from feast.types import Float64, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# ENTITIES (Join Keys)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "customer = Entity(\n",
    "    name=\"customer_id\",\n",
    "    description=\"Unique customer identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "product = Entity(\n",
    "    name=\"product_id\",\n",
    "    description=\"Unique product identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# VENTAS DOMAIN FEATURES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "ventas_offline_source = FileSource(\n",
    "    name=\"ventas_features_source\",\n",
    "    path=\"s3://mesh/ventas/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_online_source = RedisSource(\n",
    "    name=\"ventas_online\",\n",
    "    table=\"ventas_features\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_customer_features = FeatureView(\n",
    "    name=\"ventas_customer_features\",\n",
    "    description=\"Customer purchase behavior (Ventas domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=90),  # Feature validity\n",
    "    schema=[\n",
    "        Field(\n",
    "            name=\"total_purchases_7d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"total_purchases_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"transaction_count_7d\",\n",
    "            dtype=Int64,\n",
    "            description=\"Number of transactions in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"avg_basket_size_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Average basket size in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"days_since_last_purchase\",\n",
    "            dtype=Int64,\n",
    "            description=\"Days since most recent purchase\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"preferred_payment_method\",\n",
    "            dtype=String,\n",
    "            description=\"Most used payment method (last 90 days)\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"is_premium_customer\",\n",
    "            dtype=Int64,\n",
    "            description=\"1 if customer spent >$5000 in last year\"\n",
    "        )\n",
    "    ],\n",
    "    source=ventas_offline_source,\n",
    "    online=True,  # Enable online serving\n",
    "    owner=\"ventas-team@company.com\",\n",
    "    tags={\"domain\": \"ventas\", \"pii\": \"no\"}\n",
    ")\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# PRODUCTO DOMAIN FEATURES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "producto_offline_source = FileSource(\n",
    "    name=\"producto_features_source\",\n",
    "    path=\"s3://mesh/producto/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "producto_features = FeatureView(\n",
    "    name=\"producto_features\",\n",
    "    description=\"Product catalog and pricing (Producto domain)\",\n",
    "    entities=[product],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name=\"current_price\", dtype=Float64),\n",
    "        Field(name=\"category\", dtype=String),\n",
    "        Field(name=\"stock_level\", dtype=Int64),\n",
    "        Field(name=\"days_since_launch\", dtype=Int64),\n",
    "        Field(name=\"avg_rating\", dtype=Float64),\n",
    "        Field(name=\"total_reviews\", dtype=Int64)\n",
    "    ],\n",
    "    source=producto_offline_source,\n",
    "    online=True,\n",
    "    owner=\"catalog-team@company.com\",\n",
    "    tags={\"domain\": \"producto\"}\n",
    ")\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# LOGISTICA DOMAIN FEATURES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "logistica_offline_source = FileSource(\n",
    "    name=\"logistica_features_source\",\n",
    "    path=\"s3://mesh/logistica/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "logistica_features = FeatureView(\n",
    "    name=\"logistica_features\",\n",
    "    description=\"Delivery performance (Logistica domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=60),\n",
    "    schema=[\n",
    "        Field(name=\"avg_delivery_time_days\", dtype=Float64),\n",
    "        Field(name=\"on_time_delivery_rate\", dtype=Float64),\n",
    "        Field(name=\"total_shipments_30d\", dtype=Int64),\n",
    "        Field(name=\"return_rate_30d\", dtype=Float64)\n",
    "    ],\n",
    "    source=logistica_offline_source,\n",
    "    online=True,\n",
    "    owner=\"fulfillment-team@company.com\",\n",
    "    tags={\"domain\": \"logistica\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Feature Registry Configuration:**\n",
    "\n",
    "```yaml\n",
    "# feature_store.yaml\n",
    "project: ecommerce_mesh\n",
    "registry: s3://mesh/feast/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: \"redis.ecommerce.internal:6379\"\n",
    "  \n",
    "offline_store:\n",
    "  type: file  # or 'snowflake', 'bigquery', 'redshift'\n",
    "  \n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "**Apply Changes:**\n",
    "\n",
    "```bash\n",
    "# Deploy features to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# \u2705 Created entity customer_id\n",
    "# \u2705 Created entity product_id\n",
    "# \u2705 Created feature view ventas_customer_features\n",
    "# \u2705 Created feature view producto_features\n",
    "# \u2705 Created feature view logistica_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Feature Materialization (Offline \u2192 Online)**\n",
    "\n",
    "```python\n",
    "# materialize_features.py\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Materialize last 7 days to online store (Redis)\n",
    "store.materialize_incremental(\n",
    "    end_date=datetime.utcnow()\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 1 feature views from 2024-01-08 to 2024-01-15\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:23\n",
    "# \u2705 ventas_customer_features: 1.2M rows materialized to Redis\n",
    "# \u2705 producto_features: 50K rows materialized\n",
    "# \u2705 logistica_features: 800K rows materialized\n",
    "```\n",
    "\n",
    "**Airflow DAG for Incremental Materialization:**\n",
    "\n",
    "```python\n",
    "# dags/feast_materialization.py\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(\n",
    "    schedule=\"0 */4 * * *\",  # Every 4 hours\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"feast\", \"feature-store\"]\n",
    ")\n",
    "def feast_materialization():\n",
    "    \n",
    "    @task\n",
    "    def materialize_features():\n",
    "        from feast import FeatureStore\n",
    "        \n",
    "        store = FeatureStore(repo_path=\"/opt/airflow/feature_repo/\")\n",
    "        \n",
    "        # Materialize incremental\n",
    "        store.materialize_incremental(end_date=datetime.utcnow())\n",
    "        \n",
    "        print(\"\u2705 Features materialized to online store\")\n",
    "    \n",
    "    materialize_features()\n",
    "\n",
    "dag = feast_materialization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Training: Historical Features (Offline Store)**\n",
    "\n",
    "```python\n",
    "# ml/train_churn_model.py\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Training dataset: customers who churned in last 90 days\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, ...],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 2),\n",
    "        datetime(2024, 1, 3),\n",
    "        ...\n",
    "    ],\n",
    "    \"churned\": [1, 0, 1, ...]  # Label\n",
    "})\n",
    "\n",
    "# Get historical features (point-in-time correct)\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"ventas_customer_features:total_purchases_30d\",\n",
    "        \"ventas_customer_features:transaction_count_7d\",\n",
    "        \"ventas_customer_features:avg_basket_size_30d\",\n",
    "        \"ventas_customer_features:days_since_last_purchase\",\n",
    "        \"logistica_features:on_time_delivery_rate\",\n",
    "        \"logistica_features:return_rate_30d\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "# customer_id  event_timestamp  total_purchases_30d  transaction_count_7d  ...  churned\n",
    "# 1001         2024-01-01       542.30               3                     ...  1\n",
    "# 1002         2024-01-02       1250.80              8                     ...  0\n",
    "\n",
    "# Train model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = training_df.drop(columns=[\"customer_id\", \"event_timestamp\", \"churned\"])\n",
    "y = training_df[\"churned\"]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, \"churn_model_v1.pkl\")\n",
    "```\n",
    "\n",
    "**Point-in-Time Correctness:**\n",
    "\n",
    "```python\n",
    "# \u26a0\ufe0f Problem without feature store: Data leakage\n",
    "# If we compute features at training time using current data,\n",
    "# we're using information from the future!\n",
    "\n",
    "# Example: Customer churned on 2024-01-15\n",
    "# Feature: total_purchases_30d on 2024-01-15\n",
    "# \u274c Wrong: Using purchases from 2024-01-15 to 2024-02-14 (includes future)\n",
    "# \u2705 Correct: Using purchases from 2023-12-16 to 2024-01-15 (only past)\n",
    "\n",
    "# Feast handles this automatically with event_timestamp\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Serving: Real-Time Features (Online Store)**\n",
    "\n",
    "```python\n",
    "# api/prediction_service.py\n",
    "from fastapi import FastAPI\n",
    "from feast import FeatureStore\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model and feature store\n",
    "model = joblib.load(\"churn_model_v1.pkl\")\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: int\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: int\n",
    "    churn_probability: float\n",
    "    risk_level: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict_churn(request: PredictionRequest):\n",
    "    # Get features from online store (Redis) - sub-millisecond latency\n",
    "    features_dict = store.get_online_features(\n",
    "        features=[\n",
    "            \"ventas_customer_features:total_purchases_30d\",\n",
    "            \"ventas_customer_features:transaction_count_7d\",\n",
    "            \"ventas_customer_features:avg_basket_size_30d\",\n",
    "            \"ventas_customer_features:days_since_last_purchase\",\n",
    "            \"logistica_features:on_time_delivery_rate\",\n",
    "            \"logistica_features:return_rate_30d\"\n",
    "        ],\n",
    "        entity_rows=[{\"customer_id\": request.customer_id}]\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Convert to DataFrame for model\n",
    "    import pandas as pd\n",
    "    features_df = pd.DataFrame(features_dict)\n",
    "    \n",
    "    # Predict\n",
    "    churn_prob = model.predict_proba(features_df)[0][1]\n",
    "    \n",
    "    # Classify risk\n",
    "    if churn_prob > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "    elif churn_prob > 0.4:\n",
    "        risk_level = \"MEDIUM\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=request.customer_id,\n",
    "        churn_probability=churn_prob,\n",
    "        risk_level=risk_level\n",
    "    )\n",
    "\n",
    "# Latency: ~10ms (Redis lookup + model inference)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Feature Engineering Pipeline**\n",
    "\n",
    "```python\n",
    "# pipelines/compute_ventas_features.py\n",
    "\"\"\"\n",
    "Airflow DAG: Compute Ventas domain features daily\n",
    "\"\"\"\n",
    "from airflow.decorators import dag, task\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(schedule=\"@daily\", start_date=datetime(2024, 1, 1))\n",
    "def compute_ventas_features():\n",
    "    \n",
    "    @task\n",
    "    def compute_features(ds):\n",
    "        spark = SparkSession.builder.appName(\"VentasFeatures\").getOrCreate()\n",
    "        \n",
    "        # Read transactions (last 90 days)\n",
    "        transactions = spark.read.format(\"delta\").load(\"s3://mesh/ventas/curated/transactions/\")\n",
    "        transactions = transactions.filter(col(\"transaction_date\") >= date_sub(lit(ds), 90))\n",
    "        \n",
    "        # Compute features\n",
    "        features = transactions.groupBy(\"customer_id\").agg(\n",
    "            # 7-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_7d\"),\n",
    "            count(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), 1))\n",
    "                .alias(\"transaction_count_7d\"),\n",
    "            \n",
    "            # 30-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_30d\"),\n",
    "            avg(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"basket_size\")))\n",
    "                .alias(\"avg_basket_size_30d\"),\n",
    "            \n",
    "            # Recency\n",
    "            datediff(lit(ds), max(col(\"transaction_date\")))\n",
    "                .alias(\"days_since_last_purchase\"),\n",
    "            \n",
    "            # Most common payment method (last 90 days)\n",
    "            first(col(\"payment_method\"))  # After groupBy + window\n",
    "                .alias(\"preferred_payment_method\")\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        features = features \\\n",
    "            .withColumn(\"event_timestamp\", lit(ds).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"created_timestamp\", current_timestamp())\n",
    "        \n",
    "        # Write to feature store offline path (Feast reads this)\n",
    "        features.write \\\n",
    "            .format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"event_timestamp\") \\\n",
    "            .save(\"s3://mesh/ventas/curated/features/\")\n",
    "        \n",
    "        print(f\"\u2705 Computed features for {features.count()} customers\")\n",
    "    \n",
    "    compute_features()\n",
    "\n",
    "dag = compute_ventas_features()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Feature Versioning and Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/feature_quality.py\n",
    "from feast import FeatureStore\n",
    "import great_expectations as gx\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Get recent features\n",
    "features_df = store.get_historical_features(\n",
    "    entity_df=...,\n",
    "    features=[\"ventas_customer_features:total_purchases_30d\"]\n",
    ").to_df()\n",
    "\n",
    "# Validate with Great Expectations\n",
    "context = gx.get_context()\n",
    "\n",
    "suite = context.add_expectation_suite(\"ventas_features_quality\")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_be_between\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\", \"min_value\": 0, \"max_value\": 100000}\n",
    ")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\"}\n",
    ")\n",
    "\n",
    "results = context.get_validator(batch_request=...).validate()\n",
    "\n",
    "if not results.success:\n",
    "    # Alert: Feature quality degraded\n",
    "    send_slack_alert(\"Feature quality issue in ventas_customer_features\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. Tecton vs Feast Comparison**\n",
    "\n",
    "| Feature | Feast (Open Source) | Tecton (Commercial) |\n",
    "|---------|---------------------|---------------------|\n",
    "| **Cost** | Free | $$$$ (enterprise pricing) |\n",
    "| **Deployment** | Self-managed | Fully managed SaaS |\n",
    "| **Online Store** | Redis, DynamoDB | Managed Redis + optimizations |\n",
    "| **Offline Store** | S3, BigQuery, Snowflake | Snowflake, Databricks |\n",
    "| **Real-time** | Streaming via custom code | Native streaming (Flink) |\n",
    "| **Feature Engineering** | External (Spark, dbt) | Declarative transformations |\n",
    "| **Monitoring** | Custom (GE, Prometheus) | Built-in (drift, quality) |\n",
    "| **Lineage** | Manual (DataHub) | Automatic |\n",
    "| **Support** | Community | Enterprise SLA |\n",
    "\n",
    "**Decision Matrix:**\n",
    "- **Feast**: Startups, cost-sensitive, engineering resources available\n",
    "- **Tecton**: Enterprises, need SLA, limited ML engineering team\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c5b8d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdc **Data Products as APIs: Contracts, Versioning & SLOs**\n",
    "\n",
    "**1. Data Contract: API-First Design**\n",
    "\n",
    "```yaml\n",
    "# contracts/ventas-daily-revenue-api-v2.yaml\n",
    "openapi: 3.0.3\n",
    "info:\n",
    "  title: Daily Revenue API\n",
    "  version: 2.1.0\n",
    "  description: |\n",
    "    Provides daily revenue metrics aggregated by region.\n",
    "    \n",
    "    **Owner**: ventas-team@company.com\n",
    "    **SLO**: 99.9% availability, p99 latency <15 minutes\n",
    "    **Changelog**: \n",
    "      - v2.1.0 (2024-01): Added refunds_count field\n",
    "      - v2.0.0 (2023-12): Breaking - renamed 'total' to 'revenue_gross'\n",
    "  \n",
    "  contact:\n",
    "    name: Ventas Data Team\n",
    "    email: ventas-team@company.com\n",
    "    url: https://wiki.company.com/data-products/ventas-revenue\n",
    "  \n",
    "  x-slo:\n",
    "    availability: 99.9%\n",
    "    latency_p99: 900  # seconds (15 min)\n",
    "    freshness_max: 1200  # seconds (20 min)\n",
    "  \n",
    "  x-domain: ventas\n",
    "  x-cost-center: sales\n",
    "  x-tier: gold  # bronze/silver/gold\n",
    "\n",
    "servers:\n",
    "  - url: https://api.company.com/ventas/v2\n",
    "    description: Production\n",
    "  - url: https://api-staging.company.com/ventas/v2\n",
    "    description: Staging\n",
    "\n",
    "paths:\n",
    "  /daily-revenue:\n",
    "    get:\n",
    "      summary: Get daily revenue by region\n",
    "      operationId: getDailyRevenue\n",
    "      tags:\n",
    "        - Revenue\n",
    "      \n",
    "      parameters:\n",
    "        - name: date\n",
    "          in: query\n",
    "          required: true\n",
    "          schema:\n",
    "            type: string\n",
    "            format: date\n",
    "            example: \"2024-01-15\"\n",
    "          description: Revenue date (YYYY-MM-DD)\n",
    "        \n",
    "        - name: region\n",
    "          in: query\n",
    "          required: false\n",
    "          schema:\n",
    "            type: string\n",
    "            enum: [LATAM, NA, EU, APAC, ALL]\n",
    "            default: ALL\n",
    "          description: Filter by region\n",
    "      \n",
    "      responses:\n",
    "        '200':\n",
    "          description: Successful response\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/DailyRevenueResponse'\n",
    "              examples:\n",
    "                single_region:\n",
    "                  summary: Single region\n",
    "                  value:\n",
    "                    date: \"2024-01-15\"\n",
    "                    region: \"LATAM\"\n",
    "                    revenue_gross: 125000.50\n",
    "                    revenue_net: 118000.30\n",
    "                    transactions_count: 1543\n",
    "                    refunds_count: 23\n",
    "                    currency: \"USD\"\n",
    "                    calculated_at: \"2024-01-15T10:30:00Z\"\n",
    "        \n",
    "        '400':\n",
    "          description: Invalid date format\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/Error'\n",
    "        \n",
    "        '404':\n",
    "          description: No data for requested date\n",
    "        \n",
    "        '503':\n",
    "          description: Service temporarily unavailable (SLO violation)\n",
    "      \n",
    "      security:\n",
    "        - ApiKeyAuth: []\n",
    "      \n",
    "      x-rate-limit:\n",
    "        requests_per_minute: 100\n",
    "        requests_per_hour: 5000\n",
    "\n",
    "components:\n",
    "  schemas:\n",
    "    DailyRevenueResponse:\n",
    "      type: object\n",
    "      required:\n",
    "        - date\n",
    "        - region\n",
    "        - revenue_gross\n",
    "        - revenue_net\n",
    "        - transactions_count\n",
    "        - currency\n",
    "        - calculated_at\n",
    "      properties:\n",
    "        date:\n",
    "          type: string\n",
    "          format: date\n",
    "          description: Revenue date\n",
    "        region:\n",
    "          type: string\n",
    "          enum: [LATAM, NA, EU, APAC, ALL]\n",
    "          description: Geographic region\n",
    "        revenue_gross:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Total revenue before refunds/discounts\n",
    "          example: 125000.50\n",
    "        revenue_net:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Net revenue after refunds/discounts\n",
    "          example: 118000.30\n",
    "        transactions_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of transactions\n",
    "          example: 1543\n",
    "        refunds_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of refunded transactions (added v2.1)\n",
    "          example: 23\n",
    "        currency:\n",
    "          type: string\n",
    "          enum: [USD, EUR, BRL, MXN]\n",
    "          description: Currency code\n",
    "          example: \"USD\"\n",
    "        calculated_at:\n",
    "          type: string\n",
    "          format: date-time\n",
    "          description: Timestamp when metrics were calculated\n",
    "          example: \"2024-01-15T10:30:00Z\"\n",
    "        metadata:\n",
    "          type: object\n",
    "          properties:\n",
    "            data_quality_score:\n",
    "              type: number\n",
    "              description: Quality score (0-1)\n",
    "              example: 0.997\n",
    "            source_systems:\n",
    "              type: array\n",
    "              items:\n",
    "                type: string\n",
    "              example: [\"kafka_transactions\", \"sftp_refunds\"]\n",
    "    \n",
    "    Error:\n",
    "      type: object\n",
    "      required:\n",
    "        - error_code\n",
    "        - message\n",
    "      properties:\n",
    "        error_code:\n",
    "          type: string\n",
    "          example: \"INVALID_DATE_FORMAT\"\n",
    "        message:\n",
    "          type: string\n",
    "          example: \"Date must be in YYYY-MM-DD format\"\n",
    "        details:\n",
    "          type: object\n",
    "  \n",
    "  securitySchemes:\n",
    "    ApiKeyAuth:\n",
    "      type: apiKey\n",
    "      in: header\n",
    "      name: X-API-Key\n",
    "      description: Service account API key\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Schema Evolution: Backward Compatibility**\n",
    "\n",
    "```python\n",
    "# schema_evolution.py\n",
    "\"\"\"\n",
    "Rules for backward-compatible schema changes\n",
    "\"\"\"\n",
    "\n",
    "# \u2705 SAFE (Backward Compatible):\n",
    "# 1. Add optional field\n",
    "{\n",
    "    \"date\": \"2024-01-15\",\n",
    "    \"revenue_gross\": 125000.50,\n",
    "    \"refunds_count\": 23  # NEW OPTIONAL FIELD (v2.1)\n",
    "}\n",
    "\n",
    "# 2. Add new enum value\n",
    "region: [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\", \"AFRICA\"]  # NEW VALUE\n",
    "\n",
    "# 3. Widen validation (less restrictive)\n",
    "# Old: revenue_gross: minimum 100\n",
    "# New: revenue_gross: minimum 0\n",
    "\n",
    "# \u274c BREAKING (Not Backward Compatible):\n",
    "# 1. Remove field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {}  # 'total' removed\n",
    "\n",
    "# 2. Rename field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {\"revenue_gross\": 125000.50}  # 'total' renamed\n",
    "\n",
    "# 3. Change type\n",
    "# Old: {\"transactions_count\": 1543}  # integer\n",
    "# New: {\"transactions_count\": \"1543\"}  # string\n",
    "\n",
    "# 4. Make field required\n",
    "# Old: {\"date\": \"2024-01-15\"}  # refunds_count optional\n",
    "# New: {\"date\": \"2024-01-15\", \"refunds_count\": 23}  # required\n",
    "\n",
    "# 5. Narrow validation (more restrictive)\n",
    "# Old: revenue_gross: minimum 0\n",
    "# New: revenue_gross: minimum 100\n",
    "```\n",
    "\n",
    "**Breaking Change Protocol:**\n",
    "\n",
    "```markdown\n",
    "# Breaking Change Checklist\n",
    "\n",
    "## Before Implementation\n",
    "- [ ] Document breaking change in CHANGELOG\n",
    "- [ ] Notify consumers 30 days in advance (email, Slack)\n",
    "- [ ] Update API version (v2 \u2192 v3)\n",
    "- [ ] Maintain old version for deprecation period (6 months)\n",
    "\n",
    "## Implementation\n",
    "- [ ] Deploy new version (v3) alongside old (v2)\n",
    "- [ ] Monitor usage of old version\n",
    "- [ ] Provide migration guide with examples\n",
    "\n",
    "## Deprecation\n",
    "- [ ] Mark old version as deprecated (HTTP header: `Deprecation: true`)\n",
    "- [ ] Return warning in responses (Sunset header)\n",
    "- [ ] Send reminder emails at 90, 60, 30, 7 days before sunset\n",
    "- [ ] Sunset date: Remove old version after 6 months\n",
    "\n",
    "## Example Response Headers:\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Deprecation: true\n",
    "Sunset: Sat, 15 Jul 2024 00:00:00 GMT\n",
    "Link: <https://api.company.com/ventas/v3/daily-revenue>; rel=\"successor-version\"\n",
    "Warning: 299 - \"This API version will be retired on 2024-07-15. Migrate to v3\"\n",
    "```\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. API Implementation with FastAPI**\n",
    "\n",
    "```python\n",
    "# api/ventas_api.py\n",
    "from fastapi import FastAPI, Query, HTTPException, Header, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import date, datetime\n",
    "from typing import Optional, Literal\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Daily Revenue API\",\n",
    "    version=\"2.1.0\",\n",
    "    description=\"Ventas domain data product\",\n",
    "    openapi_tags=[\n",
    "        {\"name\": \"Revenue\", \"description\": \"Revenue metrics operations\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prometheus metrics\n",
    "request_count = Counter('api_requests_total', 'Total requests', ['endpoint', 'status'])\n",
    "request_duration = Histogram('api_request_duration_seconds', 'Request duration', ['endpoint'])\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# MODELS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class DailyRevenueResponse(BaseModel):\n",
    "    date: date\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "    revenue_gross: float = Field(..., ge=0, description=\"Revenue before refunds\")\n",
    "    revenue_net: float = Field(..., ge=0, description=\"Net revenue\")\n",
    "    transactions_count: int = Field(..., ge=0)\n",
    "    refunds_count: int = Field(default=0, ge=0, description=\"New in v2.1\")\n",
    "    currency: Literal[\"USD\", \"EUR\", \"BRL\", \"MXN\"]\n",
    "    calculated_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"date\": \"2024-01-15\",\n",
    "                \"region\": \"LATAM\",\n",
    "                \"revenue_gross\": 125000.50,\n",
    "                \"revenue_net\": 118000.30,\n",
    "                \"transactions_count\": 1543,\n",
    "                \"refunds_count\": 23,\n",
    "                \"currency\": \"USD\",\n",
    "                \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# DEPENDENCIES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def verify_api_key(x_api_key: str = Header(...)):\n",
    "    \"\"\"Verify API key from header\"\"\"\n",
    "    # In production: check against Secrets Manager\n",
    "    valid_keys = [\"dev-key-123\", \"prod-key-456\"]\n",
    "    if x_api_key not in valid_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "    return x_api_key\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Dependency: S3 client\"\"\"\n",
    "    return boto3.client('s3', region_name='us-east-1')\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# ENDPOINTS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@app.get(\n",
    "    \"/daily-revenue\",\n",
    "    response_model=DailyRevenueResponse,\n",
    "    tags=[\"Revenue\"],\n",
    "    summary=\"Get daily revenue by region\",\n",
    "    responses={\n",
    "        200: {\"description\": \"Successful response\"},\n",
    "        400: {\"description\": \"Invalid date format\"},\n",
    "        404: {\"description\": \"No data for requested date\"},\n",
    "        503: {\"description\": \"Service unavailable (SLO violation)\"}\n",
    "    }\n",
    ")\n",
    "async def get_daily_revenue(\n",
    "    date: date = Query(..., description=\"Revenue date (YYYY-MM-DD)\"),\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"] = Query(\"ALL\"),\n",
    "    api_key: str = Depends(verify_api_key),\n",
    "    s3: boto3.client = Depends(get_s3_client)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve daily revenue metrics for a specific date and region.\n",
    "    \n",
    "    **Data Freshness**: Updated every 15 minutes\n",
    "    **SLO**: p99 latency <15 minutes from transaction to API\n",
    "    \"\"\"\n",
    "    with request_duration.labels(endpoint=\"/daily-revenue\").time():\n",
    "        try:\n",
    "            # Query data from S3 (or cache)\n",
    "            s3_key = f\"gold/revenue/daily/dt={date}/region={region}.parquet\"\n",
    "            \n",
    "            response = s3.get_object(\n",
    "                Bucket='mesh',\n",
    "                Key=s3_key\n",
    "            )\n",
    "            \n",
    "            # Parse Parquet (simplified)\n",
    "            import pandas as pd\n",
    "            df = pd.read_parquet(response['Body'])\n",
    "            \n",
    "            if df.empty:\n",
    "                request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "                raise HTTPException(status_code=404, detail=\"No data for requested date\")\n",
    "            \n",
    "            # Convert to response model\n",
    "            row = df.iloc[0]\n",
    "            result = DailyRevenueResponse(\n",
    "                date=date,\n",
    "                region=region,\n",
    "                revenue_gross=float(row['revenue_gross']),\n",
    "                revenue_net=float(row['revenue_net']),\n",
    "                transactions_count=int(row['transactions_count']),\n",
    "                refunds_count=int(row.get('refunds_count', 0)),  # Default for old data\n",
    "                currency=row['currency'],\n",
    "                calculated_at=row['calculated_at']\n",
    "            )\n",
    "            \n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=200).inc()\n",
    "            return result\n",
    "            \n",
    "        except s3.exceptions.NoSuchKey:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "            raise HTTPException(status_code=404, detail=f\"No data for {date}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=500).inc()\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for load balancer\"\"\"\n",
    "    return {\"status\": \"healthy\", \"version\": \"2.1.0\"}\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    from fastapi import Response\n",
    "    \n",
    "    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. SLO Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/slo_monitor.py\n",
    "\"\"\"\n",
    "Monitor SLOs for data products\n",
    "\"\"\"\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# SLO metrics\n",
    "slo_availability = Gauge('data_product_availability', 'Availability %', ['product', 'domain'])\n",
    "slo_latency_p99 = Gauge('data_product_latency_p99_seconds', 'p99 latency', ['product', 'domain'])\n",
    "slo_freshness = Gauge('data_product_freshness_seconds', 'Data age', ['product', 'domain'])\n",
    "\n",
    "def check_availability(api_url: str):\n",
    "    \"\"\"Check if API is responding (availability SLO)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{api_url}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_freshness(api_url: str):\n",
    "    \"\"\"Check data freshness (how old is the data?)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{api_url}/daily-revenue\",\n",
    "            params={\"date\": datetime.utcnow().date()},\n",
    "            headers={\"X-API-Key\": \"monitoring-key\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            calculated_at = datetime.fromisoformat(data['calculated_at'].replace('Z', '+00:00'))\n",
    "            age_seconds = (datetime.utcnow() - calculated_at).total_seconds()\n",
    "            return age_seconds\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def monitor_slos():\n",
    "    \"\"\"Continuous SLO monitoring\"\"\"\n",
    "    data_products = [\n",
    "        {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"domain\": \"ventas\",\n",
    "            \"url\": \"https://api.company.com/ventas/v2\",\n",
    "            \"slo_availability\": 0.999,  # 99.9%\n",
    "            \"slo_latency_p99\": 900,  # 15 min\n",
    "            \"slo_freshness\": 1200  # 20 min\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"shipment_tracking_api\",\n",
    "            \"domain\": \"logistica\",\n",
    "            \"url\": \"https://api.company.com/logistica/v1\",\n",
    "            \"slo_availability\": 0.9995,  # 99.95%\n",
    "            \"slo_latency_p99\": 300,  # 5 min\n",
    "            \"slo_freshness\": 600  # 10 min\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        for product in data_products:\n",
    "            # Availability\n",
    "            is_available = check_availability(product['url'])\n",
    "            availability_pct = 1.0 if is_available else 0.0\n",
    "            slo_availability.labels(\n",
    "                product=product['name'],\n",
    "                domain=product['domain']\n",
    "            ).set(availability_pct)\n",
    "            \n",
    "            # Freshness\n",
    "            freshness = check_freshness(product['url'])\n",
    "            if freshness:\n",
    "                slo_freshness.labels(\n",
    "                    product=product['name'],\n",
    "                    domain=product['domain']\n",
    "                ).set(freshness)\n",
    "                \n",
    "                # Alert if SLO violated\n",
    "                if freshness > product['slo_freshness']:\n",
    "                    send_alert(\n",
    "                        f\"\ud83d\udea8 Freshness SLO violated for {product['name']}: \"\n",
    "                        f\"{freshness}s > {product['slo_freshness']}s\"\n",
    "                    )\n",
    "        \n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_http_server(8001)\n",
    "    monitor_slos()\n",
    "```\n",
    "\n",
    "**Prometheus Alert Rules:**\n",
    "\n",
    "```yaml\n",
    "# alerts/data-product-slos.yml\n",
    "groups:\n",
    "  - name: data_product_slos\n",
    "    interval: 1m\n",
    "    rules:\n",
    "      \n",
    "      - alert: DataProductAvailabilitySLOViolation\n",
    "        expr: |\n",
    "          (\n",
    "            sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d])\n",
    "          ) < 0.999\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} availability below SLO\"\n",
    "          description: \"Availability: {{ $value | humanizePercentage }} (target: 99.9%)\"\n",
    "          dashboard: \"https://grafana.company.com/d/slo?product={{ $labels.product }}\"\n",
    "      \n",
    "      - alert: DataProductFreshnessSLOViolation\n",
    "        expr: |\n",
    "          data_product_freshness_seconds > 1200\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} data is stale\"\n",
    "          description: \"Data age: {{ $value }}s (target: <1200s)\"\n",
    "      \n",
    "      - alert: DataProductErrorBudgetExhausted\n",
    "        expr: |\n",
    "          (1 - \n",
    "            (sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d]))\n",
    "          ) > 0.001  # 99.9% SLO = 0.1% error budget\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} error budget exhausted\"\n",
    "          description: |\n",
    "            Error budget used: {{ $value | humanizePercentage }}\n",
    "            Action: Freeze non-critical deployments until recovered\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Client SDK Generation**\n",
    "\n",
    "```bash\n",
    "# Generate Python client from OpenAPI spec\n",
    "openapi-generator generate \\\n",
    "  -i contracts/ventas-daily-revenue-api-v2.yaml \\\n",
    "  -g python \\\n",
    "  -o clients/python/ventas-api-client \\\n",
    "  --package-name ventas_api_client\n",
    "\n",
    "# Usage\n",
    "pip install ./clients/python/ventas-api-client\n",
    "\n",
    "# Client code\n",
    "from ventas_api_client import ApiClient, Configuration, RevenuApi\n",
    "\n",
    "config = Configuration()\n",
    "config.host = \"https://api.company.com/ventas/v2\"\n",
    "config.api_key['X-API-Key'] = \"your-api-key\"\n",
    "\n",
    "client = ApiClient(configuration=config)\n",
    "api = RevenueApi(client)\n",
    "\n",
    "# Call API\n",
    "response = api.get_daily_revenue(date=\"2024-01-15\", region=\"LATAM\")\n",
    "print(f\"Revenue: ${response.revenue_net:,.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Contract Testing**\n",
    "\n",
    "```python\n",
    "# tests/contract_tests.py\n",
    "\"\"\"\n",
    "Ensure API complies with OpenAPI contract\n",
    "\"\"\"\n",
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "from api.ventas_api import app\n",
    "import yaml\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def load_openapi_spec():\n",
    "    with open(\"contracts/ventas-daily-revenue-api-v2.yaml\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def test_response_matches_schema():\n",
    "    \"\"\"Verify response matches OpenAPI schema\"\"\"\n",
    "    spec = load_openapi_spec()\n",
    "    \n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15&region=LATAM\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    \n",
    "    # Verify required fields\n",
    "    schema = spec['components']['schemas']['DailyRevenueResponse']\n",
    "    required_fields = schema['required']\n",
    "    \n",
    "    for field in required_fields:\n",
    "        assert field in data, f\"Missing required field: {field}\"\n",
    "    \n",
    "    # Verify types\n",
    "    assert isinstance(data['revenue_gross'], (int, float))\n",
    "    assert isinstance(data['transactions_count'], int)\n",
    "    assert data['region'] in [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "\n",
    "def test_backward_compatibility():\n",
    "    \"\"\"Ensure v2.1 is backward compatible with v2.0\"\"\"\n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # v2.0 clients expect these fields\n",
    "    assert 'date' in data\n",
    "    assert 'revenue_gross' in data  # Renamed from 'total' in v2.0\n",
    "    assert 'transactions_count' in data\n",
    "    \n",
    "    # v2.1 added optional field (should have default)\n",
    "    assert 'refunds_count' in data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bd2f5",
   "metadata": {},
   "source": [
    "### \ud83c\udfdb\ufe0f **Federated Governance: Policies, Observability & Cost at Scale**\n",
    "\n",
    "**1. Computational Governance Model**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Governance Federation Model                       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502  GLOBAL POLICIES (Platform Team)                        \u2502  \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n",
    "\u2502  \u2502 \u2022 Security: IAM, encryption, PII masking                \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Compliance: GDPR, SOX, PCI-DSS                        \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Quality: Minimum SLOs (99% availability)              \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Observability: Mandatory lineage, metrics             \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Cost: Budget limits per domain                        \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                           \u25bc                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502  AUTOMATED ENFORCEMENT (Policy Engine)                  \u2502  \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n",
    "\u2502  \u2502 \u2022 OPA/Cedar: Runtime policy checks                      \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 pre-commit hooks: Prevent non-compliant code          \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 CI/CD gates: Block deployment if policies fail        \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                           \u25bc                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502 Ventas       \u2502 Logistica    \u2502 Producto     \u2502 Marketing  \u2502  \u2502\n",
    "\u2502  \u2502 (local impl) \u2502 (local impl) \u2502 (local impl) \u2502 (local)    \u2502  \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n",
    "\u2502  \u2502 \u2022 Spark      \u2502 \u2022 dbt        \u2502 \u2022 Polars     \u2502 \u2022 Airflow  \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Daily      \u2502 \u2022 Hourly     \u2502 \u2022 Weekly     \u2502 \u2022 Real-time\u2502  \u2502\n",
    "\u2502  \u2502 \u2022 50 GB/day  \u2502 \u2022 100 GB/day \u2502 \u2022 10 GB/day  \u2502 \u2022 200 GB/d \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Policy as Code: Open Policy Agent (OPA)**\n",
    "\n",
    "```rego\n",
    "# policies/data_mesh_policies.rego\n",
    "package datamesh\n",
    "\n",
    "import future.keywords.if\n",
    "import future.keywords.in\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 1: PII Must Be Masked in Shared Datasets\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.dataset.sharing_level == \"public\"\n",
    "    some column in input.dataset.columns\n",
    "    column.contains_pii == true\n",
    "    not column.is_masked\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"PII column '%s' in dataset '%s' must be masked for public sharing\",\n",
    "        [column.name, input.dataset.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 2: All Data Products Must Have Owner\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    not input.data_product.owner\n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' must have an owner defined\",\n",
    "        [input.data_product.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.owner\n",
    "    not endswith(input.data_product.owner, \"@company.com\")\n",
    "    msg := sprintf(\n",
    "        \"Data product owner '%s' must be a valid company email\",\n",
    "        [input.data_product.owner]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 3: Cost Budget Enforcement\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.domain.monthly_cost > input.domain.budget_limit\n",
    "    overage := input.domain.monthly_cost - input.domain.budget_limit\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' exceeded budget: $%.2f over limit of $%.2f\",\n",
    "        [input.domain.name, overage, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "warn[msg] if {\n",
    "    usage := input.domain.monthly_cost / input.domain.budget_limit\n",
    "    usage > 0.8\n",
    "    usage <= 1.0\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' at %.0f%% of budget ($%.2f / $%.2f)\",\n",
    "        [input.domain.name, usage * 100, input.domain.monthly_cost, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 4: Data Quality SLO Enforcement\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.quality_score < 0.95\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' quality score %.2f%% below minimum 95%%\",\n",
    "        [input.data_product.name, input.data_product.quality_score * 100]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 5: Lineage Must Be Tracked\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.pipeline.outputs_to_shared_layer\n",
    "    not input.pipeline.emits_lineage\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Pipeline '%s' must emit lineage (OpenLineage) to DataHub\",\n",
    "        [input.pipeline.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 6: API Versioning Required\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.api.is_public\n",
    "    not regex.match(`^/v[0-9]+/`, input.api.path)\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Public API path '%s' must include version prefix (e.g., /v1/)\",\n",
    "        [input.api.path]\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# POLICY 7: Cross-Domain Access Requires Approval\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "deny[msg] if {\n",
    "    input.access_request.source_domain != input.dataset.owner_domain\n",
    "    not input.access_request.approved_by_owner\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' accessing '%s' dataset requires approval from '%s' team\",\n",
    "        [\n",
    "            input.access_request.source_domain,\n",
    "            input.dataset.name,\n",
    "            input.dataset.owner_domain\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "```\n",
    "\n",
    "**Policy Enforcement in CI/CD:**\n",
    "\n",
    "```python\n",
    "# scripts/check_policies.py\n",
    "\"\"\"\n",
    "Validate policies before deployment\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def check_opa_policies(input_data: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Run OPA policy checks\n",
    "    \"\"\"\n",
    "    # Convert input to JSON\n",
    "    input_json = json.dumps(input_data)\n",
    "    \n",
    "    # Run OPA evaluation\n",
    "    result = subprocess.run(\n",
    "        [\"opa\", \"eval\", \"--data\", \"policies/\", \"--input\", \"-\", \"data.datamesh.deny\"],\n",
    "        input=input_json.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"\u274c OPA evaluation failed: {result.stderr.decode()}\")\n",
    "        return False\n",
    "    \n",
    "    output = json.loads(result.stdout)\n",
    "    violations = output['result'][0]['expressions'][0]['value']\n",
    "    \n",
    "    if violations:\n",
    "        print(\"\u274c Policy violations detected:\")\n",
    "        for violation in violations:\n",
    "            print(f\"  - {violation}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\u2705 All policies passed\")\n",
    "    return True\n",
    "\n",
    "# Example usage in pre-commit hook\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = {\n",
    "        \"data_product\": {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"owner\": \"ventas-team@company.com\",\n",
    "            \"quality_score\": 0.997\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"name\": \"ventas_curated\",\n",
    "            \"sharing_level\": \"public\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"customer_id\", \"contains_pii\": False},\n",
    "                {\"name\": \"email\", \"contains_pii\": True, \"is_masked\": True}\n",
    "            ]\n",
    "        },\n",
    "        \"domain\": {\n",
    "            \"name\": \"ventas\",\n",
    "            \"monthly_cost\": 1800,\n",
    "            \"budget_limit\": 2000\n",
    "        },\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"ventas_daily_batch\",\n",
    "            \"outputs_to_shared_layer\": True,\n",
    "            \"emits_lineage\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not check_opa_policies(input_data):\n",
    "        sys.exit(1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Multi-Tenant Observability Dashboard**\n",
    "\n",
    "```python\n",
    "# monitoring/mesh_dashboard.py\n",
    "\"\"\"\n",
    "Unified observability across all domains\n",
    "\"\"\"\n",
    "from grafana_api.grafana_face import GrafanaFace\n",
    "import json\n",
    "\n",
    "grafana = GrafanaFace(auth=\"admin:admin\", host=\"grafana.company.com\")\n",
    "\n",
    "# Create unified dashboard\n",
    "dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Data Mesh - Multi-Domain Overview\",\n",
    "        \"tags\": [\"data-mesh\", \"cross-domain\"],\n",
    "        \"timezone\": \"utc\",\n",
    "        \"panels\": [\n",
    "            # Panel 1: Domain Health Matrix\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Domain Health Matrix\",\n",
    "                \"type\": \"heatmap\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(data_product_availability[5m])\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 0, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 2: SLO Compliance by Domain\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"SLO Compliance by Domain\",\n",
    "                \"type\": \"gauge\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (\n",
    "                            sum_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                            /\n",
    "                            count_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"Availability %\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"thresholds\": {\n",
    "                            \"steps\": [\n",
    "                                {\"color\": \"red\", \"value\": 0},\n",
    "                                {\"color\": \"yellow\", \"value\": 99},\n",
    "                                {\"color\": \"green\", \"value\": 99.9}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 3: Cost by Domain\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Cost by Domain (Monthly)\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            increase(aws_billing_estimated_charges{domain!=\"\"}[30d])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 18, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 4: Data Freshness by Product\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Data Freshness (Last Update)\",\n",
    "                \"type\": \"table\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (time() - data_product_last_update_timestamp) / 60\n",
    "                    \"\"\",\n",
    "                    \"format\": \"table\",\n",
    "                    \"instant\": True\n",
    "                }],\n",
    "                \"transformations\": [{\n",
    "                    \"id\": \"organize\",\n",
    "                    \"options\": {\n",
    "                        \"excludeByName\": {},\n",
    "                        \"indexByName\": {\n",
    "                            \"domain\": 0,\n",
    "                            \"product\": 1,\n",
    "                            \"Value\": 2\n",
    "                        },\n",
    "                        \"renameByName\": {\n",
    "                            \"Value\": \"Minutes Since Update\"\n",
    "                        }\n",
    "                    }\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 5: Pipeline Success Rate\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"title\": \"Pipeline Success Rate (24h)\",\n",
    "                \"type\": \"stat\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_success{domain!=\"\"}[24h])\n",
    "                        )\n",
    "                        /\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_total{domain!=\"\"}[24h])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 6: Feature Store Usage\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"title\": \"Feature Store Requests (by domain)\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        rate(feast_feature_requests_total[5m])\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }} - {{ feature_view }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 16, \"w\": 24, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 7: Data Quality Score Trend\n",
    "            {\n",
    "                \"id\": 7,\n",
    "                \"title\": \"Data Quality Score Trend\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        avg by (domain) (\n",
    "                            data_product_quality_score\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"min\": 0,\n",
    "                        \"max\": 1,\n",
    "                        \"unit\": \"percentunit\"\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 8: Cross-Domain Dependencies\n",
    "            {\n",
    "                \"id\": 8,\n",
    "                \"title\": \"Cross-Domain API Calls\",\n",
    "                \"type\": \"nodeGraph\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (source_domain, target_domain) (\n",
    "                            rate(api_requests_total{source_domain!=\"\",target_domain!=\"\"}[5m])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            }\n",
    "        ],\n",
    "        \"templating\": {\n",
    "            \"list\": [\n",
    "                {\n",
    "                    \"name\": \"domain\",\n",
    "                    \"type\": \"query\",\n",
    "                    \"query\": \"label_values(data_product_availability, domain)\",\n",
    "                    \"multi\": False,\n",
    "                    \"includeAll\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"folderId\": 0,\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "# Create dashboard\n",
    "result = grafana.dashboard.update_dashboard(dashboard)\n",
    "print(f\"\u2705 Dashboard created: {result['url']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Cost Allocation and Chargeback**\n",
    "\n",
    "```python\n",
    "# finops/cost_allocation.py\n",
    "\"\"\"\n",
    "Allocate AWS costs to domains (chargeback model)\n",
    "\"\"\"\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "ce = boto3.client('ce', region_name='us-east-1')\n",
    "\n",
    "def get_domain_costs(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query AWS Cost Explorer with domain tags\n",
    "    \"\"\"\n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': start_date,\n",
    "            'End': end_date\n",
    "        },\n",
    "        Granularity='DAILY',\n",
    "        Metrics=['UnblendedCost', 'UsageQuantity'],\n",
    "        GroupBy=[\n",
    "            {'Type': 'TAG', 'Key': 'domain'},\n",
    "            {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n",
    "        ],\n",
    "        Filter={\n",
    "            'Tags': {\n",
    "                'Key': 'Project',\n",
    "                'Values': ['data-mesh']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse results\n",
    "    rows = []\n",
    "    for result in response['ResultsByTime']:\n",
    "        date = result['TimePeriod']['Start']\n",
    "        for group in result['Groups']:\n",
    "            domain = group['Keys'][0].split('$')[1] if '$' in group['Keys'][0] else 'untagged'\n",
    "            service = group['Keys'][1]\n",
    "            cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "            \n",
    "            rows.append({\n",
    "                'date': date,\n",
    "                'domain': domain,\n",
    "                'service': service,\n",
    "                'cost': cost\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Generate monthly report\n",
    "start = (datetime.utcnow() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "end = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "costs_df = get_domain_costs(start, end)\n",
    "\n",
    "# Aggregate by domain\n",
    "domain_summary = costs_df.groupby('domain').agg({\n",
    "    'cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "domain_summary = domain_summary.sort_values('cost', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Domain Cost Summary (Last 30 Days):\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in domain_summary.iterrows():\n",
    "    print(f\"{row['domain']:15} ${row['cost']:>10,.2f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'TOTAL':15} ${domain_summary['cost'].sum():>10,.2f}\")\n",
    "\n",
    "# Check budget overages\n",
    "budgets = {\n",
    "    'ventas': 2000,\n",
    "    'logistica': 1500,\n",
    "    'producto': 1000,\n",
    "    'marketing': 2500,\n",
    "    'finanzas': 3000\n",
    "}\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Budget Status:\")\n",
    "print(\"=\" * 50)\n",
    "for domain, budget in budgets.items():\n",
    "    actual = domain_summary[domain_summary['domain'] == domain]['cost'].sum()\n",
    "    utilization = (actual / budget) * 100\n",
    "    status = \"\u2705\" if utilization <= 100 else \"\ud83d\udea8\"\n",
    "    \n",
    "    print(f\"{status} {domain:15} ${actual:>8,.2f} / ${budget:>8,.2f} ({utilization:>5.1f}%)\")\n",
    "\n",
    "# Export to DataHub for visibility\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "for _, row in domain_summary.iterrows():\n",
    "    # Emit cost metrics to DataHub\n",
    "    domain_urn = f\"urn:li:domain:{row['domain']}\"\n",
    "    # (Implementation omitted for brevity)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. DataHub: Cross-Domain Lineage**\n",
    "\n",
    "```python\n",
    "# lineage/emit_cross_domain_lineage.py\n",
    "\"\"\"\n",
    "Emit lineage showing cross-domain dependencies\n",
    "\"\"\"\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import (\n",
    "    DatasetLineageTypeClass,\n",
    "    UpstreamClass,\n",
    "    UpstreamLineageClass\n",
    ")\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "# Example: Marketing domain consumes Ventas + Producto data\n",
    "lineage_map = {\n",
    "    # Marketing domain datasets\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.marketing.customer_segments,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            # Consumes from Ventas\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.customer_transactions,PROD)\",\n",
    "            # Consumes from Producto\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.producto.catalog,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.TRANSFORMED\n",
    "    },\n",
    "    \n",
    "    # Finanzas consumes Ventas\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.finanzas.accounting_reports,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.COPY\n",
    "    }\n",
    "}\n",
    "\n",
    "for downstream_urn, lineage_info in lineage_map.items():\n",
    "    upstreams = [\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_urn,\n",
    "            type=lineage_info['type']\n",
    "        )\n",
    "        for upstream_urn in lineage_info['upstreams']\n",
    "    ]\n",
    "    \n",
    "    lineage = UpstreamLineageClass(upstreams=upstreams)\n",
    "    \n",
    "    emitter.emit_mcp(\n",
    "        MetadataChangeProposalWrapper(\n",
    "            entityUrn=downstream_urn,\n",
    "            aspect=lineage\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Lineage emitted for {downstream_urn}\")\n",
    "```\n",
    "\n",
    "**DataHub Search: Find Cross-Domain Dependencies**\n",
    "\n",
    "```graphql\n",
    "# GraphQL query to find all datasets consuming Ventas data\n",
    "query {\n",
    "  search(\n",
    "    input: {\n",
    "      type: DATASET\n",
    "      query: \"*\"\n",
    "      filters: [\n",
    "        {\n",
    "          field: \"upstream.urn\"\n",
    "          values: [\"urn:li:domain:ventas\"]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ) {\n",
    "    searchResults {\n",
    "      entity {\n",
    "        ... on Dataset {\n",
    "          urn\n",
    "          name\n",
    "          domain {\n",
    "            name\n",
    "          }\n",
    "          upstream {\n",
    "            dataset {\n",
    "              urn\n",
    "              name\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Domain Autonomy with Guardrails**\n",
    "\n",
    "```python\n",
    "# platform/domain_provisioning.py\n",
    "\"\"\"\n",
    "Self-service domain provisioning with governance guardrails\n",
    "\"\"\"\n",
    "from typing import Dict\n",
    "import boto3\n",
    "\n",
    "class DomainProvisioner:\n",
    "    \"\"\"\n",
    "    Automated domain provisioning with policy enforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.iam = boto3.client('iam')\n",
    "        self.glue = boto3.client('glue')\n",
    "    \n",
    "    def provision_new_domain(self, domain_name: str, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Provision infrastructure for new domain\n",
    "        \n",
    "        Enforces:\n",
    "        - Naming conventions\n",
    "        - Cost budgets\n",
    "        - Security policies\n",
    "        - Observability standards\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate config against policies\n",
    "        if not self._validate_config(domain_name, config):\n",
    "            raise ValueError(\"Configuration violates governance policies\")\n",
    "        \n",
    "        resources = {}\n",
    "        \n",
    "        # 1. S3 bucket with standard structure\n",
    "        bucket_name = f\"mesh-{domain_name}\"\n",
    "        self.s3.create_bucket(Bucket=bucket_name)\n",
    "        \n",
    "        # Apply lifecycle policies (governance requirement)\n",
    "        self.s3.put_bucket_lifecycle_configuration(\n",
    "            Bucket=bucket_name,\n",
    "            LifecycleConfiguration={\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'ID': 'raw-retention',\n",
    "                        'Prefix': 'raw/',\n",
    "                        'Status': 'Enabled',\n",
    "                        'Transitions': [\n",
    "                            {'Days': 7, 'StorageClass': 'GLACIER'}\n",
    "                        ],\n",
    "                        'Expiration': {'Days': 90}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Enable versioning (governance requirement)\n",
    "        self.s3.put_bucket_versioning(\n",
    "            Bucket=bucket_name,\n",
    "            VersioningConfiguration={'Status': 'Enabled'}\n",
    "        )\n",
    "        \n",
    "        resources['s3_bucket'] = bucket_name\n",
    "        \n",
    "        # 2. IAM role with least privilege\n",
    "        role_name = f\"{domain_name}-pipeline-role\"\n",
    "        assume_role_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"emr-serverless.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        self.iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy),\n",
    "            Tags=[\n",
    "                {'Key': 'domain', 'Value': domain_name},\n",
    "                {'Key': 'managed-by', 'Value': 'platform-team'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        resources['iam_role'] = role_name\n",
    "        \n",
    "        # 3. Glue database\n",
    "        self.glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': f\"{domain_name}_db\",\n",
    "                'Description': f\"Database for {domain_name} domain\",\n",
    "                'Parameters': {\n",
    "                    'domain': domain_name,\n",
    "                    'owner': config['owner']\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resources['glue_database'] = f\"{domain_name}_db\"\n",
    "        \n",
    "        # 4. Budget alert (governance requirement)\n",
    "        budgets = boto3.client('budgets')\n",
    "        budgets.create_budget(\n",
    "            AccountId='123456789012',\n",
    "            Budget={\n",
    "                'BudgetName': f\"{domain_name}-monthly\",\n",
    "                'BudgetLimit': {\n",
    "                    'Amount': str(config['budget_limit']),\n",
    "                    'Unit': 'USD'\n",
    "                },\n",
    "                'TimeUnit': 'MONTHLY',\n",
    "                'BudgetType': 'COST',\n",
    "                'CostFilters': {\n",
    "                    'TagKeyValue': [f'domain${domain_name}']\n",
    "                }\n",
    "            },\n",
    "            NotificationsWithSubscribers=[\n",
    "                {\n",
    "                    'Notification': {\n",
    "                        'NotificationType': 'ACTUAL',\n",
    "                        'ComparisonOperator': 'GREATER_THAN',\n",
    "                        'Threshold': 80.0\n",
    "                    },\n",
    "                    'Subscribers': [{\n",
    "                        'SubscriptionType': 'EMAIL',\n",
    "                        'Address': config['owner']\n",
    "                    }]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return resources\n",
    "    \n",
    "    def _validate_config(self, domain_name: str, config: Dict) -> bool:\n",
    "        \"\"\"Validate against governance policies\"\"\"\n",
    "        \n",
    "        # Policy 1: Owner must be specified\n",
    "        if 'owner' not in config or not config['owner'].endswith('@company.com'):\n",
    "            print(\"\u274c Owner email required and must be @company.com\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 2: Budget limit required\n",
    "        if 'budget_limit' not in config or config['budget_limit'] <= 0:\n",
    "            print(\"\u274c Budget limit must be positive\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 3: Naming convention\n",
    "        if not domain_name.islower() or len(domain_name) > 20:\n",
    "            print(\"\u274c Domain name must be lowercase and <20 chars\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "provisioner = DomainProvisioner()\n",
    "\n",
    "new_domain_config = {\n",
    "    'owner': 'analytics-team@company.com',\n",
    "    'budget_limit': 1500,\n",
    "    'slo_availability': 0.999\n",
    "}\n",
    "\n",
    "resources = provisioner.provision_new_domain('analytics', new_domain_config)\n",
    "print(f\"\u2705 Domain 'analytics' provisioned: {resources}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Incident Response: Cross-Domain Impact Analysis**\n",
    "\n",
    "```python\n",
    "# sre/incident_response.py\n",
    "\"\"\"\n",
    "Analyze blast radius of incidents across domains\n",
    "\"\"\"\n",
    "from datahub.client import DataHubGraph\n",
    "\n",
    "graph = DataHubGraph(server=\"http://datahub:8080\")\n",
    "\n",
    "def analyze_impact(failed_dataset_urn: str):\n",
    "    \"\"\"\n",
    "    Find all downstream consumers of failed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query DataHub for downstream lineage\n",
    "    query = f\"\"\"\n",
    "    {{\n",
    "        dataset(urn: \"{failed_dataset_urn}\") {{\n",
    "            urn\n",
    "            name\n",
    "            domain {{ name }}\n",
    "            downstream(limit: 100) {{\n",
    "                dataset {{\n",
    "                    urn\n",
    "                    name\n",
    "                    domain {{ name }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = graph.execute_graphql(query)\n",
    "    failed = result['dataset']\n",
    "    \n",
    "    print(f\"\\n\ud83d\udea8 INCIDENT: {failed['name']} in {failed['domain']['name']} domain\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    downstreams = failed.get('downstream', [])\n",
    "    \n",
    "    if not downstreams:\n",
    "        print(\"\u2705 No downstream dependencies (isolated impact)\")\n",
    "        return\n",
    "    \n",
    "    # Group by domain\n",
    "    impacted_domains = {}\n",
    "    for downstream in downstreams:\n",
    "        ds = downstream['dataset']\n",
    "        domain = ds['domain']['name']\n",
    "        \n",
    "        if domain not in impacted_domains:\n",
    "            impacted_domains[domain] = []\n",
    "        \n",
    "        impacted_domains[domain].append(ds['name'])\n",
    "    \n",
    "    print(f\"\u26a0\ufe0f IMPACTED DOMAINS: {len(impacted_domains)}\")\n",
    "    for domain, datasets in impacted_domains.items():\n",
    "        print(f\"\\n  {domain.upper()}:\")\n",
    "        for dataset in datasets:\n",
    "            print(f\"    - {dataset}\")\n",
    "    \n",
    "    # Suggest actions\n",
    "    print(\"\\n\ud83d\udccb RECOMMENDED ACTIONS:\")\n",
    "    print(\"1. Notify impacted domain owners:\")\n",
    "    for domain in impacted_domains.keys():\n",
    "        print(f\"   - {domain}-team@company.com\")\n",
    "    print(\"2. Check if impacted datasets have fallback sources\")\n",
    "    print(\"3. Estimate ETA for fix and communicate\")\n",
    "\n",
    "# Example: Ventas daily revenue pipeline failed\n",
    "analyze_impact(\"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\")\n",
    "\n",
    "# Output:\n",
    "# \ud83d\udea8 INCIDENT: daily_revenue in ventas domain\n",
    "# ======================================================================\n",
    "# \u26a0\ufe0f IMPACTED DOMAINS: 3\n",
    "# \n",
    "#   MARKETING:\n",
    "#     - customer_segments\n",
    "#     - campaign_attribution\n",
    "# \n",
    "#   FINANZAS:\n",
    "#     - accounting_reports\n",
    "#     - revenue_forecasts\n",
    "# \n",
    "#   EXECUTIVE:\n",
    "#     - executive_dashboard\n",
    "# \n",
    "# \ud83d\udccb RECOMMENDED ACTIONS:\n",
    "# 1. Notify impacted domain owners:\n",
    "#    - marketing-team@company.com\n",
    "#    - finanzas-team@company.com\n",
    "#    - executive-team@company.com\n",
    "# 2. Check if impacted datasets have fallback sources\n",
    "# 3. Estimate ETA for fix and communicate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa74a1e",
   "metadata": {},
   "source": [
    "## 1. Contexto y requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06572913",
   "metadata": {},
   "source": [
    "**Empresa**: Marketplace multi-categor\u00eda con 5 dominios de negocio:\n",
    "- Ventas (Sales)\n",
    "- Log\u00edstica (Fulfillment)\n",
    "- Producto (Catalog)\n",
    "- Marketing (Campaigns)\n",
    "- Finanzas (Payments)\n",
    "\n",
    "**Objetivo**: Cada dominio gestiona sus propios datos como producto, con SLOs, versionado y documentaci\u00f3n. Un feature store central consume features de todos los dominios para ML.\n",
    "\n",
    "**Requerimientos**:\n",
    "- Plataforma self-service: cat\u00e1logo, CI/CD, observabilidad compartidos.\n",
    "- Gobernanza federada: pol\u00edticas de seguridad y calidad globales, aplicadas localmente.\n",
    "- Feature store (Feast/Tecton) con features de cada dominio.\n",
    "- APIs de data products con contratos versionados (OpenAPI).\n",
    "- Linaje cross-domain visible en DataHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb95d6",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Data Mesh propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_diagram = '''\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  Plataforma Self-Service                       \u2502\n",
    "\u2502  - Airflow compartido  - DataHub (cat\u00e1logo + linaje)          \u2502\n",
    "\u2502  - Grafana + Prometheus - CI/CD (GitHub Actions)              \u2502\n",
    "\u2502  - Feature Store (Feast) - Pol\u00edticas IAM centrales            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                              \u25b2\n",
    "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502                   \u2502                   \u2502\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502 Dominio      \u2502   \u2502  Dominio        \u2502  \u2502  Dominio      \u2502\n",
    "  \u2502 Ventas       \u2502   \u2502  Log\u00edstica      \u2502  \u2502  Producto     \u2502\n",
    "  \u2502 (data prod)  \u2502   \u2502  (data prod)    \u2502  \u2502  (data prod)  \u2502\n",
    "  \u2502 - raw/       \u2502   \u2502  - raw/         \u2502  \u2502  - raw/       \u2502\n",
    "  \u2502 - curated/   \u2502   \u2502  - curated/     \u2502  \u2502  - curated/   \u2502\n",
    "  \u2502 - API        \u2502   \u2502  - API          \u2502  \u2502  - API        \u2502\n",
    "  \u2502 - Features   \u2502   \u2502  - Features     \u2502  \u2502  - Features   \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502                   \u2502                   \u2502\n",
    "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                              \u25bc\n",
    "                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                   \u2502  Feature Store   \u2502\n",
    "                   \u2502  (Feast/Tecton)  \u2502\n",
    "                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                             \u2502\n",
    "                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                      \u2502 ML Models  \u2502\n",
    "                      \u2502 (training) \u2502\n",
    "                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "'''\n",
    "print(mesh_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732eb23",
   "metadata": {},
   "source": [
    "## 3. Componentes por dominio (ejemplo: Ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38990cce",
   "metadata": {},
   "source": [
    "### 3.1 Data Product Ventas\n",
    "- Owner: Equipo de Ventas.\n",
    "- Fuentes: Kafka (transacciones), archivos batch (devoluciones).\n",
    "- Storage: S3 `s3://mesh/ventas/raw/`, `s3://mesh/ventas/curated/`.\n",
    "- API: FastAPI endpoint `/ventas/v1/daily-revenue` con contrato OpenAPI.\n",
    "- Features: `cliente_total_compras_30d`, `cliente_num_transacciones_7d`.\n",
    "- SLO: latencia p99 < 15 min, disponibilidad > 99.9%.\n",
    "- Documentaci\u00f3n: README, diagramas, changelog.\n",
    "\n",
    "### 3.2 Pipeline Ventas\n",
    "- Airflow DAG propio del equipo, con validaciones GE y alertas.\n",
    "- Escribe features a Feast (offline store: S3 Parquet, online: Redis).\n",
    "- Emite linaje a DataHub v\u00eda OpenLineage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d910d0",
   "metadata": {},
   "source": [
    "## 4. Feature Store centralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feast_config = r'''\n",
    "# feature_repo/ventas_features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "ventas_source = FileSource(\n",
    "    path='s3://mesh/ventas/curated/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "ventas_fv = FeatureView(\n",
    "    name='ventas_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras_30d', dtype=Float32),\n",
    "        Field(name='num_transacciones_7d', dtype=Int64),\n",
    "    ],\n",
    "    source=ventas_source,\n",
    "    owner='ventas-team@empresa.com'\n",
    ")\n",
    "\n",
    "# Similarmente para logistica_features, producto_features, etc.\n",
    "'''\n",
    "print(feast_config.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07846920",
   "metadata": {},
   "source": [
    "## 5. Gobernanza federada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986b720",
   "metadata": {},
   "source": [
    "- Pol\u00edticas globales:\n",
    "  - Todo PII enmascarado en datasets compartidos.\n",
    "  - Validaciones m\u00ednimas de calidad (Great Expectations).\n",
    "  - Linaje obligatorio (OpenLineage).\n",
    "  - Versionado sem\u00e1ntico de APIs.\n",
    "- Autonom\u00eda local:\n",
    "  - Cada dominio elige su stack de transformaci\u00f3n (Spark/Pandas/dbt).\n",
    "  - Frecuencia de actualizaciones seg\u00fan SLO propio.\n",
    "  - Esquemas propios, evolucionables con compatibilidad (Avro/Protobuf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480061b",
   "metadata": {},
   "source": [
    "## 6. Checklist de implementaci\u00f3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = '''\n",
    "\u2610 1. Definir dominios y owners (RACI)\n",
    "\u2610 2. Crear buckets S3 por dominio (ventas/, logistica/, producto/)\n",
    "\u2610 3. DAG Airflow por dominio con validaciones y linaje\n",
    "\u2610 4. APIs FastAPI versionadas (OpenAPI specs)\n",
    "\u2610 5. Feature definitions en Feast por dominio\n",
    "\u2610 6. Feast apply y materialize-incremental en CI/CD\n",
    "\u2610 7. DataHub registrar data products con metadata\n",
    "\u2610 8. Pol\u00edticas IAM federadas (admin global + roles por dominio)\n",
    "\u2610 9. Dashboard Grafana multi-dominio con SLOs\n",
    "\u2610 10. Contratos de calidad (data contracts) versionados\n",
    "\u2610 11. Onboarding docs para nuevos dominios\n",
    "\u2610 12. Incident response playbook cross-domain\n",
    "\u2610 13. Cost allocation tags por dominio\n",
    "\u2610 14. Training model multi-dominio (consume features de Feast)\n",
    "\u2610 15. Tests de integraci\u00f3n cross-domain\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68ad77",
   "metadata": {},
   "source": [
    "## 7. Entregables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a040e",
   "metadata": {},
   "source": [
    "- Documento de dise\u00f1o Data Mesh con principios y responsabilidades.\n",
    "- Repositorio multi-dominio (monorepo o multi-repo).\n",
    "- Feature store funcional con features de \u22653 dominios.\n",
    "- Cat\u00e1logo DataHub con linaje cross-domain.\n",
    "- APIs documentadas (Swagger/OpenAPI) por dominio.\n",
    "- Dashboard unificado con m\u00e9tricas de todos los dominios.\n",
    "- Modelo ML entrenado consumiendo features federadas.\n",
    "- Presentaci\u00f3n ejecutiva (slides) con resultados y aprendizajes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe39ba",
   "metadata": {},
   "source": [
    "## 8. Evaluaci\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ac420",
   "metadata": {},
   "source": [
    "- Autonom\u00eda: \u00bfcada dominio opera independiente?\n",
    "- Gobernanza: \u00bfpol\u00edticas aplicadas consistentemente?\n",
    "- Feature store: \u00bffeatures accesibles y versionadas?\n",
    "- Observabilidad: \u00bflinaje y m\u00e9tricas cross-domain?\n",
    "- Escalabilidad: \u00bff\u00e1cil agregar nuevos dominios?\n",
    "- Costos: \u00bfoptimizaci\u00f3n por dominio visible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** Final del Nivel \ud83c\udf89\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
