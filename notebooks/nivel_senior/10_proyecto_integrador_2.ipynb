{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6b9ec7",
   "metadata": {},
   "source": [
    "# 🌐 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store\n",
    "\n",
    "Objetivo: diseñar una arquitectura Data Mesh con múltiples dominios autónomos, feature store centralizado para ML, y gobernanza federada.\n",
    "\n",
    "- Duración: 180+ min (proyecto multi-día)\n",
    "- Dificultad: Muy Alta\n",
    "- Prerrequisitos: Senior completo (01–08), experiencia organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb239f",
   "metadata": {},
   "source": [
    "### 🌐 **Data Mesh: Paradigm Shift from Centralized to Federated**\n",
    "\n",
    "**1. The Problem with Centralized Data Platforms**\n",
    "\n",
    "```\n",
    "Traditional Monolithic Data Platform:\n",
    "┌────────────────────────────────────────────┐\n",
    "│     Central Data Platform Team             │\n",
    "│  (Bottleneck: 5 engineers, 50 consumers)   │\n",
    "├────────────────────────────────────────────┤\n",
    "│ • All ETL pipelines                        │\n",
    "│ • All data quality checks                  │\n",
    "│ • All API development                      │\n",
    "│ • All documentation                        │\n",
    "│ • All incident response                    │\n",
    "└────────────────────────────────────────────┘\n",
    "           ▼  ▼  ▼  ▼  ▼\n",
    "    Requests: 200+/month\n",
    "    Lead Time: 6-12 weeks/feature\n",
    "    Quality: One team can't know all domains\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- 🚫 **Bottleneck**: Central team overwhelmed\n",
    "- 🚫 **Lack of Domain Expertise**: Platform team doesn't understand business nuances\n",
    "- 🚫 **Slow Innovation**: 3-month wait for new features\n",
    "- 🚫 **Poor Quality**: Generic validations miss domain-specific issues\n",
    "- 🚫 **No Ownership**: \"Not my problem\" mentality\n",
    "\n",
    "---\n",
    "\n",
    "**2. Data Mesh: Four Principles (Zhamak Dehghani)**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Data Mesh Principles                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  1. DOMAIN OWNERSHIP                                            │\n",
    "│     \"You build it, you own it\"                                  │\n",
    "│     - Each business domain owns its data products              │\n",
    "│     - Domain teams are accountable for quality & SLOs          │\n",
    "│                                                                  │\n",
    "│  2. DATA AS A PRODUCT                                           │\n",
    "│     \"Treat data like software products\"                         │\n",
    "│     - Discoverable (catalog)                                    │\n",
    "│     - Addressable (versioned APIs)                              │\n",
    "│     - Trustworthy (quality SLOs)                                │\n",
    "│     - Self-describing (documentation)                           │\n",
    "│     - Secure (access controls)                                  │\n",
    "│                                                                  │\n",
    "│  3. SELF-SERVE DATA PLATFORM                                    │\n",
    "│     \"Democratize infrastructure, not data\"                      │\n",
    "│     - Platform provides: storage, compute, observability       │\n",
    "│     - Domains consume: CI/CD, monitoring, governance tools     │\n",
    "│                                                                  │\n",
    "│  4. FEDERATED COMPUTATIONAL GOVERNANCE                          │\n",
    "│     \"Global policies, local execution\"                          │\n",
    "│     - Central: Security standards, PII policies, cost budgets  │\n",
    "│     - Local: Implementation details, tooling choices           │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Domain-Oriented Architecture**\n",
    "\n",
    "```python\n",
    "# domains.yml - Domain Registry\n",
    "domains:\n",
    "  \n",
    "  ventas:\n",
    "    owner: sales-team@company.com\n",
    "    mission: \"Provide revenue and transaction data products\"\n",
    "    data_products:\n",
    "      - daily_revenue_api\n",
    "      - customer_transaction_history\n",
    "      - product_performance\n",
    "    slos:\n",
    "      latency_p99: \"15 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "      data_quality: \"99.5% completeness\"\n",
    "    dependencies:\n",
    "      - producto.catalog_api  # Cross-domain dependency\n",
    "    budget_monthly: \"$2,000\"\n",
    "  \n",
    "  logistica:\n",
    "    owner: fulfillment-team@company.com\n",
    "    mission: \"Enable logistics optimization and tracking\"\n",
    "    data_products:\n",
    "      - shipment_tracking_api\n",
    "      - warehouse_inventory\n",
    "      - delivery_performance\n",
    "    slos:\n",
    "      latency_p99: \"5 minutes\"\n",
    "      availability: \"99.95%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$1,500\"\n",
    "  \n",
    "  producto:\n",
    "    owner: catalog-team@company.com\n",
    "    mission: \"Provide product catalog and pricing data\"\n",
    "    data_products:\n",
    "      - catalog_api\n",
    "      - pricing_history\n",
    "      - category_taxonomy\n",
    "    slos:\n",
    "      latency_p99: \"30 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "    dependencies: []\n",
    "    budget_monthly: \"$1,000\"\n",
    "  \n",
    "  marketing:\n",
    "    owner: campaigns-team@company.com\n",
    "    mission: \"Support campaign performance and customer segmentation\"\n",
    "    data_products:\n",
    "      - campaign_metrics\n",
    "      - customer_segments\n",
    "      - attribution_model\n",
    "    slos:\n",
    "      latency_p99: \"60 minutes\"\n",
    "      availability: \"99.5%\"\n",
    "    dependencies:\n",
    "      - ventas.customer_transaction_history\n",
    "      - producto.catalog_api\n",
    "    budget_monthly: \"$2,500\"\n",
    "  \n",
    "  finanzas:\n",
    "    owner: payments-team@company.com\n",
    "    mission: \"Financial reporting and fraud detection\"\n",
    "    data_products:\n",
    "      - payment_transactions\n",
    "      - fraud_scores\n",
    "      - accounting_reports\n",
    "    slos:\n",
    "      latency_p99: \"10 minutes\"\n",
    "      availability: \"99.99%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$3,000\"\n",
    "    compliance:\n",
    "      - PCI-DSS\n",
    "      - SOX\n",
    "```\n",
    "\n",
    "**Domain Team Structure:**\n",
    "\n",
    "```\n",
    "Ventas Domain Team (6 people):\n",
    "├── Data Product Owner (1)\n",
    "│   - Define requirements\n",
    "│   - Prioritize features\n",
    "│   - Communicate with consumers\n",
    "│\n",
    "├── Data Engineers (3)\n",
    "│   - Build pipelines\n",
    "│   - Maintain data quality\n",
    "│   - Optimize performance\n",
    "│\n",
    "├── Analytics Engineer (1)\n",
    "│   - Create gold layer aggregations\n",
    "│   - Build BI dashboards\n",
    "│\n",
    "└── SRE/DevOps (1)\n",
    "    - CI/CD pipelines\n",
    "    - Incident response\n",
    "    - Cost optimization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Data Product Canvas**\n",
    "\n",
    "```markdown\n",
    "# Data Product: Daily Revenue API\n",
    "\n",
    "## Business Context\n",
    "**Purpose**: Provide real-time revenue metrics for executive dashboards and financial reporting\n",
    "\n",
    "**Consumers**: \n",
    "- Finance team (accounting reconciliation)\n",
    "- Executive dashboards (Tableau)\n",
    "- ML models (revenue forecasting)\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "### Input Sources\n",
    "- Kafka topic: `ecommerce.ventas.transactions`\n",
    "- S3 batch: `s3://mesh/ventas/raw/refunds/`\n",
    "\n",
    "### Output Schema (v2.1)\n",
    "```json\n",
    "{\n",
    "  \"date\": \"2024-01-15\",\n",
    "  \"region\": \"LATAM\",\n",
    "  \"revenue_gross\": 125000.50,\n",
    "  \"revenue_net\": 118000.30,\n",
    "  \"transactions_count\": 1543,\n",
    "  \"refunds_count\": 23,\n",
    "  \"currency\": \"USD\",\n",
    "  \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "### SLOs\n",
    "- **Latency**: p99 < 15 minutes (from transaction to API availability)\n",
    "- **Availability**: 99.9% uptime (43 minutes downtime/month allowed)\n",
    "- **Accuracy**: ±0.5% vs accounting system\n",
    "- **Freshness**: Data no older than 20 minutes\n",
    "\n",
    "### API Endpoints\n",
    "- `GET /ventas/v2/daily-revenue?date={YYYY-MM-DD}&region={region}`\n",
    "- `GET /ventas/v2/revenue-trend?start_date={}&end_date={}`\n",
    "\n",
    "### Access Control\n",
    "- Public: No (internal only)\n",
    "- Authentication: API key (service accounts)\n",
    "- Authorization: Read-only for finance, marketing, executive teams\n",
    "\n",
    "### Dependencies\n",
    "- **Upstream**: producto.catalog_api (for product prices)\n",
    "- **Downstream**: ML feature store, executive dashboards\n",
    "\n",
    "### Costs\n",
    "- Storage: $50/month (S3 Standard)\n",
    "- Compute: $200/month (EMR Serverless)\n",
    "- API hosting: $100/month (ECS Fargate)\n",
    "- **Total**: $350/month\n",
    "\n",
    "### Metrics\n",
    "- Request rate: 500 req/day\n",
    "- p99 latency: 12 minutes (within SLO)\n",
    "- Availability (30d): 99.92%\n",
    "- Quality score: 99.7% completeness\n",
    "\n",
    "### Changelog\n",
    "- v2.1 (2024-01): Added `refunds_count` field\n",
    "- v2.0 (2023-12): Breaking change - renamed `total` to `revenue_gross`\n",
    "- v1.5 (2023-10): Added `currency` field\n",
    "- v1.0 (2023-08): Initial release\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Self-Service Platform Components**\n",
    "\n",
    "```python\n",
    "# platform/infrastructure.py\n",
    "\"\"\"\n",
    "Self-service platform capabilities provided to all domains\n",
    "\"\"\"\n",
    "\n",
    "class DataPlatform:\n",
    "    \"\"\"\n",
    "    Shared infrastructure that domains consume\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.components = {\n",
    "            # Storage\n",
    "            \"s3_buckets\": self._provision_s3(),\n",
    "            \"glue_catalog\": self._setup_glue_catalog(),\n",
    "            \n",
    "            # Compute\n",
    "            \"emr_serverless\": self._create_emr_apps(),\n",
    "            \"airflow\": self._deploy_mwaa(),\n",
    "            \n",
    "            # Observability\n",
    "            \"datahub\": self._setup_datahub(),\n",
    "            \"grafana\": self._deploy_grafana(),\n",
    "            \"prometheus\": self._deploy_prometheus(),\n",
    "            \n",
    "            # Governance\n",
    "            \"iam_roles\": self._create_domain_roles(),\n",
    "            \"kms_keys\": self._create_encryption_keys(),\n",
    "            \n",
    "            # CI/CD\n",
    "            \"github_actions\": self._setup_workflows(),\n",
    "            \"terraform_modules\": self._publish_modules()\n",
    "        }\n",
    "    \n",
    "    def provision_domain(self, domain_name: str):\n",
    "        \"\"\"\n",
    "        Self-service: Domain team provisions own infrastructure\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"s3_bucket\": f\"s3://mesh/{domain_name}/\",\n",
    "            \"iam_role\": f\"arn:aws:iam::123:role/{domain_name}-pipeline\",\n",
    "            \"airflow_connection\": f\"{domain_name}_aws\",\n",
    "            \"datahub_domain\": f\"urn:li:domain:{domain_name}\",\n",
    "            \"grafana_folder\": f\"Domains/{domain_name}\",\n",
    "            \"cost_center_tag\": domain_name\n",
    "        }\n",
    "    \n",
    "    def _provision_s3(self):\n",
    "        \"\"\"\n",
    "        Create S3 buckets with standard structure\n",
    "        \"\"\"\n",
    "        bucket_policy = {\n",
    "            \"lifecycle_rules\": [\n",
    "                {\"raw\": \"7 days → Glacier\"},\n",
    "                {\"curated\": \"90 days → IA\"},\n",
    "                {\"gold\": \"retain indefinitely\"}\n",
    "            ],\n",
    "            \"encryption\": \"AWS KMS\",\n",
    "            \"versioning\": True,\n",
    "            \"tags\": {\"ManagedBy\": \"platform-team\"}\n",
    "        }\n",
    "        return bucket_policy\n",
    "    \n",
    "    def _setup_glue_catalog(self):\n",
    "        \"\"\"\n",
    "        Central catalog with domain isolation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"database_naming\": \"{domain}_db\",\n",
    "            \"table_naming\": \"{domain}_{entity}\",\n",
    "            \"cross_domain_access\": \"IAM policies\"\n",
    "        }\n",
    "```\n",
    "\n",
    "**Platform Team Responsibilities:**\n",
    "\n",
    "| Area | Platform Team | Domain Team |\n",
    "|------|---------------|-------------|\n",
    "| **Infrastructure** | Provision & maintain | Consume via self-service |\n",
    "| **Standards** | Define (e.g., PII policy) | Implement |\n",
    "| **Tooling** | Provide (Airflow, DataHub) | Use & extend |\n",
    "| **Costs** | Set budgets | Optimize within budget |\n",
    "| **Security** | Global policies (IAM) | Local access controls |\n",
    "| **Monitoring** | Shared dashboards | Domain-specific alerts |\n",
    "| **Incidents** | Platform outages | Data quality issues |\n",
    "\n",
    "---\n",
    "\n",
    "**6. Benefits vs Challenges**\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "| Benefit | Impact |\n",
    "|---------|--------|\n",
    "| **Faster Innovation** | 12 weeks → 2 weeks (feature delivery) |\n",
    "| **Better Quality** | Domain experts validate data (80% → 95% accuracy) |\n",
    "| **Scalability** | Add domains without bottleneck |\n",
    "| **Accountability** | Clear ownership (no more \"not my problem\") |\n",
    "| **Cost Transparency** | Per-domain budgets (showback/chargeback) |\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "| Challenge | Mitigation |\n",
    "|-----------|-----------|\n",
    "| **Duplication** | Shared platform components, reusable modules |\n",
    "| **Coordination** | Data contracts, OpenAPI specs |\n",
    "| **Discoverability** | DataHub catalog with rich metadata |\n",
    "| **Governance** | Automated policy enforcement (OPA, Cedar) |\n",
    "| **Skills Gap** | Training, guilds, internal wiki |\n",
    "\n",
    "---\n",
    "\n",
    "**7. Migration Strategy: Monolith → Mesh**\n",
    "\n",
    "```\n",
    "Phase 1: Foundation (3 months)\n",
    "├── Week 1-4: Define domains and owners\n",
    "├── Week 5-8: Build self-service platform (Terraform modules)\n",
    "├── Week 9-12: Train domain teams, pilot with 1 domain\n",
    "└── Success Criteria: 1 domain fully migrated\n",
    "\n",
    "Phase 2: Scale (6 months)\n",
    "├── Month 4-5: Migrate 3 high-value domains\n",
    "├── Month 6-8: Implement federated governance\n",
    "├── Month 9: Sunset legacy central pipelines\n",
    "└── Success Criteria: 5 domains producing data products\n",
    "\n",
    "Phase 3: Maturity (ongoing)\n",
    "├── Add new domains (self-service onboarding)\n",
    "├── Advanced features (feature store, real-time)\n",
    "├── Cross-domain analytics (federated queries)\n",
    "└── Success Criteria: <1 week onboarding for new domains\n",
    "```\n",
    "\n",
    "**Pilot Domain Selection Criteria:**\n",
    "- ✅ High business value\n",
    "- ✅ Motivated team (early adopters)\n",
    "- ✅ Clear boundaries (not too many dependencies)\n",
    "- ✅ Medium complexity (not trivial, not impossible)\n",
    "\n",
    "**Example Pilot: Ventas Domain**\n",
    "- **Why**: Critical for business (revenue data)\n",
    "- **Team**: Experienced, eager to improve\n",
    "- **Scope**: Clear (transactions, revenue, customers)\n",
    "- **Timeline**: 3 months → production data product\n",
    "\n",
    "---\n",
    "\n",
    "**8. Real-World Examples**\n",
    "\n",
    "**Netflix:**\n",
    "- 50+ data domains (Content, Playback, Recommendations, etc.)\n",
    "- Each domain owns pipelines, quality, APIs\n",
    "- Central platform: Metacat (catalog), Iceberg (storage), DBT (transformations)\n",
    "- Result: 5,000+ datasets, self-serve access for 2,000+ engineers\n",
    "\n",
    "**Uber:**\n",
    "- Data domains by business unit (Rides, Eats, Freight)\n",
    "- Feature Store (Michelangelo) with domain-owned features\n",
    "- Central governance: Data Quality Portal, Lineage (DataBook)\n",
    "- Result: 10+ PB data lake, <1 week for new data products\n",
    "\n",
    "**Zalando:**\n",
    "- 200+ data products across 40 domains\n",
    "- Self-service: Nakadi (event streaming), Data Lake (S3), dbt (SQL)\n",
    "- Governance: Data Mesh Portal (discovery), Compliance Scanner\n",
    "- Result: 90% of data teams self-sufficient\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33626",
   "metadata": {},
   "source": [
    "### 🎯 **Feature Store: ML-Ready Data Infrastructure**\n",
    "\n",
    "**1. The Feature Engineering Problem**\n",
    "\n",
    "```\n",
    "Traditional ML Pipeline (Problems):\n",
    "┌────────────────────────────────────────────────┐\n",
    "│  Data Scientist A                              │\n",
    "│  ├── Extracts: customer_total_purchases        │\n",
    "│  ├── Logic: SQL query (30 days rolling)       │\n",
    "│  ├── Storage: Local CSV                        │\n",
    "│  └── Model: Churn prediction (prod)            │\n",
    "└────────────────────────────────────────────────┘\n",
    "          ↓ (6 months later)\n",
    "┌────────────────────────────────────────────────┐\n",
    "│  Data Scientist B                              │\n",
    "│  ├── Extracts: customer_total_purchases (again)│\n",
    "│  ├── Logic: Slightly different SQL            │\n",
    "│  ├── Storage: Different S3 path               │\n",
    "│  └── Model: Upsell prediction                  │\n",
    "└────────────────────────────────────────────────┘\n",
    "\n",
    "Issues:\n",
    "❌ Duplication: Same feature, different implementations\n",
    "❌ Inconsistency: Different logic → different values\n",
    "❌ Training/Serving Skew: Batch SQL vs real-time Python\n",
    "❌ No Versioning: Can't reproduce model from 6 months ago\n",
    "❌ No Discovery: Team B doesn't know Team A computed this\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Feature Store Architecture**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       Feature Store                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  ┌──────────────────┐         ┌──────────────────┐            │\n",
    "│  │  Offline Store   │         │  Online Store    │            │\n",
    "│  │  (S3/BigQuery)   │         │  (Redis/DynamoDB)│            │\n",
    "│  ├──────────────────┤         ├──────────────────┤            │\n",
    "│  │ • Training data  │         │ • Serving (ms)   │            │\n",
    "│  │ • Batch (hours)  │         │ • Low latency    │            │\n",
    "│  │ • Historical     │         │ • Recent data    │            │\n",
    "│  │ • Petabytes      │         │ • Gigabytes      │            │\n",
    "│  └──────────────────┘         └──────────────────┘            │\n",
    "│           ▲                            ▲                        │\n",
    "│           │                            │                        │\n",
    "│  ┌────────┴────────────────────────────┴─────────┐            │\n",
    "│  │         Feature Registry (Metadata)            │            │\n",
    "│  │  - Feature definitions                         │            │\n",
    "│  │  - Schema & types                              │            │\n",
    "│  │  - Owners & documentation                      │            │\n",
    "│  │  - Lineage & dependencies                      │            │\n",
    "│  └────────────────────────────────────────────────┘            │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "         ▲                                     ▲\n",
    "         │                                     │\n",
    "    ┌────┴──────┐                      ┌──────┴─────┐\n",
    "    │  Training │                      │  Serving   │\n",
    "    │  (batch)  │                      │ (real-time)│\n",
    "    └───────────┘                      └────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Feast Implementation (Open Source)**\n",
    "\n",
    "**Installation & Setup:**\n",
    "\n",
    "```python\n",
    "# Install Feast\n",
    "!pip install feast[aws,redis]\n",
    "\n",
    "# Initialize repository\n",
    "!feast init feature_repo\n",
    "cd feature_repo/\n",
    "```\n",
    "\n",
    "**Feature Definitions:**\n",
    "\n",
    "```python\n",
    "# feature_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, RedisSource\n",
    "from feast.types import Float64, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# ENTITIES (Join Keys)\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "customer = Entity(\n",
    "    name=\"customer_id\",\n",
    "    description=\"Unique customer identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "product = Entity(\n",
    "    name=\"product_id\",\n",
    "    description=\"Unique product identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# VENTAS DOMAIN FEATURES\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "ventas_offline_source = FileSource(\n",
    "    name=\"ventas_features_source\",\n",
    "    path=\"s3://mesh/ventas/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_online_source = RedisSource(\n",
    "    name=\"ventas_online\",\n",
    "    table=\"ventas_features\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_customer_features = FeatureView(\n",
    "    name=\"ventas_customer_features\",\n",
    "    description=\"Customer purchase behavior (Ventas domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=90),  # Feature validity\n",
    "    schema=[\n",
    "        Field(\n",
    "            name=\"total_purchases_7d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"total_purchases_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"transaction_count_7d\",\n",
    "            dtype=Int64,\n",
    "            description=\"Number of transactions in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"avg_basket_size_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Average basket size in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"days_since_last_purchase\",\n",
    "            dtype=Int64,\n",
    "            description=\"Days since most recent purchase\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"preferred_payment_method\",\n",
    "            dtype=String,\n",
    "            description=\"Most used payment method (last 90 days)\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"is_premium_customer\",\n",
    "            dtype=Int64,\n",
    "            description=\"1 if customer spent >$5000 in last year\"\n",
    "        )\n",
    "    ],\n",
    "    source=ventas_offline_source,\n",
    "    online=True,  # Enable online serving\n",
    "    owner=\"ventas-team@company.com\",\n",
    "    tags={\"domain\": \"ventas\", \"pii\": \"no\"}\n",
    ")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# PRODUCTO DOMAIN FEATURES\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "producto_offline_source = FileSource(\n",
    "    name=\"producto_features_source\",\n",
    "    path=\"s3://mesh/producto/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "producto_features = FeatureView(\n",
    "    name=\"producto_features\",\n",
    "    description=\"Product catalog and pricing (Producto domain)\",\n",
    "    entities=[product],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name=\"current_price\", dtype=Float64),\n",
    "        Field(name=\"category\", dtype=String),\n",
    "        Field(name=\"stock_level\", dtype=Int64),\n",
    "        Field(name=\"days_since_launch\", dtype=Int64),\n",
    "        Field(name=\"avg_rating\", dtype=Float64),\n",
    "        Field(name=\"total_reviews\", dtype=Int64)\n",
    "    ],\n",
    "    source=producto_offline_source,\n",
    "    online=True,\n",
    "    owner=\"catalog-team@company.com\",\n",
    "    tags={\"domain\": \"producto\"}\n",
    ")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# LOGISTICA DOMAIN FEATURES\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "logistica_offline_source = FileSource(\n",
    "    name=\"logistica_features_source\",\n",
    "    path=\"s3://mesh/logistica/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "logistica_features = FeatureView(\n",
    "    name=\"logistica_features\",\n",
    "    description=\"Delivery performance (Logistica domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=60),\n",
    "    schema=[\n",
    "        Field(name=\"avg_delivery_time_days\", dtype=Float64),\n",
    "        Field(name=\"on_time_delivery_rate\", dtype=Float64),\n",
    "        Field(name=\"total_shipments_30d\", dtype=Int64),\n",
    "        Field(name=\"return_rate_30d\", dtype=Float64)\n",
    "    ],\n",
    "    source=logistica_offline_source,\n",
    "    online=True,\n",
    "    owner=\"fulfillment-team@company.com\",\n",
    "    tags={\"domain\": \"logistica\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Feature Registry Configuration:**\n",
    "\n",
    "```yaml\n",
    "# feature_store.yaml\n",
    "project: ecommerce_mesh\n",
    "registry: s3://mesh/feast/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: \"redis.ecommerce.internal:6379\"\n",
    "  \n",
    "offline_store:\n",
    "  type: file  # or 'snowflake', 'bigquery', 'redshift'\n",
    "  \n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "**Apply Changes:**\n",
    "\n",
    "```bash\n",
    "# Deploy features to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# ✅ Created entity customer_id\n",
    "# ✅ Created entity product_id\n",
    "# ✅ Created feature view ventas_customer_features\n",
    "# ✅ Created feature view producto_features\n",
    "# ✅ Created feature view logistica_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Feature Materialization (Offline → Online)**\n",
    "\n",
    "```python\n",
    "# materialize_features.py\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Materialize last 7 days to online store (Redis)\n",
    "store.materialize_incremental(\n",
    "    end_date=datetime.utcnow()\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 1 feature views from 2024-01-08 to 2024-01-15\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:01:23\n",
    "# ✅ ventas_customer_features: 1.2M rows materialized to Redis\n",
    "# ✅ producto_features: 50K rows materialized\n",
    "# ✅ logistica_features: 800K rows materialized\n",
    "```\n",
    "\n",
    "**Airflow DAG for Incremental Materialization:**\n",
    "\n",
    "```python\n",
    "# dags/feast_materialization.py\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(\n",
    "    schedule=\"0 */4 * * *\",  # Every 4 hours\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"feast\", \"feature-store\"]\n",
    ")\n",
    "def feast_materialization():\n",
    "    \n",
    "    @task\n",
    "    def materialize_features():\n",
    "        from feast import FeatureStore\n",
    "        \n",
    "        store = FeatureStore(repo_path=\"/opt/airflow/feature_repo/\")\n",
    "        \n",
    "        # Materialize incremental\n",
    "        store.materialize_incremental(end_date=datetime.utcnow())\n",
    "        \n",
    "        print(\"✅ Features materialized to online store\")\n",
    "    \n",
    "    materialize_features()\n",
    "\n",
    "dag = feast_materialization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Training: Historical Features (Offline Store)**\n",
    "\n",
    "```python\n",
    "# ml/train_churn_model.py\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Training dataset: customers who churned in last 90 days\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, ...],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 2),\n",
    "        datetime(2024, 1, 3),\n",
    "        ...\n",
    "    ],\n",
    "    \"churned\": [1, 0, 1, ...]  # Label\n",
    "})\n",
    "\n",
    "# Get historical features (point-in-time correct)\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"ventas_customer_features:total_purchases_30d\",\n",
    "        \"ventas_customer_features:transaction_count_7d\",\n",
    "        \"ventas_customer_features:avg_basket_size_30d\",\n",
    "        \"ventas_customer_features:days_since_last_purchase\",\n",
    "        \"logistica_features:on_time_delivery_rate\",\n",
    "        \"logistica_features:return_rate_30d\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "# customer_id  event_timestamp  total_purchases_30d  transaction_count_7d  ...  churned\n",
    "# 1001         2024-01-01       542.30               3                     ...  1\n",
    "# 1002         2024-01-02       1250.80              8                     ...  0\n",
    "\n",
    "# Train model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = training_df.drop(columns=[\"customer_id\", \"event_timestamp\", \"churned\"])\n",
    "y = training_df[\"churned\"]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, \"churn_model_v1.pkl\")\n",
    "```\n",
    "\n",
    "**Point-in-Time Correctness:**\n",
    "\n",
    "```python\n",
    "# ⚠️ Problem without feature store: Data leakage\n",
    "# If we compute features at training time using current data,\n",
    "# we're using information from the future!\n",
    "\n",
    "# Example: Customer churned on 2024-01-15\n",
    "# Feature: total_purchases_30d on 2024-01-15\n",
    "# ❌ Wrong: Using purchases from 2024-01-15 to 2024-02-14 (includes future)\n",
    "# ✅ Correct: Using purchases from 2023-12-16 to 2024-01-15 (only past)\n",
    "\n",
    "# Feast handles this automatically with event_timestamp\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Serving: Real-Time Features (Online Store)**\n",
    "\n",
    "```python\n",
    "# api/prediction_service.py\n",
    "from fastapi import FastAPI\n",
    "from feast import FeatureStore\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model and feature store\n",
    "model = joblib.load(\"churn_model_v1.pkl\")\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: int\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: int\n",
    "    churn_probability: float\n",
    "    risk_level: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict_churn(request: PredictionRequest):\n",
    "    # Get features from online store (Redis) - sub-millisecond latency\n",
    "    features_dict = store.get_online_features(\n",
    "        features=[\n",
    "            \"ventas_customer_features:total_purchases_30d\",\n",
    "            \"ventas_customer_features:transaction_count_7d\",\n",
    "            \"ventas_customer_features:avg_basket_size_30d\",\n",
    "            \"ventas_customer_features:days_since_last_purchase\",\n",
    "            \"logistica_features:on_time_delivery_rate\",\n",
    "            \"logistica_features:return_rate_30d\"\n",
    "        ],\n",
    "        entity_rows=[{\"customer_id\": request.customer_id}]\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Convert to DataFrame for model\n",
    "    import pandas as pd\n",
    "    features_df = pd.DataFrame(features_dict)\n",
    "    \n",
    "    # Predict\n",
    "    churn_prob = model.predict_proba(features_df)[0][1]\n",
    "    \n",
    "    # Classify risk\n",
    "    if churn_prob > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "    elif churn_prob > 0.4:\n",
    "        risk_level = \"MEDIUM\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=request.customer_id,\n",
    "        churn_probability=churn_prob,\n",
    "        risk_level=risk_level\n",
    "    )\n",
    "\n",
    "# Latency: ~10ms (Redis lookup + model inference)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Feature Engineering Pipeline**\n",
    "\n",
    "```python\n",
    "# pipelines/compute_ventas_features.py\n",
    "\"\"\"\n",
    "Airflow DAG: Compute Ventas domain features daily\n",
    "\"\"\"\n",
    "from airflow.decorators import dag, task\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(schedule=\"@daily\", start_date=datetime(2024, 1, 1))\n",
    "def compute_ventas_features():\n",
    "    \n",
    "    @task\n",
    "    def compute_features(ds):\n",
    "        spark = SparkSession.builder.appName(\"VentasFeatures\").getOrCreate()\n",
    "        \n",
    "        # Read transactions (last 90 days)\n",
    "        transactions = spark.read.format(\"delta\").load(\"s3://mesh/ventas/curated/transactions/\")\n",
    "        transactions = transactions.filter(col(\"transaction_date\") >= date_sub(lit(ds), 90))\n",
    "        \n",
    "        # Compute features\n",
    "        features = transactions.groupBy(\"customer_id\").agg(\n",
    "            # 7-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_7d\"),\n",
    "            count(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), 1))\n",
    "                .alias(\"transaction_count_7d\"),\n",
    "            \n",
    "            # 30-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_30d\"),\n",
    "            avg(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"basket_size\")))\n",
    "                .alias(\"avg_basket_size_30d\"),\n",
    "            \n",
    "            # Recency\n",
    "            datediff(lit(ds), max(col(\"transaction_date\")))\n",
    "                .alias(\"days_since_last_purchase\"),\n",
    "            \n",
    "            # Most common payment method (last 90 days)\n",
    "            first(col(\"payment_method\"))  # After groupBy + window\n",
    "                .alias(\"preferred_payment_method\")\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        features = features \\\n",
    "            .withColumn(\"event_timestamp\", lit(ds).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"created_timestamp\", current_timestamp())\n",
    "        \n",
    "        # Write to feature store offline path (Feast reads this)\n",
    "        features.write \\\n",
    "            .format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"event_timestamp\") \\\n",
    "            .save(\"s3://mesh/ventas/curated/features/\")\n",
    "        \n",
    "        print(f\"✅ Computed features for {features.count()} customers\")\n",
    "    \n",
    "    compute_features()\n",
    "\n",
    "dag = compute_ventas_features()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Feature Versioning and Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/feature_quality.py\n",
    "from feast import FeatureStore\n",
    "import great_expectations as gx\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Get recent features\n",
    "features_df = store.get_historical_features(\n",
    "    entity_df=...,\n",
    "    features=[\"ventas_customer_features:total_purchases_30d\"]\n",
    ").to_df()\n",
    "\n",
    "# Validate with Great Expectations\n",
    "context = gx.get_context()\n",
    "\n",
    "suite = context.add_expectation_suite(\"ventas_features_quality\")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_be_between\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\", \"min_value\": 0, \"max_value\": 100000}\n",
    ")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\"}\n",
    ")\n",
    "\n",
    "results = context.get_validator(batch_request=...).validate()\n",
    "\n",
    "if not results.success:\n",
    "    # Alert: Feature quality degraded\n",
    "    send_slack_alert(\"Feature quality issue in ventas_customer_features\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. Tecton vs Feast Comparison**\n",
    "\n",
    "| Feature | Feast (Open Source) | Tecton (Commercial) |\n",
    "|---------|---------------------|---------------------|\n",
    "| **Cost** | Free | $$$$ (enterprise pricing) |\n",
    "| **Deployment** | Self-managed | Fully managed SaaS |\n",
    "| **Online Store** | Redis, DynamoDB | Managed Redis + optimizations |\n",
    "| **Offline Store** | S3, BigQuery, Snowflake | Snowflake, Databricks |\n",
    "| **Real-time** | Streaming via custom code | Native streaming (Flink) |\n",
    "| **Feature Engineering** | External (Spark, dbt) | Declarative transformations |\n",
    "| **Monitoring** | Custom (GE, Prometheus) | Built-in (drift, quality) |\n",
    "| **Lineage** | Manual (DataHub) | Automatic |\n",
    "| **Support** | Community | Enterprise SLA |\n",
    "\n",
    "**Decision Matrix:**\n",
    "- **Feast**: Startups, cost-sensitive, engineering resources available\n",
    "- **Tecton**: Enterprises, need SLA, limited ML engineering team\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c5b8d",
   "metadata": {},
   "source": [
    "### 📜 **Data Products as APIs: Contracts, Versioning & SLOs**\n",
    "\n",
    "**1. Data Contract: API-First Design**\n",
    "\n",
    "```yaml\n",
    "# contracts/ventas-daily-revenue-api-v2.yaml\n",
    "openapi: 3.0.3\n",
    "info:\n",
    "  title: Daily Revenue API\n",
    "  version: 2.1.0\n",
    "  description: |\n",
    "    Provides daily revenue metrics aggregated by region.\n",
    "    \n",
    "    **Owner**: ventas-team@company.com\n",
    "    **SLO**: 99.9% availability, p99 latency <15 minutes\n",
    "    **Changelog**: \n",
    "      - v2.1.0 (2024-01): Added refunds_count field\n",
    "      - v2.0.0 (2023-12): Breaking - renamed 'total' to 'revenue_gross'\n",
    "  \n",
    "  contact:\n",
    "    name: Ventas Data Team\n",
    "    email: ventas-team@company.com\n",
    "    url: https://wiki.company.com/data-products/ventas-revenue\n",
    "  \n",
    "  x-slo:\n",
    "    availability: 99.9%\n",
    "    latency_p99: 900  # seconds (15 min)\n",
    "    freshness_max: 1200  # seconds (20 min)\n",
    "  \n",
    "  x-domain: ventas\n",
    "  x-cost-center: sales\n",
    "  x-tier: gold  # bronze/silver/gold\n",
    "\n",
    "servers:\n",
    "  - url: https://api.company.com/ventas/v2\n",
    "    description: Production\n",
    "  - url: https://api-staging.company.com/ventas/v2\n",
    "    description: Staging\n",
    "\n",
    "paths:\n",
    "  /daily-revenue:\n",
    "    get:\n",
    "      summary: Get daily revenue by region\n",
    "      operationId: getDailyRevenue\n",
    "      tags:\n",
    "        - Revenue\n",
    "      \n",
    "      parameters:\n",
    "        - name: date\n",
    "          in: query\n",
    "          required: true\n",
    "          schema:\n",
    "            type: string\n",
    "            format: date\n",
    "            example: \"2024-01-15\"\n",
    "          description: Revenue date (YYYY-MM-DD)\n",
    "        \n",
    "        - name: region\n",
    "          in: query\n",
    "          required: false\n",
    "          schema:\n",
    "            type: string\n",
    "            enum: [LATAM, NA, EU, APAC, ALL]\n",
    "            default: ALL\n",
    "          description: Filter by region\n",
    "      \n",
    "      responses:\n",
    "        '200':\n",
    "          description: Successful response\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/DailyRevenueResponse'\n",
    "              examples:\n",
    "                single_region:\n",
    "                  summary: Single region\n",
    "                  value:\n",
    "                    date: \"2024-01-15\"\n",
    "                    region: \"LATAM\"\n",
    "                    revenue_gross: 125000.50\n",
    "                    revenue_net: 118000.30\n",
    "                    transactions_count: 1543\n",
    "                    refunds_count: 23\n",
    "                    currency: \"USD\"\n",
    "                    calculated_at: \"2024-01-15T10:30:00Z\"\n",
    "        \n",
    "        '400':\n",
    "          description: Invalid date format\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/Error'\n",
    "        \n",
    "        '404':\n",
    "          description: No data for requested date\n",
    "        \n",
    "        '503':\n",
    "          description: Service temporarily unavailable (SLO violation)\n",
    "      \n",
    "      security:\n",
    "        - ApiKeyAuth: []\n",
    "      \n",
    "      x-rate-limit:\n",
    "        requests_per_minute: 100\n",
    "        requests_per_hour: 5000\n",
    "\n",
    "components:\n",
    "  schemas:\n",
    "    DailyRevenueResponse:\n",
    "      type: object\n",
    "      required:\n",
    "        - date\n",
    "        - region\n",
    "        - revenue_gross\n",
    "        - revenue_net\n",
    "        - transactions_count\n",
    "        - currency\n",
    "        - calculated_at\n",
    "      properties:\n",
    "        date:\n",
    "          type: string\n",
    "          format: date\n",
    "          description: Revenue date\n",
    "        region:\n",
    "          type: string\n",
    "          enum: [LATAM, NA, EU, APAC, ALL]\n",
    "          description: Geographic region\n",
    "        revenue_gross:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Total revenue before refunds/discounts\n",
    "          example: 125000.50\n",
    "        revenue_net:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Net revenue after refunds/discounts\n",
    "          example: 118000.30\n",
    "        transactions_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of transactions\n",
    "          example: 1543\n",
    "        refunds_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of refunded transactions (added v2.1)\n",
    "          example: 23\n",
    "        currency:\n",
    "          type: string\n",
    "          enum: [USD, EUR, BRL, MXN]\n",
    "          description: Currency code\n",
    "          example: \"USD\"\n",
    "        calculated_at:\n",
    "          type: string\n",
    "          format: date-time\n",
    "          description: Timestamp when metrics were calculated\n",
    "          example: \"2024-01-15T10:30:00Z\"\n",
    "        metadata:\n",
    "          type: object\n",
    "          properties:\n",
    "            data_quality_score:\n",
    "              type: number\n",
    "              description: Quality score (0-1)\n",
    "              example: 0.997\n",
    "            source_systems:\n",
    "              type: array\n",
    "              items:\n",
    "                type: string\n",
    "              example: [\"kafka_transactions\", \"sftp_refunds\"]\n",
    "    \n",
    "    Error:\n",
    "      type: object\n",
    "      required:\n",
    "        - error_code\n",
    "        - message\n",
    "      properties:\n",
    "        error_code:\n",
    "          type: string\n",
    "          example: \"INVALID_DATE_FORMAT\"\n",
    "        message:\n",
    "          type: string\n",
    "          example: \"Date must be in YYYY-MM-DD format\"\n",
    "        details:\n",
    "          type: object\n",
    "  \n",
    "  securitySchemes:\n",
    "    ApiKeyAuth:\n",
    "      type: apiKey\n",
    "      in: header\n",
    "      name: X-API-Key\n",
    "      description: Service account API key\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Schema Evolution: Backward Compatibility**\n",
    "\n",
    "```python\n",
    "# schema_evolution.py\n",
    "\"\"\"\n",
    "Rules for backward-compatible schema changes\n",
    "\"\"\"\n",
    "\n",
    "# ✅ SAFE (Backward Compatible):\n",
    "# 1. Add optional field\n",
    "{\n",
    "    \"date\": \"2024-01-15\",\n",
    "    \"revenue_gross\": 125000.50,\n",
    "    \"refunds_count\": 23  # NEW OPTIONAL FIELD (v2.1)\n",
    "}\n",
    "\n",
    "# 2. Add new enum value\n",
    "region: [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\", \"AFRICA\"]  # NEW VALUE\n",
    "\n",
    "# 3. Widen validation (less restrictive)\n",
    "# Old: revenue_gross: minimum 100\n",
    "# New: revenue_gross: minimum 0\n",
    "\n",
    "# ❌ BREAKING (Not Backward Compatible):\n",
    "# 1. Remove field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {}  # 'total' removed\n",
    "\n",
    "# 2. Rename field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {\"revenue_gross\": 125000.50}  # 'total' renamed\n",
    "\n",
    "# 3. Change type\n",
    "# Old: {\"transactions_count\": 1543}  # integer\n",
    "# New: {\"transactions_count\": \"1543\"}  # string\n",
    "\n",
    "# 4. Make field required\n",
    "# Old: {\"date\": \"2024-01-15\"}  # refunds_count optional\n",
    "# New: {\"date\": \"2024-01-15\", \"refunds_count\": 23}  # required\n",
    "\n",
    "# 5. Narrow validation (more restrictive)\n",
    "# Old: revenue_gross: minimum 0\n",
    "# New: revenue_gross: minimum 100\n",
    "```\n",
    "\n",
    "**Breaking Change Protocol:**\n",
    "\n",
    "```markdown\n",
    "# Breaking Change Checklist\n",
    "\n",
    "## Before Implementation\n",
    "- [ ] Document breaking change in CHANGELOG\n",
    "- [ ] Notify consumers 30 days in advance (email, Slack)\n",
    "- [ ] Update API version (v2 → v3)\n",
    "- [ ] Maintain old version for deprecation period (6 months)\n",
    "\n",
    "## Implementation\n",
    "- [ ] Deploy new version (v3) alongside old (v2)\n",
    "- [ ] Monitor usage of old version\n",
    "- [ ] Provide migration guide with examples\n",
    "\n",
    "## Deprecation\n",
    "- [ ] Mark old version as deprecated (HTTP header: `Deprecation: true`)\n",
    "- [ ] Return warning in responses (Sunset header)\n",
    "- [ ] Send reminder emails at 90, 60, 30, 7 days before sunset\n",
    "- [ ] Sunset date: Remove old version after 6 months\n",
    "\n",
    "## Example Response Headers:\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Deprecation: true\n",
    "Sunset: Sat, 15 Jul 2024 00:00:00 GMT\n",
    "Link: <https://api.company.com/ventas/v3/daily-revenue>; rel=\"successor-version\"\n",
    "Warning: 299 - \"This API version will be retired on 2024-07-15. Migrate to v3\"\n",
    "```\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. API Implementation with FastAPI**\n",
    "\n",
    "```python\n",
    "# api/ventas_api.py\n",
    "from fastapi import FastAPI, Query, HTTPException, Header, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import date, datetime\n",
    "from typing import Optional, Literal\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Daily Revenue API\",\n",
    "    version=\"2.1.0\",\n",
    "    description=\"Ventas domain data product\",\n",
    "    openapi_tags=[\n",
    "        {\"name\": \"Revenue\", \"description\": \"Revenue metrics operations\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prometheus metrics\n",
    "request_count = Counter('api_requests_total', 'Total requests', ['endpoint', 'status'])\n",
    "request_duration = Histogram('api_request_duration_seconds', 'Request duration', ['endpoint'])\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# MODELS\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "class DailyRevenueResponse(BaseModel):\n",
    "    date: date\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "    revenue_gross: float = Field(..., ge=0, description=\"Revenue before refunds\")\n",
    "    revenue_net: float = Field(..., ge=0, description=\"Net revenue\")\n",
    "    transactions_count: int = Field(..., ge=0)\n",
    "    refunds_count: int = Field(default=0, ge=0, description=\"New in v2.1\")\n",
    "    currency: Literal[\"USD\", \"EUR\", \"BRL\", \"MXN\"]\n",
    "    calculated_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"date\": \"2024-01-15\",\n",
    "                \"region\": \"LATAM\",\n",
    "                \"revenue_gross\": 125000.50,\n",
    "                \"revenue_net\": 118000.30,\n",
    "                \"transactions_count\": 1543,\n",
    "                \"refunds_count\": 23,\n",
    "                \"currency\": \"USD\",\n",
    "                \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# DEPENDENCIES\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "def verify_api_key(x_api_key: str = Header(...)):\n",
    "    \"\"\"Verify API key from header\"\"\"\n",
    "    # In production: check against Secrets Manager\n",
    "    valid_keys = [\"dev-key-123\", \"prod-key-456\"]\n",
    "    if x_api_key not in valid_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "    return x_api_key\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Dependency: S3 client\"\"\"\n",
    "    return boto3.client('s3', region_name='us-east-1')\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# ENDPOINTS\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "@app.get(\n",
    "    \"/daily-revenue\",\n",
    "    response_model=DailyRevenueResponse,\n",
    "    tags=[\"Revenue\"],\n",
    "    summary=\"Get daily revenue by region\",\n",
    "    responses={\n",
    "        200: {\"description\": \"Successful response\"},\n",
    "        400: {\"description\": \"Invalid date format\"},\n",
    "        404: {\"description\": \"No data for requested date\"},\n",
    "        503: {\"description\": \"Service unavailable (SLO violation)\"}\n",
    "    }\n",
    ")\n",
    "async def get_daily_revenue(\n",
    "    date: date = Query(..., description=\"Revenue date (YYYY-MM-DD)\"),\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"] = Query(\"ALL\"),\n",
    "    api_key: str = Depends(verify_api_key),\n",
    "    s3: boto3.client = Depends(get_s3_client)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve daily revenue metrics for a specific date and region.\n",
    "    \n",
    "    **Data Freshness**: Updated every 15 minutes\n",
    "    **SLO**: p99 latency <15 minutes from transaction to API\n",
    "    \"\"\"\n",
    "    with request_duration.labels(endpoint=\"/daily-revenue\").time():\n",
    "        try:\n",
    "            # Query data from S3 (or cache)\n",
    "            s3_key = f\"gold/revenue/daily/dt={date}/region={region}.parquet\"\n",
    "            \n",
    "            response = s3.get_object(\n",
    "                Bucket='mesh',\n",
    "                Key=s3_key\n",
    "            )\n",
    "            \n",
    "            # Parse Parquet (simplified)\n",
    "            import pandas as pd\n",
    "            df = pd.read_parquet(response['Body'])\n",
    "            \n",
    "            if df.empty:\n",
    "                request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "                raise HTTPException(status_code=404, detail=\"No data for requested date\")\n",
    "            \n",
    "            # Convert to response model\n",
    "            row = df.iloc[0]\n",
    "            result = DailyRevenueResponse(\n",
    "                date=date,\n",
    "                region=region,\n",
    "                revenue_gross=float(row['revenue_gross']),\n",
    "                revenue_net=float(row['revenue_net']),\n",
    "                transactions_count=int(row['transactions_count']),\n",
    "                refunds_count=int(row.get('refunds_count', 0)),  # Default for old data\n",
    "                currency=row['currency'],\n",
    "                calculated_at=row['calculated_at']\n",
    "            )\n",
    "            \n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=200).inc()\n",
    "            return result\n",
    "            \n",
    "        except s3.exceptions.NoSuchKey:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "            raise HTTPException(status_code=404, detail=f\"No data for {date}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=500).inc()\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for load balancer\"\"\"\n",
    "    return {\"status\": \"healthy\", \"version\": \"2.1.0\"}\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    from fastapi import Response\n",
    "    \n",
    "    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. SLO Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/slo_monitor.py\n",
    "\"\"\"\n",
    "Monitor SLOs for data products\n",
    "\"\"\"\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# SLO metrics\n",
    "slo_availability = Gauge('data_product_availability', 'Availability %', ['product', 'domain'])\n",
    "slo_latency_p99 = Gauge('data_product_latency_p99_seconds', 'p99 latency', ['product', 'domain'])\n",
    "slo_freshness = Gauge('data_product_freshness_seconds', 'Data age', ['product', 'domain'])\n",
    "\n",
    "def check_availability(api_url: str):\n",
    "    \"\"\"Check if API is responding (availability SLO)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{api_url}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_freshness(api_url: str):\n",
    "    \"\"\"Check data freshness (how old is the data?)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{api_url}/daily-revenue\",\n",
    "            params={\"date\": datetime.utcnow().date()},\n",
    "            headers={\"X-API-Key\": \"monitoring-key\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            calculated_at = datetime.fromisoformat(data['calculated_at'].replace('Z', '+00:00'))\n",
    "            age_seconds = (datetime.utcnow() - calculated_at).total_seconds()\n",
    "            return age_seconds\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def monitor_slos():\n",
    "    \"\"\"Continuous SLO monitoring\"\"\"\n",
    "    data_products = [\n",
    "        {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"domain\": \"ventas\",\n",
    "            \"url\": \"https://api.company.com/ventas/v2\",\n",
    "            \"slo_availability\": 0.999,  # 99.9%\n",
    "            \"slo_latency_p99\": 900,  # 15 min\n",
    "            \"slo_freshness\": 1200  # 20 min\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"shipment_tracking_api\",\n",
    "            \"domain\": \"logistica\",\n",
    "            \"url\": \"https://api.company.com/logistica/v1\",\n",
    "            \"slo_availability\": 0.9995,  # 99.95%\n",
    "            \"slo_latency_p99\": 300,  # 5 min\n",
    "            \"slo_freshness\": 600  # 10 min\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        for product in data_products:\n",
    "            # Availability\n",
    "            is_available = check_availability(product['url'])\n",
    "            availability_pct = 1.0 if is_available else 0.0\n",
    "            slo_availability.labels(\n",
    "                product=product['name'],\n",
    "                domain=product['domain']\n",
    "            ).set(availability_pct)\n",
    "            \n",
    "            # Freshness\n",
    "            freshness = check_freshness(product['url'])\n",
    "            if freshness:\n",
    "                slo_freshness.labels(\n",
    "                    product=product['name'],\n",
    "                    domain=product['domain']\n",
    "                ).set(freshness)\n",
    "                \n",
    "                # Alert if SLO violated\n",
    "                if freshness > product['slo_freshness']:\n",
    "                    send_alert(\n",
    "                        f\"🚨 Freshness SLO violated for {product['name']}: \"\n",
    "                        f\"{freshness}s > {product['slo_freshness']}s\"\n",
    "                    )\n",
    "        \n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_http_server(8001)\n",
    "    monitor_slos()\n",
    "```\n",
    "\n",
    "**Prometheus Alert Rules:**\n",
    "\n",
    "```yaml\n",
    "# alerts/data-product-slos.yml\n",
    "groups:\n",
    "  - name: data_product_slos\n",
    "    interval: 1m\n",
    "    rules:\n",
    "      \n",
    "      - alert: DataProductAvailabilitySLOViolation\n",
    "        expr: |\n",
    "          (\n",
    "            sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d])\n",
    "          ) < 0.999\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} availability below SLO\"\n",
    "          description: \"Availability: {{ $value | humanizePercentage }} (target: 99.9%)\"\n",
    "          dashboard: \"https://grafana.company.com/d/slo?product={{ $labels.product }}\"\n",
    "      \n",
    "      - alert: DataProductFreshnessSLOViolation\n",
    "        expr: |\n",
    "          data_product_freshness_seconds > 1200\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} data is stale\"\n",
    "          description: \"Data age: {{ $value }}s (target: <1200s)\"\n",
    "      \n",
    "      - alert: DataProductErrorBudgetExhausted\n",
    "        expr: |\n",
    "          (1 - \n",
    "            (sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d]))\n",
    "          ) > 0.001  # 99.9% SLO = 0.1% error budget\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} error budget exhausted\"\n",
    "          description: |\n",
    "            Error budget used: {{ $value | humanizePercentage }}\n",
    "            Action: Freeze non-critical deployments until recovered\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Client SDK Generation**\n",
    "\n",
    "```bash\n",
    "# Generate Python client from OpenAPI spec\n",
    "openapi-generator generate \\\n",
    "  -i contracts/ventas-daily-revenue-api-v2.yaml \\\n",
    "  -g python \\\n",
    "  -o clients/python/ventas-api-client \\\n",
    "  --package-name ventas_api_client\n",
    "\n",
    "# Usage\n",
    "pip install ./clients/python/ventas-api-client\n",
    "\n",
    "# Client code\n",
    "from ventas_api_client import ApiClient, Configuration, RevenuApi\n",
    "\n",
    "config = Configuration()\n",
    "config.host = \"https://api.company.com/ventas/v2\"\n",
    "config.api_key['X-API-Key'] = \"your-api-key\"\n",
    "\n",
    "client = ApiClient(configuration=config)\n",
    "api = RevenueApi(client)\n",
    "\n",
    "# Call API\n",
    "response = api.get_daily_revenue(date=\"2024-01-15\", region=\"LATAM\")\n",
    "print(f\"Revenue: ${response.revenue_net:,.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Contract Testing**\n",
    "\n",
    "```python\n",
    "# tests/contract_tests.py\n",
    "\"\"\"\n",
    "Ensure API complies with OpenAPI contract\n",
    "\"\"\"\n",
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "from api.ventas_api import app\n",
    "import yaml\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def load_openapi_spec():\n",
    "    with open(\"contracts/ventas-daily-revenue-api-v2.yaml\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def test_response_matches_schema():\n",
    "    \"\"\"Verify response matches OpenAPI schema\"\"\"\n",
    "    spec = load_openapi_spec()\n",
    "    \n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15&region=LATAM\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    \n",
    "    # Verify required fields\n",
    "    schema = spec['components']['schemas']['DailyRevenueResponse']\n",
    "    required_fields = schema['required']\n",
    "    \n",
    "    for field in required_fields:\n",
    "        assert field in data, f\"Missing required field: {field}\"\n",
    "    \n",
    "    # Verify types\n",
    "    assert isinstance(data['revenue_gross'], (int, float))\n",
    "    assert isinstance(data['transactions_count'], int)\n",
    "    assert data['region'] in [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "\n",
    "def test_backward_compatibility():\n",
    "    \"\"\"Ensure v2.1 is backward compatible with v2.0\"\"\"\n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # v2.0 clients expect these fields\n",
    "    assert 'date' in data\n",
    "    assert 'revenue_gross' in data  # Renamed from 'total' in v2.0\n",
    "    assert 'transactions_count' in data\n",
    "    \n",
    "    # v2.1 added optional field (should have default)\n",
    "    assert 'refunds_count' in data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bd2f5",
   "metadata": {},
   "source": [
    "### 🏛️ **Federated Governance: Policies, Observability & Cost at Scale**\n",
    "\n",
    "**1. Computational Governance Model**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│              Governance Federation Model                       │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  ┌─────────────────────────────────────────────────────────┐  │\n",
    "│  │  GLOBAL POLICIES (Platform Team)                        │  │\n",
    "│  ├─────────────────────────────────────────────────────────┤  │\n",
    "│  │ • Security: IAM, encryption, PII masking                │  │\n",
    "│  │ • Compliance: GDPR, SOX, PCI-DSS                        │  │\n",
    "│  │ • Quality: Minimum SLOs (99% availability)              │  │\n",
    "│  │ • Observability: Mandatory lineage, metrics             │  │\n",
    "│  │ • Cost: Budget limits per domain                        │  │\n",
    "│  └─────────────────────────────────────────────────────────┘  │\n",
    "│                           ▼                                     │\n",
    "│  ┌─────────────────────────────────────────────────────────┐  │\n",
    "│  │  AUTOMATED ENFORCEMENT (Policy Engine)                  │  │\n",
    "│  ├─────────────────────────────────────────────────────────┤  │\n",
    "│  │ • OPA/Cedar: Runtime policy checks                      │  │\n",
    "│  │ • pre-commit hooks: Prevent non-compliant code          │  │\n",
    "│  │ • CI/CD gates: Block deployment if policies fail        │  │\n",
    "│  └─────────────────────────────────────────────────────────┘  │\n",
    "│                           ▼                                     │\n",
    "│  ┌──────────────┬──────────────┬──────────────┬────────────┐  │\n",
    "│  │ Ventas       │ Logistica    │ Producto     │ Marketing  │  │\n",
    "│  │ (local impl) │ (local impl) │ (local impl) │ (local)    │  │\n",
    "│  ├──────────────┼──────────────┼──────────────┼────────────┤  │\n",
    "│  │ • Spark      │ • dbt        │ • Polars     │ • Airflow  │  │\n",
    "│  │ • Daily      │ • Hourly     │ • Weekly     │ • Real-time│  │\n",
    "│  │ • 50 GB/day  │ • 100 GB/day │ • 10 GB/day  │ • 200 GB/d │  │\n",
    "│  └──────────────┴──────────────┴──────────────┴────────────┘  │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Policy as Code: Open Policy Agent (OPA)**\n",
    "\n",
    "```rego\n",
    "# policies/data_mesh_policies.rego\n",
    "package datamesh\n",
    "\n",
    "import future.keywords.if\n",
    "import future.keywords.in\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 1: PII Must Be Masked in Shared Datasets\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.dataset.sharing_level == \"public\"\n",
    "    some column in input.dataset.columns\n",
    "    column.contains_pii == true\n",
    "    not column.is_masked\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"PII column '%s' in dataset '%s' must be masked for public sharing\",\n",
    "        [column.name, input.dataset.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 2: All Data Products Must Have Owner\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    not input.data_product.owner\n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' must have an owner defined\",\n",
    "        [input.data_product.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.owner\n",
    "    not endswith(input.data_product.owner, \"@company.com\")\n",
    "    msg := sprintf(\n",
    "        \"Data product owner '%s' must be a valid company email\",\n",
    "        [input.data_product.owner]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 3: Cost Budget Enforcement\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.domain.monthly_cost > input.domain.budget_limit\n",
    "    overage := input.domain.monthly_cost - input.domain.budget_limit\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' exceeded budget: $%.2f over limit of $%.2f\",\n",
    "        [input.domain.name, overage, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "warn[msg] if {\n",
    "    usage := input.domain.monthly_cost / input.domain.budget_limit\n",
    "    usage > 0.8\n",
    "    usage <= 1.0\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' at %.0f%% of budget ($%.2f / $%.2f)\",\n",
    "        [input.domain.name, usage * 100, input.domain.monthly_cost, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 4: Data Quality SLO Enforcement\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.quality_score < 0.95\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' quality score %.2f%% below minimum 95%%\",\n",
    "        [input.data_product.name, input.data_product.quality_score * 100]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 5: Lineage Must Be Tracked\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.pipeline.outputs_to_shared_layer\n",
    "    not input.pipeline.emits_lineage\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Pipeline '%s' must emit lineage (OpenLineage) to DataHub\",\n",
    "        [input.pipeline.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 6: API Versioning Required\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.api.is_public\n",
    "    not regex.match(`^/v[0-9]+/`, input.api.path)\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Public API path '%s' must include version prefix (e.g., /v1/)\",\n",
    "        [input.api.path]\n",
    "    )\n",
    "}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════\n",
    "# POLICY 7: Cross-Domain Access Requires Approval\n",
    "# ═══════════════════════════════════════════════════════\n",
    "\n",
    "deny[msg] if {\n",
    "    input.access_request.source_domain != input.dataset.owner_domain\n",
    "    not input.access_request.approved_by_owner\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' accessing '%s' dataset requires approval from '%s' team\",\n",
    "        [\n",
    "            input.access_request.source_domain,\n",
    "            input.dataset.name,\n",
    "            input.dataset.owner_domain\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "```\n",
    "\n",
    "**Policy Enforcement in CI/CD:**\n",
    "\n",
    "```python\n",
    "# scripts/check_policies.py\n",
    "\"\"\"\n",
    "Validate policies before deployment\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def check_opa_policies(input_data: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Run OPA policy checks\n",
    "    \"\"\"\n",
    "    # Convert input to JSON\n",
    "    input_json = json.dumps(input_data)\n",
    "    \n",
    "    # Run OPA evaluation\n",
    "    result = subprocess.run(\n",
    "        [\"opa\", \"eval\", \"--data\", \"policies/\", \"--input\", \"-\", \"data.datamesh.deny\"],\n",
    "        input=input_json.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"❌ OPA evaluation failed: {result.stderr.decode()}\")\n",
    "        return False\n",
    "    \n",
    "    output = json.loads(result.stdout)\n",
    "    violations = output['result'][0]['expressions'][0]['value']\n",
    "    \n",
    "    if violations:\n",
    "        print(\"❌ Policy violations detected:\")\n",
    "        for violation in violations:\n",
    "            print(f\"  - {violation}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✅ All policies passed\")\n",
    "    return True\n",
    "\n",
    "# Example usage in pre-commit hook\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = {\n",
    "        \"data_product\": {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"owner\": \"ventas-team@company.com\",\n",
    "            \"quality_score\": 0.997\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"name\": \"ventas_curated\",\n",
    "            \"sharing_level\": \"public\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"customer_id\", \"contains_pii\": False},\n",
    "                {\"name\": \"email\", \"contains_pii\": True, \"is_masked\": True}\n",
    "            ]\n",
    "        },\n",
    "        \"domain\": {\n",
    "            \"name\": \"ventas\",\n",
    "            \"monthly_cost\": 1800,\n",
    "            \"budget_limit\": 2000\n",
    "        },\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"ventas_daily_batch\",\n",
    "            \"outputs_to_shared_layer\": True,\n",
    "            \"emits_lineage\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not check_opa_policies(input_data):\n",
    "        sys.exit(1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Multi-Tenant Observability Dashboard**\n",
    "\n",
    "```python\n",
    "# monitoring/mesh_dashboard.py\n",
    "\"\"\"\n",
    "Unified observability across all domains\n",
    "\"\"\"\n",
    "from grafana_api.grafana_face import GrafanaFace\n",
    "import json\n",
    "\n",
    "grafana = GrafanaFace(auth=\"admin:admin\", host=\"grafana.company.com\")\n",
    "\n",
    "# Create unified dashboard\n",
    "dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Data Mesh - Multi-Domain Overview\",\n",
    "        \"tags\": [\"data-mesh\", \"cross-domain\"],\n",
    "        \"timezone\": \"utc\",\n",
    "        \"panels\": [\n",
    "            # Panel 1: Domain Health Matrix\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Domain Health Matrix\",\n",
    "                \"type\": \"heatmap\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(data_product_availability[5m])\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 0, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 2: SLO Compliance by Domain\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"SLO Compliance by Domain\",\n",
    "                \"type\": \"gauge\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (\n",
    "                            sum_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                            /\n",
    "                            count_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"Availability %\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"thresholds\": {\n",
    "                            \"steps\": [\n",
    "                                {\"color\": \"red\", \"value\": 0},\n",
    "                                {\"color\": \"yellow\", \"value\": 99},\n",
    "                                {\"color\": \"green\", \"value\": 99.9}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 3: Cost by Domain\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Cost by Domain (Monthly)\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            increase(aws_billing_estimated_charges{domain!=\"\"}[30d])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 18, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 4: Data Freshness by Product\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Data Freshness (Last Update)\",\n",
    "                \"type\": \"table\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (time() - data_product_last_update_timestamp) / 60\n",
    "                    \"\"\",\n",
    "                    \"format\": \"table\",\n",
    "                    \"instant\": True\n",
    "                }],\n",
    "                \"transformations\": [{\n",
    "                    \"id\": \"organize\",\n",
    "                    \"options\": {\n",
    "                        \"excludeByName\": {},\n",
    "                        \"indexByName\": {\n",
    "                            \"domain\": 0,\n",
    "                            \"product\": 1,\n",
    "                            \"Value\": 2\n",
    "                        },\n",
    "                        \"renameByName\": {\n",
    "                            \"Value\": \"Minutes Since Update\"\n",
    "                        }\n",
    "                    }\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 5: Pipeline Success Rate\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"title\": \"Pipeline Success Rate (24h)\",\n",
    "                \"type\": \"stat\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_success{domain!=\"\"}[24h])\n",
    "                        )\n",
    "                        /\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_total{domain!=\"\"}[24h])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 6: Feature Store Usage\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"title\": \"Feature Store Requests (by domain)\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        rate(feast_feature_requests_total[5m])\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }} - {{ feature_view }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 16, \"w\": 24, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 7: Data Quality Score Trend\n",
    "            {\n",
    "                \"id\": 7,\n",
    "                \"title\": \"Data Quality Score Trend\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        avg by (domain) (\n",
    "                            data_product_quality_score\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"min\": 0,\n",
    "                        \"max\": 1,\n",
    "                        \"unit\": \"percentunit\"\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 8: Cross-Domain Dependencies\n",
    "            {\n",
    "                \"id\": 8,\n",
    "                \"title\": \"Cross-Domain API Calls\",\n",
    "                \"type\": \"nodeGraph\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (source_domain, target_domain) (\n",
    "                            rate(api_requests_total{source_domain!=\"\",target_domain!=\"\"}[5m])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            }\n",
    "        ],\n",
    "        \"templating\": {\n",
    "            \"list\": [\n",
    "                {\n",
    "                    \"name\": \"domain\",\n",
    "                    \"type\": \"query\",\n",
    "                    \"query\": \"label_values(data_product_availability, domain)\",\n",
    "                    \"multi\": False,\n",
    "                    \"includeAll\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"folderId\": 0,\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "# Create dashboard\n",
    "result = grafana.dashboard.update_dashboard(dashboard)\n",
    "print(f\"✅ Dashboard created: {result['url']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Cost Allocation and Chargeback**\n",
    "\n",
    "```python\n",
    "# finops/cost_allocation.py\n",
    "\"\"\"\n",
    "Allocate AWS costs to domains (chargeback model)\n",
    "\"\"\"\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "ce = boto3.client('ce', region_name='us-east-1')\n",
    "\n",
    "def get_domain_costs(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query AWS Cost Explorer with domain tags\n",
    "    \"\"\"\n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': start_date,\n",
    "            'End': end_date\n",
    "        },\n",
    "        Granularity='DAILY',\n",
    "        Metrics=['UnblendedCost', 'UsageQuantity'],\n",
    "        GroupBy=[\n",
    "            {'Type': 'TAG', 'Key': 'domain'},\n",
    "            {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n",
    "        ],\n",
    "        Filter={\n",
    "            'Tags': {\n",
    "                'Key': 'Project',\n",
    "                'Values': ['data-mesh']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse results\n",
    "    rows = []\n",
    "    for result in response['ResultsByTime']:\n",
    "        date = result['TimePeriod']['Start']\n",
    "        for group in result['Groups']:\n",
    "            domain = group['Keys'][0].split('$')[1] if '$' in group['Keys'][0] else 'untagged'\n",
    "            service = group['Keys'][1]\n",
    "            cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "            \n",
    "            rows.append({\n",
    "                'date': date,\n",
    "                'domain': domain,\n",
    "                'service': service,\n",
    "                'cost': cost\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Generate monthly report\n",
    "start = (datetime.utcnow() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "end = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "costs_df = get_domain_costs(start, end)\n",
    "\n",
    "# Aggregate by domain\n",
    "domain_summary = costs_df.groupby('domain').agg({\n",
    "    'cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "domain_summary = domain_summary.sort_values('cost', ascending=False)\n",
    "\n",
    "print(\"\\n📊 Domain Cost Summary (Last 30 Days):\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in domain_summary.iterrows():\n",
    "    print(f\"{row['domain']:15} ${row['cost']:>10,.2f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'TOTAL':15} ${domain_summary['cost'].sum():>10,.2f}\")\n",
    "\n",
    "# Check budget overages\n",
    "budgets = {\n",
    "    'ventas': 2000,\n",
    "    'logistica': 1500,\n",
    "    'producto': 1000,\n",
    "    'marketing': 2500,\n",
    "    'finanzas': 3000\n",
    "}\n",
    "\n",
    "print(\"\\n⚠️ Budget Status:\")\n",
    "print(\"=\" * 50)\n",
    "for domain, budget in budgets.items():\n",
    "    actual = domain_summary[domain_summary['domain'] == domain]['cost'].sum()\n",
    "    utilization = (actual / budget) * 100\n",
    "    status = \"✅\" if utilization <= 100 else \"🚨\"\n",
    "    \n",
    "    print(f\"{status} {domain:15} ${actual:>8,.2f} / ${budget:>8,.2f} ({utilization:>5.1f}%)\")\n",
    "\n",
    "# Export to DataHub for visibility\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "for _, row in domain_summary.iterrows():\n",
    "    # Emit cost metrics to DataHub\n",
    "    domain_urn = f\"urn:li:domain:{row['domain']}\"\n",
    "    # (Implementation omitted for brevity)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. DataHub: Cross-Domain Lineage**\n",
    "\n",
    "```python\n",
    "# lineage/emit_cross_domain_lineage.py\n",
    "\"\"\"\n",
    "Emit lineage showing cross-domain dependencies\n",
    "\"\"\"\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import (\n",
    "    DatasetLineageTypeClass,\n",
    "    UpstreamClass,\n",
    "    UpstreamLineageClass\n",
    ")\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "# Example: Marketing domain consumes Ventas + Producto data\n",
    "lineage_map = {\n",
    "    # Marketing domain datasets\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.marketing.customer_segments,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            # Consumes from Ventas\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.customer_transactions,PROD)\",\n",
    "            # Consumes from Producto\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.producto.catalog,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.TRANSFORMED\n",
    "    },\n",
    "    \n",
    "    # Finanzas consumes Ventas\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.finanzas.accounting_reports,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.COPY\n",
    "    }\n",
    "}\n",
    "\n",
    "for downstream_urn, lineage_info in lineage_map.items():\n",
    "    upstreams = [\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_urn,\n",
    "            type=lineage_info['type']\n",
    "        )\n",
    "        for upstream_urn in lineage_info['upstreams']\n",
    "    ]\n",
    "    \n",
    "    lineage = UpstreamLineageClass(upstreams=upstreams)\n",
    "    \n",
    "    emitter.emit_mcp(\n",
    "        MetadataChangeProposalWrapper(\n",
    "            entityUrn=downstream_urn,\n",
    "            aspect=lineage\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Lineage emitted for {downstream_urn}\")\n",
    "```\n",
    "\n",
    "**DataHub Search: Find Cross-Domain Dependencies**\n",
    "\n",
    "```graphql\n",
    "# GraphQL query to find all datasets consuming Ventas data\n",
    "query {\n",
    "  search(\n",
    "    input: {\n",
    "      type: DATASET\n",
    "      query: \"*\"\n",
    "      filters: [\n",
    "        {\n",
    "          field: \"upstream.urn\"\n",
    "          values: [\"urn:li:domain:ventas\"]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ) {\n",
    "    searchResults {\n",
    "      entity {\n",
    "        ... on Dataset {\n",
    "          urn\n",
    "          name\n",
    "          domain {\n",
    "            name\n",
    "          }\n",
    "          upstream {\n",
    "            dataset {\n",
    "              urn\n",
    "              name\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Domain Autonomy with Guardrails**\n",
    "\n",
    "```python\n",
    "# platform/domain_provisioning.py\n",
    "\"\"\"\n",
    "Self-service domain provisioning with governance guardrails\n",
    "\"\"\"\n",
    "from typing import Dict\n",
    "import boto3\n",
    "\n",
    "class DomainProvisioner:\n",
    "    \"\"\"\n",
    "    Automated domain provisioning with policy enforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.iam = boto3.client('iam')\n",
    "        self.glue = boto3.client('glue')\n",
    "    \n",
    "    def provision_new_domain(self, domain_name: str, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Provision infrastructure for new domain\n",
    "        \n",
    "        Enforces:\n",
    "        - Naming conventions\n",
    "        - Cost budgets\n",
    "        - Security policies\n",
    "        - Observability standards\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate config against policies\n",
    "        if not self._validate_config(domain_name, config):\n",
    "            raise ValueError(\"Configuration violates governance policies\")\n",
    "        \n",
    "        resources = {}\n",
    "        \n",
    "        # 1. S3 bucket with standard structure\n",
    "        bucket_name = f\"mesh-{domain_name}\"\n",
    "        self.s3.create_bucket(Bucket=bucket_name)\n",
    "        \n",
    "        # Apply lifecycle policies (governance requirement)\n",
    "        self.s3.put_bucket_lifecycle_configuration(\n",
    "            Bucket=bucket_name,\n",
    "            LifecycleConfiguration={\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'ID': 'raw-retention',\n",
    "                        'Prefix': 'raw/',\n",
    "                        'Status': 'Enabled',\n",
    "                        'Transitions': [\n",
    "                            {'Days': 7, 'StorageClass': 'GLACIER'}\n",
    "                        ],\n",
    "                        'Expiration': {'Days': 90}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Enable versioning (governance requirement)\n",
    "        self.s3.put_bucket_versioning(\n",
    "            Bucket=bucket_name,\n",
    "            VersioningConfiguration={'Status': 'Enabled'}\n",
    "        )\n",
    "        \n",
    "        resources['s3_bucket'] = bucket_name\n",
    "        \n",
    "        # 2. IAM role with least privilege\n",
    "        role_name = f\"{domain_name}-pipeline-role\"\n",
    "        assume_role_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"emr-serverless.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        self.iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy),\n",
    "            Tags=[\n",
    "                {'Key': 'domain', 'Value': domain_name},\n",
    "                {'Key': 'managed-by', 'Value': 'platform-team'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        resources['iam_role'] = role_name\n",
    "        \n",
    "        # 3. Glue database\n",
    "        self.glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': f\"{domain_name}_db\",\n",
    "                'Description': f\"Database for {domain_name} domain\",\n",
    "                'Parameters': {\n",
    "                    'domain': domain_name,\n",
    "                    'owner': config['owner']\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resources['glue_database'] = f\"{domain_name}_db\"\n",
    "        \n",
    "        # 4. Budget alert (governance requirement)\n",
    "        budgets = boto3.client('budgets')\n",
    "        budgets.create_budget(\n",
    "            AccountId='123456789012',\n",
    "            Budget={\n",
    "                'BudgetName': f\"{domain_name}-monthly\",\n",
    "                'BudgetLimit': {\n",
    "                    'Amount': str(config['budget_limit']),\n",
    "                    'Unit': 'USD'\n",
    "                },\n",
    "                'TimeUnit': 'MONTHLY',\n",
    "                'BudgetType': 'COST',\n",
    "                'CostFilters': {\n",
    "                    'TagKeyValue': [f'domain${domain_name}']\n",
    "                }\n",
    "            },\n",
    "            NotificationsWithSubscribers=[\n",
    "                {\n",
    "                    'Notification': {\n",
    "                        'NotificationType': 'ACTUAL',\n",
    "                        'ComparisonOperator': 'GREATER_THAN',\n",
    "                        'Threshold': 80.0\n",
    "                    },\n",
    "                    'Subscribers': [{\n",
    "                        'SubscriptionType': 'EMAIL',\n",
    "                        'Address': config['owner']\n",
    "                    }]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return resources\n",
    "    \n",
    "    def _validate_config(self, domain_name: str, config: Dict) -> bool:\n",
    "        \"\"\"Validate against governance policies\"\"\"\n",
    "        \n",
    "        # Policy 1: Owner must be specified\n",
    "        if 'owner' not in config or not config['owner'].endswith('@company.com'):\n",
    "            print(\"❌ Owner email required and must be @company.com\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 2: Budget limit required\n",
    "        if 'budget_limit' not in config or config['budget_limit'] <= 0:\n",
    "            print(\"❌ Budget limit must be positive\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 3: Naming convention\n",
    "        if not domain_name.islower() or len(domain_name) > 20:\n",
    "            print(\"❌ Domain name must be lowercase and <20 chars\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "provisioner = DomainProvisioner()\n",
    "\n",
    "new_domain_config = {\n",
    "    'owner': 'analytics-team@company.com',\n",
    "    'budget_limit': 1500,\n",
    "    'slo_availability': 0.999\n",
    "}\n",
    "\n",
    "resources = provisioner.provision_new_domain('analytics', new_domain_config)\n",
    "print(f\"✅ Domain 'analytics' provisioned: {resources}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Incident Response: Cross-Domain Impact Analysis**\n",
    "\n",
    "```python\n",
    "# sre/incident_response.py\n",
    "\"\"\"\n",
    "Analyze blast radius of incidents across domains\n",
    "\"\"\"\n",
    "from datahub.client import DataHubGraph\n",
    "\n",
    "graph = DataHubGraph(server=\"http://datahub:8080\")\n",
    "\n",
    "def analyze_impact(failed_dataset_urn: str):\n",
    "    \"\"\"\n",
    "    Find all downstream consumers of failed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query DataHub for downstream lineage\n",
    "    query = f\"\"\"\n",
    "    {{\n",
    "        dataset(urn: \"{failed_dataset_urn}\") {{\n",
    "            urn\n",
    "            name\n",
    "            domain {{ name }}\n",
    "            downstream(limit: 100) {{\n",
    "                dataset {{\n",
    "                    urn\n",
    "                    name\n",
    "                    domain {{ name }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = graph.execute_graphql(query)\n",
    "    failed = result['dataset']\n",
    "    \n",
    "    print(f\"\\n🚨 INCIDENT: {failed['name']} in {failed['domain']['name']} domain\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    downstreams = failed.get('downstream', [])\n",
    "    \n",
    "    if not downstreams:\n",
    "        print(\"✅ No downstream dependencies (isolated impact)\")\n",
    "        return\n",
    "    \n",
    "    # Group by domain\n",
    "    impacted_domains = {}\n",
    "    for downstream in downstreams:\n",
    "        ds = downstream['dataset']\n",
    "        domain = ds['domain']['name']\n",
    "        \n",
    "        if domain not in impacted_domains:\n",
    "            impacted_domains[domain] = []\n",
    "        \n",
    "        impacted_domains[domain].append(ds['name'])\n",
    "    \n",
    "    print(f\"⚠️ IMPACTED DOMAINS: {len(impacted_domains)}\")\n",
    "    for domain, datasets in impacted_domains.items():\n",
    "        print(f\"\\n  {domain.upper()}:\")\n",
    "        for dataset in datasets:\n",
    "            print(f\"    - {dataset}\")\n",
    "    \n",
    "    # Suggest actions\n",
    "    print(\"\\n📋 RECOMMENDED ACTIONS:\")\n",
    "    print(\"1. Notify impacted domain owners:\")\n",
    "    for domain in impacted_domains.keys():\n",
    "        print(f\"   - {domain}-team@company.com\")\n",
    "    print(\"2. Check if impacted datasets have fallback sources\")\n",
    "    print(\"3. Estimate ETA for fix and communicate\")\n",
    "\n",
    "# Example: Ventas daily revenue pipeline failed\n",
    "analyze_impact(\"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\")\n",
    "\n",
    "# Output:\n",
    "# 🚨 INCIDENT: daily_revenue in ventas domain\n",
    "# ======================================================================\n",
    "# ⚠️ IMPACTED DOMAINS: 3\n",
    "# \n",
    "#   MARKETING:\n",
    "#     - customer_segments\n",
    "#     - campaign_attribution\n",
    "# \n",
    "#   FINANZAS:\n",
    "#     - accounting_reports\n",
    "#     - revenue_forecasts\n",
    "# \n",
    "#   EXECUTIVE:\n",
    "#     - executive_dashboard\n",
    "# \n",
    "# 📋 RECOMMENDED ACTIONS:\n",
    "# 1. Notify impacted domain owners:\n",
    "#    - marketing-team@company.com\n",
    "#    - finanzas-team@company.com\n",
    "#    - executive-team@company.com\n",
    "# 2. Check if impacted datasets have fallback sources\n",
    "# 3. Estimate ETA for fix and communicate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa74a1e",
   "metadata": {},
   "source": [
    "## 1. Contexto y requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06572913",
   "metadata": {},
   "source": [
    "**Empresa**: Marketplace multi-categoría con 5 dominios de negocio:\n",
    "- Ventas (Sales)\n",
    "- Logística (Fulfillment)\n",
    "- Producto (Catalog)\n",
    "- Marketing (Campaigns)\n",
    "- Finanzas (Payments)\n",
    "\n",
    "**Objetivo**: Cada dominio gestiona sus propios datos como producto, con SLOs, versionado y documentación. Un feature store central consume features de todos los dominios para ML.\n",
    "\n",
    "**Requerimientos**:\n",
    "- Plataforma self-service: catálogo, CI/CD, observabilidad compartidos.\n",
    "- Gobernanza federada: políticas de seguridad y calidad globales, aplicadas localmente.\n",
    "- Feature store (Feast/Tecton) con features de cada dominio.\n",
    "- APIs de data products con contratos versionados (OpenAPI).\n",
    "- Linaje cross-domain visible en DataHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb95d6",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Data Mesh propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_diagram = '''\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                  Plataforma Self-Service                       │\n",
    "│  - Airflow compartido  - DataHub (catálogo + linaje)          │\n",
    "│  - Grafana + Prometheus - CI/CD (GitHub Actions)              │\n",
    "│  - Feature Store (Feast) - Políticas IAM centrales            │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "                              ▲\n",
    "          ┌───────────────────┼───────────────────┐\n",
    "          │                   │                   │\n",
    "  ┌───────▼──────┐   ┌────────▼────────┐  ┌──────▼────────┐\n",
    "  │ Dominio      │   │  Dominio        │  │  Dominio      │\n",
    "  │ Ventas       │   │  Logística      │  │  Producto     │\n",
    "  │ (data prod)  │   │  (data prod)    │  │  (data prod)  │\n",
    "  │ - raw/       │   │  - raw/         │  │  - raw/       │\n",
    "  │ - curated/   │   │  - curated/     │  │  - curated/   │\n",
    "  │ - API        │   │  - API          │  │  - API        │\n",
    "  │ - Features   │   │  - Features     │  │  - Features   │\n",
    "  └──────────────┘   └─────────────────┘  └───────────────┘\n",
    "          │                   │                   │\n",
    "          └───────────────────┼───────────────────┘\n",
    "                              ▼\n",
    "                   ┌──────────────────┐\n",
    "                   │  Feature Store   │\n",
    "                   │  (Feast/Tecton)  │\n",
    "                   └─────────┬────────┘\n",
    "                             │\n",
    "                      ┌──────▼─────┐\n",
    "                      │ ML Models  │\n",
    "                      │ (training) │\n",
    "                      └────────────┘\n",
    "'''\n",
    "print(mesh_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732eb23",
   "metadata": {},
   "source": [
    "## 3. Componentes por dominio (ejemplo: Ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38990cce",
   "metadata": {},
   "source": [
    "### 3.1 Data Product Ventas\n",
    "- Owner: Equipo de Ventas.\n",
    "- Fuentes: Kafka (transacciones), archivos batch (devoluciones).\n",
    "- Storage: S3 `s3://mesh/ventas/raw/`, `s3://mesh/ventas/curated/`.\n",
    "- API: FastAPI endpoint `/ventas/v1/daily-revenue` con contrato OpenAPI.\n",
    "- Features: `cliente_total_compras_30d`, `cliente_num_transacciones_7d`.\n",
    "- SLO: latencia p99 < 15 min, disponibilidad > 99.9%.\n",
    "- Documentación: README, diagramas, changelog.\n",
    "\n",
    "### 3.2 Pipeline Ventas\n",
    "- Airflow DAG propio del equipo, con validaciones GE y alertas.\n",
    "- Escribe features a Feast (offline store: S3 Parquet, online: Redis).\n",
    "- Emite linaje a DataHub vía OpenLineage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d910d0",
   "metadata": {},
   "source": [
    "## 4. Feature Store centralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feast_config = r'''\n",
    "# feature_repo/ventas_features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "ventas_source = FileSource(\n",
    "    path='s3://mesh/ventas/curated/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "ventas_fv = FeatureView(\n",
    "    name='ventas_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras_30d', dtype=Float32),\n",
    "        Field(name='num_transacciones_7d', dtype=Int64),\n",
    "    ],\n",
    "    source=ventas_source,\n",
    "    owner='ventas-team@empresa.com'\n",
    ")\n",
    "\n",
    "# Similarmente para logistica_features, producto_features, etc.\n",
    "'''\n",
    "print(feast_config.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07846920",
   "metadata": {},
   "source": [
    "## 5. Gobernanza federada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986b720",
   "metadata": {},
   "source": [
    "- Políticas globales:\n",
    "  - Todo PII enmascarado en datasets compartidos.\n",
    "  - Validaciones mínimas de calidad (Great Expectations).\n",
    "  - Linaje obligatorio (OpenLineage).\n",
    "  - Versionado semántico de APIs.\n",
    "- Autonomía local:\n",
    "  - Cada dominio elige su stack de transformación (Spark/Pandas/dbt).\n",
    "  - Frecuencia de actualizaciones según SLO propio.\n",
    "  - Esquemas propios, evolucionables con compatibilidad (Avro/Protobuf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480061b",
   "metadata": {},
   "source": [
    "## 6. Checklist de implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = '''\n",
    "☐ 1. Definir dominios y owners (RACI)\n",
    "☐ 2. Crear buckets S3 por dominio (ventas/, logistica/, producto/)\n",
    "☐ 3. DAG Airflow por dominio con validaciones y linaje\n",
    "☐ 4. APIs FastAPI versionadas (OpenAPI specs)\n",
    "☐ 5. Feature definitions en Feast por dominio\n",
    "☐ 6. Feast apply y materialize-incremental en CI/CD\n",
    "☐ 7. DataHub registrar data products con metadata\n",
    "☐ 8. Políticas IAM federadas (admin global + roles por dominio)\n",
    "☐ 9. Dashboard Grafana multi-dominio con SLOs\n",
    "☐ 10. Contratos de calidad (data contracts) versionados\n",
    "☐ 11. Onboarding docs para nuevos dominios\n",
    "☐ 12. Incident response playbook cross-domain\n",
    "☐ 13. Cost allocation tags por dominio\n",
    "☐ 14. Training model multi-dominio (consume features de Feast)\n",
    "☐ 15. Tests de integración cross-domain\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68ad77",
   "metadata": {},
   "source": [
    "## 7. Entregables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a040e",
   "metadata": {},
   "source": [
    "- Documento de diseño Data Mesh con principios y responsabilidades.\n",
    "- Repositorio multi-dominio (monorepo o multi-repo).\n",
    "- Feature store funcional con features de ≥3 dominios.\n",
    "- Catálogo DataHub con linaje cross-domain.\n",
    "- APIs documentadas (Swagger/OpenAPI) por dominio.\n",
    "- Dashboard unificado con métricas de todos los dominios.\n",
    "- Modelo ML entrenado consumiendo features federadas.\n",
    "- Presentación ejecutiva (slides) con resultados y aprendizajes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe39ba",
   "metadata": {},
   "source": [
    "## 8. Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ac420",
   "metadata": {},
   "source": [
    "- Autonomía: ¿cada dominio opera independiente?\n",
    "- Gobernanza: ¿políticas aplicadas consistentemente?\n",
    "- Feature store: ¿features accesibles y versionadas?\n",
    "- Observabilidad: ¿linaje y métricas cross-domain?\n",
    "- Escalabilidad: ¿fácil agregar nuevos dominios?\n",
    "- Costos: ¿optimización por dominio visible?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
