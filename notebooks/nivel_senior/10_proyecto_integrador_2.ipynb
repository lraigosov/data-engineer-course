{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6b9ec7",
   "metadata": {},
   "source": [
    "# ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store\n",
    "\n",
    "Objetivo: diseÃ±ar una arquitectura Data Mesh con mÃºltiples dominios autÃ³nomos, feature store centralizado para ML, y gobernanza federada.\n",
    "\n",
    "- DuraciÃ³n: 180+ min (proyecto multi-dÃ­a)\n",
    "- Dificultad: Muy Alta\n",
    "- Prerrequisitos: Senior completo (01â€“08), experiencia organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb239f",
   "metadata": {},
   "source": [
    "### ğŸŒ **Data Mesh: Paradigm Shift from Centralized to Federated**\n",
    "\n",
    "**1. The Problem with Centralized Data Platforms**\n",
    "\n",
    "```\n",
    "Traditional Monolithic Data Platform:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     Central Data Platform Team             â”‚\n",
    "â”‚  (Bottleneck: 5 engineers, 50 consumers)   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ All ETL pipelines                        â”‚\n",
    "â”‚ â€¢ All data quality checks                  â”‚\n",
    "â”‚ â€¢ All API development                      â”‚\n",
    "â”‚ â€¢ All documentation                        â”‚\n",
    "â”‚ â€¢ All incident response                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼  â–¼  â–¼  â–¼  â–¼\n",
    "    Requests: 200+/month\n",
    "    Lead Time: 6-12 weeks/feature\n",
    "    Quality: One team can't know all domains\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ğŸš« **Bottleneck**: Central team overwhelmed\n",
    "- ğŸš« **Lack of Domain Expertise**: Platform team doesn't understand business nuances\n",
    "- ğŸš« **Slow Innovation**: 3-month wait for new features\n",
    "- ğŸš« **Poor Quality**: Generic validations miss domain-specific issues\n",
    "- ğŸš« **No Ownership**: \"Not my problem\" mentality\n",
    "\n",
    "---\n",
    "\n",
    "**2. Data Mesh: Four Principles (Zhamak Dehghani)**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Data Mesh Principles                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  1. DOMAIN OWNERSHIP                                            â”‚\n",
    "â”‚     \"You build it, you own it\"                                  â”‚\n",
    "â”‚     - Each business domain owns its data products              â”‚\n",
    "â”‚     - Domain teams are accountable for quality & SLOs          â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  2. DATA AS A PRODUCT                                           â”‚\n",
    "â”‚     \"Treat data like software products\"                         â”‚\n",
    "â”‚     - Discoverable (catalog)                                    â”‚\n",
    "â”‚     - Addressable (versioned APIs)                              â”‚\n",
    "â”‚     - Trustworthy (quality SLOs)                                â”‚\n",
    "â”‚     - Self-describing (documentation)                           â”‚\n",
    "â”‚     - Secure (access controls)                                  â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  3. SELF-SERVE DATA PLATFORM                                    â”‚\n",
    "â”‚     \"Democratize infrastructure, not data\"                      â”‚\n",
    "â”‚     - Platform provides: storage, compute, observability       â”‚\n",
    "â”‚     - Domains consume: CI/CD, monitoring, governance tools     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  4. FEDERATED COMPUTATIONAL GOVERNANCE                          â”‚\n",
    "â”‚     \"Global policies, local execution\"                          â”‚\n",
    "â”‚     - Central: Security standards, PII policies, cost budgets  â”‚\n",
    "â”‚     - Local: Implementation details, tooling choices           â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Domain-Oriented Architecture**\n",
    "\n",
    "```python\n",
    "# domains.yml - Domain Registry\n",
    "domains:\n",
    "  \n",
    "  ventas:\n",
    "    owner: sales-team@company.com\n",
    "    mission: \"Provide revenue and transaction data products\"\n",
    "    data_products:\n",
    "      - daily_revenue_api\n",
    "      - customer_transaction_history\n",
    "      - product_performance\n",
    "    slos:\n",
    "      latency_p99: \"15 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "      data_quality: \"99.5% completeness\"\n",
    "    dependencies:\n",
    "      - producto.catalog_api  # Cross-domain dependency\n",
    "    budget_monthly: \"$2,000\"\n",
    "  \n",
    "  logistica:\n",
    "    owner: fulfillment-team@company.com\n",
    "    mission: \"Enable logistics optimization and tracking\"\n",
    "    data_products:\n",
    "      - shipment_tracking_api\n",
    "      - warehouse_inventory\n",
    "      - delivery_performance\n",
    "    slos:\n",
    "      latency_p99: \"5 minutes\"\n",
    "      availability: \"99.95%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$1,500\"\n",
    "  \n",
    "  producto:\n",
    "    owner: catalog-team@company.com\n",
    "    mission: \"Provide product catalog and pricing data\"\n",
    "    data_products:\n",
    "      - catalog_api\n",
    "      - pricing_history\n",
    "      - category_taxonomy\n",
    "    slos:\n",
    "      latency_p99: \"30 minutes\"\n",
    "      availability: \"99.9%\"\n",
    "    dependencies: []\n",
    "    budget_monthly: \"$1,000\"\n",
    "  \n",
    "  marketing:\n",
    "    owner: campaigns-team@company.com\n",
    "    mission: \"Support campaign performance and customer segmentation\"\n",
    "    data_products:\n",
    "      - campaign_metrics\n",
    "      - customer_segments\n",
    "      - attribution_model\n",
    "    slos:\n",
    "      latency_p99: \"60 minutes\"\n",
    "      availability: \"99.5%\"\n",
    "    dependencies:\n",
    "      - ventas.customer_transaction_history\n",
    "      - producto.catalog_api\n",
    "    budget_monthly: \"$2,500\"\n",
    "  \n",
    "  finanzas:\n",
    "    owner: payments-team@company.com\n",
    "    mission: \"Financial reporting and fraud detection\"\n",
    "    data_products:\n",
    "      - payment_transactions\n",
    "      - fraud_scores\n",
    "      - accounting_reports\n",
    "    slos:\n",
    "      latency_p99: \"10 minutes\"\n",
    "      availability: \"99.99%\"\n",
    "    dependencies:\n",
    "      - ventas.daily_revenue_api\n",
    "    budget_monthly: \"$3,000\"\n",
    "    compliance:\n",
    "      - PCI-DSS\n",
    "      - SOX\n",
    "```\n",
    "\n",
    "**Domain Team Structure:**\n",
    "\n",
    "```\n",
    "Ventas Domain Team (6 people):\n",
    "â”œâ”€â”€ Data Product Owner (1)\n",
    "â”‚   - Define requirements\n",
    "â”‚   - Prioritize features\n",
    "â”‚   - Communicate with consumers\n",
    "â”‚\n",
    "â”œâ”€â”€ Data Engineers (3)\n",
    "â”‚   - Build pipelines\n",
    "â”‚   - Maintain data quality\n",
    "â”‚   - Optimize performance\n",
    "â”‚\n",
    "â”œâ”€â”€ Analytics Engineer (1)\n",
    "â”‚   - Create gold layer aggregations\n",
    "â”‚   - Build BI dashboards\n",
    "â”‚\n",
    "â””â”€â”€ SRE/DevOps (1)\n",
    "    - CI/CD pipelines\n",
    "    - Incident response\n",
    "    - Cost optimization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Data Product Canvas**\n",
    "\n",
    "```markdown\n",
    "# Data Product: Daily Revenue API\n",
    "\n",
    "## Business Context\n",
    "**Purpose**: Provide real-time revenue metrics for executive dashboards and financial reporting\n",
    "\n",
    "**Consumers**: \n",
    "- Finance team (accounting reconciliation)\n",
    "- Executive dashboards (Tableau)\n",
    "- ML models (revenue forecasting)\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "### Input Sources\n",
    "- Kafka topic: `ecommerce.ventas.transactions`\n",
    "- S3 batch: `s3://mesh/ventas/raw/refunds/`\n",
    "\n",
    "### Output Schema (v2.1)\n",
    "```json\n",
    "{\n",
    "  \"date\": \"2024-01-15\",\n",
    "  \"region\": \"LATAM\",\n",
    "  \"revenue_gross\": 125000.50,\n",
    "  \"revenue_net\": 118000.30,\n",
    "  \"transactions_count\": 1543,\n",
    "  \"refunds_count\": 23,\n",
    "  \"currency\": \"USD\",\n",
    "  \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "### SLOs\n",
    "- **Latency**: p99 < 15 minutes (from transaction to API availability)\n",
    "- **Availability**: 99.9% uptime (43 minutes downtime/month allowed)\n",
    "- **Accuracy**: Â±0.5% vs accounting system\n",
    "- **Freshness**: Data no older than 20 minutes\n",
    "\n",
    "### API Endpoints\n",
    "- `GET /ventas/v2/daily-revenue?date={YYYY-MM-DD}&region={region}`\n",
    "- `GET /ventas/v2/revenue-trend?start_date={}&end_date={}`\n",
    "\n",
    "### Access Control\n",
    "- Public: No (internal only)\n",
    "- Authentication: API key (service accounts)\n",
    "- Authorization: Read-only for finance, marketing, executive teams\n",
    "\n",
    "### Dependencies\n",
    "- **Upstream**: producto.catalog_api (for product prices)\n",
    "- **Downstream**: ML feature store, executive dashboards\n",
    "\n",
    "### Costs\n",
    "- Storage: $50/month (S3 Standard)\n",
    "- Compute: $200/month (EMR Serverless)\n",
    "- API hosting: $100/month (ECS Fargate)\n",
    "- **Total**: $350/month\n",
    "\n",
    "### Metrics\n",
    "- Request rate: 500 req/day\n",
    "- p99 latency: 12 minutes (within SLO)\n",
    "- Availability (30d): 99.92%\n",
    "- Quality score: 99.7% completeness\n",
    "\n",
    "### Changelog\n",
    "- v2.1 (2024-01): Added `refunds_count` field\n",
    "- v2.0 (2023-12): Breaking change - renamed `total` to `revenue_gross`\n",
    "- v1.5 (2023-10): Added `currency` field\n",
    "- v1.0 (2023-08): Initial release\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Self-Service Platform Components**\n",
    "\n",
    "```python\n",
    "# platform/infrastructure.py\n",
    "\"\"\"\n",
    "Self-service platform capabilities provided to all domains\n",
    "\"\"\"\n",
    "\n",
    "class DataPlatform:\n",
    "    \"\"\"\n",
    "    Shared infrastructure that domains consume\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.components = {\n",
    "            # Storage\n",
    "            \"s3_buckets\": self._provision_s3(),\n",
    "            \"glue_catalog\": self._setup_glue_catalog(),\n",
    "            \n",
    "            # Compute\n",
    "            \"emr_serverless\": self._create_emr_apps(),\n",
    "            \"airflow\": self._deploy_mwaa(),\n",
    "            \n",
    "            # Observability\n",
    "            \"datahub\": self._setup_datahub(),\n",
    "            \"grafana\": self._deploy_grafana(),\n",
    "            \"prometheus\": self._deploy_prometheus(),\n",
    "            \n",
    "            # Governance\n",
    "            \"iam_roles\": self._create_domain_roles(),\n",
    "            \"kms_keys\": self._create_encryption_keys(),\n",
    "            \n",
    "            # CI/CD\n",
    "            \"github_actions\": self._setup_workflows(),\n",
    "            \"terraform_modules\": self._publish_modules()\n",
    "        }\n",
    "    \n",
    "    def provision_domain(self, domain_name: str):\n",
    "        \"\"\"\n",
    "        Self-service: Domain team provisions own infrastructure\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"s3_bucket\": f\"s3://mesh/{domain_name}/\",\n",
    "            \"iam_role\": f\"arn:aws:iam::123:role/{domain_name}-pipeline\",\n",
    "            \"airflow_connection\": f\"{domain_name}_aws\",\n",
    "            \"datahub_domain\": f\"urn:li:domain:{domain_name}\",\n",
    "            \"grafana_folder\": f\"Domains/{domain_name}\",\n",
    "            \"cost_center_tag\": domain_name\n",
    "        }\n",
    "    \n",
    "    def _provision_s3(self):\n",
    "        \"\"\"\n",
    "        Create S3 buckets with standard structure\n",
    "        \"\"\"\n",
    "        bucket_policy = {\n",
    "            \"lifecycle_rules\": [\n",
    "                {\"raw\": \"7 days â†’ Glacier\"},\n",
    "                {\"curated\": \"90 days â†’ IA\"},\n",
    "                {\"gold\": \"retain indefinitely\"}\n",
    "            ],\n",
    "            \"encryption\": \"AWS KMS\",\n",
    "            \"versioning\": True,\n",
    "            \"tags\": {\"ManagedBy\": \"platform-team\"}\n",
    "        }\n",
    "        return bucket_policy\n",
    "    \n",
    "    def _setup_glue_catalog(self):\n",
    "        \"\"\"\n",
    "        Central catalog with domain isolation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"database_naming\": \"{domain}_db\",\n",
    "            \"table_naming\": \"{domain}_{entity}\",\n",
    "            \"cross_domain_access\": \"IAM policies\"\n",
    "        }\n",
    "```\n",
    "\n",
    "**Platform Team Responsibilities:**\n",
    "\n",
    "| Area | Platform Team | Domain Team |\n",
    "|------|---------------|-------------|\n",
    "| **Infrastructure** | Provision & maintain | Consume via self-service |\n",
    "| **Standards** | Define (e.g., PII policy) | Implement |\n",
    "| **Tooling** | Provide (Airflow, DataHub) | Use & extend |\n",
    "| **Costs** | Set budgets | Optimize within budget |\n",
    "| **Security** | Global policies (IAM) | Local access controls |\n",
    "| **Monitoring** | Shared dashboards | Domain-specific alerts |\n",
    "| **Incidents** | Platform outages | Data quality issues |\n",
    "\n",
    "---\n",
    "\n",
    "**6. Benefits vs Challenges**\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "| Benefit | Impact |\n",
    "|---------|--------|\n",
    "| **Faster Innovation** | 12 weeks â†’ 2 weeks (feature delivery) |\n",
    "| **Better Quality** | Domain experts validate data (80% â†’ 95% accuracy) |\n",
    "| **Scalability** | Add domains without bottleneck |\n",
    "| **Accountability** | Clear ownership (no more \"not my problem\") |\n",
    "| **Cost Transparency** | Per-domain budgets (showback/chargeback) |\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "| Challenge | Mitigation |\n",
    "|-----------|-----------|\n",
    "| **Duplication** | Shared platform components, reusable modules |\n",
    "| **Coordination** | Data contracts, OpenAPI specs |\n",
    "| **Discoverability** | DataHub catalog with rich metadata |\n",
    "| **Governance** | Automated policy enforcement (OPA, Cedar) |\n",
    "| **Skills Gap** | Training, guilds, internal wiki |\n",
    "\n",
    "---\n",
    "\n",
    "**7. Migration Strategy: Monolith â†’ Mesh**\n",
    "\n",
    "```\n",
    "Phase 1: Foundation (3 months)\n",
    "â”œâ”€â”€ Week 1-4: Define domains and owners\n",
    "â”œâ”€â”€ Week 5-8: Build self-service platform (Terraform modules)\n",
    "â”œâ”€â”€ Week 9-12: Train domain teams, pilot with 1 domain\n",
    "â””â”€â”€ Success Criteria: 1 domain fully migrated\n",
    "\n",
    "Phase 2: Scale (6 months)\n",
    "â”œâ”€â”€ Month 4-5: Migrate 3 high-value domains\n",
    "â”œâ”€â”€ Month 6-8: Implement federated governance\n",
    "â”œâ”€â”€ Month 9: Sunset legacy central pipelines\n",
    "â””â”€â”€ Success Criteria: 5 domains producing data products\n",
    "\n",
    "Phase 3: Maturity (ongoing)\n",
    "â”œâ”€â”€ Add new domains (self-service onboarding)\n",
    "â”œâ”€â”€ Advanced features (feature store, real-time)\n",
    "â”œâ”€â”€ Cross-domain analytics (federated queries)\n",
    "â””â”€â”€ Success Criteria: <1 week onboarding for new domains\n",
    "```\n",
    "\n",
    "**Pilot Domain Selection Criteria:**\n",
    "- âœ… High business value\n",
    "- âœ… Motivated team (early adopters)\n",
    "- âœ… Clear boundaries (not too many dependencies)\n",
    "- âœ… Medium complexity (not trivial, not impossible)\n",
    "\n",
    "**Example Pilot: Ventas Domain**\n",
    "- **Why**: Critical for business (revenue data)\n",
    "- **Team**: Experienced, eager to improve\n",
    "- **Scope**: Clear (transactions, revenue, customers)\n",
    "- **Timeline**: 3 months â†’ production data product\n",
    "\n",
    "---\n",
    "\n",
    "**8. Real-World Examples**\n",
    "\n",
    "**Netflix:**\n",
    "- 50+ data domains (Content, Playback, Recommendations, etc.)\n",
    "- Each domain owns pipelines, quality, APIs\n",
    "- Central platform: Metacat (catalog), Iceberg (storage), DBT (transformations)\n",
    "- Result: 5,000+ datasets, self-serve access for 2,000+ engineers\n",
    "\n",
    "**Uber:**\n",
    "- Data domains by business unit (Rides, Eats, Freight)\n",
    "- Feature Store (Michelangelo) with domain-owned features\n",
    "- Central governance: Data Quality Portal, Lineage (DataBook)\n",
    "- Result: 10+ PB data lake, <1 week for new data products\n",
    "\n",
    "**Zalando:**\n",
    "- 200+ data products across 40 domains\n",
    "- Self-service: Nakadi (event streaming), Data Lake (S3), dbt (SQL)\n",
    "- Governance: Data Mesh Portal (discovery), Compliance Scanner\n",
    "- Result: 90% of data teams self-sufficient\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33626",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Feature Store: ML-Ready Data Infrastructure**\n",
    "\n",
    "**1. The Feature Engineering Problem**\n",
    "\n",
    "```\n",
    "Traditional ML Pipeline (Problems):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Data Scientist A                              â”‚\n",
    "â”‚  â”œâ”€â”€ Extracts: customer_total_purchases        â”‚\n",
    "â”‚  â”œâ”€â”€ Logic: SQL query (30 days rolling)       â”‚\n",
    "â”‚  â”œâ”€â”€ Storage: Local CSV                        â”‚\n",
    "â”‚  â””â”€â”€ Model: Churn prediction (prod)            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â†“ (6 months later)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Data Scientist B                              â”‚\n",
    "â”‚  â”œâ”€â”€ Extracts: customer_total_purchases (again)â”‚\n",
    "â”‚  â”œâ”€â”€ Logic: Slightly different SQL            â”‚\n",
    "â”‚  â”œâ”€â”€ Storage: Different S3 path               â”‚\n",
    "â”‚  â””â”€â”€ Model: Upsell prediction                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Issues:\n",
    "âŒ Duplication: Same feature, different implementations\n",
    "âŒ Inconsistency: Different logic â†’ different values\n",
    "âŒ Training/Serving Skew: Batch SQL vs real-time Python\n",
    "âŒ No Versioning: Can't reproduce model from 6 months ago\n",
    "âŒ No Discovery: Team B doesn't know Team A computed this\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Feature Store Architecture**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       Feature Store                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚  Offline Store   â”‚         â”‚  Online Store    â”‚            â”‚\n",
    "â”‚  â”‚  (S3/BigQuery)   â”‚         â”‚  (Redis/DynamoDB)â”‚            â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚\n",
    "â”‚  â”‚ â€¢ Training data  â”‚         â”‚ â€¢ Serving (ms)   â”‚            â”‚\n",
    "â”‚  â”‚ â€¢ Batch (hours)  â”‚         â”‚ â€¢ Low latency    â”‚            â”‚\n",
    "â”‚  â”‚ â€¢ Historical     â”‚         â”‚ â€¢ Recent data    â”‚            â”‚\n",
    "â”‚  â”‚ â€¢ Petabytes      â”‚         â”‚ â€¢ Gigabytes      â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚           â–²                            â–²                        â”‚\n",
    "â”‚           â”‚                            â”‚                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚         Feature Registry (Metadata)            â”‚            â”‚\n",
    "â”‚  â”‚  - Feature definitions                         â”‚            â”‚\n",
    "â”‚  â”‚  - Schema & types                              â”‚            â”‚\n",
    "â”‚  â”‚  - Owners & documentation                      â”‚            â”‚\n",
    "â”‚  â”‚  - Lineage & dependencies                      â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–²                                     â–²\n",
    "         â”‚                                     â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Training â”‚                      â”‚  Serving   â”‚\n",
    "    â”‚  (batch)  â”‚                      â”‚ (real-time)â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Feast Implementation (Open Source)**\n",
    "\n",
    "**Installation & Setup:**\n",
    "\n",
    "```python\n",
    "# Install Feast\n",
    "!pip install feast[aws,redis]\n",
    "\n",
    "# Initialize repository\n",
    "!feast init feature_repo\n",
    "cd feature_repo/\n",
    "```\n",
    "\n",
    "**Feature Definitions:**\n",
    "\n",
    "```python\n",
    "# feature_repo/features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource, RedisSource\n",
    "from feast.types import Float64, Int64, String\n",
    "from datetime import timedelta\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ENTITIES (Join Keys)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "customer = Entity(\n",
    "    name=\"customer_id\",\n",
    "    description=\"Unique customer identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "product = Entity(\n",
    "    name=\"product_id\",\n",
    "    description=\"Unique product identifier\",\n",
    "    value_type=Int64\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VENTAS DOMAIN FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ventas_offline_source = FileSource(\n",
    "    name=\"ventas_features_source\",\n",
    "    path=\"s3://mesh/ventas/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_online_source = RedisSource(\n",
    "    name=\"ventas_online\",\n",
    "    table=\"ventas_features\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "ventas_customer_features = FeatureView(\n",
    "    name=\"ventas_customer_features\",\n",
    "    description=\"Customer purchase behavior (Ventas domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=90),  # Feature validity\n",
    "    schema=[\n",
    "        Field(\n",
    "            name=\"total_purchases_7d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"total_purchases_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Total $ spent in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"transaction_count_7d\",\n",
    "            dtype=Int64,\n",
    "            description=\"Number of transactions in last 7 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"avg_basket_size_30d\",\n",
    "            dtype=Float64,\n",
    "            description=\"Average basket size in last 30 days\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"days_since_last_purchase\",\n",
    "            dtype=Int64,\n",
    "            description=\"Days since most recent purchase\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"preferred_payment_method\",\n",
    "            dtype=String,\n",
    "            description=\"Most used payment method (last 90 days)\"\n",
    "        ),\n",
    "        Field(\n",
    "            name=\"is_premium_customer\",\n",
    "            dtype=Int64,\n",
    "            description=\"1 if customer spent >$5000 in last year\"\n",
    "        )\n",
    "    ],\n",
    "    source=ventas_offline_source,\n",
    "    online=True,  # Enable online serving\n",
    "    owner=\"ventas-team@company.com\",\n",
    "    tags={\"domain\": \"ventas\", \"pii\": \"no\"}\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PRODUCTO DOMAIN FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "producto_offline_source = FileSource(\n",
    "    name=\"producto_features_source\",\n",
    "    path=\"s3://mesh/producto/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "producto_features = FeatureView(\n",
    "    name=\"producto_features\",\n",
    "    description=\"Product catalog and pricing (Producto domain)\",\n",
    "    entities=[product],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name=\"current_price\", dtype=Float64),\n",
    "        Field(name=\"category\", dtype=String),\n",
    "        Field(name=\"stock_level\", dtype=Int64),\n",
    "        Field(name=\"days_since_launch\", dtype=Int64),\n",
    "        Field(name=\"avg_rating\", dtype=Float64),\n",
    "        Field(name=\"total_reviews\", dtype=Int64)\n",
    "    ],\n",
    "    source=producto_offline_source,\n",
    "    online=True,\n",
    "    owner=\"catalog-team@company.com\",\n",
    "    tags={\"domain\": \"producto\"}\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LOGISTICA DOMAIN FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "logistica_offline_source = FileSource(\n",
    "    name=\"logistica_features_source\",\n",
    "    path=\"s3://mesh/logistica/curated/features/\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "logistica_features = FeatureView(\n",
    "    name=\"logistica_features\",\n",
    "    description=\"Delivery performance (Logistica domain)\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=60),\n",
    "    schema=[\n",
    "        Field(name=\"avg_delivery_time_days\", dtype=Float64),\n",
    "        Field(name=\"on_time_delivery_rate\", dtype=Float64),\n",
    "        Field(name=\"total_shipments_30d\", dtype=Int64),\n",
    "        Field(name=\"return_rate_30d\", dtype=Float64)\n",
    "    ],\n",
    "    source=logistica_offline_source,\n",
    "    online=True,\n",
    "    owner=\"fulfillment-team@company.com\",\n",
    "    tags={\"domain\": \"logistica\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Feature Registry Configuration:**\n",
    "\n",
    "```yaml\n",
    "# feature_store.yaml\n",
    "project: ecommerce_mesh\n",
    "registry: s3://mesh/feast/registry.db\n",
    "provider: aws\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: \"redis.ecommerce.internal:6379\"\n",
    "  \n",
    "offline_store:\n",
    "  type: file  # or 'snowflake', 'bigquery', 'redshift'\n",
    "  \n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "**Apply Changes:**\n",
    "\n",
    "```bash\n",
    "# Deploy features to registry\n",
    "feast apply\n",
    "\n",
    "# Output:\n",
    "# âœ… Created entity customer_id\n",
    "# âœ… Created entity product_id\n",
    "# âœ… Created feature view ventas_customer_features\n",
    "# âœ… Created feature view producto_features\n",
    "# âœ… Created feature view logistica_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Feature Materialization (Offline â†’ Online)**\n",
    "\n",
    "```python\n",
    "# materialize_features.py\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Materialize last 7 days to online store (Redis)\n",
    "store.materialize_incremental(\n",
    "    end_date=datetime.utcnow()\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# Materializing 1 feature views from 2024-01-08 to 2024-01-15\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:23\n",
    "# âœ… ventas_customer_features: 1.2M rows materialized to Redis\n",
    "# âœ… producto_features: 50K rows materialized\n",
    "# âœ… logistica_features: 800K rows materialized\n",
    "```\n",
    "\n",
    "**Airflow DAG for Incremental Materialization:**\n",
    "\n",
    "```python\n",
    "# dags/feast_materialization.py\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(\n",
    "    schedule=\"0 */4 * * *\",  # Every 4 hours\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"feast\", \"feature-store\"]\n",
    ")\n",
    "def feast_materialization():\n",
    "    \n",
    "    @task\n",
    "    def materialize_features():\n",
    "        from feast import FeatureStore\n",
    "        \n",
    "        store = FeatureStore(repo_path=\"/opt/airflow/feature_repo/\")\n",
    "        \n",
    "        # Materialize incremental\n",
    "        store.materialize_incremental(end_date=datetime.utcnow())\n",
    "        \n",
    "        print(\"âœ… Features materialized to online store\")\n",
    "    \n",
    "    materialize_features()\n",
    "\n",
    "dag = feast_materialization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Training: Historical Features (Offline Store)**\n",
    "\n",
    "```python\n",
    "# ml/train_churn_model.py\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Training dataset: customers who churned in last 90 days\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [1001, 1002, 1003, ...],\n",
    "    \"event_timestamp\": [\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 2),\n",
    "        datetime(2024, 1, 3),\n",
    "        ...\n",
    "    ],\n",
    "    \"churned\": [1, 0, 1, ...]  # Label\n",
    "})\n",
    "\n",
    "# Get historical features (point-in-time correct)\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"ventas_customer_features:total_purchases_30d\",\n",
    "        \"ventas_customer_features:transaction_count_7d\",\n",
    "        \"ventas_customer_features:avg_basket_size_30d\",\n",
    "        \"ventas_customer_features:days_since_last_purchase\",\n",
    "        \"logistica_features:on_time_delivery_rate\",\n",
    "        \"logistica_features:return_rate_30d\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "# customer_id  event_timestamp  total_purchases_30d  transaction_count_7d  ...  churned\n",
    "# 1001         2024-01-01       542.30               3                     ...  1\n",
    "# 1002         2024-01-02       1250.80              8                     ...  0\n",
    "\n",
    "# Train model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = training_df.drop(columns=[\"customer_id\", \"event_timestamp\", \"churned\"])\n",
    "y = training_df[\"churned\"]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, \"churn_model_v1.pkl\")\n",
    "```\n",
    "\n",
    "**Point-in-Time Correctness:**\n",
    "\n",
    "```python\n",
    "# âš ï¸ Problem without feature store: Data leakage\n",
    "# If we compute features at training time using current data,\n",
    "# we're using information from the future!\n",
    "\n",
    "# Example: Customer churned on 2024-01-15\n",
    "# Feature: total_purchases_30d on 2024-01-15\n",
    "# âŒ Wrong: Using purchases from 2024-01-15 to 2024-02-14 (includes future)\n",
    "# âœ… Correct: Using purchases from 2023-12-16 to 2024-01-15 (only past)\n",
    "\n",
    "# Feast handles this automatically with event_timestamp\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Serving: Real-Time Features (Online Store)**\n",
    "\n",
    "```python\n",
    "# api/prediction_service.py\n",
    "from fastapi import FastAPI\n",
    "from feast import FeatureStore\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Load model and feature store\n",
    "model = joblib.load(\"churn_model_v1.pkl\")\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    customer_id: int\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    customer_id: int\n",
    "    churn_probability: float\n",
    "    risk_level: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict_churn(request: PredictionRequest):\n",
    "    # Get features from online store (Redis) - sub-millisecond latency\n",
    "    features_dict = store.get_online_features(\n",
    "        features=[\n",
    "            \"ventas_customer_features:total_purchases_30d\",\n",
    "            \"ventas_customer_features:transaction_count_7d\",\n",
    "            \"ventas_customer_features:avg_basket_size_30d\",\n",
    "            \"ventas_customer_features:days_since_last_purchase\",\n",
    "            \"logistica_features:on_time_delivery_rate\",\n",
    "            \"logistica_features:return_rate_30d\"\n",
    "        ],\n",
    "        entity_rows=[{\"customer_id\": request.customer_id}]\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Convert to DataFrame for model\n",
    "    import pandas as pd\n",
    "    features_df = pd.DataFrame(features_dict)\n",
    "    \n",
    "    # Predict\n",
    "    churn_prob = model.predict_proba(features_df)[0][1]\n",
    "    \n",
    "    # Classify risk\n",
    "    if churn_prob > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "    elif churn_prob > 0.4:\n",
    "        risk_level = \"MEDIUM\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        customer_id=request.customer_id,\n",
    "        churn_probability=churn_prob,\n",
    "        risk_level=risk_level\n",
    "    )\n",
    "\n",
    "# Latency: ~10ms (Redis lookup + model inference)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Feature Engineering Pipeline**\n",
    "\n",
    "```python\n",
    "# pipelines/compute_ventas_features.py\n",
    "\"\"\"\n",
    "Airflow DAG: Compute Ventas domain features daily\n",
    "\"\"\"\n",
    "from airflow.decorators import dag, task\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(schedule=\"@daily\", start_date=datetime(2024, 1, 1))\n",
    "def compute_ventas_features():\n",
    "    \n",
    "    @task\n",
    "    def compute_features(ds):\n",
    "        spark = SparkSession.builder.appName(\"VentasFeatures\").getOrCreate()\n",
    "        \n",
    "        # Read transactions (last 90 days)\n",
    "        transactions = spark.read.format(\"delta\").load(\"s3://mesh/ventas/curated/transactions/\")\n",
    "        transactions = transactions.filter(col(\"transaction_date\") >= date_sub(lit(ds), 90))\n",
    "        \n",
    "        # Compute features\n",
    "        features = transactions.groupBy(\"customer_id\").agg(\n",
    "            # 7-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_7d\"),\n",
    "            count(when(col(\"transaction_date\") >= date_sub(lit(ds), 7), 1))\n",
    "                .alias(\"transaction_count_7d\"),\n",
    "            \n",
    "            # 30-day window\n",
    "            sum(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"amount\")).otherwise(0))\n",
    "                .alias(\"total_purchases_30d\"),\n",
    "            avg(when(col(\"transaction_date\") >= date_sub(lit(ds), 30), col(\"basket_size\")))\n",
    "                .alias(\"avg_basket_size_30d\"),\n",
    "            \n",
    "            # Recency\n",
    "            datediff(lit(ds), max(col(\"transaction_date\")))\n",
    "                .alias(\"days_since_last_purchase\"),\n",
    "            \n",
    "            # Most common payment method (last 90 days)\n",
    "            first(col(\"payment_method\"))  # After groupBy + window\n",
    "                .alias(\"preferred_payment_method\")\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        features = features \\\n",
    "            .withColumn(\"event_timestamp\", lit(ds).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"created_timestamp\", current_timestamp())\n",
    "        \n",
    "        # Write to feature store offline path (Feast reads this)\n",
    "        features.write \\\n",
    "            .format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"event_timestamp\") \\\n",
    "            .save(\"s3://mesh/ventas/curated/features/\")\n",
    "        \n",
    "        print(f\"âœ… Computed features for {features.count()} customers\")\n",
    "    \n",
    "    compute_features()\n",
    "\n",
    "dag = compute_ventas_features()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Feature Versioning and Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/feature_quality.py\n",
    "from feast import FeatureStore\n",
    "import great_expectations as gx\n",
    "\n",
    "store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "\n",
    "# Get recent features\n",
    "features_df = store.get_historical_features(\n",
    "    entity_df=...,\n",
    "    features=[\"ventas_customer_features:total_purchases_30d\"]\n",
    ").to_df()\n",
    "\n",
    "# Validate with Great Expectations\n",
    "context = gx.get_context()\n",
    "\n",
    "suite = context.add_expectation_suite(\"ventas_features_quality\")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_be_between\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\", \"min_value\": 0, \"max_value\": 100000}\n",
    ")\n",
    "suite.add_expectation(\n",
    "    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "    kwargs={\"column\": \"total_purchases_30d\"}\n",
    ")\n",
    "\n",
    "results = context.get_validator(batch_request=...).validate()\n",
    "\n",
    "if not results.success:\n",
    "    # Alert: Feature quality degraded\n",
    "    send_slack_alert(\"Feature quality issue in ventas_customer_features\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**9. Tecton vs Feast Comparison**\n",
    "\n",
    "| Feature | Feast (Open Source) | Tecton (Commercial) |\n",
    "|---------|---------------------|---------------------|\n",
    "| **Cost** | Free | $$$$ (enterprise pricing) |\n",
    "| **Deployment** | Self-managed | Fully managed SaaS |\n",
    "| **Online Store** | Redis, DynamoDB | Managed Redis + optimizations |\n",
    "| **Offline Store** | S3, BigQuery, Snowflake | Snowflake, Databricks |\n",
    "| **Real-time** | Streaming via custom code | Native streaming (Flink) |\n",
    "| **Feature Engineering** | External (Spark, dbt) | Declarative transformations |\n",
    "| **Monitoring** | Custom (GE, Prometheus) | Built-in (drift, quality) |\n",
    "| **Lineage** | Manual (DataHub) | Automatic |\n",
    "| **Support** | Community | Enterprise SLA |\n",
    "\n",
    "**Decision Matrix:**\n",
    "- **Feast**: Startups, cost-sensitive, engineering resources available\n",
    "- **Tecton**: Enterprises, need SLA, limited ML engineering team\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c5b8d",
   "metadata": {},
   "source": [
    "### ğŸ“œ **Data Products as APIs: Contracts, Versioning & SLOs**\n",
    "\n",
    "**1. Data Contract: API-First Design**\n",
    "\n",
    "```yaml\n",
    "# contracts/ventas-daily-revenue-api-v2.yaml\n",
    "openapi: 3.0.3\n",
    "info:\n",
    "  title: Daily Revenue API\n",
    "  version: 2.1.0\n",
    "  description: |\n",
    "    Provides daily revenue metrics aggregated by region.\n",
    "    \n",
    "    **Owner**: ventas-team@company.com\n",
    "    **SLO**: 99.9% availability, p99 latency <15 minutes\n",
    "    **Changelog**: \n",
    "      - v2.1.0 (2024-01): Added refunds_count field\n",
    "      - v2.0.0 (2023-12): Breaking - renamed 'total' to 'revenue_gross'\n",
    "  \n",
    "  contact:\n",
    "    name: Ventas Data Team\n",
    "    email: ventas-team@company.com\n",
    "    url: https://wiki.company.com/data-products/ventas-revenue\n",
    "  \n",
    "  x-slo:\n",
    "    availability: 99.9%\n",
    "    latency_p99: 900  # seconds (15 min)\n",
    "    freshness_max: 1200  # seconds (20 min)\n",
    "  \n",
    "  x-domain: ventas\n",
    "  x-cost-center: sales\n",
    "  x-tier: gold  # bronze/silver/gold\n",
    "\n",
    "servers:\n",
    "  - url: https://api.company.com/ventas/v2\n",
    "    description: Production\n",
    "  - url: https://api-staging.company.com/ventas/v2\n",
    "    description: Staging\n",
    "\n",
    "paths:\n",
    "  /daily-revenue:\n",
    "    get:\n",
    "      summary: Get daily revenue by region\n",
    "      operationId: getDailyRevenue\n",
    "      tags:\n",
    "        - Revenue\n",
    "      \n",
    "      parameters:\n",
    "        - name: date\n",
    "          in: query\n",
    "          required: true\n",
    "          schema:\n",
    "            type: string\n",
    "            format: date\n",
    "            example: \"2024-01-15\"\n",
    "          description: Revenue date (YYYY-MM-DD)\n",
    "        \n",
    "        - name: region\n",
    "          in: query\n",
    "          required: false\n",
    "          schema:\n",
    "            type: string\n",
    "            enum: [LATAM, NA, EU, APAC, ALL]\n",
    "            default: ALL\n",
    "          description: Filter by region\n",
    "      \n",
    "      responses:\n",
    "        '200':\n",
    "          description: Successful response\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/DailyRevenueResponse'\n",
    "              examples:\n",
    "                single_region:\n",
    "                  summary: Single region\n",
    "                  value:\n",
    "                    date: \"2024-01-15\"\n",
    "                    region: \"LATAM\"\n",
    "                    revenue_gross: 125000.50\n",
    "                    revenue_net: 118000.30\n",
    "                    transactions_count: 1543\n",
    "                    refunds_count: 23\n",
    "                    currency: \"USD\"\n",
    "                    calculated_at: \"2024-01-15T10:30:00Z\"\n",
    "        \n",
    "        '400':\n",
    "          description: Invalid date format\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                $ref: '#/components/schemas/Error'\n",
    "        \n",
    "        '404':\n",
    "          description: No data for requested date\n",
    "        \n",
    "        '503':\n",
    "          description: Service temporarily unavailable (SLO violation)\n",
    "      \n",
    "      security:\n",
    "        - ApiKeyAuth: []\n",
    "      \n",
    "      x-rate-limit:\n",
    "        requests_per_minute: 100\n",
    "        requests_per_hour: 5000\n",
    "\n",
    "components:\n",
    "  schemas:\n",
    "    DailyRevenueResponse:\n",
    "      type: object\n",
    "      required:\n",
    "        - date\n",
    "        - region\n",
    "        - revenue_gross\n",
    "        - revenue_net\n",
    "        - transactions_count\n",
    "        - currency\n",
    "        - calculated_at\n",
    "      properties:\n",
    "        date:\n",
    "          type: string\n",
    "          format: date\n",
    "          description: Revenue date\n",
    "        region:\n",
    "          type: string\n",
    "          enum: [LATAM, NA, EU, APAC, ALL]\n",
    "          description: Geographic region\n",
    "        revenue_gross:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Total revenue before refunds/discounts\n",
    "          example: 125000.50\n",
    "        revenue_net:\n",
    "          type: number\n",
    "          format: double\n",
    "          minimum: 0\n",
    "          description: Net revenue after refunds/discounts\n",
    "          example: 118000.30\n",
    "        transactions_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of transactions\n",
    "          example: 1543\n",
    "        refunds_count:\n",
    "          type: integer\n",
    "          minimum: 0\n",
    "          description: Number of refunded transactions (added v2.1)\n",
    "          example: 23\n",
    "        currency:\n",
    "          type: string\n",
    "          enum: [USD, EUR, BRL, MXN]\n",
    "          description: Currency code\n",
    "          example: \"USD\"\n",
    "        calculated_at:\n",
    "          type: string\n",
    "          format: date-time\n",
    "          description: Timestamp when metrics were calculated\n",
    "          example: \"2024-01-15T10:30:00Z\"\n",
    "        metadata:\n",
    "          type: object\n",
    "          properties:\n",
    "            data_quality_score:\n",
    "              type: number\n",
    "              description: Quality score (0-1)\n",
    "              example: 0.997\n",
    "            source_systems:\n",
    "              type: array\n",
    "              items:\n",
    "                type: string\n",
    "              example: [\"kafka_transactions\", \"sftp_refunds\"]\n",
    "    \n",
    "    Error:\n",
    "      type: object\n",
    "      required:\n",
    "        - error_code\n",
    "        - message\n",
    "      properties:\n",
    "        error_code:\n",
    "          type: string\n",
    "          example: \"INVALID_DATE_FORMAT\"\n",
    "        message:\n",
    "          type: string\n",
    "          example: \"Date must be in YYYY-MM-DD format\"\n",
    "        details:\n",
    "          type: object\n",
    "  \n",
    "  securitySchemes:\n",
    "    ApiKeyAuth:\n",
    "      type: apiKey\n",
    "      in: header\n",
    "      name: X-API-Key\n",
    "      description: Service account API key\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Schema Evolution: Backward Compatibility**\n",
    "\n",
    "```python\n",
    "# schema_evolution.py\n",
    "\"\"\"\n",
    "Rules for backward-compatible schema changes\n",
    "\"\"\"\n",
    "\n",
    "# âœ… SAFE (Backward Compatible):\n",
    "# 1. Add optional field\n",
    "{\n",
    "    \"date\": \"2024-01-15\",\n",
    "    \"revenue_gross\": 125000.50,\n",
    "    \"refunds_count\": 23  # NEW OPTIONAL FIELD (v2.1)\n",
    "}\n",
    "\n",
    "# 2. Add new enum value\n",
    "region: [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\", \"AFRICA\"]  # NEW VALUE\n",
    "\n",
    "# 3. Widen validation (less restrictive)\n",
    "# Old: revenue_gross: minimum 100\n",
    "# New: revenue_gross: minimum 0\n",
    "\n",
    "# âŒ BREAKING (Not Backward Compatible):\n",
    "# 1. Remove field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {}  # 'total' removed\n",
    "\n",
    "# 2. Rename field\n",
    "# Old: {\"total\": 125000.50}\n",
    "# New: {\"revenue_gross\": 125000.50}  # 'total' renamed\n",
    "\n",
    "# 3. Change type\n",
    "# Old: {\"transactions_count\": 1543}  # integer\n",
    "# New: {\"transactions_count\": \"1543\"}  # string\n",
    "\n",
    "# 4. Make field required\n",
    "# Old: {\"date\": \"2024-01-15\"}  # refunds_count optional\n",
    "# New: {\"date\": \"2024-01-15\", \"refunds_count\": 23}  # required\n",
    "\n",
    "# 5. Narrow validation (more restrictive)\n",
    "# Old: revenue_gross: minimum 0\n",
    "# New: revenue_gross: minimum 100\n",
    "```\n",
    "\n",
    "**Breaking Change Protocol:**\n",
    "\n",
    "```markdown\n",
    "# Breaking Change Checklist\n",
    "\n",
    "## Before Implementation\n",
    "- [ ] Document breaking change in CHANGELOG\n",
    "- [ ] Notify consumers 30 days in advance (email, Slack)\n",
    "- [ ] Update API version (v2 â†’ v3)\n",
    "- [ ] Maintain old version for deprecation period (6 months)\n",
    "\n",
    "## Implementation\n",
    "- [ ] Deploy new version (v3) alongside old (v2)\n",
    "- [ ] Monitor usage of old version\n",
    "- [ ] Provide migration guide with examples\n",
    "\n",
    "## Deprecation\n",
    "- [ ] Mark old version as deprecated (HTTP header: `Deprecation: true`)\n",
    "- [ ] Return warning in responses (Sunset header)\n",
    "- [ ] Send reminder emails at 90, 60, 30, 7 days before sunset\n",
    "- [ ] Sunset date: Remove old version after 6 months\n",
    "\n",
    "## Example Response Headers:\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Deprecation: true\n",
    "Sunset: Sat, 15 Jul 2024 00:00:00 GMT\n",
    "Link: <https://api.company.com/ventas/v3/daily-revenue>; rel=\"successor-version\"\n",
    "Warning: 299 - \"This API version will be retired on 2024-07-15. Migrate to v3\"\n",
    "```\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. API Implementation with FastAPI**\n",
    "\n",
    "```python\n",
    "# api/ventas_api.py\n",
    "from fastapi import FastAPI, Query, HTTPException, Header, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import date, datetime\n",
    "from typing import Optional, Literal\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Daily Revenue API\",\n",
    "    version=\"2.1.0\",\n",
    "    description=\"Ventas domain data product\",\n",
    "    openapi_tags=[\n",
    "        {\"name\": \"Revenue\", \"description\": \"Revenue metrics operations\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prometheus metrics\n",
    "request_count = Counter('api_requests_total', 'Total requests', ['endpoint', 'status'])\n",
    "request_duration = Histogram('api_request_duration_seconds', 'Request duration', ['endpoint'])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODELS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class DailyRevenueResponse(BaseModel):\n",
    "    date: date\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "    revenue_gross: float = Field(..., ge=0, description=\"Revenue before refunds\")\n",
    "    revenue_net: float = Field(..., ge=0, description=\"Net revenue\")\n",
    "    transactions_count: int = Field(..., ge=0)\n",
    "    refunds_count: int = Field(default=0, ge=0, description=\"New in v2.1\")\n",
    "    currency: Literal[\"USD\", \"EUR\", \"BRL\", \"MXN\"]\n",
    "    calculated_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"date\": \"2024-01-15\",\n",
    "                \"region\": \"LATAM\",\n",
    "                \"revenue_gross\": 125000.50,\n",
    "                \"revenue_net\": 118000.30,\n",
    "                \"transactions_count\": 1543,\n",
    "                \"refunds_count\": 23,\n",
    "                \"currency\": \"USD\",\n",
    "                \"calculated_at\": \"2024-01-15T10:30:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DEPENDENCIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def verify_api_key(x_api_key: str = Header(...)):\n",
    "    \"\"\"Verify API key from header\"\"\"\n",
    "    # In production: check against Secrets Manager\n",
    "    valid_keys = [\"dev-key-123\", \"prod-key-456\"]\n",
    "    if x_api_key not in valid_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "    return x_api_key\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Dependency: S3 client\"\"\"\n",
    "    return boto3.client('s3', region_name='us-east-1')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ENDPOINTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "@app.get(\n",
    "    \"/daily-revenue\",\n",
    "    response_model=DailyRevenueResponse,\n",
    "    tags=[\"Revenue\"],\n",
    "    summary=\"Get daily revenue by region\",\n",
    "    responses={\n",
    "        200: {\"description\": \"Successful response\"},\n",
    "        400: {\"description\": \"Invalid date format\"},\n",
    "        404: {\"description\": \"No data for requested date\"},\n",
    "        503: {\"description\": \"Service unavailable (SLO violation)\"}\n",
    "    }\n",
    ")\n",
    "async def get_daily_revenue(\n",
    "    date: date = Query(..., description=\"Revenue date (YYYY-MM-DD)\"),\n",
    "    region: Literal[\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"] = Query(\"ALL\"),\n",
    "    api_key: str = Depends(verify_api_key),\n",
    "    s3: boto3.client = Depends(get_s3_client)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve daily revenue metrics for a specific date and region.\n",
    "    \n",
    "    **Data Freshness**: Updated every 15 minutes\n",
    "    **SLO**: p99 latency <15 minutes from transaction to API\n",
    "    \"\"\"\n",
    "    with request_duration.labels(endpoint=\"/daily-revenue\").time():\n",
    "        try:\n",
    "            # Query data from S3 (or cache)\n",
    "            s3_key = f\"gold/revenue/daily/dt={date}/region={region}.parquet\"\n",
    "            \n",
    "            response = s3.get_object(\n",
    "                Bucket='mesh',\n",
    "                Key=s3_key\n",
    "            )\n",
    "            \n",
    "            # Parse Parquet (simplified)\n",
    "            import pandas as pd\n",
    "            df = pd.read_parquet(response['Body'])\n",
    "            \n",
    "            if df.empty:\n",
    "                request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "                raise HTTPException(status_code=404, detail=\"No data for requested date\")\n",
    "            \n",
    "            # Convert to response model\n",
    "            row = df.iloc[0]\n",
    "            result = DailyRevenueResponse(\n",
    "                date=date,\n",
    "                region=region,\n",
    "                revenue_gross=float(row['revenue_gross']),\n",
    "                revenue_net=float(row['revenue_net']),\n",
    "                transactions_count=int(row['transactions_count']),\n",
    "                refunds_count=int(row.get('refunds_count', 0)),  # Default for old data\n",
    "                currency=row['currency'],\n",
    "                calculated_at=row['calculated_at']\n",
    "            )\n",
    "            \n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=200).inc()\n",
    "            return result\n",
    "            \n",
    "        except s3.exceptions.NoSuchKey:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=404).inc()\n",
    "            raise HTTPException(status_code=404, detail=f\"No data for {date}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            request_count.labels(endpoint=\"/daily-revenue\", status=500).inc()\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for load balancer\"\"\"\n",
    "    return {\"status\": \"healthy\", \"version\": \"2.1.0\"}\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    from fastapi import Response\n",
    "    \n",
    "    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. SLO Monitoring**\n",
    "\n",
    "```python\n",
    "# monitoring/slo_monitor.py\n",
    "\"\"\"\n",
    "Monitor SLOs for data products\n",
    "\"\"\"\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# SLO metrics\n",
    "slo_availability = Gauge('data_product_availability', 'Availability %', ['product', 'domain'])\n",
    "slo_latency_p99 = Gauge('data_product_latency_p99_seconds', 'p99 latency', ['product', 'domain'])\n",
    "slo_freshness = Gauge('data_product_freshness_seconds', 'Data age', ['product', 'domain'])\n",
    "\n",
    "def check_availability(api_url: str):\n",
    "    \"\"\"Check if API is responding (availability SLO)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{api_url}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_freshness(api_url: str):\n",
    "    \"\"\"Check data freshness (how old is the data?)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{api_url}/daily-revenue\",\n",
    "            params={\"date\": datetime.utcnow().date()},\n",
    "            headers={\"X-API-Key\": \"monitoring-key\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            calculated_at = datetime.fromisoformat(data['calculated_at'].replace('Z', '+00:00'))\n",
    "            age_seconds = (datetime.utcnow() - calculated_at).total_seconds()\n",
    "            return age_seconds\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def monitor_slos():\n",
    "    \"\"\"Continuous SLO monitoring\"\"\"\n",
    "    data_products = [\n",
    "        {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"domain\": \"ventas\",\n",
    "            \"url\": \"https://api.company.com/ventas/v2\",\n",
    "            \"slo_availability\": 0.999,  # 99.9%\n",
    "            \"slo_latency_p99\": 900,  # 15 min\n",
    "            \"slo_freshness\": 1200  # 20 min\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"shipment_tracking_api\",\n",
    "            \"domain\": \"logistica\",\n",
    "            \"url\": \"https://api.company.com/logistica/v1\",\n",
    "            \"slo_availability\": 0.9995,  # 99.95%\n",
    "            \"slo_latency_p99\": 300,  # 5 min\n",
    "            \"slo_freshness\": 600  # 10 min\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        for product in data_products:\n",
    "            # Availability\n",
    "            is_available = check_availability(product['url'])\n",
    "            availability_pct = 1.0 if is_available else 0.0\n",
    "            slo_availability.labels(\n",
    "                product=product['name'],\n",
    "                domain=product['domain']\n",
    "            ).set(availability_pct)\n",
    "            \n",
    "            # Freshness\n",
    "            freshness = check_freshness(product['url'])\n",
    "            if freshness:\n",
    "                slo_freshness.labels(\n",
    "                    product=product['name'],\n",
    "                    domain=product['domain']\n",
    "                ).set(freshness)\n",
    "                \n",
    "                # Alert if SLO violated\n",
    "                if freshness > product['slo_freshness']:\n",
    "                    send_alert(\n",
    "                        f\"ğŸš¨ Freshness SLO violated for {product['name']}: \"\n",
    "                        f\"{freshness}s > {product['slo_freshness']}s\"\n",
    "                    )\n",
    "        \n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_http_server(8001)\n",
    "    monitor_slos()\n",
    "```\n",
    "\n",
    "**Prometheus Alert Rules:**\n",
    "\n",
    "```yaml\n",
    "# alerts/data-product-slos.yml\n",
    "groups:\n",
    "  - name: data_product_slos\n",
    "    interval: 1m\n",
    "    rules:\n",
    "      \n",
    "      - alert: DataProductAvailabilitySLOViolation\n",
    "        expr: |\n",
    "          (\n",
    "            sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d])\n",
    "          ) < 0.999\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} availability below SLO\"\n",
    "          description: \"Availability: {{ $value | humanizePercentage }} (target: 99.9%)\"\n",
    "          dashboard: \"https://grafana.company.com/d/slo?product={{ $labels.product }}\"\n",
    "      \n",
    "      - alert: DataProductFreshnessSLOViolation\n",
    "        expr: |\n",
    "          data_product_freshness_seconds > 1200\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} data is stale\"\n",
    "          description: \"Data age: {{ $value }}s (target: <1200s)\"\n",
    "      \n",
    "      - alert: DataProductErrorBudgetExhausted\n",
    "        expr: |\n",
    "          (1 - \n",
    "            (sum_over_time(data_product_availability[30d]) \n",
    "            / \n",
    "            count_over_time(data_product_availability[30d]))\n",
    "          ) > 0.001  # 99.9% SLO = 0.1% error budget\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: \"{{ $labels.domain }}\"\n",
    "        annotations:\n",
    "          summary: \"{{ $labels.product }} error budget exhausted\"\n",
    "          description: |\n",
    "            Error budget used: {{ $value | humanizePercentage }}\n",
    "            Action: Freeze non-critical deployments until recovered\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Client SDK Generation**\n",
    "\n",
    "```bash\n",
    "# Generate Python client from OpenAPI spec\n",
    "openapi-generator generate \\\n",
    "  -i contracts/ventas-daily-revenue-api-v2.yaml \\\n",
    "  -g python \\\n",
    "  -o clients/python/ventas-api-client \\\n",
    "  --package-name ventas_api_client\n",
    "\n",
    "# Usage\n",
    "pip install ./clients/python/ventas-api-client\n",
    "\n",
    "# Client code\n",
    "from ventas_api_client import ApiClient, Configuration, RevenuApi\n",
    "\n",
    "config = Configuration()\n",
    "config.host = \"https://api.company.com/ventas/v2\"\n",
    "config.api_key['X-API-Key'] = \"your-api-key\"\n",
    "\n",
    "client = ApiClient(configuration=config)\n",
    "api = RevenueApi(client)\n",
    "\n",
    "# Call API\n",
    "response = api.get_daily_revenue(date=\"2024-01-15\", region=\"LATAM\")\n",
    "print(f\"Revenue: ${response.revenue_net:,.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Contract Testing**\n",
    "\n",
    "```python\n",
    "# tests/contract_tests.py\n",
    "\"\"\"\n",
    "Ensure API complies with OpenAPI contract\n",
    "\"\"\"\n",
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "from api.ventas_api import app\n",
    "import yaml\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def load_openapi_spec():\n",
    "    with open(\"contracts/ventas-daily-revenue-api-v2.yaml\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def test_response_matches_schema():\n",
    "    \"\"\"Verify response matches OpenAPI schema\"\"\"\n",
    "    spec = load_openapi_spec()\n",
    "    \n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15&region=LATAM\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    \n",
    "    # Verify required fields\n",
    "    schema = spec['components']['schemas']['DailyRevenueResponse']\n",
    "    required_fields = schema['required']\n",
    "    \n",
    "    for field in required_fields:\n",
    "        assert field in data, f\"Missing required field: {field}\"\n",
    "    \n",
    "    # Verify types\n",
    "    assert isinstance(data['revenue_gross'], (int, float))\n",
    "    assert isinstance(data['transactions_count'], int)\n",
    "    assert data['region'] in [\"LATAM\", \"NA\", \"EU\", \"APAC\", \"ALL\"]\n",
    "\n",
    "def test_backward_compatibility():\n",
    "    \"\"\"Ensure v2.1 is backward compatible with v2.0\"\"\"\n",
    "    response = client.get(\n",
    "        \"/daily-revenue?date=2024-01-15\",\n",
    "        headers={\"X-API-Key\": \"test-key\"}\n",
    "    )\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # v2.0 clients expect these fields\n",
    "    assert 'date' in data\n",
    "    assert 'revenue_gross' in data  # Renamed from 'total' in v2.0\n",
    "    assert 'transactions_count' in data\n",
    "    \n",
    "    # v2.1 added optional field (should have default)\n",
    "    assert 'refunds_count' in data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bd2f5",
   "metadata": {},
   "source": [
    "### ğŸ›ï¸ **Federated Governance: Policies, Observability & Cost at Scale**\n",
    "\n",
    "**1. Computational Governance Model**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Governance Federation Model                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚  GLOBAL POLICIES (Platform Team)                        â”‚  â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚ â€¢ Security: IAM, encryption, PII masking                â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Compliance: GDPR, SOX, PCI-DSS                        â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Quality: Minimum SLOs (99% availability)              â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Observability: Mandatory lineage, metrics             â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Cost: Budget limits per domain                        â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                           â–¼                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚  AUTOMATED ENFORCEMENT (Policy Engine)                  â”‚  â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚ â€¢ OPA/Cedar: Runtime policy checks                      â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ pre-commit hooks: Prevent non-compliant code          â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ CI/CD gates: Block deployment if policies fail        â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                           â–¼                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Ventas       â”‚ Logistica    â”‚ Producto     â”‚ Marketing  â”‚  â”‚\n",
    "â”‚  â”‚ (local impl) â”‚ (local impl) â”‚ (local impl) â”‚ (local)    â”‚  â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚ â€¢ Spark      â”‚ â€¢ dbt        â”‚ â€¢ Polars     â”‚ â€¢ Airflow  â”‚  â”‚\n",
    "â”‚  â”‚ â€¢ Daily      â”‚ â€¢ Hourly     â”‚ â€¢ Weekly     â”‚ â€¢ Real-timeâ”‚  â”‚\n",
    "â”‚  â”‚ â€¢ 50 GB/day  â”‚ â€¢ 100 GB/day â”‚ â€¢ 10 GB/day  â”‚ â€¢ 200 GB/d â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Policy as Code: Open Policy Agent (OPA)**\n",
    "\n",
    "```rego\n",
    "# policies/data_mesh_policies.rego\n",
    "package datamesh\n",
    "\n",
    "import future.keywords.if\n",
    "import future.keywords.in\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 1: PII Must Be Masked in Shared Datasets\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.dataset.sharing_level == \"public\"\n",
    "    some column in input.dataset.columns\n",
    "    column.contains_pii == true\n",
    "    not column.is_masked\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"PII column '%s' in dataset '%s' must be masked for public sharing\",\n",
    "        [column.name, input.dataset.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 2: All Data Products Must Have Owner\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    not input.data_product.owner\n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' must have an owner defined\",\n",
    "        [input.data_product.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.owner\n",
    "    not endswith(input.data_product.owner, \"@company.com\")\n",
    "    msg := sprintf(\n",
    "        \"Data product owner '%s' must be a valid company email\",\n",
    "        [input.data_product.owner]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 3: Cost Budget Enforcement\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.domain.monthly_cost > input.domain.budget_limit\n",
    "    overage := input.domain.monthly_cost - input.domain.budget_limit\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' exceeded budget: $%.2f over limit of $%.2f\",\n",
    "        [input.domain.name, overage, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "warn[msg] if {\n",
    "    usage := input.domain.monthly_cost / input.domain.budget_limit\n",
    "    usage > 0.8\n",
    "    usage <= 1.0\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' at %.0f%% of budget ($%.2f / $%.2f)\",\n",
    "        [input.domain.name, usage * 100, input.domain.monthly_cost, input.domain.budget_limit]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 4: Data Quality SLO Enforcement\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.data_product.quality_score < 0.95\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Data product '%s' quality score %.2f%% below minimum 95%%\",\n",
    "        [input.data_product.name, input.data_product.quality_score * 100]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 5: Lineage Must Be Tracked\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.pipeline.outputs_to_shared_layer\n",
    "    not input.pipeline.emits_lineage\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Pipeline '%s' must emit lineage (OpenLineage) to DataHub\",\n",
    "        [input.pipeline.name]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 6: API Versioning Required\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.api.is_public\n",
    "    not regex.match(`^/v[0-9]+/`, input.api.path)\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Public API path '%s' must include version prefix (e.g., /v1/)\",\n",
    "        [input.api.path]\n",
    "    )\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POLICY 7: Cross-Domain Access Requires Approval\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "deny[msg] if {\n",
    "    input.access_request.source_domain != input.dataset.owner_domain\n",
    "    not input.access_request.approved_by_owner\n",
    "    \n",
    "    msg := sprintf(\n",
    "        \"Domain '%s' accessing '%s' dataset requires approval from '%s' team\",\n",
    "        [\n",
    "            input.access_request.source_domain,\n",
    "            input.dataset.name,\n",
    "            input.dataset.owner_domain\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "```\n",
    "\n",
    "**Policy Enforcement in CI/CD:**\n",
    "\n",
    "```python\n",
    "# scripts/check_policies.py\n",
    "\"\"\"\n",
    "Validate policies before deployment\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def check_opa_policies(input_data: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Run OPA policy checks\n",
    "    \"\"\"\n",
    "    # Convert input to JSON\n",
    "    input_json = json.dumps(input_data)\n",
    "    \n",
    "    # Run OPA evaluation\n",
    "    result = subprocess.run(\n",
    "        [\"opa\", \"eval\", \"--data\", \"policies/\", \"--input\", \"-\", \"data.datamesh.deny\"],\n",
    "        input=input_json.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"âŒ OPA evaluation failed: {result.stderr.decode()}\")\n",
    "        return False\n",
    "    \n",
    "    output = json.loads(result.stdout)\n",
    "    violations = output['result'][0]['expressions'][0]['value']\n",
    "    \n",
    "    if violations:\n",
    "        print(\"âŒ Policy violations detected:\")\n",
    "        for violation in violations:\n",
    "            print(f\"  - {violation}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ… All policies passed\")\n",
    "    return True\n",
    "\n",
    "# Example usage in pre-commit hook\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = {\n",
    "        \"data_product\": {\n",
    "            \"name\": \"daily_revenue_api\",\n",
    "            \"owner\": \"ventas-team@company.com\",\n",
    "            \"quality_score\": 0.997\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"name\": \"ventas_curated\",\n",
    "            \"sharing_level\": \"public\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"customer_id\", \"contains_pii\": False},\n",
    "                {\"name\": \"email\", \"contains_pii\": True, \"is_masked\": True}\n",
    "            ]\n",
    "        },\n",
    "        \"domain\": {\n",
    "            \"name\": \"ventas\",\n",
    "            \"monthly_cost\": 1800,\n",
    "            \"budget_limit\": 2000\n",
    "        },\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"ventas_daily_batch\",\n",
    "            \"outputs_to_shared_layer\": True,\n",
    "            \"emits_lineage\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not check_opa_policies(input_data):\n",
    "        sys.exit(1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Multi-Tenant Observability Dashboard**\n",
    "\n",
    "```python\n",
    "# monitoring/mesh_dashboard.py\n",
    "\"\"\"\n",
    "Unified observability across all domains\n",
    "\"\"\"\n",
    "from grafana_api.grafana_face import GrafanaFace\n",
    "import json\n",
    "\n",
    "grafana = GrafanaFace(auth=\"admin:admin\", host=\"grafana.company.com\")\n",
    "\n",
    "# Create unified dashboard\n",
    "dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Data Mesh - Multi-Domain Overview\",\n",
    "        \"tags\": [\"data-mesh\", \"cross-domain\"],\n",
    "        \"timezone\": \"utc\",\n",
    "        \"panels\": [\n",
    "            # Panel 1: Domain Health Matrix\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Domain Health Matrix\",\n",
    "                \"type\": \"heatmap\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(data_product_availability[5m])\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 0, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 2: SLO Compliance by Domain\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"SLO Compliance by Domain\",\n",
    "                \"type\": \"gauge\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (\n",
    "                            sum_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                            /\n",
    "                            count_over_time(data_product_availability{domain=\"$domain\"}[30d])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"Availability %\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"thresholds\": {\n",
    "                            \"steps\": [\n",
    "                                {\"color\": \"red\", \"value\": 0},\n",
    "                                {\"color\": \"yellow\", \"value\": 99},\n",
    "                                {\"color\": \"green\", \"value\": 99.9}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 3: Cost by Domain\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Cost by Domain (Monthly)\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            increase(aws_billing_estimated_charges{domain!=\"\"}[30d])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 18, \"y\": 0, \"w\": 6, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 4: Data Freshness by Product\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Data Freshness (Last Update)\",\n",
    "                \"type\": \"table\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        (time() - data_product_last_update_timestamp) / 60\n",
    "                    \"\"\",\n",
    "                    \"format\": \"table\",\n",
    "                    \"instant\": True\n",
    "                }],\n",
    "                \"transformations\": [{\n",
    "                    \"id\": \"organize\",\n",
    "                    \"options\": {\n",
    "                        \"excludeByName\": {},\n",
    "                        \"indexByName\": {\n",
    "                            \"domain\": 0,\n",
    "                            \"product\": 1,\n",
    "                            \"Value\": 2\n",
    "                        },\n",
    "                        \"renameByName\": {\n",
    "                            \"Value\": \"Minutes Since Update\"\n",
    "                        }\n",
    "                    }\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 5: Pipeline Success Rate\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"title\": \"Pipeline Success Rate (24h)\",\n",
    "                \"type\": \"stat\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_success{domain!=\"\"}[24h])\n",
    "                        )\n",
    "                        /\n",
    "                        sum by (domain) (\n",
    "                            rate(airflow_dag_run_total{domain!=\"\"}[24h])\n",
    "                        ) * 100\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 8, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 6: Feature Store Usage\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"title\": \"Feature Store Requests (by domain)\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        rate(feast_feature_requests_total[5m])\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }} - {{ feature_view }}\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 16, \"w\": 24, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 7: Data Quality Score Trend\n",
    "            {\n",
    "                \"id\": 7,\n",
    "                \"title\": \"Data Quality Score Trend\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        avg by (domain) (\n",
    "                            data_product_quality_score\n",
    "                        )\n",
    "                    \"\"\",\n",
    "                    \"legendFormat\": \"{{ domain }}\"\n",
    "                }],\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"min\": 0,\n",
    "                        \"max\": 1,\n",
    "                        \"unit\": \"percentunit\"\n",
    "                    }\n",
    "                },\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            },\n",
    "            \n",
    "            # Panel 8: Cross-Domain Dependencies\n",
    "            {\n",
    "                \"id\": 8,\n",
    "                \"title\": \"Cross-Domain API Calls\",\n",
    "                \"type\": \"nodeGraph\",\n",
    "                \"targets\": [{\n",
    "                    \"expr\": \"\"\"\n",
    "                        sum by (source_domain, target_domain) (\n",
    "                            rate(api_requests_total{source_domain!=\"\",target_domain!=\"\"}[5m])\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                }],\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 24, \"w\": 12, \"h\": 8}\n",
    "            }\n",
    "        ],\n",
    "        \"templating\": {\n",
    "            \"list\": [\n",
    "                {\n",
    "                    \"name\": \"domain\",\n",
    "                    \"type\": \"query\",\n",
    "                    \"query\": \"label_values(data_product_availability, domain)\",\n",
    "                    \"multi\": False,\n",
    "                    \"includeAll\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"folderId\": 0,\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "# Create dashboard\n",
    "result = grafana.dashboard.update_dashboard(dashboard)\n",
    "print(f\"âœ… Dashboard created: {result['url']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Cost Allocation and Chargeback**\n",
    "\n",
    "```python\n",
    "# finops/cost_allocation.py\n",
    "\"\"\"\n",
    "Allocate AWS costs to domains (chargeback model)\n",
    "\"\"\"\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "ce = boto3.client('ce', region_name='us-east-1')\n",
    "\n",
    "def get_domain_costs(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query AWS Cost Explorer with domain tags\n",
    "    \"\"\"\n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': start_date,\n",
    "            'End': end_date\n",
    "        },\n",
    "        Granularity='DAILY',\n",
    "        Metrics=['UnblendedCost', 'UsageQuantity'],\n",
    "        GroupBy=[\n",
    "            {'Type': 'TAG', 'Key': 'domain'},\n",
    "            {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n",
    "        ],\n",
    "        Filter={\n",
    "            'Tags': {\n",
    "                'Key': 'Project',\n",
    "                'Values': ['data-mesh']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse results\n",
    "    rows = []\n",
    "    for result in response['ResultsByTime']:\n",
    "        date = result['TimePeriod']['Start']\n",
    "        for group in result['Groups']:\n",
    "            domain = group['Keys'][0].split('$')[1] if '$' in group['Keys'][0] else 'untagged'\n",
    "            service = group['Keys'][1]\n",
    "            cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "            \n",
    "            rows.append({\n",
    "                'date': date,\n",
    "                'domain': domain,\n",
    "                'service': service,\n",
    "                'cost': cost\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Generate monthly report\n",
    "start = (datetime.utcnow() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "end = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "costs_df = get_domain_costs(start, end)\n",
    "\n",
    "# Aggregate by domain\n",
    "domain_summary = costs_df.groupby('domain').agg({\n",
    "    'cost': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "domain_summary = domain_summary.sort_values('cost', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Domain Cost Summary (Last 30 Days):\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in domain_summary.iterrows():\n",
    "    print(f\"{row['domain']:15} ${row['cost']:>10,.2f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'TOTAL':15} ${domain_summary['cost'].sum():>10,.2f}\")\n",
    "\n",
    "# Check budget overages\n",
    "budgets = {\n",
    "    'ventas': 2000,\n",
    "    'logistica': 1500,\n",
    "    'producto': 1000,\n",
    "    'marketing': 2500,\n",
    "    'finanzas': 3000\n",
    "}\n",
    "\n",
    "print(\"\\nâš ï¸ Budget Status:\")\n",
    "print(\"=\" * 50)\n",
    "for domain, budget in budgets.items():\n",
    "    actual = domain_summary[domain_summary['domain'] == domain]['cost'].sum()\n",
    "    utilization = (actual / budget) * 100\n",
    "    status = \"âœ…\" if utilization <= 100 else \"ğŸš¨\"\n",
    "    \n",
    "    print(f\"{status} {domain:15} ${actual:>8,.2f} / ${budget:>8,.2f} ({utilization:>5.1f}%)\")\n",
    "\n",
    "# Export to DataHub for visibility\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "for _, row in domain_summary.iterrows():\n",
    "    # Emit cost metrics to DataHub\n",
    "    domain_urn = f\"urn:li:domain:{row['domain']}\"\n",
    "    # (Implementation omitted for brevity)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. DataHub: Cross-Domain Lineage**\n",
    "\n",
    "```python\n",
    "# lineage/emit_cross_domain_lineage.py\n",
    "\"\"\"\n",
    "Emit lineage showing cross-domain dependencies\n",
    "\"\"\"\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import (\n",
    "    DatasetLineageTypeClass,\n",
    "    UpstreamClass,\n",
    "    UpstreamLineageClass\n",
    ")\n",
    "\n",
    "emitter = DatahubRestEmitter('http://datahub:8080')\n",
    "\n",
    "# Example: Marketing domain consumes Ventas + Producto data\n",
    "lineage_map = {\n",
    "    # Marketing domain datasets\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.marketing.customer_segments,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            # Consumes from Ventas\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.customer_transactions,PROD)\",\n",
    "            # Consumes from Producto\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.producto.catalog,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.TRANSFORMED\n",
    "    },\n",
    "    \n",
    "    # Finanzas consumes Ventas\n",
    "    \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.finanzas.accounting_reports,PROD)\": {\n",
    "        \"upstreams\": [\n",
    "            \"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\"\n",
    "        ],\n",
    "        \"type\": DatasetLineageTypeClass.COPY\n",
    "    }\n",
    "}\n",
    "\n",
    "for downstream_urn, lineage_info in lineage_map.items():\n",
    "    upstreams = [\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_urn,\n",
    "            type=lineage_info['type']\n",
    "        )\n",
    "        for upstream_urn in lineage_info['upstreams']\n",
    "    ]\n",
    "    \n",
    "    lineage = UpstreamLineageClass(upstreams=upstreams)\n",
    "    \n",
    "    emitter.emit_mcp(\n",
    "        MetadataChangeProposalWrapper(\n",
    "            entityUrn=downstream_urn,\n",
    "            aspect=lineage\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Lineage emitted for {downstream_urn}\")\n",
    "```\n",
    "\n",
    "**DataHub Search: Find Cross-Domain Dependencies**\n",
    "\n",
    "```graphql\n",
    "# GraphQL query to find all datasets consuming Ventas data\n",
    "query {\n",
    "  search(\n",
    "    input: {\n",
    "      type: DATASET\n",
    "      query: \"*\"\n",
    "      filters: [\n",
    "        {\n",
    "          field: \"upstream.urn\"\n",
    "          values: [\"urn:li:domain:ventas\"]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ) {\n",
    "    searchResults {\n",
    "      entity {\n",
    "        ... on Dataset {\n",
    "          urn\n",
    "          name\n",
    "          domain {\n",
    "            name\n",
    "          }\n",
    "          upstream {\n",
    "            dataset {\n",
    "              urn\n",
    "              name\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Domain Autonomy with Guardrails**\n",
    "\n",
    "```python\n",
    "# platform/domain_provisioning.py\n",
    "\"\"\"\n",
    "Self-service domain provisioning with governance guardrails\n",
    "\"\"\"\n",
    "from typing import Dict\n",
    "import boto3\n",
    "\n",
    "class DomainProvisioner:\n",
    "    \"\"\"\n",
    "    Automated domain provisioning with policy enforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.iam = boto3.client('iam')\n",
    "        self.glue = boto3.client('glue')\n",
    "    \n",
    "    def provision_new_domain(self, domain_name: str, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Provision infrastructure for new domain\n",
    "        \n",
    "        Enforces:\n",
    "        - Naming conventions\n",
    "        - Cost budgets\n",
    "        - Security policies\n",
    "        - Observability standards\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate config against policies\n",
    "        if not self._validate_config(domain_name, config):\n",
    "            raise ValueError(\"Configuration violates governance policies\")\n",
    "        \n",
    "        resources = {}\n",
    "        \n",
    "        # 1. S3 bucket with standard structure\n",
    "        bucket_name = f\"mesh-{domain_name}\"\n",
    "        self.s3.create_bucket(Bucket=bucket_name)\n",
    "        \n",
    "        # Apply lifecycle policies (governance requirement)\n",
    "        self.s3.put_bucket_lifecycle_configuration(\n",
    "            Bucket=bucket_name,\n",
    "            LifecycleConfiguration={\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'ID': 'raw-retention',\n",
    "                        'Prefix': 'raw/',\n",
    "                        'Status': 'Enabled',\n",
    "                        'Transitions': [\n",
    "                            {'Days': 7, 'StorageClass': 'GLACIER'}\n",
    "                        ],\n",
    "                        'Expiration': {'Days': 90}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Enable versioning (governance requirement)\n",
    "        self.s3.put_bucket_versioning(\n",
    "            Bucket=bucket_name,\n",
    "            VersioningConfiguration={'Status': 'Enabled'}\n",
    "        )\n",
    "        \n",
    "        resources['s3_bucket'] = bucket_name\n",
    "        \n",
    "        # 2. IAM role with least privilege\n",
    "        role_name = f\"{domain_name}-pipeline-role\"\n",
    "        assume_role_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"emr-serverless.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        self.iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(assume_role_policy),\n",
    "            Tags=[\n",
    "                {'Key': 'domain', 'Value': domain_name},\n",
    "                {'Key': 'managed-by', 'Value': 'platform-team'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        resources['iam_role'] = role_name\n",
    "        \n",
    "        # 3. Glue database\n",
    "        self.glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': f\"{domain_name}_db\",\n",
    "                'Description': f\"Database for {domain_name} domain\",\n",
    "                'Parameters': {\n",
    "                    'domain': domain_name,\n",
    "                    'owner': config['owner']\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resources['glue_database'] = f\"{domain_name}_db\"\n",
    "        \n",
    "        # 4. Budget alert (governance requirement)\n",
    "        budgets = boto3.client('budgets')\n",
    "        budgets.create_budget(\n",
    "            AccountId='123456789012',\n",
    "            Budget={\n",
    "                'BudgetName': f\"{domain_name}-monthly\",\n",
    "                'BudgetLimit': {\n",
    "                    'Amount': str(config['budget_limit']),\n",
    "                    'Unit': 'USD'\n",
    "                },\n",
    "                'TimeUnit': 'MONTHLY',\n",
    "                'BudgetType': 'COST',\n",
    "                'CostFilters': {\n",
    "                    'TagKeyValue': [f'domain${domain_name}']\n",
    "                }\n",
    "            },\n",
    "            NotificationsWithSubscribers=[\n",
    "                {\n",
    "                    'Notification': {\n",
    "                        'NotificationType': 'ACTUAL',\n",
    "                        'ComparisonOperator': 'GREATER_THAN',\n",
    "                        'Threshold': 80.0\n",
    "                    },\n",
    "                    'Subscribers': [{\n",
    "                        'SubscriptionType': 'EMAIL',\n",
    "                        'Address': config['owner']\n",
    "                    }]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return resources\n",
    "    \n",
    "    def _validate_config(self, domain_name: str, config: Dict) -> bool:\n",
    "        \"\"\"Validate against governance policies\"\"\"\n",
    "        \n",
    "        # Policy 1: Owner must be specified\n",
    "        if 'owner' not in config or not config['owner'].endswith('@company.com'):\n",
    "            print(\"âŒ Owner email required and must be @company.com\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 2: Budget limit required\n",
    "        if 'budget_limit' not in config or config['budget_limit'] <= 0:\n",
    "            print(\"âŒ Budget limit must be positive\")\n",
    "            return False\n",
    "        \n",
    "        # Policy 3: Naming convention\n",
    "        if not domain_name.islower() or len(domain_name) > 20:\n",
    "            print(\"âŒ Domain name must be lowercase and <20 chars\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "provisioner = DomainProvisioner()\n",
    "\n",
    "new_domain_config = {\n",
    "    'owner': 'analytics-team@company.com',\n",
    "    'budget_limit': 1500,\n",
    "    'slo_availability': 0.999\n",
    "}\n",
    "\n",
    "resources = provisioner.provision_new_domain('analytics', new_domain_config)\n",
    "print(f\"âœ… Domain 'analytics' provisioned: {resources}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Incident Response: Cross-Domain Impact Analysis**\n",
    "\n",
    "```python\n",
    "# sre/incident_response.py\n",
    "\"\"\"\n",
    "Analyze blast radius of incidents across domains\n",
    "\"\"\"\n",
    "from datahub.client import DataHubGraph\n",
    "\n",
    "graph = DataHubGraph(server=\"http://datahub:8080\")\n",
    "\n",
    "def analyze_impact(failed_dataset_urn: str):\n",
    "    \"\"\"\n",
    "    Find all downstream consumers of failed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query DataHub for downstream lineage\n",
    "    query = f\"\"\"\n",
    "    {{\n",
    "        dataset(urn: \"{failed_dataset_urn}\") {{\n",
    "            urn\n",
    "            name\n",
    "            domain {{ name }}\n",
    "            downstream(limit: 100) {{\n",
    "                dataset {{\n",
    "                    urn\n",
    "                    name\n",
    "                    domain {{ name }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = graph.execute_graphql(query)\n",
    "    failed = result['dataset']\n",
    "    \n",
    "    print(f\"\\nğŸš¨ INCIDENT: {failed['name']} in {failed['domain']['name']} domain\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    downstreams = failed.get('downstream', [])\n",
    "    \n",
    "    if not downstreams:\n",
    "        print(\"âœ… No downstream dependencies (isolated impact)\")\n",
    "        return\n",
    "    \n",
    "    # Group by domain\n",
    "    impacted_domains = {}\n",
    "    for downstream in downstreams:\n",
    "        ds = downstream['dataset']\n",
    "        domain = ds['domain']['name']\n",
    "        \n",
    "        if domain not in impacted_domains:\n",
    "            impacted_domains[domain] = []\n",
    "        \n",
    "        impacted_domains[domain].append(ds['name'])\n",
    "    \n",
    "    print(f\"âš ï¸ IMPACTED DOMAINS: {len(impacted_domains)}\")\n",
    "    for domain, datasets in impacted_domains.items():\n",
    "        print(f\"\\n  {domain.upper()}:\")\n",
    "        for dataset in datasets:\n",
    "            print(f\"    - {dataset}\")\n",
    "    \n",
    "    # Suggest actions\n",
    "    print(\"\\nğŸ“‹ RECOMMENDED ACTIONS:\")\n",
    "    print(\"1. Notify impacted domain owners:\")\n",
    "    for domain in impacted_domains.keys():\n",
    "        print(f\"   - {domain}-team@company.com\")\n",
    "    print(\"2. Check if impacted datasets have fallback sources\")\n",
    "    print(\"3. Estimate ETA for fix and communicate\")\n",
    "\n",
    "# Example: Ventas daily revenue pipeline failed\n",
    "analyze_impact(\"urn:li:dataset:(urn:li:dataPlatform:s3,mesh.ventas.daily_revenue,PROD)\")\n",
    "\n",
    "# Output:\n",
    "# ğŸš¨ INCIDENT: daily_revenue in ventas domain\n",
    "# ======================================================================\n",
    "# âš ï¸ IMPACTED DOMAINS: 3\n",
    "# \n",
    "#   MARKETING:\n",
    "#     - customer_segments\n",
    "#     - campaign_attribution\n",
    "# \n",
    "#   FINANZAS:\n",
    "#     - accounting_reports\n",
    "#     - revenue_forecasts\n",
    "# \n",
    "#   EXECUTIVE:\n",
    "#     - executive_dashboard\n",
    "# \n",
    "# ğŸ“‹ RECOMMENDED ACTIONS:\n",
    "# 1. Notify impacted domain owners:\n",
    "#    - marketing-team@company.com\n",
    "#    - finanzas-team@company.com\n",
    "#    - executive-team@company.com\n",
    "# 2. Check if impacted datasets have fallback sources\n",
    "# 3. Estimate ETA for fix and communicate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa74a1e",
   "metadata": {},
   "source": [
    "## 1. Contexto y requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06572913",
   "metadata": {},
   "source": [
    "**Empresa**: Marketplace multi-categorÃ­a con 5 dominios de negocio:\n",
    "- Ventas (Sales)\n",
    "- LogÃ­stica (Fulfillment)\n",
    "- Producto (Catalog)\n",
    "- Marketing (Campaigns)\n",
    "- Finanzas (Payments)\n",
    "\n",
    "**Objetivo**: Cada dominio gestiona sus propios datos como producto, con SLOs, versionado y documentaciÃ³n. Un feature store central consume features de todos los dominios para ML.\n",
    "\n",
    "**Requerimientos**:\n",
    "- Plataforma self-service: catÃ¡logo, CI/CD, observabilidad compartidos.\n",
    "- Gobernanza federada: polÃ­ticas de seguridad y calidad globales, aplicadas localmente.\n",
    "- Feature store (Feast/Tecton) con features de cada dominio.\n",
    "- APIs de data products con contratos versionados (OpenAPI).\n",
    "- Linaje cross-domain visible en DataHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6483037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ğŸŒ PROYECTO INTEGRADOR SENIOR 2: DATA MESH + FEATURE STORE\n",
      "==========================================================================================\n",
      "\n",
      "Arquitectura: Federated Domains + Centralized Feature Store + Governance\n",
      "\n",
      "==========================================================================================\n",
      "1ï¸âƒ£ DOMAIN-ORIENTED ARCHITECTURE\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ¢ Dominios configurados: 4\n",
      "\n",
      "   ğŸ“¦ VENTAS:\n",
      "      Team: sales-analytics@company.com\n",
      "      Mission: Revenue intelligence & transaction analytics\n",
      "      Data Products: 3\n",
      "      Budget: $2000/month\n",
      "      Engineers: 3\n",
      "      SLOs: 99.9% avail, P95 <15min\n",
      "      Dependencies: producto.catalog_api\n",
      "\n",
      "   ğŸ“¦ LOGISTICA:\n",
      "      Team: fulfillment@company.com\n",
      "      Mission: Logistics optimization & tracking\n",
      "      Data Products: 3\n",
      "      Budget: $1500/month\n",
      "      Engineers: 2\n",
      "      SLOs: 99.5% avail, P95 <30min\n",
      "      Dependencies: ventas.customer_transactions\n",
      "\n",
      "   ğŸ“¦ PRODUCTO:\n",
      "      Team: catalog-team@company.com\n",
      "      Mission: Product catalog & pricing\n",
      "      Data Products: 3\n",
      "      Budget: $1000/month\n",
      "      Engineers: 2\n",
      "      SLOs: 99.9% avail, P95 <10min\n",
      "\n",
      "   ğŸ“¦ MARKETING:\n",
      "      Team: growth@company.com\n",
      "      Mission: Campaign effectiveness & attribution\n",
      "      Data Products: 3\n",
      "      Budget: $2500/month\n",
      "      Engineers: 4\n",
      "      SLOs: 99.0% avail, P95 <60min\n",
      "      Dependencies: ventas.customer_transactions, producto.catalog_api\n",
      "\n",
      "   ğŸ’° Total budget: $7,000/month\n",
      "   ğŸ‘¥ Total engineers: 11\n",
      "\n",
      "==========================================================================================\n",
      "2ï¸âƒ£ DATA PRODUCTS: APIs + Quality SLOs\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“Š Data Products Performance (30 dÃ­as):\n",
      "\n",
      "   âŒ VENTAS:\n",
      "      Total requests: 481,622\n",
      "      Error rate: 0.578%\n",
      "      P95 latency: 12.8min\n",
      "      SLO compliance: 73.3%\n",
      "   âŒ LOGISTICA:\n",
      "      Total requests: 480,995\n",
      "      Error rate: 0.546%\n",
      "      P95 latency: 25.7min\n",
      "      SLO compliance: 68.9%\n",
      "   âŒ PRODUCTO:\n",
      "      Total requests: 533,995\n",
      "      Error rate: 0.577%\n",
      "      P95 latency: 8.4min\n",
      "      SLO compliance: 71.1%\n",
      "   âŒ MARKETING:\n",
      "      Total requests: 486,137\n",
      "      Error rate: 0.534%\n",
      "      P95 latency: 48.3min\n",
      "      SLO compliance: 75.6%\n",
      "\n",
      "==========================================================================================\n",
      "3ï¸âƒ£ FEDERATED GOVERNANCE: Global Policies + Local Execution\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“œ Global Policies:\n",
      "\n",
      "   ğŸ”’ SECURITY:\n",
      "      â€¢ pii_encryption: REQUIRED\n",
      "      â€¢ access_control: RBAC\n",
      "      â€¢ audit_logging: ENABLED\n",
      "      â€¢ data_classification: PUBLIC, INTERNAL, CONFIDENTIAL, RESTRICTED\n",
      "   ğŸ”’ QUALITY:\n",
      "      â€¢ min_data_quality_score: 95\n",
      "      â€¢ max_null_rate: 0.05\n",
      "      â€¢ freshness_slo: 24h\n",
      "      â€¢ schema_validation: REQUIRED\n",
      "   ğŸ”’ COST:\n",
      "      â€¢ budget_approval_threshold: 5000\n",
      "      â€¢ cost_allocation_required: True\n",
      "      â€¢ monthly_review: True\n",
      "   ğŸ”’ COMPLIANCE:\n",
      "      â€¢ gdpr_compliance: REQUIRED\n",
      "      â€¢ data_retention_max_years: 7\n",
      "      â€¢ right_to_deletion: ENABLED\n",
      "\n",
      "ğŸ“Š Compliance Audit:\n",
      "\n",
      "   VENTAS:\n",
      "      âš ï¸ Security: 86.7/100\n",
      "      âš ï¸ Quality: 94.2/100\n",
      "      âœ… Cost: Compliant\n",
      "      âœ… GDPR: Compliant\n",
      "   LOGISTICA:\n",
      "      âš ï¸ Security: 85.8/100\n",
      "      âš ï¸ Quality: 94.6/100\n",
      "      âœ… Cost: Compliant\n",
      "      âœ… GDPR: Compliant\n",
      "   PRODUCTO:\n",
      "      âœ… Security: 94.8/100\n",
      "      âš ï¸ Quality: 93.6/100\n",
      "      âœ… Cost: Compliant\n",
      "      âœ… GDPR: Compliant\n",
      "   MARKETING:\n",
      "      âœ… Security: 98.3/100\n",
      "      âœ… Quality: 95.3/100\n",
      "      âœ… Cost: Compliant\n",
      "      âœ… GDPR: Compliant\n",
      "\n",
      "==========================================================================================\n",
      "4ï¸âƒ£ CENTRALIZED FEATURE STORE: ML Features at Scale\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ¯ Feature Groups: 4\n",
      "\n",
      "   ğŸ“Š customer_behavioral:\n",
      "      Source: ventas domain\n",
      "      Features: 4 (total_purchases_30d, avg_order_value...)\n",
      "      Freshness: <1h\n",
      "      Consumers: churn_model, recommendation_engine, segmentation_model\n",
      "   ğŸ“Š product_engagement:\n",
      "      Source: producto domain\n",
      "      Features: 4 (view_count_7d, add_to_cart_rate...)\n",
      "      Freshness: <6h\n",
      "      Consumers: recommendation_engine, pricing_optimizer\n",
      "   ğŸ“Š logistics_metrics:\n",
      "      Source: logistica domain\n",
      "      Features: 4 (delivery_time_avg, return_rate...)\n",
      "      Freshness: <24h\n",
      "      Consumers: demand_forecasting, inventory_optimization\n",
      "   ğŸ“Š campaign_attribution:\n",
      "      Source: marketing domain\n",
      "      Features: 4 (last_campaign_id, campaign_touches_30d...)\n",
      "      Freshness: <12h\n",
      "      Consumers: attribution_model, campaign_optimizer\n",
      "\n",
      "   ğŸ“ˆ Total features: 16\n",
      "   ğŸ¤– ML models consuming: 8\n",
      "\n",
      "ğŸ“Š Feature Serving Performance (7 dÃ­as):\n",
      "\n",
      "   â€¢ Total requests: 1,731,830\n",
      "   â€¢ Avg P95 latency: 25.8ms\n",
      "   â€¢ Cache hit rate: 83.8%\n",
      "\n",
      "==========================================================================================\n",
      "5ï¸âƒ£ CROSS-DOMAIN DATA LINEAGE\n",
      "==========================================================================================\n",
      "\n",
      "ğŸŒ³ Grafo de linaje cross-domain:\n",
      "\n",
      "   ğŸ“„ Productos fuente (sin dependencias):\n",
      "      â€¢ producto.catalog_api\n",
      "      â€¢ producto.pricing_history\n",
      "      â€¢ producto.category_metrics\n",
      "\n",
      "   ğŸ”— Dependencias:\n",
      "      ventas.daily_revenue_api depends on:\n",
      "         â””â”€ producto.catalog_api\n",
      "      ventas.customer_transactions depends on:\n",
      "         â””â”€ producto.catalog_api\n",
      "      ventas.product_performance depends on:\n",
      "         â””â”€ producto.catalog_api\n",
      "      logistica.shipment_tracking depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "      logistica.warehouse_inventory depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "      logistica.delivery_metrics depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "      marketing.campaign_performance depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "         â””â”€ producto.catalog_api\n",
      "      marketing.customer_segments depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "         â””â”€ producto.catalog_api\n",
      "      marketing.attribution_model depends on:\n",
      "         â””â”€ ventas.customer_transactions\n",
      "         â””â”€ producto.catalog_api\n",
      "\n",
      "   âš ï¸ AnÃ¡lisis de impacto:\n",
      "      ğŸ”¥ ventas.customer_transactions es el mÃ¡s dependido:\n",
      "         Consumido por: logistica, marketing\n",
      "         âš ï¸ Cambios aquÃ­ impactan a 2 dominios\n",
      "      ğŸ”¥ producto.catalog_api es el mÃ¡s dependido:\n",
      "         Consumido por: ventas, marketing\n",
      "         âš ï¸ Cambios aquÃ­ impactan a 2 dominios\n",
      "\n",
      "==========================================================================================\n",
      "6ï¸âƒ£ SELF-SERVE DATA PLATFORM: Infrastructure as Code\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ› ï¸ Platform Services:\n",
      "\n",
      "   â€¢ COMPUTE: Spark Clusters\n",
      "      Provisioning: 5 minutes\n",
      "      Cost: $15/hour\n",
      "      Auto-scaling: âœ…\n",
      "   â€¢ STORAGE: Delta Lake on S3\n",
      "      Provisioning: Immediate\n",
      "      Cost: $23/TB/month\n",
      "      Features: ACID, Time Travel, Schema Evolution\n",
      "   â€¢ ORCHESTRATION: Managed Airflow\n",
      "      Provisioning: 10 minutes\n",
      "      Cost: $350/month\n",
      "      Features: Auto-scaling, HA, Monitoring\n",
      "   â€¢ OBSERVABILITY: Unified Monitoring\n",
      "      Provisioning: Immediate\n",
      "      Cost: $200/month\n",
      "      Features: Logs, Metrics, Traces, Alerting\n",
      "   â€¢ GOVERNANCE: Data Catalog + Access Control\n",
      "      Provisioning: Immediate\n",
      "      Cost: $150/month\n",
      "      Features: Metadata, Lineage, RBAC, Audit\n",
      "\n",
      "==========================================================================================\n",
      "7ï¸âƒ£ RESUMEN EJECUTIVO: DATA MESH ADOPTION\n",
      "==========================================================================================\n",
      "\n",
      "Metric                         Value                         \n",
      "============================================================\n",
      "Architecture                   Data Mesh (Federated)\n",
      "Domains                        4\n",
      "Data Products                  12\n",
      "Engineers (Total)              11\n",
      "Monthly Budget                 $7,000\n",
      "SLO Compliance                 72.2%\n",
      "Avg Data Quality               94.4/100\n",
      "Feature Store                  16 features, 4 groups\n",
      "ML Models                      8\n",
      "Feature Serving                1,731,830 requests/7d\n",
      "Governance                     Federated (global policies + local execution)\n",
      "Platform                       Self-serve (IaC, auto-provisioning)\n",
      "\n",
      "ğŸ“Š Data Mesh vs Centralized Platform:\n",
      "\n",
      "   Time to Market:\n",
      "      Centralized: 6-12 weeks\n",
      "      Data Mesh: 1-2 weeks\n",
      "      â†’ Data Mesh âœ…\n",
      "   Domain Expertise:\n",
      "      Centralized: Limited (1 team)\n",
      "      Data Mesh: High (domain teams)\n",
      "      â†’ Data Mesh âœ…\n",
      "   Scalability:\n",
      "      Centralized: Limited by team size\n",
      "      Data Mesh: Horizontal (add domains)\n",
      "      â†’ Data Mesh âœ…\n",
      "   Initial Complexity:\n",
      "      Centralized: Low\n",
      "      Data Mesh: High\n",
      "      â†’ Centralized âœ…\n",
      "\n",
      "==========================================================================================\n",
      "âœ… Proyecto Integrador 2 completado\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ PROYECTO INTEGRADOR 2: Data Mesh Multi-Dominio + Feature Store\n",
    "# SimulaciÃ³n de arquitectura federada con dominios autÃ³nomos y ML centralizado\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"ğŸŒ PROYECTO INTEGRADOR SENIOR 2: DATA MESH + FEATURE STORE\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nArquitectura: Federated Domains + Centralized Feature Store + Governance\")\n",
    "\n",
    "# ============= 1. DOMAIN ARCHITECTURE =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"1ï¸âƒ£ DOMAIN-ORIENTED ARCHITECTURE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Definir dominios\n",
    "domains = {\n",
    "    'ventas': {\n",
    "        'team': 'sales-analytics@company.com',\n",
    "        'mission': 'Revenue intelligence & transaction analytics',\n",
    "        'data_products': ['daily_revenue_api', 'customer_transactions', 'product_performance'],\n",
    "        'budget_monthly': 2000,\n",
    "        'engineers': 3,\n",
    "        'slo_availability': 99.9,\n",
    "        'slo_latency_p95': 15,  # minutos\n",
    "        'dependencies': ['producto.catalog_api']\n",
    "    },\n",
    "    'logistica': {\n",
    "        'team': 'fulfillment@company.com',\n",
    "        'mission': 'Logistics optimization & tracking',\n",
    "        'data_products': ['shipment_tracking', 'warehouse_inventory', 'delivery_metrics'],\n",
    "        'budget_monthly': 1500,\n",
    "        'engineers': 2,\n",
    "        'slo_availability': 99.5,\n",
    "        'slo_latency_p95': 30,\n",
    "        'dependencies': ['ventas.customer_transactions']\n",
    "    },\n",
    "    'producto': {\n",
    "        'team': 'catalog-team@company.com',\n",
    "        'mission': 'Product catalog & pricing',\n",
    "        'data_products': ['catalog_api', 'pricing_history', 'category_metrics'],\n",
    "        'budget_monthly': 1000,\n",
    "        'engineers': 2,\n",
    "        'slo_availability': 99.9,\n",
    "        'slo_latency_p95': 10,\n",
    "        'dependencies': []\n",
    "    },\n",
    "    'marketing': {\n",
    "        'team': 'growth@company.com',\n",
    "        'mission': 'Campaign effectiveness & attribution',\n",
    "        'data_products': ['campaign_performance', 'customer_segments', 'attribution_model'],\n",
    "        'budget_monthly': 2500,\n",
    "        'engineers': 4,\n",
    "        'slo_availability': 99.0,\n",
    "        'slo_latency_p95': 60,\n",
    "        'dependencies': ['ventas.customer_transactions', 'producto.catalog_api']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¢ Dominios configurados: {len(domains)}\\n\")\n",
    "\n",
    "total_budget = 0\n",
    "total_engineers = 0\n",
    "for domain_name, config in domains.items():\n",
    "    print(f\"   ğŸ“¦ {domain_name.upper()}:\")\n",
    "    print(f\"      Team: {config['team']}\")\n",
    "    print(f\"      Mission: {config['mission']}\")\n",
    "    print(f\"      Data Products: {len(config['data_products'])}\")\n",
    "    print(f\"      Budget: ${config['budget_monthly']}/month\")\n",
    "    print(f\"      Engineers: {config['engineers']}\")\n",
    "    print(f\"      SLOs: {config['slo_availability']}% avail, P95 <{config['slo_latency_p95']}min\")\n",
    "    if config['dependencies']:\n",
    "        print(f\"      Dependencies: {', '.join(config['dependencies'])}\")\n",
    "    print()\n",
    "    \n",
    "    total_budget += config['budget_monthly']\n",
    "    total_engineers += config['engineers']\n",
    "\n",
    "print(f\"   ğŸ’° Total budget: ${total_budget:,}/month\")\n",
    "print(f\"   ğŸ‘¥ Total engineers: {total_engineers}\")\n",
    "\n",
    "# ============= 2. DATA PRODUCTS =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"2ï¸âƒ£ DATA PRODUCTS: APIs + Quality SLOs\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Generar mÃ©tricas de data products (30 dÃ­as)\n",
    "data_products_metrics = []\n",
    "\n",
    "for day in range(30):\n",
    "    date = (now - timedelta(days=30-day)).date()\n",
    "    \n",
    "    for domain_name, domain_config in domains.items():\n",
    "        for product in domain_config['data_products']:\n",
    "            # Simular mÃ©tricas\n",
    "            requests = np.random.randint(1000, 10000)\n",
    "            errors = int(requests * np.random.uniform(0.001, 0.01))\n",
    "            latency_p95 = domain_config['slo_latency_p95'] * np.random.uniform(0.5, 1.2)\n",
    "            \n",
    "            data_products_metrics.append({\n",
    "                'date': date,\n",
    "                'domain': domain_name,\n",
    "                'product': product,\n",
    "                'requests': requests,\n",
    "                'errors': errors,\n",
    "                'error_rate': errors / requests,\n",
    "                'latency_p95_minutes': latency_p95,\n",
    "                'slo_availability': domain_config['slo_availability'],\n",
    "                'slo_latency_p95': domain_config['slo_latency_p95'],\n",
    "                'slo_breached': (errors / requests > 0.01) or (latency_p95 > domain_config['slo_latency_p95'])\n",
    "            })\n",
    "\n",
    "df_products = pd.DataFrame(data_products_metrics)\n",
    "\n",
    "print(f\"\\nğŸ“Š Data Products Performance (30 dÃ­as):\\n\")\n",
    "\n",
    "for domain_name in domains.keys():\n",
    "    domain_data = df_products[df_products['domain'] == domain_name]\n",
    "    \n",
    "    total_requests = domain_data['requests'].sum()\n",
    "    avg_error_rate = domain_data['error_rate'].mean() * 100\n",
    "    avg_latency = domain_data['latency_p95_minutes'].mean()\n",
    "    slo_compliance = (1 - domain_data['slo_breached'].mean()) * 100\n",
    "    \n",
    "    status = 'âœ…' if slo_compliance >= 95 else ('âš ï¸' if slo_compliance >= 90 else 'âŒ')\n",
    "    \n",
    "    print(f\"   {status} {domain_name.upper()}:\")\n",
    "    print(f\"      Total requests: {total_requests:,}\")\n",
    "    print(f\"      Error rate: {avg_error_rate:.3f}%\")\n",
    "    print(f\"      P95 latency: {avg_latency:.1f}min\")\n",
    "    print(f\"      SLO compliance: {slo_compliance:.1f}%\")\n",
    "\n",
    "# ============= 3. FEDERATED GOVERNANCE =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"3ï¸âƒ£ FEDERATED GOVERNANCE: Global Policies + Local Execution\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# PolÃ­ticas globales\n",
    "global_policies = {\n",
    "    'security': {\n",
    "        'pii_encryption': 'REQUIRED',\n",
    "        'access_control': 'RBAC',\n",
    "        'audit_logging': 'ENABLED',\n",
    "        'data_classification': ['PUBLIC', 'INTERNAL', 'CONFIDENTIAL', 'RESTRICTED']\n",
    "    },\n",
    "    'quality': {\n",
    "        'min_data_quality_score': 95,\n",
    "        'max_null_rate': 0.05,\n",
    "        'freshness_slo': '24h',\n",
    "        'schema_validation': 'REQUIRED'\n",
    "    },\n",
    "    'cost': {\n",
    "        'budget_approval_threshold': 5000,\n",
    "        'cost_allocation_required': True,\n",
    "        'monthly_review': True\n",
    "    },\n",
    "    'compliance': {\n",
    "        'gdpr_compliance': 'REQUIRED',\n",
    "        'data_retention_max_years': 7,\n",
    "        'right_to_deletion': 'ENABLED'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“œ Global Policies:\\n\")\n",
    "for policy_area, rules in global_policies.items():\n",
    "    print(f\"   ğŸ”’ {policy_area.upper()}:\")\n",
    "    for rule_name, rule_value in rules.items():\n",
    "        if isinstance(rule_value, list):\n",
    "            print(f\"      â€¢ {rule_name}: {', '.join(rule_value)}\")\n",
    "        else:\n",
    "            print(f\"      â€¢ {rule_name}: {rule_value}\")\n",
    "\n",
    "# Audit compliance por dominio\n",
    "compliance_scores = {}\n",
    "for domain_name in domains.keys():\n",
    "    # Simular audit\n",
    "    compliance_scores[domain_name] = {\n",
    "        'security_score': np.random.uniform(85, 100),\n",
    "        'quality_score': np.random.uniform(90, 100),\n",
    "        'cost_compliance': np.random.choice([True, False], p=[0.9, 0.1]),\n",
    "        'gdpr_compliant': np.random.choice([True, False], p=[0.95, 0.05])\n",
    "    }\n",
    "\n",
    "print(f\"\\nğŸ“Š Compliance Audit:\\n\")\n",
    "for domain_name, scores in compliance_scores.items():\n",
    "    sec_status = 'âœ…' if scores['security_score'] >= 90 else 'âš ï¸'\n",
    "    qual_status = 'âœ…' if scores['quality_score'] >= 95 else 'âš ï¸'\n",
    "    cost_status = 'âœ…' if scores['cost_compliance'] else 'âŒ'\n",
    "    gdpr_status = 'âœ…' if scores['gdpr_compliant'] else 'âŒ'\n",
    "    \n",
    "    print(f\"   {domain_name.upper()}:\")\n",
    "    print(f\"      {sec_status} Security: {scores['security_score']:.1f}/100\")\n",
    "    print(f\"      {qual_status} Quality: {scores['quality_score']:.1f}/100\")\n",
    "    print(f\"      {cost_status} Cost: {'Compliant' if scores['cost_compliance'] else 'Over budget'}\")\n",
    "    print(f\"      {gdpr_status} GDPR: {'Compliant' if scores['gdpr_compliant'] else 'Violations detected'}\")\n",
    "\n",
    "# ============= 4. FEATURE STORE =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"4ï¸âƒ£ CENTRALIZED FEATURE STORE: ML Features at Scale\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Definir feature groups\n",
    "feature_groups = {\n",
    "    'customer_behavioral': {\n",
    "        'source_domain': 'ventas',\n",
    "        'features': ['total_purchases_30d', 'avg_order_value', 'recency_days', 'favorite_category'],\n",
    "        'freshness': '1h',\n",
    "        'consumers': ['churn_model', 'recommendation_engine', 'segmentation_model']\n",
    "    },\n",
    "    'product_engagement': {\n",
    "        'source_domain': 'producto',\n",
    "        'features': ['view_count_7d', 'add_to_cart_rate', 'conversion_rate', 'avg_rating'],\n",
    "        'freshness': '6h',\n",
    "        'consumers': ['recommendation_engine', 'pricing_optimizer']\n",
    "    },\n",
    "    'logistics_metrics': {\n",
    "        'source_domain': 'logistica',\n",
    "        'features': ['delivery_time_avg', 'return_rate', 'shipping_cost', 'stock_level'],\n",
    "        'freshness': '24h',\n",
    "        'consumers': ['demand_forecasting', 'inventory_optimization']\n",
    "    },\n",
    "    'campaign_attribution': {\n",
    "        'source_domain': 'marketing',\n",
    "        'features': ['last_campaign_id', 'campaign_touches_30d', 'channel_preference', 'response_rate'],\n",
    "        'freshness': '12h',\n",
    "        'consumers': ['attribution_model', 'campaign_optimizer']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¯ Feature Groups: {len(feature_groups)}\\n\")\n",
    "\n",
    "total_features = 0\n",
    "unique_consumers = set()\n",
    "\n",
    "for group_name, config in feature_groups.items():\n",
    "    print(f\"   ğŸ“Š {group_name}:\")\n",
    "    print(f\"      Source: {config['source_domain']} domain\")\n",
    "    print(f\"      Features: {len(config['features'])} ({', '.join(config['features'][:2])}...)\")\n",
    "    print(f\"      Freshness: <{config['freshness']}\")\n",
    "    print(f\"      Consumers: {', '.join(config['consumers'])}\")\n",
    "    \n",
    "    total_features += len(config['features'])\n",
    "    unique_consumers.update(config['consumers'])\n",
    "\n",
    "print(f\"\\n   ğŸ“ˆ Total features: {total_features}\")\n",
    "print(f\"   ğŸ¤– ML models consuming: {len(unique_consumers)}\")\n",
    "\n",
    "# Generar feature serving metrics (7 dÃ­as)\n",
    "feature_serving = []\n",
    "for day in range(7):\n",
    "    date = (now - timedelta(days=7-day)).date()\n",
    "    \n",
    "    for group_name, config in feature_groups.items():\n",
    "        requests = np.random.randint(10000, 100000)\n",
    "        latency_p95 = np.random.uniform(5, 50)  # ms\n",
    "        cache_hit_rate = np.random.uniform(0.7, 0.95)\n",
    "        \n",
    "        feature_serving.append({\n",
    "            'date': date,\n",
    "            'feature_group': group_name,\n",
    "            'requests': requests,\n",
    "            'latency_p95_ms': latency_p95,\n",
    "            'cache_hit_rate': cache_hit_rate\n",
    "        })\n",
    "\n",
    "df_features = pd.DataFrame(feature_serving)\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Serving Performance (7 dÃ­as):\\n\")\n",
    "print(f\"   â€¢ Total requests: {df_features['requests'].sum():,}\")\n",
    "print(f\"   â€¢ Avg P95 latency: {df_features['latency_p95_ms'].mean():.1f}ms\")\n",
    "print(f\"   â€¢ Cache hit rate: {df_features['cache_hit_rate'].mean() * 100:.1f}%\")\n",
    "\n",
    "# ============= 5. CROSS-DOMAIN LINEAGE =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"5ï¸âƒ£ CROSS-DOMAIN DATA LINEAGE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Construir grafo de dependencias\n",
    "lineage = {}\n",
    "for domain_name, config in domains.items():\n",
    "    for product in config['data_products']:\n",
    "        lineage[f\"{domain_name}.{product}\"] = {\n",
    "            'dependencies': config['dependencies'],\n",
    "            'consumers': []\n",
    "        }\n",
    "\n",
    "# Agregar consumers\n",
    "for domain_name, config in domains.items():\n",
    "    for dep in config['dependencies']:\n",
    "        if dep in lineage:\n",
    "            lineage[dep]['consumers'].append(domain_name)\n",
    "\n",
    "print(f\"\\nğŸŒ³ Grafo de linaje cross-domain:\\n\")\n",
    "\n",
    "# Mostrar productos sin dependencias (sources)\n",
    "sources = [k for k, v in lineage.items() if not v['dependencies']]\n",
    "print(f\"   ğŸ“„ Productos fuente (sin dependencias):\")\n",
    "for source in sources:\n",
    "    print(f\"      â€¢ {source}\")\n",
    "\n",
    "print(f\"\\n   ğŸ”— Dependencias:\")\n",
    "for product, info in lineage.items():\n",
    "    if info['dependencies']:\n",
    "        print(f\"      {product} depends on:\")\n",
    "        for dep in info['dependencies']:\n",
    "            print(f\"         â””â”€ {dep}\")\n",
    "\n",
    "print(f\"\\n   âš ï¸ AnÃ¡lisis de impacto:\")\n",
    "# Encontrar producto mÃ¡s usado\n",
    "max_consumers = max([len(v['consumers']) for v in lineage.values()])\n",
    "for product, info in lineage.items():\n",
    "    if len(info['consumers']) == max_consumers and max_consumers > 0:\n",
    "        print(f\"      ğŸ”¥ {product} es el mÃ¡s dependido:\")\n",
    "        print(f\"         Consumido por: {', '.join(info['consumers'])}\")\n",
    "        print(f\"         âš ï¸ Cambios aquÃ­ impactan a {len(info['consumers'])} dominios\")\n",
    "\n",
    "# ============= 6. SELF-SERVE PLATFORM =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"6ï¸âƒ£ SELF-SERVE DATA PLATFORM: Infrastructure as Code\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "platform_services = {\n",
    "    'compute': {\n",
    "        'service': 'Spark Clusters',\n",
    "        'provisioning_time': '5 minutes',\n",
    "        'cost_per_hour': 15,\n",
    "        'auto_scaling': True\n",
    "    },\n",
    "    'storage': {\n",
    "        'service': 'Delta Lake on S3',\n",
    "        'provisioning_time': 'Immediate',\n",
    "        'cost_per_tb_month': 23,\n",
    "        'features': ['ACID', 'Time Travel', 'Schema Evolution']\n",
    "    },\n",
    "    'orchestration': {\n",
    "        'service': 'Managed Airflow',\n",
    "        'provisioning_time': '10 minutes',\n",
    "        'cost_per_month': 350,\n",
    "        'features': ['Auto-scaling', 'HA', 'Monitoring']\n",
    "    },\n",
    "    'observability': {\n",
    "        'service': 'Unified Monitoring',\n",
    "        'provisioning_time': 'Immediate',\n",
    "        'cost_per_month': 200,\n",
    "        'features': ['Logs', 'Metrics', 'Traces', 'Alerting']\n",
    "    },\n",
    "    'governance': {\n",
    "        'service': 'Data Catalog + Access Control',\n",
    "        'provisioning_time': 'Immediate',\n",
    "        'cost_per_month': 150,\n",
    "        'features': ['Metadata', 'Lineage', 'RBAC', 'Audit']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ› ï¸ Platform Services:\\n\")\n",
    "for service_name, config in platform_services.items():\n",
    "    print(f\"   â€¢ {service_name.upper()}: {config['service']}\")\n",
    "    print(f\"      Provisioning: {config['provisioning_time']}\")\n",
    "    if 'cost_per_hour' in config:\n",
    "        print(f\"      Cost: ${config['cost_per_hour']}/hour\")\n",
    "    elif 'cost_per_tb_month' in config:\n",
    "        print(f\"      Cost: ${config['cost_per_tb_month']}/TB/month\")\n",
    "    else:\n",
    "        print(f\"      Cost: ${config['cost_per_month']}/month\")\n",
    "    if 'features' in config:\n",
    "        print(f\"      Features: {', '.join(config['features'])}\")\n",
    "    if 'auto_scaling' in config:\n",
    "        print(f\"      Auto-scaling: âœ…\")\n",
    "\n",
    "# ============= 7. RESUMEN EJECUTIVO =============\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"7ï¸âƒ£ RESUMEN EJECUTIVO: DATA MESH ADOPTION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calcular mÃ©tricas agregadas\n",
    "total_products = sum([len(d['data_products']) for d in domains.values()])\n",
    "avg_slo_compliance = (1 - df_products['slo_breached'].mean()) * 100\n",
    "avg_quality = sum([s['quality_score'] for s in compliance_scores.values()]) / len(compliance_scores)\n",
    "total_ml_consumers = len(unique_consumers)\n",
    "\n",
    "summary = {\n",
    "    'Architecture': 'Data Mesh (Federated)',\n",
    "    'Domains': len(domains),\n",
    "    'Data Products': total_products,\n",
    "    'Engineers (Total)': total_engineers,\n",
    "    'Monthly Budget': f\"${total_budget:,}\",\n",
    "    'SLO Compliance': f\"{avg_slo_compliance:.1f}%\",\n",
    "    'Avg Data Quality': f\"{avg_quality:.1f}/100\",\n",
    "    'Feature Store': f\"{total_features} features, {len(feature_groups)} groups\",\n",
    "    'ML Models': total_ml_consumers,\n",
    "    'Feature Serving': f\"{df_features['requests'].sum():,} requests/7d\",\n",
    "    'Governance': 'Federated (global policies + local execution)',\n",
    "    'Platform': 'Self-serve (IaC, auto-provisioning)'\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Value':<30}\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in summary.items():\n",
    "    print(f\"{metric:<30} {value}\")\n",
    "\n",
    "# ComparaciÃ³n: Centralizado vs Data Mesh\n",
    "print(f\"\\nğŸ“Š Data Mesh vs Centralized Platform:\\n\")\n",
    "\n",
    "comparison = {\n",
    "    'Time to Market': {\n",
    "        'Centralized': '6-12 weeks',\n",
    "        'Data Mesh': '1-2 weeks',\n",
    "        'Winner': 'Data Mesh âœ…'\n",
    "    },\n",
    "    'Domain Expertise': {\n",
    "        'Centralized': 'Limited (1 team)',\n",
    "        'Data Mesh': 'High (domain teams)',\n",
    "        'Winner': 'Data Mesh âœ…'\n",
    "    },\n",
    "    'Scalability': {\n",
    "        'Centralized': 'Limited by team size',\n",
    "        'Data Mesh': 'Horizontal (add domains)',\n",
    "        'Winner': 'Data Mesh âœ…'\n",
    "    },\n",
    "    'Initial Complexity': {\n",
    "        'Centralized': 'Low',\n",
    "        'Data Mesh': 'High',\n",
    "        'Winner': 'Centralized âœ…'\n",
    "    }\n",
    "}\n",
    "\n",
    "for aspect, scores in comparison.items():\n",
    "    print(f\"   {aspect}:\")\n",
    "    print(f\"      Centralized: {scores['Centralized']}\")\n",
    "    print(f\"      Data Mesh: {scores['Data Mesh']}\")\n",
    "    print(f\"      â†’ {scores['Winner']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… Proyecto Integrador 2 completado\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb95d6",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Data Mesh propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b5d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                  Plataforma Self-Service                       â”‚\n",
      "â”‚  - Airflow compartido  - DataHub (catÃ¡logo + linaje)          â”‚\n",
      "â”‚  - Grafana + Prometheus - CI/CD (GitHub Actions)              â”‚\n",
      "â”‚  - Feature Store (Feast) - PolÃ­ticas IAM centrales            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                              â–²\n",
      "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "          â”‚                   â”‚                   â”‚\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "  â”‚ Dominio      â”‚   â”‚  Dominio        â”‚  â”‚  Dominio      â”‚\n",
      "  â”‚ Ventas       â”‚   â”‚  LogÃ­stica      â”‚  â”‚  Producto     â”‚\n",
      "  â”‚ (data prod)  â”‚   â”‚  (data prod)    â”‚  â”‚  (data prod)  â”‚\n",
      "  â”‚ - raw/       â”‚   â”‚  - raw/         â”‚  â”‚  - raw/       â”‚\n",
      "  â”‚ - curated/   â”‚   â”‚  - curated/     â”‚  â”‚  - curated/   â”‚\n",
      "  â”‚ - API        â”‚   â”‚  - API          â”‚  â”‚  - API        â”‚\n",
      "  â”‚ - Features   â”‚   â”‚  - Features     â”‚  â”‚  - Features   â”‚\n",
      "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚                   â”‚                   â”‚\n",
      "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                              â–¼\n",
      "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "                   â”‚  Feature Store   â”‚\n",
      "                   â”‚  (Feast/Tecton)  â”‚\n",
      "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                             â”‚\n",
      "                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
      "                      â”‚ ML Models  â”‚\n",
      "                      â”‚ (training) â”‚\n",
      "                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mesh_diagram = '''\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  Plataforma Self-Service                       â”‚\n",
    "â”‚  - Airflow compartido  - DataHub (catÃ¡logo + linaje)          â”‚\n",
    "â”‚  - Grafana + Prometheus - CI/CD (GitHub Actions)              â”‚\n",
    "â”‚  - Feature Store (Feast) - PolÃ­ticas IAM centrales            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â–²\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚                   â”‚                   â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ Dominio      â”‚   â”‚  Dominio        â”‚  â”‚  Dominio      â”‚\n",
    "  â”‚ Ventas       â”‚   â”‚  LogÃ­stica      â”‚  â”‚  Producto     â”‚\n",
    "  â”‚ (data prod)  â”‚   â”‚  (data prod)    â”‚  â”‚  (data prod)  â”‚\n",
    "  â”‚ - raw/       â”‚   â”‚  - raw/         â”‚  â”‚  - raw/       â”‚\n",
    "  â”‚ - curated/   â”‚   â”‚  - curated/     â”‚  â”‚  - curated/   â”‚\n",
    "  â”‚ - API        â”‚   â”‚  - API          â”‚  â”‚  - API        â”‚\n",
    "  â”‚ - Features   â”‚   â”‚  - Features     â”‚  â”‚  - Features   â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                   â”‚                   â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â–¼\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                   â”‚  Feature Store   â”‚\n",
    "                   â”‚  (Feast/Tecton)  â”‚\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "                      â”‚ ML Models  â”‚\n",
    "                      â”‚ (training) â”‚\n",
    "                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "'''\n",
    "print(mesh_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732eb23",
   "metadata": {},
   "source": [
    "## 3. Componentes por dominio (ejemplo: Ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38990cce",
   "metadata": {},
   "source": [
    "### 3.1 Data Product Ventas\n",
    "- Owner: Equipo de Ventas.\n",
    "- Fuentes: Kafka (transacciones), archivos batch (devoluciones).\n",
    "- Storage: S3 `s3://mesh/ventas/raw/`, `s3://mesh/ventas/curated/`.\n",
    "- API: FastAPI endpoint `/ventas/v1/daily-revenue` con contrato OpenAPI.\n",
    "- Features: `cliente_total_compras_30d`, `cliente_num_transacciones_7d`.\n",
    "- SLO: latencia p99 < 15 min, disponibilidad > 99.9%.\n",
    "- DocumentaciÃ³n: README, diagramas, changelog.\n",
    "\n",
    "### 3.2 Pipeline Ventas\n",
    "- Airflow DAG propio del equipo, con validaciones GE y alertas.\n",
    "- Escribe features a Feast (offline store: S3 Parquet, online: Redis).\n",
    "- Emite linaje a DataHub vÃ­a OpenLineage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d910d0",
   "metadata": {},
   "source": [
    "## 4. Feature Store centralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acb149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '# feature_repo/ventas_features.py', 'from feast import Entity, FeatureView, Field, FileSource', 'from feast.types import Float32, Int64', 'from datetime import timedelta', '', \"cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\", '', 'ventas_source = FileSource(', \"    path='s3://mesh/ventas/curated/features.parquet',\", \"    timestamp_field='event_timestamp'\", ')', '', 'ventas_fv = FeatureView(', \"    name='ventas_features',\", '    entities=[cliente],', '    ttl=timedelta(days=30),', '    schema=[', \"        Field(name='total_compras_30d', dtype=Float32),\", \"        Field(name='num_transacciones_7d', dtype=Int64),\", '    ],', '    source=ventas_source,', \"    owner='ventas-team@empresa.com'\", ')', '']\n"
     ]
    }
   ],
   "source": [
    "feast_config = r'''\n",
    "# feature_repo/ventas_features.py\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "cliente = Entity(name='cliente_id', join_keys=['cliente_id'])\n",
    "\n",
    "ventas_source = FileSource(\n",
    "    path='s3://mesh/ventas/curated/features.parquet',\n",
    "    timestamp_field='event_timestamp'\n",
    ")\n",
    "\n",
    "ventas_fv = FeatureView(\n",
    "    name='ventas_features',\n",
    "    entities=[cliente],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name='total_compras_30d', dtype=Float32),\n",
    "        Field(name='num_transacciones_7d', dtype=Int64),\n",
    "    ],\n",
    "    source=ventas_source,\n",
    "    owner='ventas-team@empresa.com'\n",
    ")\n",
    "\n",
    "# Similarmente para logistica_features, producto_features, etc.\n",
    "'''\n",
    "print(feast_config.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07846920",
   "metadata": {},
   "source": [
    "## 5. Gobernanza federada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986b720",
   "metadata": {},
   "source": [
    "- PolÃ­ticas globales:\n",
    "  - Todo PII enmascarado en datasets compartidos.\n",
    "  - Validaciones mÃ­nimas de calidad (Great Expectations).\n",
    "  - Linaje obligatorio (OpenLineage).\n",
    "  - Versionado semÃ¡ntico de APIs.\n",
    "- AutonomÃ­a local:\n",
    "  - Cada dominio elige su stack de transformaciÃ³n (Spark/Pandas/dbt).\n",
    "  - Frecuencia de actualizaciones segÃºn SLO propio.\n",
    "  - Esquemas propios, evolucionables con compatibilidad (Avro/Protobuf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480061b",
   "metadata": {},
   "source": [
    "## 6. Checklist de implementaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9e753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â˜ 1. Definir dominios y owners (RACI)\n",
      "â˜ 2. Crear buckets S3 por dominio (ventas/, logistica/, producto/)\n",
      "â˜ 3. DAG Airflow por dominio con validaciones y linaje\n",
      "â˜ 4. APIs FastAPI versionadas (OpenAPI specs)\n",
      "â˜ 5. Feature definitions en Feast por dominio\n",
      "â˜ 6. Feast apply y materialize-incremental en CI/CD\n",
      "â˜ 7. DataHub registrar data products con metadata\n",
      "â˜ 8. PolÃ­ticas IAM federadas (admin global + roles por dominio)\n",
      "â˜ 9. Dashboard Grafana multi-dominio con SLOs\n",
      "â˜ 10. Contratos de calidad (data contracts) versionados\n",
      "â˜ 11. Onboarding docs para nuevos dominios\n",
      "â˜ 12. Incident response playbook cross-domain\n",
      "â˜ 13. Cost allocation tags por dominio\n",
      "â˜ 14. Training model multi-dominio (consume features de Feast)\n",
      "â˜ 15. Tests de integraciÃ³n cross-domain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checklist = '''\n",
    "â˜ 1. Definir dominios y owners (RACI)\n",
    "â˜ 2. Crear buckets S3 por dominio (ventas/, logistica/, producto/)\n",
    "â˜ 3. DAG Airflow por dominio con validaciones y linaje\n",
    "â˜ 4. APIs FastAPI versionadas (OpenAPI specs)\n",
    "â˜ 5. Feature definitions en Feast por dominio\n",
    "â˜ 6. Feast apply y materialize-incremental en CI/CD\n",
    "â˜ 7. DataHub registrar data products con metadata\n",
    "â˜ 8. PolÃ­ticas IAM federadas (admin global + roles por dominio)\n",
    "â˜ 9. Dashboard Grafana multi-dominio con SLOs\n",
    "â˜ 10. Contratos de calidad (data contracts) versionados\n",
    "â˜ 11. Onboarding docs para nuevos dominios\n",
    "â˜ 12. Incident response playbook cross-domain\n",
    "â˜ 13. Cost allocation tags por dominio\n",
    "â˜ 14. Training model multi-dominio (consume features de Feast)\n",
    "â˜ 15. Tests de integraciÃ³n cross-domain\n",
    "'''\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68ad77",
   "metadata": {},
   "source": [
    "## 7. Entregables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a040e",
   "metadata": {},
   "source": [
    "- Documento de diseÃ±o Data Mesh con principios y responsabilidades.\n",
    "- Repositorio multi-dominio (monorepo o multi-repo).\n",
    "- Feature store funcional con features de â‰¥3 dominios.\n",
    "- CatÃ¡logo DataHub con linaje cross-domain.\n",
    "- APIs documentadas (Swagger/OpenAPI) por dominio.\n",
    "- Dashboard unificado con mÃ©tricas de todos los dominios.\n",
    "- Modelo ML entrenado consumiendo features federadas.\n",
    "- PresentaciÃ³n ejecutiva (slides) con resultados y aprendizajes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe39ba",
   "metadata": {},
   "source": [
    "## 8. EvaluaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ac420",
   "metadata": {},
   "source": [
    "- AutonomÃ­a: Â¿cada dominio opera independiente?\n",
    "- Gobernanza: Â¿polÃ­ticas aplicadas consistentemente?\n",
    "- Feature store: Â¿features accesibles y versionadas?\n",
    "- Observabilidad: Â¿linaje y mÃ©tricas cross-domain?\n",
    "- Escalabilidad: Â¿fÃ¡cil agregar nuevos dominios?\n",
    "- Costos: Â¿optimizaciÃ³n por dominio visible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "\n",
    "**Siguiente â†’:** Final del Nivel ğŸ‰\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Senior:**\n",
    "- [ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [ğŸ—ï¸ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y prÃ¡ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [ğŸ“Š Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
