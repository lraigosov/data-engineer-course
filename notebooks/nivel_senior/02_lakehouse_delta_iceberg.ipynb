{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da7d8cb",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)\n",
    "\n",
    "Objetivo: comprender los principios de Lakehouse y practicar un flujo b√°sico con Parquet (local) y notas de c√≥mo migrar a Delta Lake o Apache Iceberg.\n",
    "\n",
    "- Duraci√≥n: 120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Mid 03 (AWS/S3) y 07 (Particionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a71667",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Lakehouse Architecture: Unificando Data Warehouse y Data Lake**\n",
    "\n",
    "**La Evoluci√≥n de Arquitecturas de Datos:**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 1 (2000s): Data Warehouse                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  Structured Data ‚Üí RDBMS (Oracle)  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  OLAP ‚Üí Star/Snowflake Schema      ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  BI Tools ‚Üí SQL Queries            ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ ACID, Performance      ‚ùå Caro, No soporta ML        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 2 (2010s): Data Lake                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  All Data (structured + unstr.)    ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Üí S3/HDFS (cheap storage)         ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Üí Spark/Presto (compute layer)    ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ Escalable, Barato    ‚ùå No ACID, Data Swamp          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 3 (2020s): LAKEHOUSE                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Metadata Layer (Delta/Ice)  ‚îÇ  ‚îÇ ‚Üê Transacciones  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Columnar Format (Parquet)   ‚îÇ  ‚îÇ ‚Üê Performance    ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Object Storage (S3/ADLS)    ‚îÇ  ‚îÇ ‚Üê Escalabilidad  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ ACID + Escala + Barato + ML/BI    ‚ùå Complejidad     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**¬øQu√© es un Lakehouse?**\n",
    "\n",
    "Arquitectura que combina lo mejor de **Data Warehouse** (transacciones, performance) con **Data Lake** (escalabilidad, costo-efectividad).\n",
    "\n",
    "**Componentes Clave:**\n",
    "\n",
    "```python\n",
    "lakehouse_stack = {\n",
    "    'Storage Layer': {\n",
    "        'Technology': 'S3, ADLS, GCS',\n",
    "        'Format': 'Parquet (columnar)',\n",
    "        'Cost': '$0.023/GB/mes',\n",
    "        'Benefit': 'Almacenamiento infinito y barato'\n",
    "    },\n",
    "    'Metadata Layer': {\n",
    "        'Technology': 'Delta Lake, Apache Iceberg, Apache Hudi',\n",
    "        'Features': 'ACID, Time Travel, Schema Evolution',\n",
    "        'Benefit': 'Transacciones sobre object storage'\n",
    "    },\n",
    "    'Catalog Layer': {\n",
    "        'Technology': 'AWS Glue, Unity Catalog, Hive Metastore',\n",
    "        'Features': 'Metadata management, Permissions',\n",
    "        'Benefit': 'Single source of truth'\n",
    "    },\n",
    "    'Compute Layer': {\n",
    "        'Technology': 'Spark, Presto, Athena, Trino',\n",
    "        'Features': 'SQL queries, Distributed processing',\n",
    "        'Benefit': 'Separaci√≥n storage/compute'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Table Formats Comparison:**\n",
    "\n",
    "| Feature | **Delta Lake** | **Apache Iceberg** | **Apache Hudi** |\n",
    "|---------|----------------|-------------------|-----------------|\n",
    "| **Creator** | Databricks (2019) | Netflix (2017) | Uber (2016) |\n",
    "| **ACID** | ‚úÖ Optimistic locking | ‚úÖ Snapshot isolation | ‚úÖ MVCC |\n",
    "| **Time Travel** | ‚úÖ Version history | ‚úÖ Snapshot-based | ‚úÖ Commit timeline |\n",
    "| **Schema Evolution** | ‚úÖ ADD/DROP cols | ‚úÖ Full evolution | ‚úÖ Partial |\n",
    "| **Partition Evolution** | ‚ùå Manual | ‚úÖ Hidden partitions | ‚ùå Manual |\n",
    "| **Streaming** | ‚úÖ Spark Streaming | ‚úÖ Flink, Spark | ‚úÖ DeltaStreamer |\n",
    "| **Engines** | Spark, Presto, Trino | Spark, Flink, Trino, Athena | Spark, Presto |\n",
    "| **Maturity** | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê Medium |\n",
    "| **Governance** | Databricks (vendor) | Apache (neutral) | Apache (neutral) |\n",
    "| **Best For** | Databricks users | Multi-engine, AWS | Upserts, CDC |\n",
    "\n",
    "**Real-World Adoption:**\n",
    "\n",
    "```\n",
    "Delta Lake:\n",
    "  - Databricks customers (obviamente)\n",
    "  - Comcast (Petabyte-scale)\n",
    "  - Riot Games (Gaming analytics)\n",
    "\n",
    "Apache Iceberg:\n",
    "  - Netflix (originator, 100+ PB)\n",
    "  - Apple (iCloud data)\n",
    "  - Adobe (Experience Cloud)\n",
    "  - AWS (Athena native support)\n",
    "\n",
    "Apache Hudi:\n",
    "  - Uber (ride-hailing data)\n",
    "  - Amazon (internal use)\n",
    "  - Disney+ (streaming analytics)\n",
    "```\n",
    "\n",
    "**Arquitectura de Datos Moderna:**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     DATA SOURCES                         ‚îÇ\n",
    "‚îÇ  Databases, APIs, Streams, Files, SaaS, IoT             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Ingest (Fivetran, Airbyte, Custom)\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    BRONZE LAYER                          ‚îÇ\n",
    "‚îÇ              (Raw data, append-only)                     ‚îÇ\n",
    "‚îÇ  Format: Parquet, Delta, Iceberg                        ‚îÇ\n",
    "‚îÇ  Partitioned by: ingestion_date                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Transformation (dbt, Spark)\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    SILVER LAYER                          ‚îÇ\n",
    "‚îÇ         (Cleaned, validated, deduplicated)              ‚îÇ\n",
    "‚îÇ  Format: Delta Lake (ACID needed)                       ‚îÇ\n",
    "‚îÇ  Partitioned by: business dimensions                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Aggregation, Business Logic\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     GOLD LAYER                           ‚îÇ\n",
    "‚îÇ        (Aggregated, business-ready datasets)            ‚îÇ\n",
    "‚îÇ  Format: Delta Lake or Iceberg                          ‚îÇ\n",
    "‚îÇ  Consumed by: BI Tools, ML Models, APIs                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Beneficios del Lakehouse:**\n",
    "\n",
    "1. **Unified Platform**:\n",
    "   - Single storage para BI, ML, Data Science\n",
    "   - No m√°s \"copy data from DW to DL for ML\"\n",
    "\n",
    "2. **Cost Reduction**:\n",
    "   - Storage: $0.023/GB vs $25/TB (Snowflake)\n",
    "   - Compute: Pay-per-query (Athena) o Spot instances\n",
    "\n",
    "3. **Performance**:\n",
    "   - Columnar format (10x faster queries)\n",
    "   - Partition pruning (scan solo datos relevantes)\n",
    "   - Caching + predicate pushdown\n",
    "\n",
    "4. **Governance**:\n",
    "   - ACID garantiza consistencia\n",
    "   - Time Travel para auditor√≠a\n",
    "   - Fine-grained access control\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - M√∫ltiples engines (no vendor lock-in)\n",
    "   - Schema evolution sin downtime\n",
    "   - Support structured + semi-structured\n",
    "\n",
    "**¬øCu√°ndo usar Lakehouse?**\n",
    "\n",
    "‚úÖ **S√ç usar cuando:**\n",
    "- Vol√∫menes > 100 TB\n",
    "- Necesitas ML + BI sobre mismos datos\n",
    "- M√∫ltiples teams con diferentes tools\n",
    "- Budget limitado vs DW tradicional\n",
    "\n",
    "‚ùå **NO usar cuando:**\n",
    "- Datasets < 1 TB (PostgreSQL suficiente)\n",
    "- Team peque√±o (< 5) con skills limitados\n",
    "- Latencia cr√≠tica < 100ms (considerar OLTP)\n",
    "- Compliance requiere on-prem (limitaciones cloud)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822026",
   "metadata": {},
   "source": [
    "## 1. Lakehouse en pocas l√≠neas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31879",
   "metadata": {},
   "source": [
    "### üìê **Lakehouse Fundamentals: Storage + Metadata + Catalog**\n",
    "\n",
    "**1. Storage Layer: Object Storage como Foundation**\n",
    "\n",
    "```python\n",
    "# ¬øPor qu√© S3/ADLS/GCS y no HDFS o RDBMS?\n",
    "\n",
    "storage_comparison = {\n",
    "    'HDFS (Hadoop)': {\n",
    "        'cost': '$$$',\n",
    "        'scalability': 'Limited by cluster',\n",
    "        'durability': '99.9% (3 replicas)',\n",
    "        'latency': 'Low (local)',\n",
    "        'ops_complexity': 'High (manage cluster)',\n",
    "        'verdict': '‚ùå Legacy, avoid for new projects'\n",
    "    },\n",
    "    'RDBMS (PostgreSQL)': {\n",
    "        'cost': '$$$$',\n",
    "        'scalability': 'Vertical (TB-scale)',\n",
    "        'durability': '99.99% (replication)',\n",
    "        'latency': 'Very Low (<10ms)',\n",
    "        'ops_complexity': 'Medium',\n",
    "        'verdict': '‚úÖ For OLTP, ‚ùå for Analytics at scale'\n",
    "    },\n",
    "    'Object Storage (S3)': {\n",
    "        'cost': '$',\n",
    "        'scalability': 'Infinite (PB-EB scale)',\n",
    "        'durability': '99.999999999% (11 nines)',\n",
    "        'latency': 'Medium (network)',\n",
    "        'ops_complexity': 'Very Low (managed)',\n",
    "        'verdict': '‚úÖ Perfect for Lakehouse'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**S3 as Database? The Challenges:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Problem 1: No ACID transactions\n",
    "# Dos writers simult√°neos ‚Üí race condition\n",
    "writer_1_writes_file('s3://bucket/data/part-001.parquet')\n",
    "writer_2_writes_file('s3://bucket/data/part-001.parquet')  # Overwrite!\n",
    "\n",
    "# ‚ùå Problem 2: No consistency\n",
    "list_objects('s3://bucket/data/')  # May not see latest write immediately\n",
    "\n",
    "# ‚ùå Problem 3: No indexing\n",
    "# Para find record con id=X ‚Üí scan all files (slow!)\n",
    "\n",
    "# ‚ùå Problem 4: No schema enforcement\n",
    "# Nada previene que alguien escriba schema incompatible\n",
    "```\n",
    "\n",
    "**2. Metadata Layer: The Secret Sauce**\n",
    "\n",
    "**Delta Lake Transaction Log:**\n",
    "\n",
    "```\n",
    "table_path/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json  ‚Üê Version 0 (CREATE TABLE)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json  ‚Üê Version 1 (INSERT)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json  ‚Üê Version 2 (UPDATE)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000003.json  ‚Üê Version 3 (DELETE)\n",
    "‚îú‚îÄ‚îÄ part-00000-xxx.parquet\n",
    "‚îú‚îÄ‚îÄ part-00001-xxx.parquet\n",
    "‚îî‚îÄ‚îÄ part-00002-xxx.parquet\n",
    "\n",
    "# Contenido de 00000000000000000001.json:\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"timestamp\": 1730246400000,\n",
    "    \"operation\": \"WRITE\",\n",
    "    \"operationMetrics\": {\n",
    "      \"numFiles\": \"2\",\n",
    "      \"numOutputRows\": \"1000\"\n",
    "    }\n",
    "  },\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-xxx.parquet\",\n",
    "    \"size\": 52428800,\n",
    "    \"partitionValues\": {\"year\": \"2025\", \"month\": \"10\"},\n",
    "    \"dataChange\": true,\n",
    "    \"stats\": \"{\\\"numRecords\\\":500,\\\"minValues\\\":{\\\"id\\\":1},\\\"maxValues\\\":{\\\"id\\\":500}}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**C√≥mo funciona ACID:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ Atomicity: All-or-nothing\n",
    "def write_to_delta(df, table_path):\n",
    "    # Step 1: Write Parquet files (no one can see yet)\n",
    "    temp_files = df.write.parquet(f\"{table_path}/_tmp/\")\n",
    "    \n",
    "    # Step 2: Create transaction log entry\n",
    "    version = get_next_version(table_path)\n",
    "    log_entry = {\n",
    "        \"add\": [{\"path\": f, \"size\": size} for f in temp_files]\n",
    "    }\n",
    "    \n",
    "    # Step 3: Atomic commit (write JSON file)\n",
    "    write_json(f\"{table_path}/_delta_log/{version:020d}.json\", log_entry)\n",
    "    # Solo cuando este archivo existe ‚Üí datos visibles\n",
    "    \n",
    "    # Si crash antes del Step 3 ‚Üí temp files hu√©rfanos (no problema)\n",
    "\n",
    "# ‚úÖ Consistency: Schema enforcement\n",
    "def validate_write(df, existing_schema):\n",
    "    if df.schema != existing_schema:\n",
    "        if not compatible(df.schema, existing_schema):\n",
    "            raise SchemaIncompatibleException()\n",
    "\n",
    "# ‚úÖ Isolation: Optimistic concurrency control\n",
    "def concurrent_write(df1, df2):\n",
    "    # Writer 1 reads version 5, writes based on v5\n",
    "    # Writer 2 reads version 5, writes based on v5\n",
    "    \n",
    "    # Writer 1 tries to commit version 6\n",
    "    if current_version == 5:  # OK\n",
    "        commit_version_6()\n",
    "    \n",
    "    # Writer 2 tries to commit version 6\n",
    "    if current_version == 5:  # CONFLICT! (now it's 6)\n",
    "        raise ConcurrentModificationException()\n",
    "        # Writer 2 must retry: read v6, apply changes, commit v7\n",
    "\n",
    "# ‚úÖ Durability: S3's 11 nines\n",
    "# Once committed, data is durable (S3 guarantee)\n",
    "```\n",
    "\n",
    "**Time Travel (Versioning):**\n",
    "\n",
    "```python\n",
    "# Read current version\n",
    "df = spark.read.format(\"delta\").load(\"s3://bucket/sales\")\n",
    "\n",
    "# Read version from 7 days ago\n",
    "df_v7d = spark.read.format(\"delta\").option(\"versionAsOf\", 7).load(...)\n",
    "\n",
    "# Read version at specific timestamp\n",
    "df_oct20 = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20\") \\\n",
    "    .load(...)\n",
    "\n",
    "# Use cases:\n",
    "# - Reproducibility: \"Show me data exactly as ML model saw it\"\n",
    "# - Auditing: \"What changed between yesterday and today?\"\n",
    "# - Rollback: \"Undo accidental DELETE\"\n",
    "# - A/B testing: \"Compare old vs new transformations\"\n",
    "```\n",
    "\n",
    "**3. Catalog Layer: Metadata Management**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              DATA CATALOG                         ‚îÇ\n",
    "‚îÇ                                                   ‚îÇ\n",
    "‚îÇ  Database: sales_prod                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Table: orders                                ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Location: s3://bucket/gold/orders/      ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Format: delta                            ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Schema: {id: bigint, total: double, ...}‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Partitions: [year, month]               ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Owner: data-team@company.com            ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Tags: [PII, critical]                   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îî‚îÄ Last Updated: 2025-10-30 14:30:00       ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Table: customers                             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Table: products                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Catalog Implementations:**\n",
    "\n",
    "| Catalog | Provider | Best For | Limitations |\n",
    "|---------|----------|----------|-------------|\n",
    "| **Hive Metastore** | Apache | Open-source, Spark native | Single point of failure |\n",
    "| **AWS Glue Catalog** | AWS | Serverless, AWS-integrated | AWS lock-in |\n",
    "| **Unity Catalog** | Databricks | Unified governance | Databricks only |\n",
    "| **Polaris** | Snowflake | Open-source Iceberg | New (2024) |\n",
    "\n",
    "**Example: Creating Table in Catalog:**\n",
    "\n",
    "```python\n",
    "# Spark + Delta Lake + Glue Catalog\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE sales_prod.orders\n",
    "  USING delta\n",
    "  LOCATION 's3://bucket/gold/orders/'\n",
    "  PARTITIONED BY (year, month)\n",
    "  TBLPROPERTIES (\n",
    "    'delta.dataSkippingNumIndexedCols' = '5',\n",
    "    'delta.deletedFileRetentionDuration' = 'interval 7 days',\n",
    "    'owner' = 'data-team@company.com',\n",
    "    'pii' = 'true'\n",
    "  )\n",
    "  AS SELECT * FROM staging.orders_raw\n",
    "\"\"\")\n",
    "\n",
    "# Query from any engine\n",
    "# Athena:\n",
    "SELECT * FROM sales_prod.orders WHERE year=2025 AND month=10\n",
    "\n",
    "# Presto:\n",
    "SELECT * FROM glue.sales_prod.orders WHERE total > 1000\n",
    "\n",
    "# Spark:\n",
    "spark.table(\"sales_prod.orders\").filter(\"year = 2025\").show()\n",
    "```\n",
    "\n",
    "**Metadata Caching & Performance:**\n",
    "\n",
    "```python\n",
    "# Problem: Reading metadata for each query is slow\n",
    "# Solution: Caching\n",
    "\n",
    "# Delta Lake: Stats in transaction log\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000.parquet\",\n",
    "    \"stats\": \"{\\\"numRecords\\\":1000,\\\"minValues\\\":{\\\"id\\\":1,\\\"date\\\":\\\"2025-10-01\\\"},\\\"maxValues\\\":{\\\"id\\\":1000,\\\"date\\\":\\\"2025-10-31\\\"}}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Query: SELECT * FROM orders WHERE id = 500\n",
    "# Engine reads stats ‚Üí knows id ‚àà [1, 1000] ‚Üí scan this file\n",
    "# Query: SELECT * FROM orders WHERE id = 5000\n",
    "# Engine reads stats ‚Üí knows id ‚àà [1, 1000] ‚Üí SKIP this file (data skipping)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a20ee",
   "metadata": {},
   "source": [
    "- Tabla de datos en formato columna (Parquet) sobre object storage.\n",
    "- Transaccionalidad y versiones con capas de metadatos (Delta/Iceberg/Hudi).\n",
    "- Cat√°logo central (Glue/Unity/Metastore) y gobernanza integrada.\n",
    "- Lectores: engines SQL (Athena/Trino/Spark) + ML + BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a329f1",
   "metadata": {},
   "source": [
    "## 2. Hands-on: tabla Parquet particionada local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c1f50",
   "metadata": {},
   "source": [
    "### üíæ **Parquet: The Columnar Storage Format**\n",
    "\n",
    "**¬øPor qu√© Parquet y no CSV/JSON?**\n",
    "\n",
    "**Query Performance Example:**\n",
    "\n",
    "```python\n",
    "# Dataset: 1M filas √ó 100 columnas = 100M valores\n",
    "# Query: SELECT AVG(salary) FROM employees WHERE department = 'Engineering'\n",
    "\n",
    "# ‚ùå CSV (Row-based):\n",
    "for row in read_csv('employees.csv'):  # Lee TODAS las columnas\n",
    "    if row['department'] == 'Engineering':\n",
    "        salaries.append(row['salary'])  # Solo usa 2 columnas\n",
    "\n",
    "# I/O: Lee 100M valores\n",
    "# Time: ~30 segundos\n",
    "\n",
    "# ‚úÖ Parquet (Columnar):\n",
    "departments = read_column('employees.parquet', 'department')\n",
    "salaries = read_column('employees.parquet', 'salary')\n",
    "avg = mean([s for d, s in zip(departments, salaries) if d == 'Engineering'])\n",
    "\n",
    "# I/O: Lee 2M valores (solo 2 columnas)\n",
    "# Time: ~0.6 segundos (50x faster!)\n",
    "```\n",
    "\n",
    "**Parquet File Structure:**\n",
    "\n",
    "```\n",
    "file.parquet\n",
    "‚îú‚îÄ‚îÄ Header (4 bytes magic: \"PAR1\")\n",
    "‚îú‚îÄ‚îÄ Row Group 1 (default: 128 MB)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: id\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Data Pages (compressed)\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Statistics (min, max, null_count)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: name\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: salary\n",
    "‚îú‚îÄ‚îÄ Row Group 2\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: id\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: name\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: salary\n",
    "‚îú‚îÄ‚îÄ Footer Metadata\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Schema\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column statistics (per row group)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Compression codec\n",
    "‚îî‚îÄ‚îÄ Footer (4 bytes magic: \"PAR1\")\n",
    "```\n",
    "\n",
    "**Compression Codecs:**\n",
    "\n",
    "```python\n",
    "df.to_parquet('data.parquet', compression='snappy')\n",
    "\n",
    "# Benchmark: 1M rows, 10 columns\n",
    "compression_results = {\n",
    "    'none': {\n",
    "        'size_mb': 100,\n",
    "        'write_s': 2.1,\n",
    "        'read_s': 1.5,\n",
    "        'ratio': '1x'\n",
    "    },\n",
    "    'snappy': {\n",
    "        'size_mb': 33,\n",
    "        'write_s': 2.8,\n",
    "        'read_s': 1.8,\n",
    "        'ratio': '3x',\n",
    "        'verdict': '‚úÖ Default (balance speed/compression)'\n",
    "    },\n",
    "    'gzip': {\n",
    "        'size_mb': 20,\n",
    "        'write_s': 8.5,\n",
    "        'read_s': 4.2,\n",
    "        'ratio': '5x',\n",
    "        'verdict': '‚ö†Ô∏è Better compression, slower'\n",
    "    },\n",
    "    'zstd': {\n",
    "        'size_mb': 22,\n",
    "        'write_s': 3.5,\n",
    "        'read_s': 2.0,\n",
    "        'ratio': '4.5x',\n",
    "        'verdict': '‚úÖ Modern (better than snappy)'\n",
    "    },\n",
    "    'lz4': {\n",
    "        'size_mb': 35,\n",
    "        'write_s': 2.2,\n",
    "        'read_s': 1.6,\n",
    "        'ratio': '2.8x',\n",
    "        'verdict': '‚ö° Fastest compression'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Encoding Schemes:**\n",
    "\n",
    "```python\n",
    "# Plain Encoding (default for small datasets)\n",
    "values = [100, 200, 150, 175, 100]\n",
    "encoded = b'\\x64\\x00\\xc8\\x00\\x96\\x00\\xaf\\x00\\x64\\x00'  # Raw bytes\n",
    "\n",
    "# Dictionary Encoding (for low cardinality)\n",
    "# Column: [\"Engineering\", \"Sales\", \"Engineering\", \"Engineering\", \"Sales\"]\n",
    "dictionary = [\"Engineering\", \"Sales\"]\n",
    "indices = [0, 1, 0, 0, 1]  # Store indices (2 bits each vs 11 bytes per string)\n",
    "# Compression: ~85%\n",
    "\n",
    "# Run-Length Encoding (for repeated values)\n",
    "# Column: [100, 100, 100, 200, 200, 300]\n",
    "rle = [(100, 3), (200, 2), (300, 1)]  # (value, count)\n",
    "\n",
    "# Delta Encoding (for sorted/incremental data)\n",
    "# Column: [1000, 1001, 1002, 1003, 1004]\n",
    "delta = [1000, 1, 1, 1, 1]  # Base + deltas\n",
    "```\n",
    "\n",
    "**Partition Pruning (Data Skipping):**\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "‚îú‚îÄ‚îÄ year=2023/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ month=02/data.parquet\n",
    "‚îú‚îÄ‚îÄ year=2024/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ month=12/data.parquet\n",
    "‚îî‚îÄ‚îÄ year=2025/\n",
    "    ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "    ‚îî‚îÄ‚îÄ month=10/data.parquet  ‚Üê Only scan this!\n",
    "\n",
    "# Query:\n",
    "SELECT * FROM sales WHERE year=2025 AND month=10\n",
    "\n",
    "# Without partitioning: Scan 7 files\n",
    "# With partitioning: Scan 1 file (85% reduction!)\n",
    "```\n",
    "\n",
    "**Small Files Problem:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Anti-pattern: Too many small files\n",
    "for record in stream:\n",
    "    df = pd.DataFrame([record])\n",
    "    df.to_parquet(f's3://bucket/data/record_{record[\"id\"]}.parquet')\n",
    "\n",
    "# Result: 1M files √ó 1 KB each = Overhead disaster!\n",
    "# Athena cost: $5/TB scanned + $0.002/file = $$$$\n",
    "\n",
    "# ‚úÖ Solution 1: Buffering\n",
    "buffer = []\n",
    "for record in stream:\n",
    "    buffer.append(record)\n",
    "    if len(buffer) >= 10000:\n",
    "        pd.DataFrame(buffer).to_parquet(f's3://bucket/data/batch_{timestamp}.parquet')\n",
    "        buffer = []\n",
    "\n",
    "# ‚úÖ Solution 2: Compaction (OPTIMIZE in Delta)\n",
    "spark.sql(\"OPTIMIZE sales_table\")\n",
    "# Merges small files into 128MB-1GB files\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Over-partitioning (too granular)\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\", \"hour\", \"customer_id\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Result: Millions of partitions ‚Üí slow metadata operations\n",
    "\n",
    "# ‚úÖ Balanced partitioning\n",
    "df.write.partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Target: 128MB-1GB per partition\n",
    "# Rule: Partition columns used in 80%+ of queries\n",
    "\n",
    "# ‚ö° Z-ordering (Delta Lake optimization)\n",
    "spark.sql(\"OPTIMIZE sales_table ZORDER BY (customer_id, product_id)\")\n",
    "# Co-locates related data for better data skipping\n",
    "```\n",
    "\n",
    "**Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# V1: Initial schema\n",
    "df_v1 = pd.DataFrame({\n",
    "    'id': [1, 2],\n",
    "    'name': ['Alice', 'Bob']\n",
    "})\n",
    "df_v1.to_parquet('users_v1.parquet')\n",
    "\n",
    "# V2: Add column (compatible)\n",
    "df_v2 = pd.DataFrame({\n",
    "    'id': [3, 4],\n",
    "    'name': ['Charlie', 'Diana'],\n",
    "    'email': ['c@x.com', 'd@x.com']  # New column\n",
    "})\n",
    "df_v2.to_parquet('users_v2.parquet')\n",
    "\n",
    "# Reading both files:\n",
    "df = pd.concat([\n",
    "    pd.read_parquet('users_v1.parquet'),  # email will be null\n",
    "    pd.read_parquet('users_v2.parquet')\n",
    "])\n",
    "# Parquet handles missing columns gracefully\n",
    "\n",
    "# ‚ùå Breaking change: Rename/delete column\n",
    "df_v3 = pd.DataFrame({\n",
    "    'user_id': [5, 6],  # Renamed from 'id'\n",
    "    'full_name': ['Eve', 'Frank']  # Renamed from 'name'\n",
    "})\n",
    "# This will break queries expecting 'id' column!\n",
    "```\n",
    "\n",
    "**Parquet + Pandas/Polars:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read specific columns (projection)\n",
    "df = pd.read_parquet('sales.parquet', columns=['id', 'total'])\n",
    "\n",
    "# Read with filters (predicate pushdown)\n",
    "df = pd.read_parquet('sales.parquet', \n",
    "                     filters=[('year', '=', 2025), ('total', '>', 1000)])\n",
    "\n",
    "# Read row groups (chunked reading for large files)\n",
    "parquet_file = pq.ParquetFile('sales.parquet')\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    df = batch.to_pandas()\n",
    "    process(df)\n",
    "\n",
    "# Read from S3 directly\n",
    "df = pd.read_parquet('s3://bucket/sales/year=2025/month=10/*.parquet')\n",
    "```\n",
    "\n",
    "**Monitoring Parquet Health:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def analyze_parquet_file(path):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    metadata = pf.metadata\n",
    "    \n",
    "    metrics = {\n",
    "        'num_row_groups': metadata.num_row_groups,\n",
    "        'num_rows': metadata.num_rows,\n",
    "        'num_columns': metadata.num_columns,\n",
    "        'file_size_mb': os.path.getsize(path) / (1024**2),\n",
    "        'avg_row_group_size_mb': os.path.getsize(path) / metadata.num_row_groups / (1024**2),\n",
    "        'compression': pf.schema_arrow.metadata.get(b'compression', b'unknown').decode()\n",
    "    }\n",
    "    \n",
    "    # Health checks\n",
    "    if metrics['avg_row_group_size_mb'] < 64:\n",
    "        print(\"‚ö†Ô∏è Row groups too small (target: 128MB)\")\n",
    "    \n",
    "    if metrics['file_size_mb'] < 10:\n",
    "        print(\"‚ö†Ô∏è File too small (target: >100MB)\")\n",
    "    \n",
    "    if metrics['num_row_groups'] > 10:\n",
    "        print(\"‚ö†Ô∏è Too many row groups (consider repartitioning)\")\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7878388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets\\\\processed\\\\lakehouse_demo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path('datasets/processed/lakehouse_demo')\n",
    "(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'id':[1,2,3,4],\n",
    "  'fecha':['2025-10-01','2025-10-02','2025-10-02','2025-10-03'],\n",
    "  'producto_id':[101,102,101,103],\n",
    "  'cantidad':[1,2,1,3],\n",
    "  'precio':[100.0,50.0,100.0,20.0]\n",
    "})\n",
    "df['total'] = df['cantidad'] * df['precio']\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.strftime('%Y-%m')\n",
    "\n",
    "for (anio, mes), part in df.groupby(['anio','mes']):\n",
    "    part_dir = BASE / f'anio={anio}' / f'mes={mes}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fp = part_dir / f'ventas_{int(time.time())}.parquet'\n",
    "    part.to_parquet(fp, index=False)\n",
    "str(BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457603",
   "metadata": {},
   "source": [
    "### 2.1 Lectura particionada y pruning manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9c198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fecha",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "producto_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cantidad",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "anio",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "mes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "afcaab6a-f678-4f6f-a13c-47405adb18c2",
       "rows": [
        [
         "0",
         "1",
         "2025-10-01",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "1",
         "2",
         "2025-10-02",
         "102",
         "2",
         "50.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "2",
         "3",
         "2025-10-02",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "3",
         "4",
         "2025-10-03",
         "103",
         "3",
         "20.0",
         "60.0",
         "2025",
         "2025-10"
        ],
        [
         "0",
         "1",
         "2025-10-01",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fecha</th>\n",
       "      <th>producto_id</th>\n",
       "      <th>cantidad</th>\n",
       "      <th>precio</th>\n",
       "      <th>total</th>\n",
       "      <th>anio</th>\n",
       "      <th>mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       fecha  producto_id  cantidad  precio  total  anio      mes\n",
       "0   1  2025-10-01          101         1   100.0  100.0  2025  2025-10\n",
       "1   2  2025-10-02          102         2    50.0  100.0  2025  2025-10\n",
       "2   3  2025-10-02          101         1   100.0  100.0  2025  2025-10\n",
       "3   4  2025-10-03          103         3    20.0   60.0  2025  2025-10\n",
       "0   1  2025-10-01          101         1   100.0  100.0  2025  2025-10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = list((BASE / 'anio=2025' / 'mes=2025-10').glob('*.parquet'))\n",
    "pd.concat([pd.read_parquet(p) for p in parts]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549cff5",
   "metadata": {},
   "source": [
    "## 3. Delta Lake/Iceberg: c√≥mo y cu√°ndo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e4cb",
   "metadata": {},
   "source": [
    "### ‚ö° **Delta Lake vs Apache Iceberg: Deep Dive**\n",
    "\n",
    "**When to Choose Which?**\n",
    "\n",
    "```python\n",
    "decision_matrix = {\n",
    "    'Use Delta Lake if': [\n",
    "        'Already using Databricks',\n",
    "        'Primary engine is Spark',\n",
    "        'Need mature ecosystem (more tools)',\n",
    "        'Streaming workloads (Spark Structured Streaming)',\n",
    "        'Team familiar with Delta Lake'\n",
    "    ],\n",
    "    'Use Apache Iceberg if': [\n",
    "        'Multi-engine strategy (Spark + Trino + Flink + Athena)',\n",
    "        'AWS-centric (Athena native support)',\n",
    "        'Need hidden partitioning (automatic partition management)',\n",
    "        'Open governance important (Apache vs vendor)',\n",
    "        'Future-proofing (growing adoption)'\n",
    "    ],\n",
    "    'Use Apache Hudi if': [\n",
    "        'Heavy upsert/CDC workloads',\n",
    "        'Record-level updates critical',\n",
    "        'Uber-style use case'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Comparison:**\n",
    "\n",
    "**1. ACID Transactions:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Optimistic Concurrency Control\n",
    "@transaction\n",
    "def update_delta_table():\n",
    "    current_version = read_version()  # v5\n",
    "    # ... perform transformation ...\n",
    "    try:\n",
    "        commit_new_version(v6, based_on=v5)\n",
    "    except ConflictException:\n",
    "        # Another writer committed v6 first\n",
    "        retry_with_new_base(v6)\n",
    "\n",
    "# Apache Iceberg: Snapshot Isolation\n",
    "@transaction\n",
    "def update_iceberg_table():\n",
    "    snapshot_id = current_snapshot()\n",
    "    # ... perform transformation ...\n",
    "    new_snapshot = create_snapshot(changes)\n",
    "    atomic_swap(current_snapshot, new_snapshot)\n",
    "    # Readers on old snapshot unaffected\n",
    "```\n",
    "\n",
    "**2. Time Travel:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# Files: _delta_log/00000000000000000005.json\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 5) \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20 00:00:00\") \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "# Apache Iceberg\n",
    "# Files: metadata/snap-xxxx-1-yy.avro\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", 12345678) \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"as-of-timestamp\", \"1730246400000\") \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "# Iceberg advantage: Faster time travel (O(1) vs O(n))\n",
    "# Delta: Must replay transaction log from start\n",
    "# Iceberg: Direct snapshot lookup\n",
    "```\n",
    "\n",
    "**3. Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# ‚úÖ ADD COLUMN (supported)\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (discount DOUBLE)\")\n",
    "\n",
    "# ‚ö†Ô∏è RENAME COLUMN (manual migration needed)\n",
    "# Step 1: Add new column\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (customer_email STRING)\")\n",
    "# Step 2: Backfill\n",
    "spark.sql(\"UPDATE orders SET customer_email = email\")\n",
    "# Step 3: Drop old column\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN email\")\n",
    "\n",
    "# ‚ùå CHANGE COLUMN TYPE (not supported)\n",
    "# Must create new table\n",
    "\n",
    "# Apache Iceberg\n",
    "# ‚úÖ ADD COLUMN\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMN discount double\")\n",
    "\n",
    "# ‚úÖ RENAME COLUMN (supported!)\n",
    "spark.sql(\"ALTER TABLE orders RENAME COLUMN email TO customer_email\")\n",
    "\n",
    "# ‚úÖ DROP COLUMN\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN discount\")\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE TYPE (limited support)\n",
    "spark.sql(\"ALTER TABLE orders ALTER COLUMN id TYPE bigint\")  # int ‚Üí bigint OK\n",
    "# int ‚Üí string: Not supported (data loss risk)\n",
    "```\n",
    "\n",
    "**4. Partition Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Manual partition management\n",
    "# Initial: Partitioned by date\n",
    "df.write.partitionBy(\"date\").format(\"delta\").save(\"orders\")\n",
    "\n",
    "# Later: Want to partition by date + country\n",
    "# ‚ùå Problem: Must rewrite all data\n",
    "df_all = spark.read.format(\"delta\").load(\"orders\")\n",
    "df_all.write.partitionBy(\"date\", \"country\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"orders_new\")\n",
    "\n",
    "# Apache Iceberg: Hidden partitions (automatic!)\n",
    "# Initial: Partitioned by date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE orders (\n",
    "        id bigint,\n",
    "        date date,\n",
    "        country string,\n",
    "        total double\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(date))\n",
    "\"\"\")\n",
    "\n",
    "# Later: Change partitioning strategy (NO data rewrite!)\n",
    "spark.sql(\"ALTER TABLE orders DROP PARTITION FIELD days(date)\")\n",
    "spark.sql(\"ALTER TABLE orders ADD PARTITION FIELD bucket(10, country)\")\n",
    "\n",
    "# Iceberg tracks partition evolution in metadata\n",
    "# Old data: date partitions\n",
    "# New data: country buckets\n",
    "# Queries work seamlessly across both!\n",
    "```\n",
    "\n",
    "**5. File Management (VACUUM/OPTIMIZE):**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# OPTIMIZE: Compact small files\n",
    "spark.sql(\"OPTIMIZE orders\")\n",
    "# Target: 1GB files\n",
    "\n",
    "# Z-ORDER: Co-locate data\n",
    "spark.sql(\"OPTIMIZE orders ZORDER BY (customer_id, product_id)\")\n",
    "\n",
    "# VACUUM: Delete old files\n",
    "spark.sql(\"VACUUM orders RETAIN 168 HOURS\")  # Keep 7 days\n",
    "# Deletes:\n",
    "# - Old data files (overwritten by OPTIMIZE)\n",
    "# - Transaction log checkpoints\n",
    "\n",
    "# Apache Iceberg\n",
    "# REWRITE DATA FILES (equivalent to OPTIMIZE)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.rewrite_data_files(\n",
    "        table => 'db.orders',\n",
    "        options => map(\n",
    "            'target-file-size-bytes', '1073741824',  -- 1GB\n",
    "            'min-input-files', '5'\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# EXPIRE SNAPSHOTS (equivalent to VACUUM)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.expire_snapshots(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-23 00:00:00',\n",
    "        retain_last => 7\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# REMOVE ORPHAN FILES\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.remove_orphan_files(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-20 00:00:00'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**6. Multi-Engine Support:**\n",
    "\n",
    "| Engine | Delta Lake | Iceberg |\n",
    "|--------|------------|---------|\n",
    "| **Spark** | ‚úÖ Native | ‚úÖ Native |\n",
    "| **Presto/Trino** | ‚úÖ Connector | ‚úÖ Native |\n",
    "| **Flink** | ‚ùå Limited | ‚úÖ Native |\n",
    "| **AWS Athena** | ‚ö†Ô∏è Via manifests | ‚úÖ Native |\n",
    "| **Dremio** | ‚úÖ | ‚úÖ |\n",
    "| **Snowflake** | ‚ö†Ô∏è Via external tables | ‚úÖ Iceberg Tables |\n",
    "\n",
    "**7. Metadata Performance:**\n",
    "\n",
    "```python\n",
    "# Scenario: Table with 10,000 partitions\n",
    "\n",
    "# Delta Lake:\n",
    "# - Metadata: JSON files in _delta_log/\n",
    "# - List partitions: O(n) scan of transaction log\n",
    "# - Overhead: ~100MB metadata for large tables\n",
    "\n",
    "# Iceberg:\n",
    "# - Metadata: Avro files with manifest list ‚Üí manifest files\n",
    "# - List partitions: O(log n) via manifest index\n",
    "# - Overhead: ~10MB metadata for same table\n",
    "\n",
    "# Benchmark: SHOW PARTITIONS\n",
    "# Delta: 15 seconds\n",
    "# Iceberg: 0.5 seconds (30x faster!)\n",
    "```\n",
    "\n",
    "**8. Streaming Support:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Spark Structured Streaming native\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"orders\")\n",
    "\n",
    "stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints\") \\\n",
    "    .start(\"orders_processed\")\n",
    "\n",
    "# Apache Iceberg: Flink + Spark Streaming\n",
    "# Flink (primary streaming engine for Iceberg)\n",
    "TableEnvironment tableEnv = ...\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    CREATE TABLE orders (...)\n",
    "    WITH ('connector' = 'iceberg', ...)\n",
    "\"\"\")\n",
    "\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    INSERT INTO orders_processed\n",
    "    SELECT * FROM orders\n",
    "    WHERE total > 1000\n",
    "\"\"\")\n",
    "\n",
    "# Spark Streaming (also supported)\n",
    "spark.readStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"orders\")\n",
    "```\n",
    "\n",
    "**Real-World Migration Story:**\n",
    "\n",
    "```python\n",
    "# Company: Netflix ‚Üí Apple (hypothetical)\n",
    "\n",
    "# Phase 1: Dual-write (6 months)\n",
    "df.write.format(\"delta\").save(\"s3://bucket/delta/orders\")\n",
    "df.write.format(\"iceberg\").save(\"s3://bucket/iceberg/orders\")\n",
    "\n",
    "# Phase 2: Validate (3 months)\n",
    "delta_count = spark.read.format(\"delta\").load(...).count()\n",
    "iceberg_count = spark.read.format(\"iceberg\").load(...).count()\n",
    "assert delta_count == iceberg_count\n",
    "\n",
    "# Phase 3: Cutover (1 week)\n",
    "# Redirect readers to Iceberg\n",
    "# Stop Delta writes\n",
    "\n",
    "# Phase 4: Cleanup (1 month)\n",
    "# Delete Delta files\n",
    "# Reclaim storage\n",
    "\n",
    "# Total timeline: 10 months\n",
    "# Cost: ~$50K engineering effort\n",
    "# Benefit: 30% query performance improvement\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00324956",
   "metadata": {},
   "source": [
    "- Delta Lake agrega ACID, time travel y MERGE INTO sobre Parquet.\n",
    "- Iceberg optimiza la gesti√≥n de metadatos, particiones ocultas y evoluci√≥n de esquema.\n",
    "- Requiere motor como Spark/Trino/Flint y un cat√°logo (Glue/REST).\n",
    "- Coste/beneficio: eval√∫a volumen, concurrencia, latencia y SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e6",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo Delta Lake (referencia con PySpark) [opcional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d983386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'from pyspark.sql import SparkSession', 'spark = (SparkSession.builder', \"    .appName('DeltaDemo')\", \"    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\", \"    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\", '    .getOrCreate())', '', \"df = spark.read.parquet('s3://bucket/curated/ventas/')\", \"df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\", '', \"delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\", \"delta.createOrReplaceTempView('ventas')\", 'spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()']\n"
     ]
    }
   ],
   "source": [
    "delta_demo = r'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName('DeltaDemo')\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.read.parquet('s3://bucket/curated/ventas/')\n",
    "df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\n",
    "\n",
    "delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\n",
    "delta.createOrReplaceTempView('ventas')\n",
    "spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()\n",
    "'''\n",
    "print(delta_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b564",
   "metadata": {},
   "source": [
    "## 4. Buenas pr√°cticas de Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccb0bc",
   "metadata": {},
   "source": [
    "- Definir contratos de datos y versionado de esquemas.\n",
    "- Gestionar tama√±os de archivos y compaction (OPTIMIZE/VACUUM).\n",
    "- Catalogaci√≥n y pol√≠ticas de acceso por dominio (Data Mesh).\n",
    "- Observabilidad y linaje (OpenLineage/Marquez, DataHub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß≠ Navegaci√≥n\n",
    "\n",
    "**‚Üê Anterior:** [üèõÔ∏è Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "\n",
    "**Siguiente ‚Üí:** [Apache Spark Streaming: Procesamiento en Tiempo Real ‚Üí](03_spark_streaming.ipynb)\n",
    "\n",
    "**üìö √çndice de Nivel Senior:**\n",
    "- [üèõÔ∏è Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)](02_lakehouse_delta_iceberg.ipynb) ‚Üê üîµ Est√°s aqu√≠\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [üèõÔ∏è Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ü§ñ ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [üí∞ Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [üîê Seguridad, Compliance y Auditor√≠a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [üìä Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [üèÜ Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [üåê Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**üéì Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
