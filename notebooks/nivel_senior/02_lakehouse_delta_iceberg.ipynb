{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da7d8cb",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)\n",
    "\n",
    "Objetivo: comprender los principios de Lakehouse y practicar un flujo b√°sico con Parquet (local) y notas de c√≥mo migrar a Delta Lake o Apache Iceberg.\n",
    "\n",
    "- Duraci√≥n: 120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Mid 03 (AWS/S3) y 07 (Particionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822026",
   "metadata": {},
   "source": [
    "## 1. Lakehouse en pocas l√≠neas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a20ee",
   "metadata": {},
   "source": [
    "- Tabla de datos en formato columna (Parquet) sobre object storage.\n",
    "- Transaccionalidad y versiones con capas de metadatos (Delta/Iceberg/Hudi).\n",
    "- Cat√°logo central (Glue/Unity/Metastore) y gobernanza integrada.\n",
    "- Lectores: engines SQL (Athena/Trino/Spark) + ML + BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a329f1",
   "metadata": {},
   "source": [
    "## 2. Hands-on: tabla Parquet particionada local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path('datasets/processed/lakehouse_demo')\n",
    "(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'id':[1,2,3,4],\n",
    "  'fecha':['2025-10-01','2025-10-02','2025-10-02','2025-10-03'],\n",
    "  'producto_id':[101,102,101,103],\n",
    "  'cantidad':[1,2,1,3],\n",
    "  'precio':[100.0,50.0,100.0,20.0]\n",
    "})\n",
    "df['total'] = df['cantidad'] * df['precio']\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.strftime('%Y-%m')\n",
    "\n",
    "for (anio, mes), part in df.groupby(['anio','mes']):\n",
    "    part_dir = BASE / f'anio={anio}' / f'mes={mes}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fp = part_dir / f'ventas_{int(time.time())}.parquet'\n",
    "    part.to_parquet(fp, index=False)\n",
    "str(BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457603",
   "metadata": {},
   "source": [
    "### 2.1 Lectura particionada y pruning manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = list((BASE / 'anio=2025' / 'mes=2025-10').glob('*.parquet'))\n",
    "pd.concat([pd.read_parquet(p) for p in parts]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549cff5",
   "metadata": {},
   "source": [
    "## 3. Delta Lake/Iceberg: c√≥mo y cu√°ndo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00324956",
   "metadata": {},
   "source": [
    "- Delta Lake agrega ACID, time travel y MERGE INTO sobre Parquet.\n",
    "- Iceberg optimiza la gesti√≥n de metadatos, particiones ocultas y evoluci√≥n de esquema.\n",
    "- Requiere motor como Spark/Trino/Flint y un cat√°logo (Glue/REST).\n",
    "- Coste/beneficio: eval√∫a volumen, concurrencia, latencia y SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e6",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo Delta Lake (referencia con PySpark) [opcional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_demo = r'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName('DeltaDemo')\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.read.parquet('s3://bucket/curated/ventas/')\n",
    "df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\n",
    "\n",
    "delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\n",
    "delta.createOrReplaceTempView('ventas')\n",
    "spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()\n",
    "'''\n",
    "print(delta_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b564",
   "metadata": {},
   "source": [
    "## 4. Buenas pr√°cticas de Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccb0bc",
   "metadata": {},
   "source": [
    "- Definir contratos de datos y versionado de esquemas.\n",
    "- Gestionar tama√±os de archivos y compaction (OPTIMIZE/VACUUM).\n",
    "- Catalogaci√≥n y pol√≠ticas de acceso por dominio (Data Mesh).\n",
    "- Observabilidad y linaje (OpenLineage/Marquez, DataHub)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
