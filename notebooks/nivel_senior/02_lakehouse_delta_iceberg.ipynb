{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da7d8cb",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)\n",
    "\n",
    "Objetivo: comprender los principios de Lakehouse y practicar un flujo b√°sico con Parquet (local) y notas de c√≥mo migrar a Delta Lake o Apache Iceberg.\n",
    "\n",
    "- Duraci√≥n: 120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Mid 03 (AWS/S3) y 07 (Particionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a71667",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Lakehouse Architecture: Unificando Data Warehouse y Data Lake**\n",
    "\n",
    "**La Evoluci√≥n de Arquitecturas de Datos:**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 1 (2000s): Data Warehouse                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  Structured Data ‚Üí RDBMS (Oracle)  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  OLAP ‚Üí Star/Snowflake Schema      ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  BI Tools ‚Üí SQL Queries            ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ ACID, Performance      ‚ùå Caro, No soporta ML        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 2 (2010s): Data Lake                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  All Data (structured + unstr.)    ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Üí S3/HDFS (cheap storage)         ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Üí Spark/Presto (compute layer)    ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ Escalable, Barato    ‚ùå No ACID, Data Swamp          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GENERACI√ìN 3 (2020s): LAKEHOUSE                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Metadata Layer (Delta/Ice)  ‚îÇ  ‚îÇ ‚Üê Transacciones  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Columnar Format (Parquet)   ‚îÇ  ‚îÇ ‚Üê Performance    ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îÇ  Object Storage (S3/ADLS)    ‚îÇ  ‚îÇ ‚Üê Escalabilidad  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "‚îÇ  ‚úÖ ACID + Escala + Barato + ML/BI    ‚ùå Complejidad     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**¬øQu√© es un Lakehouse?**\n",
    "\n",
    "Arquitectura que combina lo mejor de **Data Warehouse** (transacciones, performance) con **Data Lake** (escalabilidad, costo-efectividad).\n",
    "\n",
    "**Componentes Clave:**\n",
    "\n",
    "```python\n",
    "lakehouse_stack = {\n",
    "    'Storage Layer': {\n",
    "        'Technology': 'S3, ADLS, GCS',\n",
    "        'Format': 'Parquet (columnar)',\n",
    "        'Cost': '$0.023/GB/mes',\n",
    "        'Benefit': 'Almacenamiento infinito y barato'\n",
    "    },\n",
    "    'Metadata Layer': {\n",
    "        'Technology': 'Delta Lake, Apache Iceberg, Apache Hudi',\n",
    "        'Features': 'ACID, Time Travel, Schema Evolution',\n",
    "        'Benefit': 'Transacciones sobre object storage'\n",
    "    },\n",
    "    'Catalog Layer': {\n",
    "        'Technology': 'AWS Glue, Unity Catalog, Hive Metastore',\n",
    "        'Features': 'Metadata management, Permissions',\n",
    "        'Benefit': 'Single source of truth'\n",
    "    },\n",
    "    'Compute Layer': {\n",
    "        'Technology': 'Spark, Presto, Athena, Trino',\n",
    "        'Features': 'SQL queries, Distributed processing',\n",
    "        'Benefit': 'Separaci√≥n storage/compute'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Table Formats Comparison:**\n",
    "\n",
    "| Feature | **Delta Lake** | **Apache Iceberg** | **Apache Hudi** |\n",
    "|---------|----------------|-------------------|-----------------|\n",
    "| **Creator** | Databricks (2019) | Netflix (2017) | Uber (2016) |\n",
    "| **ACID** | ‚úÖ Optimistic locking | ‚úÖ Snapshot isolation | ‚úÖ MVCC |\n",
    "| **Time Travel** | ‚úÖ Version history | ‚úÖ Snapshot-based | ‚úÖ Commit timeline |\n",
    "| **Schema Evolution** | ‚úÖ ADD/DROP cols | ‚úÖ Full evolution | ‚úÖ Partial |\n",
    "| **Partition Evolution** | ‚ùå Manual | ‚úÖ Hidden partitions | ‚ùå Manual |\n",
    "| **Streaming** | ‚úÖ Spark Streaming | ‚úÖ Flink, Spark | ‚úÖ DeltaStreamer |\n",
    "| **Engines** | Spark, Presto, Trino | Spark, Flink, Trino, Athena | Spark, Presto |\n",
    "| **Maturity** | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê Medium |\n",
    "| **Governance** | Databricks (vendor) | Apache (neutral) | Apache (neutral) |\n",
    "| **Best For** | Databricks users | Multi-engine, AWS | Upserts, CDC |\n",
    "\n",
    "**Real-World Adoption:**\n",
    "\n",
    "```\n",
    "Delta Lake:\n",
    "  - Databricks customers (obviamente)\n",
    "  - Comcast (Petabyte-scale)\n",
    "  - Riot Games (Gaming analytics)\n",
    "\n",
    "Apache Iceberg:\n",
    "  - Netflix (originator, 100+ PB)\n",
    "  - Apple (iCloud data)\n",
    "  - Adobe (Experience Cloud)\n",
    "  - AWS (Athena native support)\n",
    "\n",
    "Apache Hudi:\n",
    "  - Uber (ride-hailing data)\n",
    "  - Amazon (internal use)\n",
    "  - Disney+ (streaming analytics)\n",
    "```\n",
    "\n",
    "**Arquitectura de Datos Moderna:**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     DATA SOURCES                         ‚îÇ\n",
    "‚îÇ  Databases, APIs, Streams, Files, SaaS, IoT             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Ingest (Fivetran, Airbyte, Custom)\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    BRONZE LAYER                          ‚îÇ\n",
    "‚îÇ              (Raw data, append-only)                     ‚îÇ\n",
    "‚îÇ  Format: Parquet, Delta, Iceberg                        ‚îÇ\n",
    "‚îÇ  Partitioned by: ingestion_date                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Transformation (dbt, Spark)\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    SILVER LAYER                          ‚îÇ\n",
    "‚îÇ         (Cleaned, validated, deduplicated)              ‚îÇ\n",
    "‚îÇ  Format: Delta Lake (ACID needed)                       ‚îÇ\n",
    "‚îÇ  Partitioned by: business dimensions                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ Aggregation, Business Logic\n",
    "                       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     GOLD LAYER                           ‚îÇ\n",
    "‚îÇ        (Aggregated, business-ready datasets)            ‚îÇ\n",
    "‚îÇ  Format: Delta Lake or Iceberg                          ‚îÇ\n",
    "‚îÇ  Consumed by: BI Tools, ML Models, APIs                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Beneficios del Lakehouse:**\n",
    "\n",
    "1. **Unified Platform**:\n",
    "   - Single storage para BI, ML, Data Science\n",
    "   - No m√°s \"copy data from DW to DL for ML\"\n",
    "\n",
    "2. **Cost Reduction**:\n",
    "   - Storage: $0.023/GB vs $25/TB (Snowflake)\n",
    "   - Compute: Pay-per-query (Athena) o Spot instances\n",
    "\n",
    "3. **Performance**:\n",
    "   - Columnar format (10x faster queries)\n",
    "   - Partition pruning (scan solo datos relevantes)\n",
    "   - Caching + predicate pushdown\n",
    "\n",
    "4. **Governance**:\n",
    "   - ACID garantiza consistencia\n",
    "   - Time Travel para auditor√≠a\n",
    "   - Fine-grained access control\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - M√∫ltiples engines (no vendor lock-in)\n",
    "   - Schema evolution sin downtime\n",
    "   - Support structured + semi-structured\n",
    "\n",
    "**¬øCu√°ndo usar Lakehouse?**\n",
    "\n",
    "‚úÖ **S√ç usar cuando:**\n",
    "- Vol√∫menes > 100 TB\n",
    "- Necesitas ML + BI sobre mismos datos\n",
    "- M√∫ltiples teams con diferentes tools\n",
    "- Budget limitado vs DW tradicional\n",
    "\n",
    "‚ùå **NO usar cuando:**\n",
    "- Datasets < 1 TB (PostgreSQL suficiente)\n",
    "- Team peque√±o (< 5) con skills limitados\n",
    "- Latencia cr√≠tica < 100ms (considerar OLTP)\n",
    "- Compliance requiere on-prem (limitaciones cloud)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822026",
   "metadata": {},
   "source": [
    "## 1. Lakehouse en pocas l√≠neas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31879",
   "metadata": {},
   "source": [
    "### üìê **Lakehouse Fundamentals: Storage + Metadata + Catalog**\n",
    "\n",
    "**1. Storage Layer: Object Storage como Foundation**\n",
    "\n",
    "```python\n",
    "# ¬øPor qu√© S3/ADLS/GCS y no HDFS o RDBMS?\n",
    "\n",
    "storage_comparison = {\n",
    "    'HDFS (Hadoop)': {\n",
    "        'cost': '$$$',\n",
    "        'scalability': 'Limited by cluster',\n",
    "        'durability': '99.9% (3 replicas)',\n",
    "        'latency': 'Low (local)',\n",
    "        'ops_complexity': 'High (manage cluster)',\n",
    "        'verdict': '‚ùå Legacy, avoid for new projects'\n",
    "    },\n",
    "    'RDBMS (PostgreSQL)': {\n",
    "        'cost': '$$$$',\n",
    "        'scalability': 'Vertical (TB-scale)',\n",
    "        'durability': '99.99% (replication)',\n",
    "        'latency': 'Very Low (<10ms)',\n",
    "        'ops_complexity': 'Medium',\n",
    "        'verdict': '‚úÖ For OLTP, ‚ùå for Analytics at scale'\n",
    "    },\n",
    "    'Object Storage (S3)': {\n",
    "        'cost': '$',\n",
    "        'scalability': 'Infinite (PB-EB scale)',\n",
    "        'durability': '99.999999999% (11 nines)',\n",
    "        'latency': 'Medium (network)',\n",
    "        'ops_complexity': 'Very Low (managed)',\n",
    "        'verdict': '‚úÖ Perfect for Lakehouse'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**S3 as Database? The Challenges:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Problem 1: No ACID transactions\n",
    "# Dos writers simult√°neos ‚Üí race condition\n",
    "writer_1_writes_file('s3://bucket/data/part-001.parquet')\n",
    "writer_2_writes_file('s3://bucket/data/part-001.parquet')  # Overwrite!\n",
    "\n",
    "# ‚ùå Problem 2: No consistency\n",
    "list_objects('s3://bucket/data/')  # May not see latest write immediately\n",
    "\n",
    "# ‚ùå Problem 3: No indexing\n",
    "# Para find record con id=X ‚Üí scan all files (slow!)\n",
    "\n",
    "# ‚ùå Problem 4: No schema enforcement\n",
    "# Nada previene que alguien escriba schema incompatible\n",
    "```\n",
    "\n",
    "**2. Metadata Layer: The Secret Sauce**\n",
    "\n",
    "**Delta Lake Transaction Log:**\n",
    "\n",
    "```\n",
    "table_path/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json  ‚Üê Version 0 (CREATE TABLE)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json  ‚Üê Version 1 (INSERT)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json  ‚Üê Version 2 (UPDATE)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000003.json  ‚Üê Version 3 (DELETE)\n",
    "‚îú‚îÄ‚îÄ part-00000-xxx.parquet\n",
    "‚îú‚îÄ‚îÄ part-00001-xxx.parquet\n",
    "‚îî‚îÄ‚îÄ part-00002-xxx.parquet\n",
    "\n",
    "# Contenido de 00000000000000000001.json:\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"timestamp\": 1730246400000,\n",
    "    \"operation\": \"WRITE\",\n",
    "    \"operationMetrics\": {\n",
    "      \"numFiles\": \"2\",\n",
    "      \"numOutputRows\": \"1000\"\n",
    "    }\n",
    "  },\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-xxx.parquet\",\n",
    "    \"size\": 52428800,\n",
    "    \"partitionValues\": {\"year\": \"2025\", \"month\": \"10\"},\n",
    "    \"dataChange\": true,\n",
    "    \"stats\": \"{\\\"numRecords\\\":500,\\\"minValues\\\":{\\\"id\\\":1},\\\"maxValues\\\":{\\\"id\\\":500}}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**C√≥mo funciona ACID:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ Atomicity: All-or-nothing\n",
    "def write_to_delta(df, table_path):\n",
    "    # Step 1: Write Parquet files (no one can see yet)\n",
    "    temp_files = df.write.parquet(f\"{table_path}/_tmp/\")\n",
    "    \n",
    "    # Step 2: Create transaction log entry\n",
    "    version = get_next_version(table_path)\n",
    "    log_entry = {\n",
    "        \"add\": [{\"path\": f, \"size\": size} for f in temp_files]\n",
    "    }\n",
    "    \n",
    "    # Step 3: Atomic commit (write JSON file)\n",
    "    write_json(f\"{table_path}/_delta_log/{version:020d}.json\", log_entry)\n",
    "    # Solo cuando este archivo existe ‚Üí datos visibles\n",
    "    \n",
    "    # Si crash antes del Step 3 ‚Üí temp files hu√©rfanos (no problema)\n",
    "\n",
    "# ‚úÖ Consistency: Schema enforcement\n",
    "def validate_write(df, existing_schema):\n",
    "    if df.schema != existing_schema:\n",
    "        if not compatible(df.schema, existing_schema):\n",
    "            raise SchemaIncompatibleException()\n",
    "\n",
    "# ‚úÖ Isolation: Optimistic concurrency control\n",
    "def concurrent_write(df1, df2):\n",
    "    # Writer 1 reads version 5, writes based on v5\n",
    "    # Writer 2 reads version 5, writes based on v5\n",
    "    \n",
    "    # Writer 1 tries to commit version 6\n",
    "    if current_version == 5:  # OK\n",
    "        commit_version_6()\n",
    "    \n",
    "    # Writer 2 tries to commit version 6\n",
    "    if current_version == 5:  # CONFLICT! (now it's 6)\n",
    "        raise ConcurrentModificationException()\n",
    "        # Writer 2 must retry: read v6, apply changes, commit v7\n",
    "\n",
    "# ‚úÖ Durability: S3's 11 nines\n",
    "# Once committed, data is durable (S3 guarantee)\n",
    "```\n",
    "\n",
    "**Time Travel (Versioning):**\n",
    "\n",
    "```python\n",
    "# Read current version\n",
    "df = spark.read.format(\"delta\").load(\"s3://bucket/sales\")\n",
    "\n",
    "# Read version from 7 days ago\n",
    "df_v7d = spark.read.format(\"delta\").option(\"versionAsOf\", 7).load(...)\n",
    "\n",
    "# Read version at specific timestamp\n",
    "df_oct20 = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20\") \\\n",
    "    .load(...)\n",
    "\n",
    "# Use cases:\n",
    "# - Reproducibility: \"Show me data exactly as ML model saw it\"\n",
    "# - Auditing: \"What changed between yesterday and today?\"\n",
    "# - Rollback: \"Undo accidental DELETE\"\n",
    "# - A/B testing: \"Compare old vs new transformations\"\n",
    "```\n",
    "\n",
    "**3. Catalog Layer: Metadata Management**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              DATA CATALOG                         ‚îÇ\n",
    "‚îÇ                                                   ‚îÇ\n",
    "‚îÇ  Database: sales_prod                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Table: orders                                ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Location: s3://bucket/gold/orders/      ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Format: delta                            ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Schema: {id: bigint, total: double, ...}‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Partitions: [year, month]               ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Owner: data-team@company.com            ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îú‚îÄ Tags: [PII, critical]                   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îî‚îÄ Last Updated: 2025-10-30 14:30:00       ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Table: customers                             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Table: products                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Catalog Implementations:**\n",
    "\n",
    "| Catalog | Provider | Best For | Limitations |\n",
    "|---------|----------|----------|-------------|\n",
    "| **Hive Metastore** | Apache | Open-source, Spark native | Single point of failure |\n",
    "| **AWS Glue Catalog** | AWS | Serverless, AWS-integrated | AWS lock-in |\n",
    "| **Unity Catalog** | Databricks | Unified governance | Databricks only |\n",
    "| **Polaris** | Snowflake | Open-source Iceberg | New (2024) |\n",
    "\n",
    "**Example: Creating Table in Catalog:**\n",
    "\n",
    "```python\n",
    "# Spark + Delta Lake + Glue Catalog\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE sales_prod.orders\n",
    "  USING delta\n",
    "  LOCATION 's3://bucket/gold/orders/'\n",
    "  PARTITIONED BY (year, month)\n",
    "  TBLPROPERTIES (\n",
    "    'delta.dataSkippingNumIndexedCols' = '5',\n",
    "    'delta.deletedFileRetentionDuration' = 'interval 7 days',\n",
    "    'owner' = 'data-team@company.com',\n",
    "    'pii' = 'true'\n",
    "  )\n",
    "  AS SELECT * FROM staging.orders_raw\n",
    "\"\"\")\n",
    "\n",
    "# Query from any engine\n",
    "# Athena:\n",
    "SELECT * FROM sales_prod.orders WHERE year=2025 AND month=10\n",
    "\n",
    "# Presto:\n",
    "SELECT * FROM glue.sales_prod.orders WHERE total > 1000\n",
    "\n",
    "# Spark:\n",
    "spark.table(\"sales_prod.orders\").filter(\"year = 2025\").show()\n",
    "```\n",
    "\n",
    "**Metadata Caching & Performance:**\n",
    "\n",
    "```python\n",
    "# Problem: Reading metadata for each query is slow\n",
    "# Solution: Caching\n",
    "\n",
    "# Delta Lake: Stats in transaction log\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000.parquet\",\n",
    "    \"stats\": \"{\\\"numRecords\\\":1000,\\\"minValues\\\":{\\\"id\\\":1,\\\"date\\\":\\\"2025-10-01\\\"},\\\"maxValues\\\":{\\\"id\\\":1000,\\\"date\\\":\\\"2025-10-31\\\"}}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Query: SELECT * FROM orders WHERE id = 500\n",
    "# Engine reads stats ‚Üí knows id ‚àà [1, 1000] ‚Üí scan this file\n",
    "# Query: SELECT * FROM orders WHERE id = 5000\n",
    "# Engine reads stats ‚Üí knows id ‚àà [1, 1000] ‚Üí SKIP this file (data skipping)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a20ee",
   "metadata": {},
   "source": [
    "- Tabla de datos en formato columna (Parquet) sobre object storage.\n",
    "- Transaccionalidad y versiones con capas de metadatos (Delta/Iceberg/Hudi).\n",
    "- Cat√°logo central (Glue/Unity/Metastore) y gobernanza integrada.\n",
    "- Lectores: engines SQL (Athena/Trino/Spark) + ML + BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a329f1",
   "metadata": {},
   "source": [
    "## 2. Hands-on: tabla Parquet particionada local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c1f50",
   "metadata": {},
   "source": [
    "### üíæ **Parquet: The Columnar Storage Format**\n",
    "\n",
    "**¬øPor qu√© Parquet y no CSV/JSON?**\n",
    "\n",
    "**Query Performance Example:**\n",
    "\n",
    "```python\n",
    "# Dataset: 1M filas √ó 100 columnas = 100M valores\n",
    "# Query: SELECT AVG(salary) FROM employees WHERE department = 'Engineering'\n",
    "\n",
    "# ‚ùå CSV (Row-based):\n",
    "for row in read_csv('employees.csv'):  # Lee TODAS las columnas\n",
    "    if row['department'] == 'Engineering':\n",
    "        salaries.append(row['salary'])  # Solo usa 2 columnas\n",
    "\n",
    "# I/O: Lee 100M valores\n",
    "# Time: ~30 segundos\n",
    "\n",
    "# ‚úÖ Parquet (Columnar):\n",
    "departments = read_column('employees.parquet', 'department')\n",
    "salaries = read_column('employees.parquet', 'salary')\n",
    "avg = mean([s for d, s in zip(departments, salaries) if d == 'Engineering'])\n",
    "\n",
    "# I/O: Lee 2M valores (solo 2 columnas)\n",
    "# Time: ~0.6 segundos (50x faster!)\n",
    "```\n",
    "\n",
    "**Parquet File Structure:**\n",
    "\n",
    "```\n",
    "file.parquet\n",
    "‚îú‚îÄ‚îÄ Header (4 bytes magic: \"PAR1\")\n",
    "‚îú‚îÄ‚îÄ Row Group 1 (default: 128 MB)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: id\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Data Pages (compressed)\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Statistics (min, max, null_count)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: name\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: salary\n",
    "‚îú‚îÄ‚îÄ Row Group 2\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: id\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: name\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: salary\n",
    "‚îú‚îÄ‚îÄ Footer Metadata\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Schema\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Column statistics (per row group)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Compression codec\n",
    "‚îî‚îÄ‚îÄ Footer (4 bytes magic: \"PAR1\")\n",
    "```\n",
    "\n",
    "**Compression Codecs:**\n",
    "\n",
    "```python\n",
    "df.to_parquet('data.parquet', compression='snappy')\n",
    "\n",
    "# Benchmark: 1M rows, 10 columns\n",
    "compression_results = {\n",
    "    'none': {\n",
    "        'size_mb': 100,\n",
    "        'write_s': 2.1,\n",
    "        'read_s': 1.5,\n",
    "        'ratio': '1x'\n",
    "    },\n",
    "    'snappy': {\n",
    "        'size_mb': 33,\n",
    "        'write_s': 2.8,\n",
    "        'read_s': 1.8,\n",
    "        'ratio': '3x',\n",
    "        'verdict': '‚úÖ Default (balance speed/compression)'\n",
    "    },\n",
    "    'gzip': {\n",
    "        'size_mb': 20,\n",
    "        'write_s': 8.5,\n",
    "        'read_s': 4.2,\n",
    "        'ratio': '5x',\n",
    "        'verdict': '‚ö†Ô∏è Better compression, slower'\n",
    "    },\n",
    "    'zstd': {\n",
    "        'size_mb': 22,\n",
    "        'write_s': 3.5,\n",
    "        'read_s': 2.0,\n",
    "        'ratio': '4.5x',\n",
    "        'verdict': '‚úÖ Modern (better than snappy)'\n",
    "    },\n",
    "    'lz4': {\n",
    "        'size_mb': 35,\n",
    "        'write_s': 2.2,\n",
    "        'read_s': 1.6,\n",
    "        'ratio': '2.8x',\n",
    "        'verdict': '‚ö° Fastest compression'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Encoding Schemes:**\n",
    "\n",
    "```python\n",
    "# Plain Encoding (default for small datasets)\n",
    "values = [100, 200, 150, 175, 100]\n",
    "encoded = b'\\x64\\x00\\xc8\\x00\\x96\\x00\\xaf\\x00\\x64\\x00'  # Raw bytes\n",
    "\n",
    "# Dictionary Encoding (for low cardinality)\n",
    "# Column: [\"Engineering\", \"Sales\", \"Engineering\", \"Engineering\", \"Sales\"]\n",
    "dictionary = [\"Engineering\", \"Sales\"]\n",
    "indices = [0, 1, 0, 0, 1]  # Store indices (2 bits each vs 11 bytes per string)\n",
    "# Compression: ~85%\n",
    "\n",
    "# Run-Length Encoding (for repeated values)\n",
    "# Column: [100, 100, 100, 200, 200, 300]\n",
    "rle = [(100, 3), (200, 2), (300, 1)]  # (value, count)\n",
    "\n",
    "# Delta Encoding (for sorted/incremental data)\n",
    "# Column: [1000, 1001, 1002, 1003, 1004]\n",
    "delta = [1000, 1, 1, 1, 1]  # Base + deltas\n",
    "```\n",
    "\n",
    "**Partition Pruning (Data Skipping):**\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "‚îú‚îÄ‚îÄ year=2023/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ month=02/data.parquet\n",
    "‚îú‚îÄ‚îÄ year=2024/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ month=12/data.parquet\n",
    "‚îî‚îÄ‚îÄ year=2025/\n",
    "    ‚îú‚îÄ‚îÄ month=01/data.parquet\n",
    "    ‚îî‚îÄ‚îÄ month=10/data.parquet  ‚Üê Only scan this!\n",
    "\n",
    "# Query:\n",
    "SELECT * FROM sales WHERE year=2025 AND month=10\n",
    "\n",
    "# Without partitioning: Scan 7 files\n",
    "# With partitioning: Scan 1 file (85% reduction!)\n",
    "```\n",
    "\n",
    "**Small Files Problem:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Anti-pattern: Too many small files\n",
    "for record in stream:\n",
    "    df = pd.DataFrame([record])\n",
    "    df.to_parquet(f's3://bucket/data/record_{record[\"id\"]}.parquet')\n",
    "\n",
    "# Result: 1M files √ó 1 KB each = Overhead disaster!\n",
    "# Athena cost: $5/TB scanned + $0.002/file = $$$$\n",
    "\n",
    "# ‚úÖ Solution 1: Buffering\n",
    "buffer = []\n",
    "for record in stream:\n",
    "    buffer.append(record)\n",
    "    if len(buffer) >= 10000:\n",
    "        pd.DataFrame(buffer).to_parquet(f's3://bucket/data/batch_{timestamp}.parquet')\n",
    "        buffer = []\n",
    "\n",
    "# ‚úÖ Solution 2: Compaction (OPTIMIZE in Delta)\n",
    "spark.sql(\"OPTIMIZE sales_table\")\n",
    "# Merges small files into 128MB-1GB files\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# ‚ùå Over-partitioning (too granular)\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\", \"hour\", \"customer_id\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Result: Millions of partitions ‚Üí slow metadata operations\n",
    "\n",
    "# ‚úÖ Balanced partitioning\n",
    "df.write.partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Target: 128MB-1GB per partition\n",
    "# Rule: Partition columns used in 80%+ of queries\n",
    "\n",
    "# ‚ö° Z-ordering (Delta Lake optimization)\n",
    "spark.sql(\"OPTIMIZE sales_table ZORDER BY (customer_id, product_id)\")\n",
    "# Co-locates related data for better data skipping\n",
    "```\n",
    "\n",
    "**Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# V1: Initial schema\n",
    "df_v1 = pd.DataFrame({\n",
    "    'id': [1, 2],\n",
    "    'name': ['Alice', 'Bob']\n",
    "})\n",
    "df_v1.to_parquet('users_v1.parquet')\n",
    "\n",
    "# V2: Add column (compatible)\n",
    "df_v2 = pd.DataFrame({\n",
    "    'id': [3, 4],\n",
    "    'name': ['Charlie', 'Diana'],\n",
    "    'email': ['c@x.com', 'd@x.com']  # New column\n",
    "})\n",
    "df_v2.to_parquet('users_v2.parquet')\n",
    "\n",
    "# Reading both files:\n",
    "df = pd.concat([\n",
    "    pd.read_parquet('users_v1.parquet'),  # email will be null\n",
    "    pd.read_parquet('users_v2.parquet')\n",
    "])\n",
    "# Parquet handles missing columns gracefully\n",
    "\n",
    "# ‚ùå Breaking change: Rename/delete column\n",
    "df_v3 = pd.DataFrame({\n",
    "    'user_id': [5, 6],  # Renamed from 'id'\n",
    "    'full_name': ['Eve', 'Frank']  # Renamed from 'name'\n",
    "})\n",
    "# This will break queries expecting 'id' column!\n",
    "```\n",
    "\n",
    "**Parquet + Pandas/Polars:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read specific columns (projection)\n",
    "df = pd.read_parquet('sales.parquet', columns=['id', 'total'])\n",
    "\n",
    "# Read with filters (predicate pushdown)\n",
    "df = pd.read_parquet('sales.parquet', \n",
    "                     filters=[('year', '=', 2025), ('total', '>', 1000)])\n",
    "\n",
    "# Read row groups (chunked reading for large files)\n",
    "parquet_file = pq.ParquetFile('sales.parquet')\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    df = batch.to_pandas()\n",
    "    process(df)\n",
    "\n",
    "# Read from S3 directly\n",
    "df = pd.read_parquet('s3://bucket/sales/year=2025/month=10/*.parquet')\n",
    "```\n",
    "\n",
    "**Monitoring Parquet Health:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def analyze_parquet_file(path):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    metadata = pf.metadata\n",
    "    \n",
    "    metrics = {\n",
    "        'num_row_groups': metadata.num_row_groups,\n",
    "        'num_rows': metadata.num_rows,\n",
    "        'num_columns': metadata.num_columns,\n",
    "        'file_size_mb': os.path.getsize(path) / (1024**2),\n",
    "        'avg_row_group_size_mb': os.path.getsize(path) / metadata.num_row_groups / (1024**2),\n",
    "        'compression': pf.schema_arrow.metadata.get(b'compression', b'unknown').decode()\n",
    "    }\n",
    "    \n",
    "    # Health checks\n",
    "    if metrics['avg_row_group_size_mb'] < 64:\n",
    "        print(\"‚ö†Ô∏è Row groups too small (target: 128MB)\")\n",
    "    \n",
    "    if metrics['file_size_mb'] < 10:\n",
    "        print(\"‚ö†Ô∏è File too small (target: >100MB)\")\n",
    "    \n",
    "    if metrics['num_row_groups'] > 10:\n",
    "        print(\"‚ö†Ô∏è Too many row groups (consider repartitioning)\")\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7878388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets\\\\processed\\\\lakehouse_demo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path('datasets/processed/lakehouse_demo')\n",
    "(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'id':[1,2,3,4],\n",
    "  'fecha':['2025-10-01','2025-10-02','2025-10-02','2025-10-03'],\n",
    "  'producto_id':[101,102,101,103],\n",
    "  'cantidad':[1,2,1,3],\n",
    "  'precio':[100.0,50.0,100.0,20.0]\n",
    "})\n",
    "df['total'] = df['cantidad'] * df['precio']\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.strftime('%Y-%m')\n",
    "\n",
    "for (anio, mes), part in df.groupby(['anio','mes']):\n",
    "    part_dir = BASE / f'anio={anio}' / f'mes={mes}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fp = part_dir / f'ventas_{int(time.time())}.parquet'\n",
    "    part.to_parquet(fp, index=False)\n",
    "str(BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457603",
   "metadata": {},
   "source": [
    "### 2.1 Lectura particionada y pruning manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9c198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fecha",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "producto_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cantidad",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "anio",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "mes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "afcaab6a-f678-4f6f-a13c-47405adb18c2",
       "rows": [
        [
         "0",
         "1",
         "2025-10-01",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "1",
         "2",
         "2025-10-02",
         "102",
         "2",
         "50.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "2",
         "3",
         "2025-10-02",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ],
        [
         "3",
         "4",
         "2025-10-03",
         "103",
         "3",
         "20.0",
         "60.0",
         "2025",
         "2025-10"
        ],
        [
         "0",
         "1",
         "2025-10-01",
         "101",
         "1",
         "100.0",
         "100.0",
         "2025",
         "2025-10"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fecha</th>\n",
       "      <th>producto_id</th>\n",
       "      <th>cantidad</th>\n",
       "      <th>precio</th>\n",
       "      <th>total</th>\n",
       "      <th>anio</th>\n",
       "      <th>mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       fecha  producto_id  cantidad  precio  total  anio      mes\n",
       "0   1  2025-10-01          101         1   100.0  100.0  2025  2025-10\n",
       "1   2  2025-10-02          102         2    50.0  100.0  2025  2025-10\n",
       "2   3  2025-10-02          101         1   100.0  100.0  2025  2025-10\n",
       "3   4  2025-10-03          103         3    20.0   60.0  2025  2025-10\n",
       "0   1  2025-10-01          101         1   100.0  100.0  2025  2025-10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = list((BASE / 'anio=2025' / 'mes=2025-10').glob('*.parquet'))\n",
    "pd.concat([pd.read_parquet(p) for p in parts]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c5f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Demostrando compresi√≥n Parquet...\n",
      "\n",
      "   snappy  : 0.33 MB\n",
      "   gzip    : 0.21 MB\n",
      "   none    : 0.81 MB\n",
      "\n",
      "üìä Ratios de compresi√≥n (vs sin comprimir):\n",
      "   snappy: 59.5% reducci√≥n\n",
      "   gzip: 74.4% reducci√≥n\n",
      "\n",
      "‚úÖ Mejor compresi√≥n: gzip (m√°s lento pero m√°s peque√±o)\n",
      "‚úÖ Mejor balance: snappy (r√°pido y buena compresi√≥n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis\\AppData\\Local\\Temp\\ipykernel_37712\\2040506150.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'fecha': pd.date_range('2024-01-01', periods=10000, freq='H'),\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de compresi√≥n de Parquet con diferentes codecs\n",
    "print(\"üîß Demostrando compresi√≥n Parquet...\\n\")\n",
    "\n",
    "# Crear dataset m√°s grande para ver diferencias de compresi√≥n\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset con 10,000 registros\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(1, 10001),\n",
    "    'fecha': pd.date_range('2024-01-01', periods=10000, freq='H'),\n",
    "    'producto_id': np.random.randint(100, 200, 10000),\n",
    "    'cantidad': np.random.randint(1, 10, 10000),\n",
    "    'precio': np.random.uniform(10, 1000, 10000).round(2),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C', 'D'], 10000),\n",
    "    'descripcion': ['Producto ' + str(i) * 10 for i in range(10000)]  # Datos repetitivos\n",
    "})\n",
    "\n",
    "# Probar diferentes compresiones\n",
    "compression_results = {}\n",
    "for codec in ['snappy', 'gzip', 'none']:\n",
    "    filepath = BASE / f'test_{codec}.parquet'\n",
    "    large_df.to_parquet(filepath, compression=codec, index=False)\n",
    "    size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "    compression_results[codec] = size_mb\n",
    "    print(f\"   {codec:8s}: {size_mb:.2f} MB\")\n",
    "\n",
    "# Calcular ratios de compresi√≥n\n",
    "base_size = compression_results['none']\n",
    "print(f\"\\nüìä Ratios de compresi√≥n (vs sin comprimir):\")\n",
    "for codec, size in compression_results.items():\n",
    "    if codec != 'none':\n",
    "        ratio = (1 - size/base_size) * 100\n",
    "        print(f\"   {codec}: {ratio:.1f}% reducci√≥n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Mejor compresi√≥n: gzip (m√°s lento pero m√°s peque√±o)\")\n",
    "print(f\"‚úÖ Mejor balance: snappy (r√°pido y buena compresi√≥n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aad2c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Demostrando partition pruning...\n",
      "\n",
      "üìÅ Escribiendo 720 registros particionados...\n",
      "\n",
      "‚è±Ô∏è Rendimiento:\n",
      "   Leer TODO (720 registros): 0.0799s\n",
      "   Leer D√çA 15 (24 registros): 0.0030s\n",
      "   Speedup: 26.6x m√°s r√°pido con pruning\n",
      "\n",
      "‚úÖ Partition pruning evita leer 29 archivos innecesarios!\n",
      "\n",
      "‚è±Ô∏è Rendimiento:\n",
      "   Leer TODO (720 registros): 0.0799s\n",
      "   Leer D√çA 15 (24 registros): 0.0030s\n",
      "   Speedup: 26.6x m√°s r√°pido con pruning\n",
      "\n",
      "‚úÖ Partition pruning evita leer 29 archivos innecesarios!\n"
     ]
    }
   ],
   "source": [
    "# Demostraci√≥n de partition pruning (lectura selectiva)\n",
    "print(\"‚ö° Demostrando partition pruning...\\n\")\n",
    "\n",
    "# Crear dataset particionado por fecha m√°s grande\n",
    "sales_data = []\n",
    "for day in range(1, 31):  # 30 d√≠as\n",
    "    for hour in range(24):  # 24 horas por d√≠a\n",
    "        sales_data.append({\n",
    "            'timestamp': pd.Timestamp(2025, 10, day, hour),\n",
    "            'producto_id': np.random.randint(1, 100),\n",
    "            'cantidad': np.random.randint(1, 5),\n",
    "            'precio': np.random.uniform(10, 500),\n",
    "            'tienda_id': np.random.randint(1, 10)\n",
    "        })\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "sales_df['fecha'] = sales_df['timestamp'].dt.date\n",
    "sales_df['anio'] = sales_df['timestamp'].dt.year\n",
    "sales_df['mes'] = sales_df['timestamp'].dt.month\n",
    "sales_df['dia'] = sales_df['timestamp'].dt.day\n",
    "\n",
    "# Escribir particionado por a√±o/mes/d√≠a\n",
    "partition_path = BASE / 'sales_partitioned'\n",
    "print(f\"üìÅ Escribiendo {len(sales_df):,} registros particionados...\")\n",
    "\n",
    "for (anio, mes, dia), group in sales_df.groupby(['anio', 'mes', 'dia']):\n",
    "    part_dir = partition_path / f'anio={anio}' / f'mes={mes:02d}' / f'dia={dia:02d}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    group.to_parquet(part_dir / 'data.parquet', index=False)\n",
    "\n",
    "# Medir lectura con y sin pruning\n",
    "import time\n",
    "\n",
    "# Sin pruning: leer todo\n",
    "t0 = time.time()\n",
    "all_files = list(partition_path.rglob('*.parquet'))\n",
    "df_all = pd.concat([pd.read_parquet(f) for f in all_files])\n",
    "time_all = time.time() - t0\n",
    "\n",
    "# Con pruning: leer solo un d√≠a espec√≠fico\n",
    "t0 = time.time()\n",
    "specific_day = partition_path / 'anio=2025' / 'mes=10' / 'dia=15'\n",
    "df_specific = pd.read_parquet(specific_day / 'data.parquet')\n",
    "time_specific = time.time() - t0\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Rendimiento:\")\n",
    "print(f\"   Leer TODO ({len(df_all):,} registros): {time_all:.4f}s\")\n",
    "print(f\"   Leer D√çA 15 ({len(df_specific):,} registros): {time_specific:.4f}s\")\n",
    "print(f\"   Speedup: {time_all/time_specific:.1f}x m√°s r√°pido con pruning\")\n",
    "print(f\"\\n‚úÖ Partition pruning evita leer {len(all_files) - 1} archivos innecesarios!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bf8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Demostrando column pruning (lectura columnar)...\n",
      "\n",
      "üìÅ Archivo creado: 0.50 MB\n",
      "   Dimensiones: (5000, 12)\n",
      "\n",
      "‚ö° Comparaci√≥n de lectura:\n",
      "   Todas las columnas (12): 0.0055s\n",
      "   Solo 3 columnas: 0.0031s\n",
      "   Speedup: 1.8x m√°s r√°pido\n",
      "\n",
      "üìà Estad√≠sticas del archivo Parquet:\n",
      "   Row groups: 1\n",
      "   Filas totales: 5,000\n",
      "   Columnas: 12\n",
      "   Tama√±o serializado: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Demostraci√≥n de Column Pruning (proyecci√≥n de columnas)\n",
    "print(\"üìä Demostrando column pruning (lectura columnar)...\\n\")\n",
    "\n",
    "# Crear dataset ancho (muchas columnas)\n",
    "wide_df = pd.DataFrame({\n",
    "    'id': range(1, 5001),\n",
    "    'col_1': np.random.rand(5000),\n",
    "    'col_2': np.random.rand(5000),\n",
    "    'col_3': np.random.rand(5000),\n",
    "    'col_4': np.random.rand(5000),\n",
    "    'col_5': np.random.rand(5000),\n",
    "    'col_6': np.random.rand(5000),\n",
    "    'col_7': np.random.rand(5000),\n",
    "    'col_8': np.random.rand(5000),\n",
    "    'col_9': np.random.rand(5000),\n",
    "    'col_10': np.random.rand(5000),\n",
    "    'text_data': ['Lorem ipsum ' * 50] * 5000  # Columna grande\n",
    "})\n",
    "\n",
    "# Guardar en parquet\n",
    "wide_file = BASE / 'wide_table.parquet'\n",
    "wide_df.to_parquet(wide_file, index=False)\n",
    "\n",
    "print(f\"üìÅ Archivo creado: {os.path.getsize(wide_file) / (1024*1024):.2f} MB\")\n",
    "print(f\"   Dimensiones: {wide_df.shape}\")\n",
    "\n",
    "# Leer solo columnas espec√≠ficas\n",
    "t0 = time.time()\n",
    "df_all_cols = pd.read_parquet(wide_file)\n",
    "time_all_cols = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "df_few_cols = pd.read_parquet(wide_file, columns=['id', 'col_1', 'col_2'])\n",
    "time_few_cols = time.time() - t0\n",
    "\n",
    "print(f\"\\n‚ö° Comparaci√≥n de lectura:\")\n",
    "print(f\"   Todas las columnas ({len(df_all_cols.columns)}): {time_all_cols:.4f}s\")\n",
    "print(f\"   Solo 3 columnas: {time_few_cols:.4f}s\")\n",
    "print(f\"   Speedup: {time_all_cols/time_few_cols:.1f}x m√°s r√°pido\")\n",
    "\n",
    "# Mostrar estad√≠sticas del archivo Parquet\n",
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile(wide_file)\n",
    "metadata = parquet_file.metadata\n",
    "\n",
    "print(f\"\\nüìà Estad√≠sticas del archivo Parquet:\")\n",
    "print(f\"   Row groups: {metadata.num_row_groups}\")\n",
    "print(f\"   Filas totales: {metadata.num_rows:,}\")\n",
    "print(f\"   Columnas: {metadata.num_columns}\")\n",
    "print(f\"   Tama√±o serializado: {metadata.serialized_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1cefd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Problemas comunes de Parquet y c√≥mo evitarlos\n",
      "\n",
      "1Ô∏è‚É£ Small Files Problem\n",
      "   ‚ùå Creados: 100 archivos\n",
      "   ‚ùå Tama√±o promedio: 1.62 KB\n",
      "   ‚ùå Problema: Overhead de metadatos, slow queries\n",
      "\n",
      "   ‚úÖ Soluci√≥n: Consolidar en 1 archivo\n",
      "   ‚úÖ Tama√±o: 2.61 KB\n",
      "   ‚úÖ Mejora: 100x menos archivos\n",
      "\n",
      "2Ô∏è‚É£ Schema Evolution / Inconsistencia\n",
      "   ‚ö†Ô∏è Esquemas diferentes detectados\n",
      "   ‚úÖ Pandas maneja autom√°ticamente (NaN para columnas faltantes)\n",
      "   Resultado:\n",
      "   id   name   age   salary\n",
      "0   1  Alice  30.0      NaN\n",
      "0   2    Bob   NaN  50000.0\n",
      "\n",
      "3Ô∏è‚É£ Tipos de datos y valores nulos\n",
      "   üìä Tipos de datos preservados:\n",
      "id       float64\n",
      "value    float64\n",
      "text      object\n",
      "dtype: object\n",
      "\n",
      "   ‚úÖ Parquet maneja nulls correctamente en todas las columnas\n",
      "   ‚ùå Creados: 100 archivos\n",
      "   ‚ùå Tama√±o promedio: 1.62 KB\n",
      "   ‚ùå Problema: Overhead de metadatos, slow queries\n",
      "\n",
      "   ‚úÖ Soluci√≥n: Consolidar en 1 archivo\n",
      "   ‚úÖ Tama√±o: 2.61 KB\n",
      "   ‚úÖ Mejora: 100x menos archivos\n",
      "\n",
      "2Ô∏è‚É£ Schema Evolution / Inconsistencia\n",
      "   ‚ö†Ô∏è Esquemas diferentes detectados\n",
      "   ‚úÖ Pandas maneja autom√°ticamente (NaN para columnas faltantes)\n",
      "   Resultado:\n",
      "   id   name   age   salary\n",
      "0   1  Alice  30.0      NaN\n",
      "0   2    Bob   NaN  50000.0\n",
      "\n",
      "3Ô∏è‚É£ Tipos de datos y valores nulos\n",
      "   üìä Tipos de datos preservados:\n",
      "id       float64\n",
      "value    float64\n",
      "text      object\n",
      "dtype: object\n",
      "\n",
      "   ‚úÖ Parquet maneja nulls correctamente en todas las columnas\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de problemas comunes y soluciones\n",
    "print(\"‚ö†Ô∏è Problemas comunes de Parquet y c√≥mo evitarlos\\n\")\n",
    "\n",
    "# Problema 1: Small Files Problem\n",
    "print(\"1Ô∏è‚É£ Small Files Problem\")\n",
    "small_files_dir = BASE / 'small_files_problem'\n",
    "small_files_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Crear muchos archivos peque√±os (anti-pattern)\n",
    "for i in range(100):\n",
    "    tiny_df = pd.DataFrame({'id': [i], 'value': [i * 2]})\n",
    "    tiny_df.to_parquet(small_files_dir / f'file_{i:03d}.parquet', index=False)\n",
    "\n",
    "small_files = list(small_files_dir.glob('*.parquet'))\n",
    "total_size = sum(os.path.getsize(f) for f in small_files)\n",
    "avg_size = total_size / len(small_files) / 1024\n",
    "\n",
    "print(f\"   ‚ùå Creados: {len(small_files)} archivos\")\n",
    "print(f\"   ‚ùå Tama√±o promedio: {avg_size:.2f} KB\")\n",
    "print(f\"   ‚ùå Problema: Overhead de metadatos, slow queries\")\n",
    "\n",
    "# Soluci√≥n: Consolidar archivos\n",
    "consolidated = pd.concat([pd.read_parquet(f) for f in small_files])\n",
    "consolidated_file = BASE / 'consolidated.parquet'\n",
    "consolidated.to_parquet(consolidated_file, index=False)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Soluci√≥n: Consolidar en 1 archivo\")\n",
    "print(f\"   ‚úÖ Tama√±o: {os.path.getsize(consolidated_file) / 1024:.2f} KB\")\n",
    "print(f\"   ‚úÖ Mejora: {len(small_files)}x menos archivos\\n\")\n",
    "\n",
    "# Problema 2: Esquema inconsistente\n",
    "print(\"2Ô∏è‚É£ Schema Evolution / Inconsistencia\")\n",
    "try:\n",
    "    df1 = pd.DataFrame({'id': [1], 'name': ['Alice'], 'age': [30]})\n",
    "    df2 = pd.DataFrame({'id': [2], 'name': ['Bob'], 'salary': [50000]})  # Nueva columna\n",
    "    \n",
    "    df1.to_parquet(BASE / 'schema_v1.parquet', index=False)\n",
    "    df2.to_parquet(BASE / 'schema_v2.parquet', index=False)\n",
    "    \n",
    "    # Leer ambos archivos\n",
    "    combined = pd.concat([\n",
    "        pd.read_parquet(BASE / 'schema_v1.parquet'),\n",
    "        pd.read_parquet(BASE / 'schema_v2.parquet')\n",
    "    ])\n",
    "    \n",
    "    print(f\"   ‚ö†Ô∏è Esquemas diferentes detectados\")\n",
    "    print(f\"   ‚úÖ Pandas maneja autom√°ticamente (NaN para columnas faltantes)\")\n",
    "    print(f\"   Resultado:\\n{combined}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Problema 3: Valores nulos y tipos de datos\n",
    "print(\"3Ô∏è‚É£ Tipos de datos y valores nulos\")\n",
    "mixed_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, None],  # Int con NULL\n",
    "    'value': [1.5, 2.5, None, 4.5],  # Float con NULL\n",
    "    'text': ['a', 'b', None, 'd']  # String con NULL\n",
    "})\n",
    "\n",
    "mixed_file = BASE / 'mixed_types.parquet'\n",
    "mixed_df.to_parquet(mixed_file, index=False)\n",
    "loaded = pd.read_parquet(mixed_file)\n",
    "\n",
    "print(f\"   üìä Tipos de datos preservados:\")\n",
    "print(loaded.dtypes)\n",
    "print(f\"\\n   ‚úÖ Parquet maneja nulls correctamente en todas las columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61bb19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä RESUMEN DE EXPERIMENTOS PARQUET\n",
      "============================================================\n",
      "\n",
      "‚úÖ CONCEPTOS DEMOSTRADOS:\n",
      "   1. ‚úì Particionamiento por fecha (anio/mes/dia)\n",
      "   2. ‚úì Compresi√≥n (snappy vs gzip vs none)\n",
      "   3. ‚úì Partition pruning (26.6x speedup)\n",
      "   4. ‚úì Column pruning (1.8x speedup)\n",
      "   5. ‚úì Schema evolution (manejo de nulls)\n",
      "   6. ‚úì Small files problem (consolidaci√≥n)\n",
      "\n",
      "üìà RESULTADOS CLAVE:\n",
      "   ‚Ä¢ Compresi√≥n snappy: 59.5% reducci√≥n de tama√±o\n",
      "   ‚Ä¢ Compresi√≥n gzip: 74.4% reducci√≥n de tama√±o\n",
      "   ‚Ä¢ Partition pruning: 26.6x m√°s r√°pido\n",
      "   ‚Ä¢ Column pruning: 1.8x m√°s r√°pido\n",
      "   ‚Ä¢ Small files: 100 archivos ‚Üí 1 archivo consolidado\n",
      "\n",
      "üí° MEJORES PR√ÅCTICAS:\n",
      "   ‚Ä¢ Usar particiones de 64-128 MB por archivo\n",
      "   ‚Ä¢ Preferir snappy para balance velocidad/compresi√≥n\n",
      "   ‚Ä¢ Particionar por fechas o claves frecuentes\n",
      "   ‚Ä¢ Leer solo columnas necesarias\n",
      "   ‚Ä¢ Evitar small files (<10MB)\n",
      "   ‚Ä¢ Mantener row groups de ~128MB\n",
      "\n",
      "üöÄ LISTO PARA PRODUCCI√ìN:\n",
      "   ‚Ä¢ Implementar en Spark/Dask para escala\n",
      "   ‚Ä¢ Agregar Delta Lake para ACID\n",
      "   ‚Ä¢ Usar Iceberg para table evolution\n",
      "   ‚Ä¢ Integrar con cat√°logo (Hive/Glue)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Todos los experimentos ejecutados exitosamente\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Resumen de experimentos\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä RESUMEN DE EXPERIMENTOS PARQUET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ CONCEPTOS DEMOSTRADOS:\")\n",
    "print(\"   1. ‚úì Particionamiento por fecha (anio/mes/dia)\")\n",
    "print(\"   2. ‚úì Compresi√≥n (snappy vs gzip vs none)\")\n",
    "print(\"   3. ‚úì Partition pruning (26.6x speedup)\")\n",
    "print(\"   4. ‚úì Column pruning (1.8x speedup)\")\n",
    "print(\"   5. ‚úì Schema evolution (manejo de nulls)\")\n",
    "print(\"   6. ‚úì Small files problem (consolidaci√≥n)\")\n",
    "\n",
    "print(\"\\nüìà RESULTADOS CLAVE:\")\n",
    "print(f\"   ‚Ä¢ Compresi√≥n snappy: 59.5% reducci√≥n de tama√±o\")\n",
    "print(f\"   ‚Ä¢ Compresi√≥n gzip: 74.4% reducci√≥n de tama√±o\")\n",
    "print(f\"   ‚Ä¢ Partition pruning: 26.6x m√°s r√°pido\")\n",
    "print(f\"   ‚Ä¢ Column pruning: 1.8x m√°s r√°pido\")\n",
    "print(f\"   ‚Ä¢ Small files: 100 archivos ‚Üí 1 archivo consolidado\")\n",
    "\n",
    "print(\"\\nüí° MEJORES PR√ÅCTICAS:\")\n",
    "print(\"   ‚Ä¢ Usar particiones de 64-128 MB por archivo\")\n",
    "print(\"   ‚Ä¢ Preferir snappy para balance velocidad/compresi√≥n\")\n",
    "print(\"   ‚Ä¢ Particionar por fechas o claves frecuentes\")\n",
    "print(\"   ‚Ä¢ Leer solo columnas necesarias\")\n",
    "print(\"   ‚Ä¢ Evitar small files (<10MB)\")\n",
    "print(\"   ‚Ä¢ Mantener row groups de ~128MB\")\n",
    "\n",
    "print(\"\\nüöÄ LISTO PARA PRODUCCI√ìN:\")\n",
    "print(\"   ‚Ä¢ Implementar en Spark/Dask para escala\")\n",
    "print(\"   ‚Ä¢ Agregar Delta Lake para ACID\")\n",
    "print(\"   ‚Ä¢ Usar Iceberg para table evolution\")\n",
    "print(\"   ‚Ä¢ Integrar con cat√°logo (Hive/Glue)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Todos los experimentos ejecutados exitosamente\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549cff5",
   "metadata": {},
   "source": [
    "## 3. Delta Lake/Iceberg: c√≥mo y cu√°ndo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e4cb",
   "metadata": {},
   "source": [
    "### ‚ö° **Delta Lake vs Apache Iceberg: Deep Dive**\n",
    "\n",
    "**When to Choose Which?**\n",
    "\n",
    "```python\n",
    "decision_matrix = {\n",
    "    'Use Delta Lake if': [\n",
    "        'Already using Databricks',\n",
    "        'Primary engine is Spark',\n",
    "        'Need mature ecosystem (more tools)',\n",
    "        'Streaming workloads (Spark Structured Streaming)',\n",
    "        'Team familiar with Delta Lake'\n",
    "    ],\n",
    "    'Use Apache Iceberg if': [\n",
    "        'Multi-engine strategy (Spark + Trino + Flink + Athena)',\n",
    "        'AWS-centric (Athena native support)',\n",
    "        'Need hidden partitioning (automatic partition management)',\n",
    "        'Open governance important (Apache vs vendor)',\n",
    "        'Future-proofing (growing adoption)'\n",
    "    ],\n",
    "    'Use Apache Hudi if': [\n",
    "        'Heavy upsert/CDC workloads',\n",
    "        'Record-level updates critical',\n",
    "        'Uber-style use case'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Comparison:**\n",
    "\n",
    "**1. ACID Transactions:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Optimistic Concurrency Control\n",
    "@transaction\n",
    "def update_delta_table():\n",
    "    current_version = read_version()  # v5\n",
    "    # ... perform transformation ...\n",
    "    try:\n",
    "        commit_new_version(v6, based_on=v5)\n",
    "    except ConflictException:\n",
    "        # Another writer committed v6 first\n",
    "        retry_with_new_base(v6)\n",
    "\n",
    "# Apache Iceberg: Snapshot Isolation\n",
    "@transaction\n",
    "def update_iceberg_table():\n",
    "    snapshot_id = current_snapshot()\n",
    "    # ... perform transformation ...\n",
    "    new_snapshot = create_snapshot(changes)\n",
    "    atomic_swap(current_snapshot, new_snapshot)\n",
    "    # Readers on old snapshot unaffected\n",
    "```\n",
    "\n",
    "**2. Time Travel:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# Files: _delta_log/00000000000000000005.json\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 5) \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20 00:00:00\") \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "# Apache Iceberg\n",
    "# Files: metadata/snap-xxxx-1-yy.avro\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", 12345678) \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"as-of-timestamp\", \"1730246400000\") \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "# Iceberg advantage: Faster time travel (O(1) vs O(n))\n",
    "# Delta: Must replay transaction log from start\n",
    "# Iceberg: Direct snapshot lookup\n",
    "```\n",
    "\n",
    "**3. Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# ‚úÖ ADD COLUMN (supported)\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (discount DOUBLE)\")\n",
    "\n",
    "# ‚ö†Ô∏è RENAME COLUMN (manual migration needed)\n",
    "# Step 1: Add new column\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (customer_email STRING)\")\n",
    "# Step 2: Backfill\n",
    "spark.sql(\"UPDATE orders SET customer_email = email\")\n",
    "# Step 3: Drop old column\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN email\")\n",
    "\n",
    "# ‚ùå CHANGE COLUMN TYPE (not supported)\n",
    "# Must create new table\n",
    "\n",
    "# Apache Iceberg\n",
    "# ‚úÖ ADD COLUMN\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMN discount double\")\n",
    "\n",
    "# ‚úÖ RENAME COLUMN (supported!)\n",
    "spark.sql(\"ALTER TABLE orders RENAME COLUMN email TO customer_email\")\n",
    "\n",
    "# ‚úÖ DROP COLUMN\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN discount\")\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE TYPE (limited support)\n",
    "spark.sql(\"ALTER TABLE orders ALTER COLUMN id TYPE bigint\")  # int ‚Üí bigint OK\n",
    "# int ‚Üí string: Not supported (data loss risk)\n",
    "```\n",
    "\n",
    "**4. Partition Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Manual partition management\n",
    "# Initial: Partitioned by date\n",
    "df.write.partitionBy(\"date\").format(\"delta\").save(\"orders\")\n",
    "\n",
    "# Later: Want to partition by date + country\n",
    "# ‚ùå Problem: Must rewrite all data\n",
    "df_all = spark.read.format(\"delta\").load(\"orders\")\n",
    "df_all.write.partitionBy(\"date\", \"country\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"orders_new\")\n",
    "\n",
    "# Apache Iceberg: Hidden partitions (automatic!)\n",
    "# Initial: Partitioned by date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE orders (\n",
    "        id bigint,\n",
    "        date date,\n",
    "        country string,\n",
    "        total double\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(date))\n",
    "\"\"\")\n",
    "\n",
    "# Later: Change partitioning strategy (NO data rewrite!)\n",
    "spark.sql(\"ALTER TABLE orders DROP PARTITION FIELD days(date)\")\n",
    "spark.sql(\"ALTER TABLE orders ADD PARTITION FIELD bucket(10, country)\")\n",
    "\n",
    "# Iceberg tracks partition evolution in metadata\n",
    "# Old data: date partitions\n",
    "# New data: country buckets\n",
    "# Queries work seamlessly across both!\n",
    "```\n",
    "\n",
    "**5. File Management (VACUUM/OPTIMIZE):**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# OPTIMIZE: Compact small files\n",
    "spark.sql(\"OPTIMIZE orders\")\n",
    "# Target: 1GB files\n",
    "\n",
    "# Z-ORDER: Co-locate data\n",
    "spark.sql(\"OPTIMIZE orders ZORDER BY (customer_id, product_id)\")\n",
    "\n",
    "# VACUUM: Delete old files\n",
    "spark.sql(\"VACUUM orders RETAIN 168 HOURS\")  # Keep 7 days\n",
    "# Deletes:\n",
    "# - Old data files (overwritten by OPTIMIZE)\n",
    "# - Transaction log checkpoints\n",
    "\n",
    "# Apache Iceberg\n",
    "# REWRITE DATA FILES (equivalent to OPTIMIZE)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.rewrite_data_files(\n",
    "        table => 'db.orders',\n",
    "        options => map(\n",
    "            'target-file-size-bytes', '1073741824',  -- 1GB\n",
    "            'min-input-files', '5'\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# EXPIRE SNAPSHOTS (equivalent to VACUUM)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.expire_snapshots(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-23 00:00:00',\n",
    "        retain_last => 7\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# REMOVE ORPHAN FILES\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.remove_orphan_files(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-20 00:00:00'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**6. Multi-Engine Support:**\n",
    "\n",
    "| Engine | Delta Lake | Iceberg |\n",
    "|--------|------------|---------|\n",
    "| **Spark** | ‚úÖ Native | ‚úÖ Native |\n",
    "| **Presto/Trino** | ‚úÖ Connector | ‚úÖ Native |\n",
    "| **Flink** | ‚ùå Limited | ‚úÖ Native |\n",
    "| **AWS Athena** | ‚ö†Ô∏è Via manifests | ‚úÖ Native |\n",
    "| **Dremio** | ‚úÖ | ‚úÖ |\n",
    "| **Snowflake** | ‚ö†Ô∏è Via external tables | ‚úÖ Iceberg Tables |\n",
    "\n",
    "**7. Metadata Performance:**\n",
    "\n",
    "```python\n",
    "# Scenario: Table with 10,000 partitions\n",
    "\n",
    "# Delta Lake:\n",
    "# - Metadata: JSON files in _delta_log/\n",
    "# - List partitions: O(n) scan of transaction log\n",
    "# - Overhead: ~100MB metadata for large tables\n",
    "\n",
    "# Iceberg:\n",
    "# - Metadata: Avro files with manifest list ‚Üí manifest files\n",
    "# - List partitions: O(log n) via manifest index\n",
    "# - Overhead: ~10MB metadata for same table\n",
    "\n",
    "# Benchmark: SHOW PARTITIONS\n",
    "# Delta: 15 seconds\n",
    "# Iceberg: 0.5 seconds (30x faster!)\n",
    "```\n",
    "\n",
    "**8. Streaming Support:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Spark Structured Streaming native\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"orders\")\n",
    "\n",
    "stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints\") \\\n",
    "    .start(\"orders_processed\")\n",
    "\n",
    "# Apache Iceberg: Flink + Spark Streaming\n",
    "# Flink (primary streaming engine for Iceberg)\n",
    "TableEnvironment tableEnv = ...\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    CREATE TABLE orders (...)\n",
    "    WITH ('connector' = 'iceberg', ...)\n",
    "\"\"\")\n",
    "\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    INSERT INTO orders_processed\n",
    "    SELECT * FROM orders\n",
    "    WHERE total > 1000\n",
    "\"\"\")\n",
    "\n",
    "# Spark Streaming (also supported)\n",
    "spark.readStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"orders\")\n",
    "```\n",
    "\n",
    "**Real-World Migration Story:**\n",
    "\n",
    "```python\n",
    "# Company: Netflix ‚Üí Apple (hypothetical)\n",
    "\n",
    "# Phase 1: Dual-write (6 months)\n",
    "df.write.format(\"delta\").save(\"s3://bucket/delta/orders\")\n",
    "df.write.format(\"iceberg\").save(\"s3://bucket/iceberg/orders\")\n",
    "\n",
    "# Phase 2: Validate (3 months)\n",
    "delta_count = spark.read.format(\"delta\").load(...).count()\n",
    "iceberg_count = spark.read.format(\"iceberg\").load(...).count()\n",
    "assert delta_count == iceberg_count\n",
    "\n",
    "# Phase 3: Cutover (1 week)\n",
    "# Redirect readers to Iceberg\n",
    "# Stop Delta writes\n",
    "\n",
    "# Phase 4: Cleanup (1 month)\n",
    "# Delete Delta files\n",
    "# Reclaim storage\n",
    "\n",
    "# Total timeline: 10 months\n",
    "# Cost: ~$50K engineering effort\n",
    "# Benefit: 30% query performance improvement\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00324956",
   "metadata": {},
   "source": [
    "- Delta Lake agrega ACID, time travel y MERGE INTO sobre Parquet.\n",
    "- Iceberg optimiza la gesti√≥n de metadatos, particiones ocultas y evoluci√≥n de esquema.\n",
    "- Requiere motor como Spark/Trino/Flint y un cat√°logo (Glue/REST).\n",
    "- Coste/beneficio: eval√∫a volumen, concurrencia, latencia y SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e6",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo Delta Lake (referencia con PySpark) [opcional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d983386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'from pyspark.sql import SparkSession', 'spark = (SparkSession.builder', \"    .appName('DeltaDemo')\", \"    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\", \"    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\", '    .getOrCreate())', '', \"df = spark.read.parquet('s3://bucket/curated/ventas/')\", \"df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\", '', \"delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\", \"delta.createOrReplaceTempView('ventas')\", 'spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()']\n"
     ]
    }
   ],
   "source": [
    "delta_demo = r'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName('DeltaDemo')\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.read.parquet('s3://bucket/curated/ventas/')\n",
    "df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\n",
    "\n",
    "delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\n",
    "delta.createOrReplaceTempView('ventas')\n",
    "spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()\n",
    "'''\n",
    "print(delta_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b564",
   "metadata": {},
   "source": [
    "## 4. Buenas pr√°cticas de Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccb0bc",
   "metadata": {},
   "source": [
    "- Definir contratos de datos y versionado de esquemas.\n",
    "- Gestionar tama√±os de archivos y compaction (OPTIMIZE/VACUUM).\n",
    "- Catalogaci√≥n y pol√≠ticas de acceso por dominio (Data Mesh).\n",
    "- Observabilidad y linaje (OpenLineage/Marquez, DataHub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß≠ Navegaci√≥n\n",
    "\n",
    "**‚Üê Anterior:** [üèõÔ∏è Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "\n",
    "**Siguiente ‚Üí:** [Apache Spark Streaming: Procesamiento en Tiempo Real ‚Üí](03_spark_streaming.ipynb)\n",
    "\n",
    "**üìö √çndice de Nivel Senior:**\n",
    "- [üèõÔ∏è Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [üèóÔ∏è Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr√°ctica ligera)](02_lakehouse_delta_iceberg.ipynb) ‚Üê üîµ Est√°s aqu√≠\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [üèõÔ∏è Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [ü§ñ ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [üí∞ Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [üîê Seguridad, Compliance y Auditor√≠a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [üìä Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [üèÜ Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [üåê Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**üéì Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
