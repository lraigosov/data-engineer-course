{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da7d8cb",
   "metadata": {},
   "source": [
    "# 🏗️ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y práctica ligera)\n",
    "\n",
    "Objetivo: comprender los principios de Lakehouse y practicar un flujo básico con Parquet (local) y notas de cómo migrar a Delta Lake o Apache Iceberg.\n",
    "\n",
    "- Duración: 120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Mid 03 (AWS/S3) y 07 (Particionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a71667",
   "metadata": {},
   "source": [
    "### 🏗️ **Lakehouse Architecture: Unificando Data Warehouse y Data Lake**\n",
    "\n",
    "**La Evolución de Arquitecturas de Datos:**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│  GENERACIÓN 1 (2000s): Data Warehouse                    │\n",
    "│  ┌────────────────────────────────────┐                  │\n",
    "│  │  Structured Data → RDBMS (Oracle)  │                  │\n",
    "│  │  OLAP → Star/Snowflake Schema      │                  │\n",
    "│  │  BI Tools → SQL Queries            │                  │\n",
    "│  └────────────────────────────────────┘                  │\n",
    "│  ✅ ACID, Performance      ❌ Caro, No soporta ML        │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│  GENERACIÓN 2 (2010s): Data Lake                         │\n",
    "│  ┌────────────────────────────────────┐                  │\n",
    "│  │  All Data (structured + unstr.)    │                  │\n",
    "│  │  → S3/HDFS (cheap storage)         │                  │\n",
    "│  │  → Spark/Presto (compute layer)    │                  │\n",
    "│  └────────────────────────────────────┘                  │\n",
    "│  ✅ Escalable, Barato    ❌ No ACID, Data Swamp          │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│  GENERACIÓN 3 (2020s): LAKEHOUSE                         │\n",
    "│  ┌────────────────────────────────────┐                  │\n",
    "│  │  ┌──────────────────────────────┐  │                  │\n",
    "│  │  │  Metadata Layer (Delta/Ice)  │  │ ← Transacciones  │\n",
    "│  │  └──────────────────────────────┘  │                  │\n",
    "│  │  ┌──────────────────────────────┐  │                  │\n",
    "│  │  │  Columnar Format (Parquet)   │  │ ← Performance    │\n",
    "│  │  └──────────────────────────────┘  │                  │\n",
    "│  │  ┌──────────────────────────────┐  │                  │\n",
    "│  │  │  Object Storage (S3/ADLS)    │  │ ← Escalabilidad  │\n",
    "│  │  └──────────────────────────────┘  │                  │\n",
    "│  └────────────────────────────────────┘                  │\n",
    "│  ✅ ACID + Escala + Barato + ML/BI    ❌ Complejidad     │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**¿Qué es un Lakehouse?**\n",
    "\n",
    "Arquitectura que combina lo mejor de **Data Warehouse** (transacciones, performance) con **Data Lake** (escalabilidad, costo-efectividad).\n",
    "\n",
    "**Componentes Clave:**\n",
    "\n",
    "```python\n",
    "lakehouse_stack = {\n",
    "    'Storage Layer': {\n",
    "        'Technology': 'S3, ADLS, GCS',\n",
    "        'Format': 'Parquet (columnar)',\n",
    "        'Cost': '$0.023/GB/mes',\n",
    "        'Benefit': 'Almacenamiento infinito y barato'\n",
    "    },\n",
    "    'Metadata Layer': {\n",
    "        'Technology': 'Delta Lake, Apache Iceberg, Apache Hudi',\n",
    "        'Features': 'ACID, Time Travel, Schema Evolution',\n",
    "        'Benefit': 'Transacciones sobre object storage'\n",
    "    },\n",
    "    'Catalog Layer': {\n",
    "        'Technology': 'AWS Glue, Unity Catalog, Hive Metastore',\n",
    "        'Features': 'Metadata management, Permissions',\n",
    "        'Benefit': 'Single source of truth'\n",
    "    },\n",
    "    'Compute Layer': {\n",
    "        'Technology': 'Spark, Presto, Athena, Trino',\n",
    "        'Features': 'SQL queries, Distributed processing',\n",
    "        'Benefit': 'Separación storage/compute'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Table Formats Comparison:**\n",
    "\n",
    "| Feature | **Delta Lake** | **Apache Iceberg** | **Apache Hudi** |\n",
    "|---------|----------------|-------------------|-----------------|\n",
    "| **Creator** | Databricks (2019) | Netflix (2017) | Uber (2016) |\n",
    "| **ACID** | ✅ Optimistic locking | ✅ Snapshot isolation | ✅ MVCC |\n",
    "| **Time Travel** | ✅ Version history | ✅ Snapshot-based | ✅ Commit timeline |\n",
    "| **Schema Evolution** | ✅ ADD/DROP cols | ✅ Full evolution | ✅ Partial |\n",
    "| **Partition Evolution** | ❌ Manual | ✅ Hidden partitions | ❌ Manual |\n",
    "| **Streaming** | ✅ Spark Streaming | ✅ Flink, Spark | ✅ DeltaStreamer |\n",
    "| **Engines** | Spark, Presto, Trino | Spark, Flink, Trino, Athena | Spark, Presto |\n",
    "| **Maturity** | ⭐⭐⭐ High | ⭐⭐⭐ High | ⭐⭐ Medium |\n",
    "| **Governance** | Databricks (vendor) | Apache (neutral) | Apache (neutral) |\n",
    "| **Best For** | Databricks users | Multi-engine, AWS | Upserts, CDC |\n",
    "\n",
    "**Real-World Adoption:**\n",
    "\n",
    "```\n",
    "Delta Lake:\n",
    "  - Databricks customers (obviamente)\n",
    "  - Comcast (Petabyte-scale)\n",
    "  - Riot Games (Gaming analytics)\n",
    "\n",
    "Apache Iceberg:\n",
    "  - Netflix (originator, 100+ PB)\n",
    "  - Apple (iCloud data)\n",
    "  - Adobe (Experience Cloud)\n",
    "  - AWS (Athena native support)\n",
    "\n",
    "Apache Hudi:\n",
    "  - Uber (ride-hailing data)\n",
    "  - Amazon (internal use)\n",
    "  - Disney+ (streaming analytics)\n",
    "```\n",
    "\n",
    "**Arquitectura de Datos Moderna:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                     DATA SOURCES                         │\n",
    "│  Databases, APIs, Streams, Files, SaaS, IoT             │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │ Ingest (Fivetran, Airbyte, Custom)\n",
    "                       ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    BRONZE LAYER                          │\n",
    "│              (Raw data, append-only)                     │\n",
    "│  Format: Parquet, Delta, Iceberg                        │\n",
    "│  Partitioned by: ingestion_date                         │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │ Transformation (dbt, Spark)\n",
    "                       ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    SILVER LAYER                          │\n",
    "│         (Cleaned, validated, deduplicated)              │\n",
    "│  Format: Delta Lake (ACID needed)                       │\n",
    "│  Partitioned by: business dimensions                    │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │ Aggregation, Business Logic\n",
    "                       ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                     GOLD LAYER                           │\n",
    "│        (Aggregated, business-ready datasets)            │\n",
    "│  Format: Delta Lake or Iceberg                          │\n",
    "│  Consumed by: BI Tools, ML Models, APIs                 │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Beneficios del Lakehouse:**\n",
    "\n",
    "1. **Unified Platform**:\n",
    "   - Single storage para BI, ML, Data Science\n",
    "   - No más \"copy data from DW to DL for ML\"\n",
    "\n",
    "2. **Cost Reduction**:\n",
    "   - Storage: $0.023/GB vs $25/TB (Snowflake)\n",
    "   - Compute: Pay-per-query (Athena) o Spot instances\n",
    "\n",
    "3. **Performance**:\n",
    "   - Columnar format (10x faster queries)\n",
    "   - Partition pruning (scan solo datos relevantes)\n",
    "   - Caching + predicate pushdown\n",
    "\n",
    "4. **Governance**:\n",
    "   - ACID garantiza consistencia\n",
    "   - Time Travel para auditoría\n",
    "   - Fine-grained access control\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - Múltiples engines (no vendor lock-in)\n",
    "   - Schema evolution sin downtime\n",
    "   - Support structured + semi-structured\n",
    "\n",
    "**¿Cuándo usar Lakehouse?**\n",
    "\n",
    "✅ **SÍ usar cuando:**\n",
    "- Volúmenes > 100 TB\n",
    "- Necesitas ML + BI sobre mismos datos\n",
    "- Múltiples teams con diferentes tools\n",
    "- Budget limitado vs DW tradicional\n",
    "\n",
    "❌ **NO usar cuando:**\n",
    "- Datasets < 1 TB (PostgreSQL suficiente)\n",
    "- Team pequeño (< 5) con skills limitados\n",
    "- Latencia crítica < 100ms (considerar OLTP)\n",
    "- Compliance requiere on-prem (limitaciones cloud)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822026",
   "metadata": {},
   "source": [
    "## 1. Lakehouse en pocas líneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31879",
   "metadata": {},
   "source": [
    "### 📐 **Lakehouse Fundamentals: Storage + Metadata + Catalog**\n",
    "\n",
    "**1. Storage Layer: Object Storage como Foundation**\n",
    "\n",
    "```python\n",
    "# ¿Por qué S3/ADLS/GCS y no HDFS o RDBMS?\n",
    "\n",
    "storage_comparison = {\n",
    "    'HDFS (Hadoop)': {\n",
    "        'cost': '$$$',\n",
    "        'scalability': 'Limited by cluster',\n",
    "        'durability': '99.9% (3 replicas)',\n",
    "        'latency': 'Low (local)',\n",
    "        'ops_complexity': 'High (manage cluster)',\n",
    "        'verdict': '❌ Legacy, avoid for new projects'\n",
    "    },\n",
    "    'RDBMS (PostgreSQL)': {\n",
    "        'cost': '$$$$',\n",
    "        'scalability': 'Vertical (TB-scale)',\n",
    "        'durability': '99.99% (replication)',\n",
    "        'latency': 'Very Low (<10ms)',\n",
    "        'ops_complexity': 'Medium',\n",
    "        'verdict': '✅ For OLTP, ❌ for Analytics at scale'\n",
    "    },\n",
    "    'Object Storage (S3)': {\n",
    "        'cost': '$',\n",
    "        'scalability': 'Infinite (PB-EB scale)',\n",
    "        'durability': '99.999999999% (11 nines)',\n",
    "        'latency': 'Medium (network)',\n",
    "        'ops_complexity': 'Very Low (managed)',\n",
    "        'verdict': '✅ Perfect for Lakehouse'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**S3 as Database? The Challenges:**\n",
    "\n",
    "```python\n",
    "# ❌ Problem 1: No ACID transactions\n",
    "# Dos writers simultáneos → race condition\n",
    "writer_1_writes_file('s3://bucket/data/part-001.parquet')\n",
    "writer_2_writes_file('s3://bucket/data/part-001.parquet')  # Overwrite!\n",
    "\n",
    "# ❌ Problem 2: No consistency\n",
    "list_objects('s3://bucket/data/')  # May not see latest write immediately\n",
    "\n",
    "# ❌ Problem 3: No indexing\n",
    "# Para find record con id=X → scan all files (slow!)\n",
    "\n",
    "# ❌ Problem 4: No schema enforcement\n",
    "# Nada previene que alguien escriba schema incompatible\n",
    "```\n",
    "\n",
    "**2. Metadata Layer: The Secret Sauce**\n",
    "\n",
    "**Delta Lake Transaction Log:**\n",
    "\n",
    "```\n",
    "table_path/\n",
    "├── _delta_log/\n",
    "│   ├── 00000000000000000000.json  ← Version 0 (CREATE TABLE)\n",
    "│   ├── 00000000000000000001.json  ← Version 1 (INSERT)\n",
    "│   ├── 00000000000000000002.json  ← Version 2 (UPDATE)\n",
    "│   └── 00000000000000000003.json  ← Version 3 (DELETE)\n",
    "├── part-00000-xxx.parquet\n",
    "├── part-00001-xxx.parquet\n",
    "└── part-00002-xxx.parquet\n",
    "\n",
    "# Contenido de 00000000000000000001.json:\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"timestamp\": 1730246400000,\n",
    "    \"operation\": \"WRITE\",\n",
    "    \"operationMetrics\": {\n",
    "      \"numFiles\": \"2\",\n",
    "      \"numOutputRows\": \"1000\"\n",
    "    }\n",
    "  },\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-xxx.parquet\",\n",
    "    \"size\": 52428800,\n",
    "    \"partitionValues\": {\"year\": \"2025\", \"month\": \"10\"},\n",
    "    \"dataChange\": true,\n",
    "    \"stats\": \"{\\\"numRecords\\\":500,\\\"minValues\\\":{\\\"id\\\":1},\\\"maxValues\\\":{\\\"id\\\":500}}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Cómo funciona ACID:**\n",
    "\n",
    "```python\n",
    "# ✅ Atomicity: All-or-nothing\n",
    "def write_to_delta(df, table_path):\n",
    "    # Step 1: Write Parquet files (no one can see yet)\n",
    "    temp_files = df.write.parquet(f\"{table_path}/_tmp/\")\n",
    "    \n",
    "    # Step 2: Create transaction log entry\n",
    "    version = get_next_version(table_path)\n",
    "    log_entry = {\n",
    "        \"add\": [{\"path\": f, \"size\": size} for f in temp_files]\n",
    "    }\n",
    "    \n",
    "    # Step 3: Atomic commit (write JSON file)\n",
    "    write_json(f\"{table_path}/_delta_log/{version:020d}.json\", log_entry)\n",
    "    # Solo cuando este archivo existe → datos visibles\n",
    "    \n",
    "    # Si crash antes del Step 3 → temp files huérfanos (no problema)\n",
    "\n",
    "# ✅ Consistency: Schema enforcement\n",
    "def validate_write(df, existing_schema):\n",
    "    if df.schema != existing_schema:\n",
    "        if not compatible(df.schema, existing_schema):\n",
    "            raise SchemaIncompatibleException()\n",
    "\n",
    "# ✅ Isolation: Optimistic concurrency control\n",
    "def concurrent_write(df1, df2):\n",
    "    # Writer 1 reads version 5, writes based on v5\n",
    "    # Writer 2 reads version 5, writes based on v5\n",
    "    \n",
    "    # Writer 1 tries to commit version 6\n",
    "    if current_version == 5:  # OK\n",
    "        commit_version_6()\n",
    "    \n",
    "    # Writer 2 tries to commit version 6\n",
    "    if current_version == 5:  # CONFLICT! (now it's 6)\n",
    "        raise ConcurrentModificationException()\n",
    "        # Writer 2 must retry: read v6, apply changes, commit v7\n",
    "\n",
    "# ✅ Durability: S3's 11 nines\n",
    "# Once committed, data is durable (S3 guarantee)\n",
    "```\n",
    "\n",
    "**Time Travel (Versioning):**\n",
    "\n",
    "```python\n",
    "# Read current version\n",
    "df = spark.read.format(\"delta\").load(\"s3://bucket/sales\")\n",
    "\n",
    "# Read version from 7 days ago\n",
    "df_v7d = spark.read.format(\"delta\").option(\"versionAsOf\", 7).load(...)\n",
    "\n",
    "# Read version at specific timestamp\n",
    "df_oct20 = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20\") \\\n",
    "    .load(...)\n",
    "\n",
    "# Use cases:\n",
    "# - Reproducibility: \"Show me data exactly as ML model saw it\"\n",
    "# - Auditing: \"What changed between yesterday and today?\"\n",
    "# - Rollback: \"Undo accidental DELETE\"\n",
    "# - A/B testing: \"Compare old vs new transformations\"\n",
    "```\n",
    "\n",
    "**3. Catalog Layer: Metadata Management**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────┐\n",
    "│              DATA CATALOG                         │\n",
    "│                                                   │\n",
    "│  Database: sales_prod                            │\n",
    "│  ├─ Table: orders                                │\n",
    "│  │   ├─ Location: s3://bucket/gold/orders/      │\n",
    "│  │   ├─ Format: delta                            │\n",
    "│  │   ├─ Schema: {id: bigint, total: double, ...}│\n",
    "│  │   ├─ Partitions: [year, month]               │\n",
    "│  │   ├─ Owner: data-team@company.com            │\n",
    "│  │   ├─ Tags: [PII, critical]                   │\n",
    "│  │   └─ Last Updated: 2025-10-30 14:30:00       │\n",
    "│  ├─ Table: customers                             │\n",
    "│  └─ Table: products                              │\n",
    "└──────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Catalog Implementations:**\n",
    "\n",
    "| Catalog | Provider | Best For | Limitations |\n",
    "|---------|----------|----------|-------------|\n",
    "| **Hive Metastore** | Apache | Open-source, Spark native | Single point of failure |\n",
    "| **AWS Glue Catalog** | AWS | Serverless, AWS-integrated | AWS lock-in |\n",
    "| **Unity Catalog** | Databricks | Unified governance | Databricks only |\n",
    "| **Polaris** | Snowflake | Open-source Iceberg | New (2024) |\n",
    "\n",
    "**Example: Creating Table in Catalog:**\n",
    "\n",
    "```python\n",
    "# Spark + Delta Lake + Glue Catalog\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE sales_prod.orders\n",
    "  USING delta\n",
    "  LOCATION 's3://bucket/gold/orders/'\n",
    "  PARTITIONED BY (year, month)\n",
    "  TBLPROPERTIES (\n",
    "    'delta.dataSkippingNumIndexedCols' = '5',\n",
    "    'delta.deletedFileRetentionDuration' = 'interval 7 days',\n",
    "    'owner' = 'data-team@company.com',\n",
    "    'pii' = 'true'\n",
    "  )\n",
    "  AS SELECT * FROM staging.orders_raw\n",
    "\"\"\")\n",
    "\n",
    "# Query from any engine\n",
    "# Athena:\n",
    "SELECT * FROM sales_prod.orders WHERE year=2025 AND month=10\n",
    "\n",
    "# Presto:\n",
    "SELECT * FROM glue.sales_prod.orders WHERE total > 1000\n",
    "\n",
    "# Spark:\n",
    "spark.table(\"sales_prod.orders\").filter(\"year = 2025\").show()\n",
    "```\n",
    "\n",
    "**Metadata Caching & Performance:**\n",
    "\n",
    "```python\n",
    "# Problem: Reading metadata for each query is slow\n",
    "# Solution: Caching\n",
    "\n",
    "# Delta Lake: Stats in transaction log\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000.parquet\",\n",
    "    \"stats\": \"{\\\"numRecords\\\":1000,\\\"minValues\\\":{\\\"id\\\":1,\\\"date\\\":\\\"2025-10-01\\\"},\\\"maxValues\\\":{\\\"id\\\":1000,\\\"date\\\":\\\"2025-10-31\\\"}}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Query: SELECT * FROM orders WHERE id = 500\n",
    "# Engine reads stats → knows id ∈ [1, 1000] → scan this file\n",
    "# Query: SELECT * FROM orders WHERE id = 5000\n",
    "# Engine reads stats → knows id ∈ [1, 1000] → SKIP this file (data skipping)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a20ee",
   "metadata": {},
   "source": [
    "- Tabla de datos en formato columna (Parquet) sobre object storage.\n",
    "- Transaccionalidad y versiones con capas de metadatos (Delta/Iceberg/Hudi).\n",
    "- Catálogo central (Glue/Unity/Metastore) y gobernanza integrada.\n",
    "- Lectores: engines SQL (Athena/Trino/Spark) + ML + BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a329f1",
   "metadata": {},
   "source": [
    "## 2. Hands-on: tabla Parquet particionada local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c1f50",
   "metadata": {},
   "source": [
    "### 💾 **Parquet: The Columnar Storage Format**\n",
    "\n",
    "**¿Por qué Parquet y no CSV/JSON?**\n",
    "\n",
    "**Query Performance Example:**\n",
    "\n",
    "```python\n",
    "# Dataset: 1M filas × 100 columnas = 100M valores\n",
    "# Query: SELECT AVG(salary) FROM employees WHERE department = 'Engineering'\n",
    "\n",
    "# ❌ CSV (Row-based):\n",
    "for row in read_csv('employees.csv'):  # Lee TODAS las columnas\n",
    "    if row['department'] == 'Engineering':\n",
    "        salaries.append(row['salary'])  # Solo usa 2 columnas\n",
    "\n",
    "# I/O: Lee 100M valores\n",
    "# Time: ~30 segundos\n",
    "\n",
    "# ✅ Parquet (Columnar):\n",
    "departments = read_column('employees.parquet', 'department')\n",
    "salaries = read_column('employees.parquet', 'salary')\n",
    "avg = mean([s for d, s in zip(departments, salaries) if d == 'Engineering'])\n",
    "\n",
    "# I/O: Lee 2M valores (solo 2 columnas)\n",
    "# Time: ~0.6 segundos (50x faster!)\n",
    "```\n",
    "\n",
    "**Parquet File Structure:**\n",
    "\n",
    "```\n",
    "file.parquet\n",
    "├── Header (4 bytes magic: \"PAR1\")\n",
    "├── Row Group 1 (default: 128 MB)\n",
    "│   ├── Column Chunk: id\n",
    "│   │   ├── Data Pages (compressed)\n",
    "│   │   └── Statistics (min, max, null_count)\n",
    "│   ├── Column Chunk: name\n",
    "│   └── Column Chunk: salary\n",
    "├── Row Group 2\n",
    "│   ├── Column Chunk: id\n",
    "│   ├── Column Chunk: name\n",
    "│   └── Column Chunk: salary\n",
    "├── Footer Metadata\n",
    "│   ├── Schema\n",
    "│   ├── Column statistics (per row group)\n",
    "│   └── Compression codec\n",
    "└── Footer (4 bytes magic: \"PAR1\")\n",
    "```\n",
    "\n",
    "**Compression Codecs:**\n",
    "\n",
    "```python\n",
    "df.to_parquet('data.parquet', compression='snappy')\n",
    "\n",
    "# Benchmark: 1M rows, 10 columns\n",
    "compression_results = {\n",
    "    'none': {\n",
    "        'size_mb': 100,\n",
    "        'write_s': 2.1,\n",
    "        'read_s': 1.5,\n",
    "        'ratio': '1x'\n",
    "    },\n",
    "    'snappy': {\n",
    "        'size_mb': 33,\n",
    "        'write_s': 2.8,\n",
    "        'read_s': 1.8,\n",
    "        'ratio': '3x',\n",
    "        'verdict': '✅ Default (balance speed/compression)'\n",
    "    },\n",
    "    'gzip': {\n",
    "        'size_mb': 20,\n",
    "        'write_s': 8.5,\n",
    "        'read_s': 4.2,\n",
    "        'ratio': '5x',\n",
    "        'verdict': '⚠️ Better compression, slower'\n",
    "    },\n",
    "    'zstd': {\n",
    "        'size_mb': 22,\n",
    "        'write_s': 3.5,\n",
    "        'read_s': 2.0,\n",
    "        'ratio': '4.5x',\n",
    "        'verdict': '✅ Modern (better than snappy)'\n",
    "    },\n",
    "    'lz4': {\n",
    "        'size_mb': 35,\n",
    "        'write_s': 2.2,\n",
    "        'read_s': 1.6,\n",
    "        'ratio': '2.8x',\n",
    "        'verdict': '⚡ Fastest compression'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Encoding Schemes:**\n",
    "\n",
    "```python\n",
    "# Plain Encoding (default for small datasets)\n",
    "values = [100, 200, 150, 175, 100]\n",
    "encoded = b'\\x64\\x00\\xc8\\x00\\x96\\x00\\xaf\\x00\\x64\\x00'  # Raw bytes\n",
    "\n",
    "# Dictionary Encoding (for low cardinality)\n",
    "# Column: [\"Engineering\", \"Sales\", \"Engineering\", \"Engineering\", \"Sales\"]\n",
    "dictionary = [\"Engineering\", \"Sales\"]\n",
    "indices = [0, 1, 0, 0, 1]  # Store indices (2 bits each vs 11 bytes per string)\n",
    "# Compression: ~85%\n",
    "\n",
    "# Run-Length Encoding (for repeated values)\n",
    "# Column: [100, 100, 100, 200, 200, 300]\n",
    "rle = [(100, 3), (200, 2), (300, 1)]  # (value, count)\n",
    "\n",
    "# Delta Encoding (for sorted/incremental data)\n",
    "# Column: [1000, 1001, 1002, 1003, 1004]\n",
    "delta = [1000, 1, 1, 1, 1]  # Base + deltas\n",
    "```\n",
    "\n",
    "**Partition Pruning (Data Skipping):**\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "├── year=2023/\n",
    "│   ├── month=01/data.parquet\n",
    "│   └── month=02/data.parquet\n",
    "├── year=2024/\n",
    "│   ├── month=01/data.parquet\n",
    "│   └── month=12/data.parquet\n",
    "└── year=2025/\n",
    "    ├── month=01/data.parquet\n",
    "    └── month=10/data.parquet  ← Only scan this!\n",
    "\n",
    "# Query:\n",
    "SELECT * FROM sales WHERE year=2025 AND month=10\n",
    "\n",
    "# Without partitioning: Scan 7 files\n",
    "# With partitioning: Scan 1 file (85% reduction!)\n",
    "```\n",
    "\n",
    "**Small Files Problem:**\n",
    "\n",
    "```python\n",
    "# ❌ Anti-pattern: Too many small files\n",
    "for record in stream:\n",
    "    df = pd.DataFrame([record])\n",
    "    df.to_parquet(f's3://bucket/data/record_{record[\"id\"]}.parquet')\n",
    "\n",
    "# Result: 1M files × 1 KB each = Overhead disaster!\n",
    "# Athena cost: $5/TB scanned + $0.002/file = $$$$\n",
    "\n",
    "# ✅ Solution 1: Buffering\n",
    "buffer = []\n",
    "for record in stream:\n",
    "    buffer.append(record)\n",
    "    if len(buffer) >= 10000:\n",
    "        pd.DataFrame(buffer).to_parquet(f's3://bucket/data/batch_{timestamp}.parquet')\n",
    "        buffer = []\n",
    "\n",
    "# ✅ Solution 2: Compaction (OPTIMIZE in Delta)\n",
    "spark.sql(\"OPTIMIZE sales_table\")\n",
    "# Merges small files into 128MB-1GB files\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# ❌ Over-partitioning (too granular)\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\", \"hour\", \"customer_id\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Result: Millions of partitions → slow metadata operations\n",
    "\n",
    "# ✅ Balanced partitioning\n",
    "df.write.partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Target: 128MB-1GB per partition\n",
    "# Rule: Partition columns used in 80%+ of queries\n",
    "\n",
    "# ⚡ Z-ordering (Delta Lake optimization)\n",
    "spark.sql(\"OPTIMIZE sales_table ZORDER BY (customer_id, product_id)\")\n",
    "# Co-locates related data for better data skipping\n",
    "```\n",
    "\n",
    "**Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# V1: Initial schema\n",
    "df_v1 = pd.DataFrame({\n",
    "    'id': [1, 2],\n",
    "    'name': ['Alice', 'Bob']\n",
    "})\n",
    "df_v1.to_parquet('users_v1.parquet')\n",
    "\n",
    "# V2: Add column (compatible)\n",
    "df_v2 = pd.DataFrame({\n",
    "    'id': [3, 4],\n",
    "    'name': ['Charlie', 'Diana'],\n",
    "    'email': ['c@x.com', 'd@x.com']  # New column\n",
    "})\n",
    "df_v2.to_parquet('users_v2.parquet')\n",
    "\n",
    "# Reading both files:\n",
    "df = pd.concat([\n",
    "    pd.read_parquet('users_v1.parquet'),  # email will be null\n",
    "    pd.read_parquet('users_v2.parquet')\n",
    "])\n",
    "# Parquet handles missing columns gracefully\n",
    "\n",
    "# ❌ Breaking change: Rename/delete column\n",
    "df_v3 = pd.DataFrame({\n",
    "    'user_id': [5, 6],  # Renamed from 'id'\n",
    "    'full_name': ['Eve', 'Frank']  # Renamed from 'name'\n",
    "})\n",
    "# This will break queries expecting 'id' column!\n",
    "```\n",
    "\n",
    "**Parquet + Pandas/Polars:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read specific columns (projection)\n",
    "df = pd.read_parquet('sales.parquet', columns=['id', 'total'])\n",
    "\n",
    "# Read with filters (predicate pushdown)\n",
    "df = pd.read_parquet('sales.parquet', \n",
    "                     filters=[('year', '=', 2025), ('total', '>', 1000)])\n",
    "\n",
    "# Read row groups (chunked reading for large files)\n",
    "parquet_file = pq.ParquetFile('sales.parquet')\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    df = batch.to_pandas()\n",
    "    process(df)\n",
    "\n",
    "# Read from S3 directly\n",
    "df = pd.read_parquet('s3://bucket/sales/year=2025/month=10/*.parquet')\n",
    "```\n",
    "\n",
    "**Monitoring Parquet Health:**\n",
    "\n",
    "```python\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def analyze_parquet_file(path):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    metadata = pf.metadata\n",
    "    \n",
    "    metrics = {\n",
    "        'num_row_groups': metadata.num_row_groups,\n",
    "        'num_rows': metadata.num_rows,\n",
    "        'num_columns': metadata.num_columns,\n",
    "        'file_size_mb': os.path.getsize(path) / (1024**2),\n",
    "        'avg_row_group_size_mb': os.path.getsize(path) / metadata.num_row_groups / (1024**2),\n",
    "        'compression': pf.schema_arrow.metadata.get(b'compression', b'unknown').decode()\n",
    "    }\n",
    "    \n",
    "    # Health checks\n",
    "    if metrics['avg_row_group_size_mb'] < 64:\n",
    "        print(\"⚠️ Row groups too small (target: 128MB)\")\n",
    "    \n",
    "    if metrics['file_size_mb'] < 10:\n",
    "        print(\"⚠️ File too small (target: >100MB)\")\n",
    "    \n",
    "    if metrics['num_row_groups'] > 10:\n",
    "        print(\"⚠️ Too many row groups (consider repartitioning)\")\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path('datasets/processed/lakehouse_demo')\n",
    "(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'id':[1,2,3,4],\n",
    "  'fecha':['2025-10-01','2025-10-02','2025-10-02','2025-10-03'],\n",
    "  'producto_id':[101,102,101,103],\n",
    "  'cantidad':[1,2,1,3],\n",
    "  'precio':[100.0,50.0,100.0,20.0]\n",
    "})\n",
    "df['total'] = df['cantidad'] * df['precio']\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.strftime('%Y-%m')\n",
    "\n",
    "for (anio, mes), part in df.groupby(['anio','mes']):\n",
    "    part_dir = BASE / f'anio={anio}' / f'mes={mes}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fp = part_dir / f'ventas_{int(time.time())}.parquet'\n",
    "    part.to_parquet(fp, index=False)\n",
    "str(BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457603",
   "metadata": {},
   "source": [
    "### 2.1 Lectura particionada y pruning manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = list((BASE / 'anio=2025' / 'mes=2025-10').glob('*.parquet'))\n",
    "pd.concat([pd.read_parquet(p) for p in parts]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549cff5",
   "metadata": {},
   "source": [
    "## 3. Delta Lake/Iceberg: cómo y cuándo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e4cb",
   "metadata": {},
   "source": [
    "### ⚡ **Delta Lake vs Apache Iceberg: Deep Dive**\n",
    "\n",
    "**When to Choose Which?**\n",
    "\n",
    "```python\n",
    "decision_matrix = {\n",
    "    'Use Delta Lake if': [\n",
    "        'Already using Databricks',\n",
    "        'Primary engine is Spark',\n",
    "        'Need mature ecosystem (more tools)',\n",
    "        'Streaming workloads (Spark Structured Streaming)',\n",
    "        'Team familiar with Delta Lake'\n",
    "    ],\n",
    "    'Use Apache Iceberg if': [\n",
    "        'Multi-engine strategy (Spark + Trino + Flink + Athena)',\n",
    "        'AWS-centric (Athena native support)',\n",
    "        'Need hidden partitioning (automatic partition management)',\n",
    "        'Open governance important (Apache vs vendor)',\n",
    "        'Future-proofing (growing adoption)'\n",
    "    ],\n",
    "    'Use Apache Hudi if': [\n",
    "        'Heavy upsert/CDC workloads',\n",
    "        'Record-level updates critical',\n",
    "        'Uber-style use case'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Comparison:**\n",
    "\n",
    "**1. ACID Transactions:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Optimistic Concurrency Control\n",
    "@transaction\n",
    "def update_delta_table():\n",
    "    current_version = read_version()  # v5\n",
    "    # ... perform transformation ...\n",
    "    try:\n",
    "        commit_new_version(v6, based_on=v5)\n",
    "    except ConflictException:\n",
    "        # Another writer committed v6 first\n",
    "        retry_with_new_base(v6)\n",
    "\n",
    "# Apache Iceberg: Snapshot Isolation\n",
    "@transaction\n",
    "def update_iceberg_table():\n",
    "    snapshot_id = current_snapshot()\n",
    "    # ... perform transformation ...\n",
    "    new_snapshot = create_snapshot(changes)\n",
    "    atomic_swap(current_snapshot, new_snapshot)\n",
    "    # Readers on old snapshot unaffected\n",
    "```\n",
    "\n",
    "**2. Time Travel:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# Files: _delta_log/00000000000000000005.json\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 5) \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20 00:00:00\") \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "# Apache Iceberg\n",
    "# Files: metadata/snap-xxxx-1-yy.avro\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", 12345678) \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"as-of-timestamp\", \"1730246400000\") \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "# Iceberg advantage: Faster time travel (O(1) vs O(n))\n",
    "# Delta: Must replay transaction log from start\n",
    "# Iceberg: Direct snapshot lookup\n",
    "```\n",
    "\n",
    "**3. Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# ✅ ADD COLUMN (supported)\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (discount DOUBLE)\")\n",
    "\n",
    "# ⚠️ RENAME COLUMN (manual migration needed)\n",
    "# Step 1: Add new column\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (customer_email STRING)\")\n",
    "# Step 2: Backfill\n",
    "spark.sql(\"UPDATE orders SET customer_email = email\")\n",
    "# Step 3: Drop old column\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN email\")\n",
    "\n",
    "# ❌ CHANGE COLUMN TYPE (not supported)\n",
    "# Must create new table\n",
    "\n",
    "# Apache Iceberg\n",
    "# ✅ ADD COLUMN\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMN discount double\")\n",
    "\n",
    "# ✅ RENAME COLUMN (supported!)\n",
    "spark.sql(\"ALTER TABLE orders RENAME COLUMN email TO customer_email\")\n",
    "\n",
    "# ✅ DROP COLUMN\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN discount\")\n",
    "\n",
    "# ⚠️ CHANGE TYPE (limited support)\n",
    "spark.sql(\"ALTER TABLE orders ALTER COLUMN id TYPE bigint\")  # int → bigint OK\n",
    "# int → string: Not supported (data loss risk)\n",
    "```\n",
    "\n",
    "**4. Partition Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Manual partition management\n",
    "# Initial: Partitioned by date\n",
    "df.write.partitionBy(\"date\").format(\"delta\").save(\"orders\")\n",
    "\n",
    "# Later: Want to partition by date + country\n",
    "# ❌ Problem: Must rewrite all data\n",
    "df_all = spark.read.format(\"delta\").load(\"orders\")\n",
    "df_all.write.partitionBy(\"date\", \"country\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"orders_new\")\n",
    "\n",
    "# Apache Iceberg: Hidden partitions (automatic!)\n",
    "# Initial: Partitioned by date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE orders (\n",
    "        id bigint,\n",
    "        date date,\n",
    "        country string,\n",
    "        total double\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(date))\n",
    "\"\"\")\n",
    "\n",
    "# Later: Change partitioning strategy (NO data rewrite!)\n",
    "spark.sql(\"ALTER TABLE orders DROP PARTITION FIELD days(date)\")\n",
    "spark.sql(\"ALTER TABLE orders ADD PARTITION FIELD bucket(10, country)\")\n",
    "\n",
    "# Iceberg tracks partition evolution in metadata\n",
    "# Old data: date partitions\n",
    "# New data: country buckets\n",
    "# Queries work seamlessly across both!\n",
    "```\n",
    "\n",
    "**5. File Management (VACUUM/OPTIMIZE):**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# OPTIMIZE: Compact small files\n",
    "spark.sql(\"OPTIMIZE orders\")\n",
    "# Target: 1GB files\n",
    "\n",
    "# Z-ORDER: Co-locate data\n",
    "spark.sql(\"OPTIMIZE orders ZORDER BY (customer_id, product_id)\")\n",
    "\n",
    "# VACUUM: Delete old files\n",
    "spark.sql(\"VACUUM orders RETAIN 168 HOURS\")  # Keep 7 days\n",
    "# Deletes:\n",
    "# - Old data files (overwritten by OPTIMIZE)\n",
    "# - Transaction log checkpoints\n",
    "\n",
    "# Apache Iceberg\n",
    "# REWRITE DATA FILES (equivalent to OPTIMIZE)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.rewrite_data_files(\n",
    "        table => 'db.orders',\n",
    "        options => map(\n",
    "            'target-file-size-bytes', '1073741824',  -- 1GB\n",
    "            'min-input-files', '5'\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# EXPIRE SNAPSHOTS (equivalent to VACUUM)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.expire_snapshots(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-23 00:00:00',\n",
    "        retain_last => 7\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# REMOVE ORPHAN FILES\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.remove_orphan_files(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-20 00:00:00'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**6. Multi-Engine Support:**\n",
    "\n",
    "| Engine | Delta Lake | Iceberg |\n",
    "|--------|------------|---------|\n",
    "| **Spark** | ✅ Native | ✅ Native |\n",
    "| **Presto/Trino** | ✅ Connector | ✅ Native |\n",
    "| **Flink** | ❌ Limited | ✅ Native |\n",
    "| **AWS Athena** | ⚠️ Via manifests | ✅ Native |\n",
    "| **Dremio** | ✅ | ✅ |\n",
    "| **Snowflake** | ⚠️ Via external tables | ✅ Iceberg Tables |\n",
    "\n",
    "**7. Metadata Performance:**\n",
    "\n",
    "```python\n",
    "# Scenario: Table with 10,000 partitions\n",
    "\n",
    "# Delta Lake:\n",
    "# - Metadata: JSON files in _delta_log/\n",
    "# - List partitions: O(n) scan of transaction log\n",
    "# - Overhead: ~100MB metadata for large tables\n",
    "\n",
    "# Iceberg:\n",
    "# - Metadata: Avro files with manifest list → manifest files\n",
    "# - List partitions: O(log n) via manifest index\n",
    "# - Overhead: ~10MB metadata for same table\n",
    "\n",
    "# Benchmark: SHOW PARTITIONS\n",
    "# Delta: 15 seconds\n",
    "# Iceberg: 0.5 seconds (30x faster!)\n",
    "```\n",
    "\n",
    "**8. Streaming Support:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Spark Structured Streaming native\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"orders\")\n",
    "\n",
    "stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints\") \\\n",
    "    .start(\"orders_processed\")\n",
    "\n",
    "# Apache Iceberg: Flink + Spark Streaming\n",
    "# Flink (primary streaming engine for Iceberg)\n",
    "TableEnvironment tableEnv = ...\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    CREATE TABLE orders (...)\n",
    "    WITH ('connector' = 'iceberg', ...)\n",
    "\"\"\")\n",
    "\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    INSERT INTO orders_processed\n",
    "    SELECT * FROM orders\n",
    "    WHERE total > 1000\n",
    "\"\"\")\n",
    "\n",
    "# Spark Streaming (also supported)\n",
    "spark.readStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"orders\")\n",
    "```\n",
    "\n",
    "**Real-World Migration Story:**\n",
    "\n",
    "```python\n",
    "# Company: Netflix → Apple (hypothetical)\n",
    "\n",
    "# Phase 1: Dual-write (6 months)\n",
    "df.write.format(\"delta\").save(\"s3://bucket/delta/orders\")\n",
    "df.write.format(\"iceberg\").save(\"s3://bucket/iceberg/orders\")\n",
    "\n",
    "# Phase 2: Validate (3 months)\n",
    "delta_count = spark.read.format(\"delta\").load(...).count()\n",
    "iceberg_count = spark.read.format(\"iceberg\").load(...).count()\n",
    "assert delta_count == iceberg_count\n",
    "\n",
    "# Phase 3: Cutover (1 week)\n",
    "# Redirect readers to Iceberg\n",
    "# Stop Delta writes\n",
    "\n",
    "# Phase 4: Cleanup (1 month)\n",
    "# Delete Delta files\n",
    "# Reclaim storage\n",
    "\n",
    "# Total timeline: 10 months\n",
    "# Cost: ~$50K engineering effort\n",
    "# Benefit: 30% query performance improvement\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00324956",
   "metadata": {},
   "source": [
    "- Delta Lake agrega ACID, time travel y MERGE INTO sobre Parquet.\n",
    "- Iceberg optimiza la gestión de metadatos, particiones ocultas y evolución de esquema.\n",
    "- Requiere motor como Spark/Trino/Flint y un catálogo (Glue/REST).\n",
    "- Coste/beneficio: evalúa volumen, concurrencia, latencia y SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e6",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo Delta Lake (referencia con PySpark) [opcional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_demo = r'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName('DeltaDemo')\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.read.parquet('s3://bucket/curated/ventas/')\n",
    "df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\n",
    "\n",
    "delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\n",
    "delta.createOrReplaceTempView('ventas')\n",
    "spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()\n",
    "'''\n",
    "print(delta_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b564",
   "metadata": {},
   "source": [
    "## 4. Buenas prácticas de Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccb0bc",
   "metadata": {},
   "source": [
    "- Definir contratos de datos y versionado de esquemas.\n",
    "- Gestionar tamaños de archivos y compaction (OPTIMIZE/VACUUM).\n",
    "- Catalogación y políticas de acceso por dominio (Data Mesh).\n",
    "- Observabilidad y linaje (OpenLineage/Marquez, DataHub)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
