{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da7d8cb",
   "metadata": {},
   "source": [
    "# \ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)\n",
    "\n",
    "Objetivo: comprender los principios de Lakehouse y practicar un flujo b\u00e1sico con Parquet (local) y notas de c\u00f3mo migrar a Delta Lake o Apache Iceberg.\n",
    "\n",
    "- Duraci\u00f3n: 120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Mid 03 (AWS/S3) y 07 (Particionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a71667",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Lakehouse Architecture: Unificando Data Warehouse y Data Lake**\n",
    "\n",
    "**La Evoluci\u00f3n de Arquitecturas de Datos:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  GENERACI\u00d3N 1 (2000s): Data Warehouse                    \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502  Structured Data \u2192 RDBMS (Oracle)  \u2502                  \u2502\n",
    "\u2502  \u2502  OLAP \u2192 Star/Snowflake Schema      \u2502                  \u2502\n",
    "\u2502  \u2502  BI Tools \u2192 SQL Queries            \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502  \u2705 ACID, Performance      \u274c Caro, No soporta ML        \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  GENERACI\u00d3N 2 (2010s): Data Lake                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502  All Data (structured + unstr.)    \u2502                  \u2502\n",
    "\u2502  \u2502  \u2192 S3/HDFS (cheap storage)         \u2502                  \u2502\n",
    "\u2502  \u2502  \u2192 Spark/Presto (compute layer)    \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502  \u2705 Escalable, Barato    \u274c No ACID, Data Swamp          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  GENERACI\u00d3N 3 (2020s): LAKEHOUSE                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
    "\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                  \u2502\n",
    "\u2502  \u2502  \u2502  Metadata Layer (Delta/Ice)  \u2502  \u2502 \u2190 Transacciones  \u2502\n",
    "\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u2502\n",
    "\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                  \u2502\n",
    "\u2502  \u2502  \u2502  Columnar Format (Parquet)   \u2502  \u2502 \u2190 Performance    \u2502\n",
    "\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u2502\n",
    "\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                  \u2502\n",
    "\u2502  \u2502  \u2502  Object Storage (S3/ADLS)    \u2502  \u2502 \u2190 Escalabilidad  \u2502\n",
    "\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
    "\u2502  \u2705 ACID + Escala + Barato + ML/BI    \u274c Complejidad     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**\u00bfQu\u00e9 es un Lakehouse?**\n",
    "\n",
    "Arquitectura que combina lo mejor de **Data Warehouse** (transacciones, performance) con **Data Lake** (escalabilidad, costo-efectividad).\n",
    "\n",
    "**Componentes Clave:**\n",
    "\n",
    "```python\n",
    "lakehouse_stack = {\n",
    "    'Storage Layer': {\n",
    "        'Technology': 'S3, ADLS, GCS',\n",
    "        'Format': 'Parquet (columnar)',\n",
    "        'Cost': '$0.023/GB/mes',\n",
    "        'Benefit': 'Almacenamiento infinito y barato'\n",
    "    },\n",
    "    'Metadata Layer': {\n",
    "        'Technology': 'Delta Lake, Apache Iceberg, Apache Hudi',\n",
    "        'Features': 'ACID, Time Travel, Schema Evolution',\n",
    "        'Benefit': 'Transacciones sobre object storage'\n",
    "    },\n",
    "    'Catalog Layer': {\n",
    "        'Technology': 'AWS Glue, Unity Catalog, Hive Metastore',\n",
    "        'Features': 'Metadata management, Permissions',\n",
    "        'Benefit': 'Single source of truth'\n",
    "    },\n",
    "    'Compute Layer': {\n",
    "        'Technology': 'Spark, Presto, Athena, Trino',\n",
    "        'Features': 'SQL queries, Distributed processing',\n",
    "        'Benefit': 'Separaci\u00f3n storage/compute'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Table Formats Comparison:**\n",
    "\n",
    "| Feature | **Delta Lake** | **Apache Iceberg** | **Apache Hudi** |\n",
    "|---------|----------------|-------------------|-----------------|\n",
    "| **Creator** | Databricks (2019) | Netflix (2017) | Uber (2016) |\n",
    "| **ACID** | \u2705 Optimistic locking | \u2705 Snapshot isolation | \u2705 MVCC |\n",
    "| **Time Travel** | \u2705 Version history | \u2705 Snapshot-based | \u2705 Commit timeline |\n",
    "| **Schema Evolution** | \u2705 ADD/DROP cols | \u2705 Full evolution | \u2705 Partial |\n",
    "| **Partition Evolution** | \u274c Manual | \u2705 Hidden partitions | \u274c Manual |\n",
    "| **Streaming** | \u2705 Spark Streaming | \u2705 Flink, Spark | \u2705 DeltaStreamer |\n",
    "| **Engines** | Spark, Presto, Trino | Spark, Flink, Trino, Athena | Spark, Presto |\n",
    "| **Maturity** | \u2b50\u2b50\u2b50 High | \u2b50\u2b50\u2b50 High | \u2b50\u2b50 Medium |\n",
    "| **Governance** | Databricks (vendor) | Apache (neutral) | Apache (neutral) |\n",
    "| **Best For** | Databricks users | Multi-engine, AWS | Upserts, CDC |\n",
    "\n",
    "**Real-World Adoption:**\n",
    "\n",
    "```\n",
    "Delta Lake:\n",
    "  - Databricks customers (obviamente)\n",
    "  - Comcast (Petabyte-scale)\n",
    "  - Riot Games (Gaming analytics)\n",
    "\n",
    "Apache Iceberg:\n",
    "  - Netflix (originator, 100+ PB)\n",
    "  - Apple (iCloud data)\n",
    "  - Adobe (Experience Cloud)\n",
    "  - AWS (Athena native support)\n",
    "\n",
    "Apache Hudi:\n",
    "  - Uber (ride-hailing data)\n",
    "  - Amazon (internal use)\n",
    "  - Disney+ (streaming analytics)\n",
    "```\n",
    "\n",
    "**Arquitectura de Datos Moderna:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     DATA SOURCES                         \u2502\n",
    "\u2502  Databases, APIs, Streams, Files, SaaS, IoT             \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                       \u2502 Ingest (Fivetran, Airbyte, Custom)\n",
    "                       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    BRONZE LAYER                          \u2502\n",
    "\u2502              (Raw data, append-only)                     \u2502\n",
    "\u2502  Format: Parquet, Delta, Iceberg                        \u2502\n",
    "\u2502  Partitioned by: ingestion_date                         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                       \u2502 Transformation (dbt, Spark)\n",
    "                       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    SILVER LAYER                          \u2502\n",
    "\u2502         (Cleaned, validated, deduplicated)              \u2502\n",
    "\u2502  Format: Delta Lake (ACID needed)                       \u2502\n",
    "\u2502  Partitioned by: business dimensions                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                       \u2502 Aggregation, Business Logic\n",
    "                       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     GOLD LAYER                           \u2502\n",
    "\u2502        (Aggregated, business-ready datasets)            \u2502\n",
    "\u2502  Format: Delta Lake or Iceberg                          \u2502\n",
    "\u2502  Consumed by: BI Tools, ML Models, APIs                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Beneficios del Lakehouse:**\n",
    "\n",
    "1. **Unified Platform**:\n",
    "   - Single storage para BI, ML, Data Science\n",
    "   - No m\u00e1s \"copy data from DW to DL for ML\"\n",
    "\n",
    "2. **Cost Reduction**:\n",
    "   - Storage: $0.023/GB vs $25/TB (Snowflake)\n",
    "   - Compute: Pay-per-query (Athena) o Spot instances\n",
    "\n",
    "3. **Performance**:\n",
    "   - Columnar format (10x faster queries)\n",
    "   - Partition pruning (scan solo datos relevantes)\n",
    "   - Caching + predicate pushdown\n",
    "\n",
    "4. **Governance**:\n",
    "   - ACID garantiza consistencia\n",
    "   - Time Travel para auditor\u00eda\n",
    "   - Fine-grained access control\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - M\u00faltiples engines (no vendor lock-in)\n",
    "   - Schema evolution sin downtime\n",
    "   - Support structured + semi-structured\n",
    "\n",
    "**\u00bfCu\u00e1ndo usar Lakehouse?**\n",
    "\n",
    "\u2705 **S\u00cd usar cuando:**\n",
    "- Vol\u00famenes > 100 TB\n",
    "- Necesitas ML + BI sobre mismos datos\n",
    "- M\u00faltiples teams con diferentes tools\n",
    "- Budget limitado vs DW tradicional\n",
    "\n",
    "\u274c **NO usar cuando:**\n",
    "- Datasets < 1 TB (PostgreSQL suficiente)\n",
    "- Team peque\u00f1o (< 5) con skills limitados\n",
    "- Latencia cr\u00edtica < 100ms (considerar OLTP)\n",
    "- Compliance requiere on-prem (limitaciones cloud)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822026",
   "metadata": {},
   "source": [
    "## 1. Lakehouse en pocas l\u00edneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31879",
   "metadata": {},
   "source": [
    "### \ud83d\udcd0 **Lakehouse Fundamentals: Storage + Metadata + Catalog**\n",
    "\n",
    "**1. Storage Layer: Object Storage como Foundation**\n",
    "\n",
    "```python\n",
    "# \u00bfPor qu\u00e9 S3/ADLS/GCS y no HDFS o RDBMS?\n",
    "\n",
    "storage_comparison = {\n",
    "    'HDFS (Hadoop)': {\n",
    "        'cost': '$$$',\n",
    "        'scalability': 'Limited by cluster',\n",
    "        'durability': '99.9% (3 replicas)',\n",
    "        'latency': 'Low (local)',\n",
    "        'ops_complexity': 'High (manage cluster)',\n",
    "        'verdict': '\u274c Legacy, avoid for new projects'\n",
    "    },\n",
    "    'RDBMS (PostgreSQL)': {\n",
    "        'cost': '$$$$',\n",
    "        'scalability': 'Vertical (TB-scale)',\n",
    "        'durability': '99.99% (replication)',\n",
    "        'latency': 'Very Low (<10ms)',\n",
    "        'ops_complexity': 'Medium',\n",
    "        'verdict': '\u2705 For OLTP, \u274c for Analytics at scale'\n",
    "    },\n",
    "    'Object Storage (S3)': {\n",
    "        'cost': '$',\n",
    "        'scalability': 'Infinite (PB-EB scale)',\n",
    "        'durability': '99.999999999% (11 nines)',\n",
    "        'latency': 'Medium (network)',\n",
    "        'ops_complexity': 'Very Low (managed)',\n",
    "        'verdict': '\u2705 Perfect for Lakehouse'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**S3 as Database? The Challenges:**\n",
    "\n",
    "```python\n",
    "# \u274c Problem 1: No ACID transactions\n",
    "# Dos writers simult\u00e1neos \u2192 race condition\n",
    "writer_1_writes_file('s3://bucket/data/part-001.parquet')\n",
    "writer_2_writes_file('s3://bucket/data/part-001.parquet')  # Overwrite!\n",
    "\n",
    "# \u274c Problem 2: No consistency\n",
    "list_objects('s3://bucket/data/')  # May not see latest write immediately\n",
    "\n",
    "# \u274c Problem 3: No indexing\n",
    "# Para find record con id=X \u2192 scan all files (slow!)\n",
    "\n",
    "# \u274c Problem 4: No schema enforcement\n",
    "# Nada previene que alguien escriba schema incompatible\n",
    "```\n",
    "\n",
    "**2. Metadata Layer: The Secret Sauce**\n",
    "\n",
    "**Delta Lake Transaction Log:**\n",
    "\n",
    "```\n",
    "table_path/\n",
    "\u251c\u2500\u2500 _delta_log/\n",
    "\u2502   \u251c\u2500\u2500 00000000000000000000.json  \u2190 Version 0 (CREATE TABLE)\n",
    "\u2502   \u251c\u2500\u2500 00000000000000000001.json  \u2190 Version 1 (INSERT)\n",
    "\u2502   \u251c\u2500\u2500 00000000000000000002.json  \u2190 Version 2 (UPDATE)\n",
    "\u2502   \u2514\u2500\u2500 00000000000000000003.json  \u2190 Version 3 (DELETE)\n",
    "\u251c\u2500\u2500 part-00000-xxx.parquet\n",
    "\u251c\u2500\u2500 part-00001-xxx.parquet\n",
    "\u2514\u2500\u2500 part-00002-xxx.parquet\n",
    "\n",
    "# Contenido de 00000000000000000001.json:\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"timestamp\": 1730246400000,\n",
    "    \"operation\": \"WRITE\",\n",
    "    \"operationMetrics\": {\n",
    "      \"numFiles\": \"2\",\n",
    "      \"numOutputRows\": \"1000\"\n",
    "    }\n",
    "  },\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-xxx.parquet\",\n",
    "    \"size\": 52428800,\n",
    "    \"partitionValues\": {\"year\": \"2025\", \"month\": \"10\"},\n",
    "    \"dataChange\": true,\n",
    "    \"stats\": \"{\\\"numRecords\\\":500,\\\"minValues\\\":{\\\"id\\\":1},\\\"maxValues\\\":{\\\"id\\\":500}}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**C\u00f3mo funciona ACID:**\n",
    "\n",
    "```python\n",
    "# \u2705 Atomicity: All-or-nothing\n",
    "def write_to_delta(df, table_path):\n",
    "    # Step 1: Write Parquet files (no one can see yet)\n",
    "    temp_files = df.write.parquet(f\"{table_path}/_tmp/\")\n",
    "    \n",
    "    # Step 2: Create transaction log entry\n",
    "    version = get_next_version(table_path)\n",
    "    log_entry = {\n",
    "        \"add\": [{\"path\": f, \"size\": size} for f in temp_files]\n",
    "    }\n",
    "    \n",
    "    # Step 3: Atomic commit (write JSON file)\n",
    "    write_json(f\"{table_path}/_delta_log/{version:020d}.json\", log_entry)\n",
    "    # Solo cuando este archivo existe \u2192 datos visibles\n",
    "    \n",
    "    # Si crash antes del Step 3 \u2192 temp files hu\u00e9rfanos (no problema)\n",
    "\n",
    "# \u2705 Consistency: Schema enforcement\n",
    "def validate_write(df, existing_schema):\n",
    "    if df.schema != existing_schema:\n",
    "        if not compatible(df.schema, existing_schema):\n",
    "            raise SchemaIncompatibleException()\n",
    "\n",
    "# \u2705 Isolation: Optimistic concurrency control\n",
    "def concurrent_write(df1, df2):\n",
    "    # Writer 1 reads version 5, writes based on v5\n",
    "    # Writer 2 reads version 5, writes based on v5\n",
    "    \n",
    "    # Writer 1 tries to commit version 6\n",
    "    if current_version == 5:  # OK\n",
    "        commit_version_6()\n",
    "    \n",
    "    # Writer 2 tries to commit version 6\n",
    "    if current_version == 5:  # CONFLICT! (now it's 6)\n",
    "        raise ConcurrentModificationException()\n",
    "        # Writer 2 must retry: read v6, apply changes, commit v7\n",
    "\n",
    "# \u2705 Durability: S3's 11 nines\n",
    "# Once committed, data is durable (S3 guarantee)\n",
    "```\n",
    "\n",
    "**Time Travel (Versioning):**\n",
    "\n",
    "```python\n",
    "# Read current version\n",
    "df = spark.read.format(\"delta\").load(\"s3://bucket/sales\")\n",
    "\n",
    "# Read version from 7 days ago\n",
    "df_v7d = spark.read.format(\"delta\").option(\"versionAsOf\", 7).load(...)\n",
    "\n",
    "# Read version at specific timestamp\n",
    "df_oct20 = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20\") \\\n",
    "    .load(...)\n",
    "\n",
    "# Use cases:\n",
    "# - Reproducibility: \"Show me data exactly as ML model saw it\"\n",
    "# - Auditing: \"What changed between yesterday and today?\"\n",
    "# - Rollback: \"Undo accidental DELETE\"\n",
    "# - A/B testing: \"Compare old vs new transformations\"\n",
    "```\n",
    "\n",
    "**3. Catalog Layer: Metadata Management**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              DATA CATALOG                         \u2502\n",
    "\u2502                                                   \u2502\n",
    "\u2502  Database: sales_prod                            \u2502\n",
    "\u2502  \u251c\u2500 Table: orders                                \u2502\n",
    "\u2502  \u2502   \u251c\u2500 Location: s3://bucket/gold/orders/      \u2502\n",
    "\u2502  \u2502   \u251c\u2500 Format: delta                            \u2502\n",
    "\u2502  \u2502   \u251c\u2500 Schema: {id: bigint, total: double, ...}\u2502\n",
    "\u2502  \u2502   \u251c\u2500 Partitions: [year, month]               \u2502\n",
    "\u2502  \u2502   \u251c\u2500 Owner: data-team@company.com            \u2502\n",
    "\u2502  \u2502   \u251c\u2500 Tags: [PII, critical]                   \u2502\n",
    "\u2502  \u2502   \u2514\u2500 Last Updated: 2025-10-30 14:30:00       \u2502\n",
    "\u2502  \u251c\u2500 Table: customers                             \u2502\n",
    "\u2502  \u2514\u2500 Table: products                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Catalog Implementations:**\n",
    "\n",
    "| Catalog | Provider | Best For | Limitations |\n",
    "|---------|----------|----------|-------------|\n",
    "| **Hive Metastore** | Apache | Open-source, Spark native | Single point of failure |\n",
    "| **AWS Glue Catalog** | AWS | Serverless, AWS-integrated | AWS lock-in |\n",
    "| **Unity Catalog** | Databricks | Unified governance | Databricks only |\n",
    "| **Polaris** | Snowflake | Open-source Iceberg | New (2024) |\n",
    "\n",
    "**Example: Creating Table in Catalog:**\n",
    "\n",
    "```python\n",
    "# Spark + Delta Lake + Glue Catalog\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE sales_prod.orders\n",
    "  USING delta\n",
    "  LOCATION 's3://bucket/gold/orders/'\n",
    "  PARTITIONED BY (year, month)\n",
    "  TBLPROPERTIES (\n",
    "    'delta.dataSkippingNumIndexedCols' = '5',\n",
    "    'delta.deletedFileRetentionDuration' = 'interval 7 days',\n",
    "    'owner' = 'data-team@company.com',\n",
    "    'pii' = 'true'\n",
    "  )\n",
    "  AS SELECT * FROM staging.orders_raw\n",
    "\"\"\")\n",
    "\n",
    "# Query from any engine\n",
    "# Athena:\n",
    "SELECT * FROM sales_prod.orders WHERE year=2025 AND month=10\n",
    "\n",
    "# Presto:\n",
    "SELECT * FROM glue.sales_prod.orders WHERE total > 1000\n",
    "\n",
    "# Spark:\n",
    "spark.table(\"sales_prod.orders\").filter(\"year = 2025\").show()\n",
    "```\n",
    "\n",
    "**Metadata Caching & Performance:**\n",
    "\n",
    "```python\n",
    "# Problem: Reading metadata for each query is slow\n",
    "# Solution: Caching\n",
    "\n",
    "# Delta Lake: Stats in transaction log\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000.parquet\",\n",
    "    \"stats\": \"{\\\"numRecords\\\":1000,\\\"minValues\\\":{\\\"id\\\":1,\\\"date\\\":\\\"2025-10-01\\\"},\\\"maxValues\\\":{\\\"id\\\":1000,\\\"date\\\":\\\"2025-10-31\\\"}}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Query: SELECT * FROM orders WHERE id = 500\n",
    "# Engine reads stats \u2192 knows id \u2208 [1, 1000] \u2192 scan this file\n",
    "# Query: SELECT * FROM orders WHERE id = 5000\n",
    "# Engine reads stats \u2192 knows id \u2208 [1, 1000] \u2192 SKIP this file (data skipping)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a20ee",
   "metadata": {},
   "source": [
    "- Tabla de datos en formato columna (Parquet) sobre object storage.\n",
    "- Transaccionalidad y versiones con capas de metadatos (Delta/Iceberg/Hudi).\n",
    "- Cat\u00e1logo central (Glue/Unity/Metastore) y gobernanza integrada.\n",
    "- Lectores: engines SQL (Athena/Trino/Spark) + ML + BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a329f1",
   "metadata": {},
   "source": [
    "## 2. Hands-on: tabla Parquet particionada local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c1f50",
   "metadata": {},
   "source": [
    "### \ud83d\udcbe **Parquet: The Columnar Storage Format**\n",
    "\n",
    "**\u00bfPor qu\u00e9 Parquet y no CSV/JSON?**\n",
    "\n",
    "**Query Performance Example:**\n",
    "\n",
    "```python\n",
    "# Dataset: 1M filas \u00d7 100 columnas = 100M valores\n",
    "# Query: SELECT AVG(salary) FROM employees WHERE department = 'Engineering'\n",
    "\n",
    "# \u274c CSV (Row-based):\n",
    "for row in read_csv('employees.csv'):  # Lee TODAS las columnas\n",
    "    if row['department'] == 'Engineering':\n",
    "        salaries.append(row['salary'])  # Solo usa 2 columnas\n",
    "\n",
    "# I/O: Lee 100M valores\n",
    "# Time: ~30 segundos\n",
    "\n",
    "# \u2705 Parquet (Columnar):\n",
    "departments = read_column('employees.parquet', 'department')\n",
    "salaries = read_column('employees.parquet', 'salary')\n",
    "avg = mean([s for d, s in zip(departments, salaries) if d == 'Engineering'])\n",
    "\n",
    "# I/O: Lee 2M valores (solo 2 columnas)\n",
    "# Time: ~0.6 segundos (50x faster!)\n",
    "```\n",
    "\n",
    "**Parquet File Structure:**\n",
    "\n",
    "```\n",
    "file.parquet\n",
    "\u251c\u2500\u2500 Header (4 bytes magic: \"PAR1\")\n",
    "\u251c\u2500\u2500 Row Group 1 (default: 128 MB)\n",
    "\u2502   \u251c\u2500\u2500 Column Chunk: id\n",
    "\u2502   \u2502   \u251c\u2500\u2500 Data Pages (compressed)\n",
    "\u2502   \u2502   \u2514\u2500\u2500 Statistics (min, max, null_count)\n",
    "\u2502   \u251c\u2500\u2500 Column Chunk: name\n",
    "\u2502   \u2514\u2500\u2500 Column Chunk: salary\n",
    "\u251c\u2500\u2500 Row Group 2\n",
    "\u2502   \u251c\u2500\u2500 Column Chunk: id\n",
    "\u2502   \u251c\u2500\u2500 Column Chunk: name\n",
    "\u2502   \u2514\u2500\u2500 Column Chunk: salary\n",
    "\u251c\u2500\u2500 Footer Metadata\n",
    "\u2502   \u251c\u2500\u2500 Schema\n",
    "\u2502   \u251c\u2500\u2500 Column statistics (per row group)\n",
    "\u2502   \u2514\u2500\u2500 Compression codec\n",
    "\u2514\u2500\u2500 Footer (4 bytes magic: \"PAR1\")\n",
    "```\n",
    "\n",
    "**Compression Codecs:**\n",
    "\n",
    "```python\n",
    "df.to_parquet('data.parquet', compression='snappy')\n",
    "\n",
    "# Benchmark: 1M rows, 10 columns\n",
    "compression_results = {\n",
    "    'none': {\n",
    "        'size_mb': 100,\n",
    "        'write_s': 2.1,\n",
    "        'read_s': 1.5,\n",
    "        'ratio': '1x'\n",
    "    },\n",
    "    'snappy': {\n",
    "        'size_mb': 33,\n",
    "        'write_s': 2.8,\n",
    "        'read_s': 1.8,\n",
    "        'ratio': '3x',\n",
    "        'verdict': '\u2705 Default (balance speed/compression)'\n",
    "    },\n",
    "    'gzip': {\n",
    "        'size_mb': 20,\n",
    "        'write_s': 8.5,\n",
    "        'read_s': 4.2,\n",
    "        'ratio': '5x',\n",
    "        'verdict': '\u26a0\ufe0f Better compression, slower'\n",
    "    },\n",
    "    'zstd': {\n",
    "        'size_mb': 22,\n",
    "        'write_s': 3.5,\n",
    "        'read_s': 2.0,\n",
    "        'ratio': '4.5x',\n",
    "        'verdict': '\u2705 Modern (better than snappy)'\n",
    "    },\n",
    "    'lz4': {\n",
    "        'size_mb': 35,\n",
    "        'write_s': 2.2,\n",
    "        'read_s': 1.6,\n",
    "        'ratio': '2.8x',\n",
    "        'verdict': '\u26a1 Fastest compression'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Encoding Schemes:**\n",
    "\n",
    "```python\n",
    "# Plain Encoding (default for small datasets)\n",
    "values = [100, 200, 150, 175, 100]\n",
    "encoded = b'\\x64\\x00\\xc8\\x00\\x96\\x00\\xaf\\x00\\x64\\x00'  # Raw bytes\n",
    "\n",
    "# Dictionary Encoding (for low cardinality)\n",
    "# Column: [\"Engineering\", \"Sales\", \"Engineering\", \"Engineering\", \"Sales\"]\n",
    "dictionary = [\"Engineering\", \"Sales\"]\n",
    "indices = [0, 1, 0, 0, 1]  # Store indices (2 bits each vs 11 bytes per string)\n",
    "# Compression: ~85%\n",
    "\n",
    "# Run-Length Encoding (for repeated values)\n",
    "# Column: [100, 100, 100, 200, 200, 300]\n",
    "rle = [(100, 3), (200, 2), (300, 1)]  # (value, count)\n",
    "\n",
    "# Delta Encoding (for sorted/incremental data)\n",
    "# Column: [1000, 1001, 1002, 1003, 1004]\n",
    "delta = [1000, 1, 1, 1, 1]  # Base + deltas\n",
    "```\n",
    "\n",
    "**Partition Pruning (Data Skipping):**\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "\u251c\u2500\u2500 year=2023/\n",
    "\u2502   \u251c\u2500\u2500 month=01/data.parquet\n",
    "\u2502   \u2514\u2500\u2500 month=02/data.parquet\n",
    "\u251c\u2500\u2500 year=2024/\n",
    "\u2502   \u251c\u2500\u2500 month=01/data.parquet\n",
    "\u2502   \u2514\u2500\u2500 month=12/data.parquet\n",
    "\u2514\u2500\u2500 year=2025/\n",
    "    \u251c\u2500\u2500 month=01/data.parquet\n",
    "    \u2514\u2500\u2500 month=10/data.parquet  \u2190 Only scan this!\n",
    "\n",
    "# Query:\n",
    "SELECT * FROM sales WHERE year=2025 AND month=10\n",
    "\n",
    "# Without partitioning: Scan 7 files\n",
    "# With partitioning: Scan 1 file (85% reduction!)\n",
    "```\n",
    "\n",
    "**Small Files Problem:**\n",
    "\n",
    "```python\n",
    "# \u274c Anti-pattern: Too many small files\n",
    "for record in stream:\n",
    "    df = pd.DataFrame([record])\n",
    "    df.to_parquet(f's3://bucket/data/record_{record[\"id\"]}.parquet')\n",
    "\n",
    "# Result: 1M files \u00d7 1 KB each = Overhead disaster!\n",
    "# Athena cost: $5/TB scanned + $0.002/file = $$$$\n",
    "\n",
    "# \u2705 Solution 1: Buffering\n",
    "buffer = []\n",
    "for record in stream:\n",
    "    buffer.append(record)\n",
    "    if len(buffer) >= 10000:\n",
    "        pd.DataFrame(buffer).to_parquet(f's3://bucket/data/batch_{timestamp}.parquet')\n",
    "        buffer = []\n",
    "\n",
    "# \u2705 Solution 2: Compaction (OPTIMIZE in Delta)\n",
    "spark.sql(\"OPTIMIZE sales_table\")\n",
    "# Merges small files into 128MB-1GB files\n",
    "```\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "```python\n",
    "# \u274c Over-partitioning (too granular)\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\", \"hour\", \"customer_id\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Result: Millions of partitions \u2192 slow metadata operations\n",
    "\n",
    "# \u2705 Balanced partitioning\n",
    "df.write.partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://bucket/sales\")\n",
    "# Target: 128MB-1GB per partition\n",
    "# Rule: Partition columns used in 80%+ of queries\n",
    "\n",
    "# \u26a1 Z-ordering (Delta Lake optimization)\n",
    "spark.sql(\"OPTIMIZE sales_table ZORDER BY (customer_id, product_id)\")\n",
    "# Co-locates related data for better data skipping\n",
    "```\n",
    "\n",
    "**Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# V1: Initial schema\n",
    "df_v1 = pd.DataFrame({\n",
    "    'id': [1, 2],\n",
    "    'name': ['Alice', 'Bob']\n",
    "})\n",
    "df_v1.to_parquet('users_v1.parquet')\n",
    "\n",
    "# V2: Add column (compatible)\n",
    "df_v2 = pd.DataFrame({\n",
    "    'id': [3, 4],\n",
    "    'name': ['Charlie', 'Diana'],\n",
    "    'email': ['c@x.com', 'd@x.com']  # New column\n",
    "})\n",
    "df_v2.to_parquet('users_v2.parquet')\n",
    "\n",
    "# Reading both files:\n",
    "df = pd.concat([\n",
    "    pd.read_parquet('users_v1.parquet'),  # email will be null\n",
    "    pd.read_parquet('users_v2.parquet')\n",
    "])\n",
    "# Parquet handles missing columns gracefully\n",
    "\n",
    "# \u274c Breaking change: Rename/delete column\n",
    "df_v3 = pd.DataFrame({\n",
    "    'user_id': [5, 6],  # Renamed from 'id'\n",
    "    'full_name': ['Eve', 'Frank']  # Renamed from 'name'\n",
    "})\n",
    "# This will break queries expecting 'id' column!\n",
    "```\n",
    "\n",
    "**Parquet + Pandas/Polars:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read specific columns (projection)\n",
    "df = pd.read_parquet('sales.parquet', columns=['id', 'total'])\n",
    "\n",
    "# Read with filters (predicate pushdown)\n",
    "df = pd.read_parquet('sales.parquet', \n",
    "                     filters=[('year', '=', 2025), ('total', '>', 1000)])\n",
    "\n",
    "# Read row groups (chunked reading for large files)\n",
    "parquet_file = pq.ParquetFile('sales.parquet')\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    df = batch.to_pandas()\n",
    "    process(df)\n",
    "\n",
    "# Read from S3 directly\n",
    "df = pd.read_parquet('s3://bucket/sales/year=2025/month=10/*.parquet')\n",
    "```\n",
    "\n",
    "**Monitoring Parquet Health:**\n",
    "\n",
    "```python\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def analyze_parquet_file(path):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    metadata = pf.metadata\n",
    "    \n",
    "    metrics = {\n",
    "        'num_row_groups': metadata.num_row_groups,\n",
    "        'num_rows': metadata.num_rows,\n",
    "        'num_columns': metadata.num_columns,\n",
    "        'file_size_mb': os.path.getsize(path) / (1024**2),\n",
    "        'avg_row_group_size_mb': os.path.getsize(path) / metadata.num_row_groups / (1024**2),\n",
    "        'compression': pf.schema_arrow.metadata.get(b'compression', b'unknown').decode()\n",
    "    }\n",
    "    \n",
    "    # Health checks\n",
    "    if metrics['avg_row_group_size_mb'] < 64:\n",
    "        print(\"\u26a0\ufe0f Row groups too small (target: 128MB)\")\n",
    "    \n",
    "    if metrics['file_size_mb'] < 10:\n",
    "        print(\"\u26a0\ufe0f File too small (target: >100MB)\")\n",
    "    \n",
    "    if metrics['num_row_groups'] > 10:\n",
    "        print(\"\u26a0\ufe0f Too many row groups (consider repartitioning)\")\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path('datasets/processed/lakehouse_demo')\n",
    "(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'id':[1,2,3,4],\n",
    "  'fecha':['2025-10-01','2025-10-02','2025-10-02','2025-10-03'],\n",
    "  'producto_id':[101,102,101,103],\n",
    "  'cantidad':[1,2,1,3],\n",
    "  'precio':[100.0,50.0,100.0,20.0]\n",
    "})\n",
    "df['total'] = df['cantidad'] * df['precio']\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.strftime('%Y-%m')\n",
    "\n",
    "for (anio, mes), part in df.groupby(['anio','mes']):\n",
    "    part_dir = BASE / f'anio={anio}' / f'mes={mes}'\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fp = part_dir / f'ventas_{int(time.time())}.parquet'\n",
    "    part.to_parquet(fp, index=False)\n",
    "str(BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457603",
   "metadata": {},
   "source": [
    "### 2.1 Lectura particionada y pruning manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = list((BASE / 'anio=2025' / 'mes=2025-10').glob('*.parquet'))\n",
    "pd.concat([pd.read_parquet(p) for p in parts]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549cff5",
   "metadata": {},
   "source": [
    "## 3. Delta Lake/Iceberg: c\u00f3mo y cu\u00e1ndo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e4cb",
   "metadata": {},
   "source": [
    "### \u26a1 **Delta Lake vs Apache Iceberg: Deep Dive**\n",
    "\n",
    "**When to Choose Which?**\n",
    "\n",
    "```python\n",
    "decision_matrix = {\n",
    "    'Use Delta Lake if': [\n",
    "        'Already using Databricks',\n",
    "        'Primary engine is Spark',\n",
    "        'Need mature ecosystem (more tools)',\n",
    "        'Streaming workloads (Spark Structured Streaming)',\n",
    "        'Team familiar with Delta Lake'\n",
    "    ],\n",
    "    'Use Apache Iceberg if': [\n",
    "        'Multi-engine strategy (Spark + Trino + Flink + Athena)',\n",
    "        'AWS-centric (Athena native support)',\n",
    "        'Need hidden partitioning (automatic partition management)',\n",
    "        'Open governance important (Apache vs vendor)',\n",
    "        'Future-proofing (growing adoption)'\n",
    "    ],\n",
    "    'Use Apache Hudi if': [\n",
    "        'Heavy upsert/CDC workloads',\n",
    "        'Record-level updates critical',\n",
    "        'Uber-style use case'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Comparison:**\n",
    "\n",
    "**1. ACID Transactions:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Optimistic Concurrency Control\n",
    "@transaction\n",
    "def update_delta_table():\n",
    "    current_version = read_version()  # v5\n",
    "    # ... perform transformation ...\n",
    "    try:\n",
    "        commit_new_version(v6, based_on=v5)\n",
    "    except ConflictException:\n",
    "        # Another writer committed v6 first\n",
    "        retry_with_new_base(v6)\n",
    "\n",
    "# Apache Iceberg: Snapshot Isolation\n",
    "@transaction\n",
    "def update_iceberg_table():\n",
    "    snapshot_id = current_snapshot()\n",
    "    # ... perform transformation ...\n",
    "    new_snapshot = create_snapshot(changes)\n",
    "    atomic_swap(current_snapshot, new_snapshot)\n",
    "    # Readers on old snapshot unaffected\n",
    "```\n",
    "\n",
    "**2. Time Travel:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# Files: _delta_log/00000000000000000005.json\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 5) \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2025-10-20 00:00:00\") \\\n",
    "    .load(\"s3://bucket/orders\")\n",
    "\n",
    "# Apache Iceberg\n",
    "# Files: metadata/snap-xxxx-1-yy.avro\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", 12345678) \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"as-of-timestamp\", \"1730246400000\") \\\n",
    "    .load(\"catalog.db.orders\")\n",
    "\n",
    "# Iceberg advantage: Faster time travel (O(1) vs O(n))\n",
    "# Delta: Must replay transaction log from start\n",
    "# Iceberg: Direct snapshot lookup\n",
    "```\n",
    "\n",
    "**3. Schema Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# \u2705 ADD COLUMN (supported)\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (discount DOUBLE)\")\n",
    "\n",
    "# \u26a0\ufe0f RENAME COLUMN (manual migration needed)\n",
    "# Step 1: Add new column\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMNS (customer_email STRING)\")\n",
    "# Step 2: Backfill\n",
    "spark.sql(\"UPDATE orders SET customer_email = email\")\n",
    "# Step 3: Drop old column\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN email\")\n",
    "\n",
    "# \u274c CHANGE COLUMN TYPE (not supported)\n",
    "# Must create new table\n",
    "\n",
    "# Apache Iceberg\n",
    "# \u2705 ADD COLUMN\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMN discount double\")\n",
    "\n",
    "# \u2705 RENAME COLUMN (supported!)\n",
    "spark.sql(\"ALTER TABLE orders RENAME COLUMN email TO customer_email\")\n",
    "\n",
    "# \u2705 DROP COLUMN\n",
    "spark.sql(\"ALTER TABLE orders DROP COLUMN discount\")\n",
    "\n",
    "# \u26a0\ufe0f CHANGE TYPE (limited support)\n",
    "spark.sql(\"ALTER TABLE orders ALTER COLUMN id TYPE bigint\")  # int \u2192 bigint OK\n",
    "# int \u2192 string: Not supported (data loss risk)\n",
    "```\n",
    "\n",
    "**4. Partition Evolution:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Manual partition management\n",
    "# Initial: Partitioned by date\n",
    "df.write.partitionBy(\"date\").format(\"delta\").save(\"orders\")\n",
    "\n",
    "# Later: Want to partition by date + country\n",
    "# \u274c Problem: Must rewrite all data\n",
    "df_all = spark.read.format(\"delta\").load(\"orders\")\n",
    "df_all.write.partitionBy(\"date\", \"country\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"orders_new\")\n",
    "\n",
    "# Apache Iceberg: Hidden partitions (automatic!)\n",
    "# Initial: Partitioned by date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE orders (\n",
    "        id bigint,\n",
    "        date date,\n",
    "        country string,\n",
    "        total double\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(date))\n",
    "\"\"\")\n",
    "\n",
    "# Later: Change partitioning strategy (NO data rewrite!)\n",
    "spark.sql(\"ALTER TABLE orders DROP PARTITION FIELD days(date)\")\n",
    "spark.sql(\"ALTER TABLE orders ADD PARTITION FIELD bucket(10, country)\")\n",
    "\n",
    "# Iceberg tracks partition evolution in metadata\n",
    "# Old data: date partitions\n",
    "# New data: country buckets\n",
    "# Queries work seamlessly across both!\n",
    "```\n",
    "\n",
    "**5. File Management (VACUUM/OPTIMIZE):**\n",
    "\n",
    "```python\n",
    "# Delta Lake\n",
    "# OPTIMIZE: Compact small files\n",
    "spark.sql(\"OPTIMIZE orders\")\n",
    "# Target: 1GB files\n",
    "\n",
    "# Z-ORDER: Co-locate data\n",
    "spark.sql(\"OPTIMIZE orders ZORDER BY (customer_id, product_id)\")\n",
    "\n",
    "# VACUUM: Delete old files\n",
    "spark.sql(\"VACUUM orders RETAIN 168 HOURS\")  # Keep 7 days\n",
    "# Deletes:\n",
    "# - Old data files (overwritten by OPTIMIZE)\n",
    "# - Transaction log checkpoints\n",
    "\n",
    "# Apache Iceberg\n",
    "# REWRITE DATA FILES (equivalent to OPTIMIZE)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.rewrite_data_files(\n",
    "        table => 'db.orders',\n",
    "        options => map(\n",
    "            'target-file-size-bytes', '1073741824',  -- 1GB\n",
    "            'min-input-files', '5'\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# EXPIRE SNAPSHOTS (equivalent to VACUUM)\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.expire_snapshots(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-23 00:00:00',\n",
    "        retain_last => 7\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# REMOVE ORPHAN FILES\n",
    "spark.sql(\"\"\"\n",
    "    CALL catalog.system.remove_orphan_files(\n",
    "        table => 'db.orders',\n",
    "        older_than => TIMESTAMP '2025-10-20 00:00:00'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**6. Multi-Engine Support:**\n",
    "\n",
    "| Engine | Delta Lake | Iceberg |\n",
    "|--------|------------|---------|\n",
    "| **Spark** | \u2705 Native | \u2705 Native |\n",
    "| **Presto/Trino** | \u2705 Connector | \u2705 Native |\n",
    "| **Flink** | \u274c Limited | \u2705 Native |\n",
    "| **AWS Athena** | \u26a0\ufe0f Via manifests | \u2705 Native |\n",
    "| **Dremio** | \u2705 | \u2705 |\n",
    "| **Snowflake** | \u26a0\ufe0f Via external tables | \u2705 Iceberg Tables |\n",
    "\n",
    "**7. Metadata Performance:**\n",
    "\n",
    "```python\n",
    "# Scenario: Table with 10,000 partitions\n",
    "\n",
    "# Delta Lake:\n",
    "# - Metadata: JSON files in _delta_log/\n",
    "# - List partitions: O(n) scan of transaction log\n",
    "# - Overhead: ~100MB metadata for large tables\n",
    "\n",
    "# Iceberg:\n",
    "# - Metadata: Avro files with manifest list \u2192 manifest files\n",
    "# - List partitions: O(log n) via manifest index\n",
    "# - Overhead: ~10MB metadata for same table\n",
    "\n",
    "# Benchmark: SHOW PARTITIONS\n",
    "# Delta: 15 seconds\n",
    "# Iceberg: 0.5 seconds (30x faster!)\n",
    "```\n",
    "\n",
    "**8. Streaming Support:**\n",
    "\n",
    "```python\n",
    "# Delta Lake: Spark Structured Streaming native\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"orders\")\n",
    "\n",
    "stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints\") \\\n",
    "    .start(\"orders_processed\")\n",
    "\n",
    "# Apache Iceberg: Flink + Spark Streaming\n",
    "# Flink (primary streaming engine for Iceberg)\n",
    "TableEnvironment tableEnv = ...\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    CREATE TABLE orders (...)\n",
    "    WITH ('connector' = 'iceberg', ...)\n",
    "\"\"\")\n",
    "\n",
    "tableEnv.executeSql(\"\"\"\n",
    "    INSERT INTO orders_processed\n",
    "    SELECT * FROM orders\n",
    "    WHERE total > 1000\n",
    "\"\"\")\n",
    "\n",
    "# Spark Streaming (also supported)\n",
    "spark.readStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"orders\")\n",
    "```\n",
    "\n",
    "**Real-World Migration Story:**\n",
    "\n",
    "```python\n",
    "# Company: Netflix \u2192 Apple (hypothetical)\n",
    "\n",
    "# Phase 1: Dual-write (6 months)\n",
    "df.write.format(\"delta\").save(\"s3://bucket/delta/orders\")\n",
    "df.write.format(\"iceberg\").save(\"s3://bucket/iceberg/orders\")\n",
    "\n",
    "# Phase 2: Validate (3 months)\n",
    "delta_count = spark.read.format(\"delta\").load(...).count()\n",
    "iceberg_count = spark.read.format(\"iceberg\").load(...).count()\n",
    "assert delta_count == iceberg_count\n",
    "\n",
    "# Phase 3: Cutover (1 week)\n",
    "# Redirect readers to Iceberg\n",
    "# Stop Delta writes\n",
    "\n",
    "# Phase 4: Cleanup (1 month)\n",
    "# Delete Delta files\n",
    "# Reclaim storage\n",
    "\n",
    "# Total timeline: 10 months\n",
    "# Cost: ~$50K engineering effort\n",
    "# Benefit: 30% query performance improvement\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00324956",
   "metadata": {},
   "source": [
    "- Delta Lake agrega ACID, time travel y MERGE INTO sobre Parquet.\n",
    "- Iceberg optimiza la gesti\u00f3n de metadatos, particiones ocultas y evoluci\u00f3n de esquema.\n",
    "- Requiere motor como Spark/Trino/Flint y un cat\u00e1logo (Glue/REST).\n",
    "- Coste/beneficio: eval\u00faa volumen, concurrencia, latencia y SLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e6",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo Delta Lake (referencia con PySpark) [opcional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_demo = r'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName('DeltaDemo')\n",
    "    .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.read.parquet('s3://bucket/curated/ventas/')\n",
    "df.write.format('delta').mode('overwrite').save('s3://bucket/delta/ventas/')\n",
    "\n",
    "delta = spark.read.format('delta').load('s3://bucket/delta/ventas/')\n",
    "delta.createOrReplaceTempView('ventas')\n",
    "spark.sql(\"SELECT mes, SUM(total) FROM ventas GROUP BY mes\").show()\n",
    "'''\n",
    "print(delta_demo.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b564",
   "metadata": {},
   "source": [
    "## 4. Buenas pr\u00e1cticas de Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccb0bc",
   "metadata": {},
   "source": [
    "- Definir contratos de datos y versionado de esquemas.\n",
    "- Gestionar tama\u00f1os de archivos y compaction (OPTIMIZE/VACUUM).\n",
    "- Catalogaci\u00f3n y pol\u00edticas de acceso por dominio (Data Mesh).\n",
    "- Observabilidad y linaje (OpenLineage/Marquez, DataHub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [Apache Spark Streaming: Procesamiento en Tiempo Real \u2192](03_spark_streaming.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
