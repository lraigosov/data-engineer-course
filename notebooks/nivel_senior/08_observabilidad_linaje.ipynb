{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19a6223",
   "metadata": {},
   "source": [
    "# 📊 Observabilidad y Linaje de Datos\n",
    "\n",
    "Objetivo: implementar observabilidad end-to-end con logs estructurados, métricas (Prometheus), trazas (OpenTelemetry), linaje de datos (OpenLineage/DataHub) y SLOs.\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Senior 01 (Governance), pipelines en producción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e040c57",
   "metadata": {},
   "source": [
    "### 📡 **Observability: From Monitoring to Full Visibility**\n",
    "\n",
    "**Evolution of Monitoring**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│  TRADITIONAL MONITORING (2000s)                        │\n",
    "│  • Metrics: CPU, memory, disk                          │\n",
    "│  • Dashboards: Static graphs                           │\n",
    "│  • Alerts: Threshold-based (CPU > 80%)                 │\n",
    "│  • Problem: \"What is broken?\" ❌                       │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  OBSERVABILITY (2020s)                                 │\n",
    "│  • Logs + Metrics + Traces (3 pillars)                 │\n",
    "│  • Distributed tracing                                 │\n",
    "│  • High-cardinality queries                            │\n",
    "│  • Problem: \"Why is it broken?\" ✅                     │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Three Pillars of Observability**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  1. LOGS (Events)                                       │\n",
    "│  • What: Discrete events (errors, warnings, debug)      │\n",
    "│  • Format: Structured JSON with context                 │\n",
    "│  • Tools: ELK, Loki, CloudWatch Logs                    │\n",
    "│  • Query: \"Show me all errors in pipeline X last hour\"  │\n",
    "│                                                          │\n",
    "│  2. METRICS (Aggregates)                                │\n",
    "│  • What: Numeric measurements over time                 │\n",
    "│  • Types: Counter, Gauge, Histogram, Summary            │\n",
    "│  • Tools: Prometheus, InfluxDB, Datadog                 │\n",
    "│  • Query: \"What's p99 latency for pipeline X?\"          │\n",
    "│                                                          │\n",
    "│  3. TRACES (Flows)                                      │\n",
    "│  • What: Request flow through distributed system        │\n",
    "│  • Standard: OpenTelemetry                              │\n",
    "│  • Tools: Jaeger, Zipkin, Tempo                         │\n",
    "│  • Query: \"Why is this request slow? Where?\"            │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Structured Logging: From Strings to Context**\n",
    "\n",
    "```python\n",
    "# ❌ BAD: String logging\n",
    "import logging\n",
    "logging.info(\"Pipeline ventas_etl started\")\n",
    "logging.error(\"Validation failed with 5 errors: [1,2,3,4,5]\")\n",
    "\n",
    "# Output: Unparseable strings\n",
    "# 2025-10-30 10:00:00 INFO Pipeline ventas_etl started\n",
    "# 2025-10-30 10:01:00 ERROR Validation failed with 5 errors: [1,2,3,4,5]\n",
    "\n",
    "# ✅ GOOD: Structured logging\n",
    "from loguru import logger\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Configure JSON output\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"{time} {level} {message}\",\n",
    "    serialize=True  # JSON output\n",
    ")\n",
    "\n",
    "# Add context\n",
    "logger.bind(\n",
    "    pipeline=\"ventas_etl\",\n",
    "    version=\"v1.2.3\",\n",
    "    environment=\"production\"\n",
    ").info(\"pipeline_started\")\n",
    "\n",
    "logger.bind(\n",
    "    pipeline=\"ventas_etl\",\n",
    "    error_count=5,\n",
    "    sample_ids=[1, 2, 3, 4, 5],\n",
    "    validation_type=\"schema\"\n",
    ").error(\"validation_failed\")\n",
    "\n",
    "# Output: Queryable JSON\n",
    "\"\"\"\n",
    "{\n",
    "  \"text\": \"pipeline_started\",\n",
    "  \"record\": {\n",
    "    \"time\": {\"timestamp\": 1730286000.123},\n",
    "    \"level\": {\"name\": \"INFO\"},\n",
    "    \"message\": \"pipeline_started\",\n",
    "    \"extra\": {\n",
    "      \"pipeline\": \"ventas_etl\",\n",
    "      \"version\": \"v1.2.3\",\n",
    "      \"environment\": \"production\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Now can query: \"Show all errors where pipeline=ventas_etl AND error_count > 3\"\n",
    "```\n",
    "\n",
    "**Logging Best Practices for Data Pipelines**\n",
    "\n",
    "```python\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from functools import wraps\n",
    "\n",
    "class DataPipelineLogger:\n",
    "    \"\"\"Logging wrapper para data pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str, run_id: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # Configure logger\n",
    "        logger.remove()\n",
    "        logger.add(\n",
    "            f\"logs/{pipeline_name}/{{time}}.log\",\n",
    "            rotation=\"500 MB\",\n",
    "            retention=\"30 days\",\n",
    "            serialize=True,\n",
    "            format=\"{time} {level} {message}\"\n",
    "        )\n",
    "        \n",
    "        # Add common context\n",
    "        self.logger = logger.bind(\n",
    "            pipeline=pipeline_name,\n",
    "            run_id=run_id,\n",
    "            environment=\"production\"\n",
    "        )\n",
    "    \n",
    "    def log_stage_start(self, stage: str, **kwargs):\n",
    "        \"\"\"Log inicio de stage\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"stage_started\",\n",
    "            stage=stage,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def log_stage_complete(self, stage: str, records_processed: int, duration_sec: float):\n",
    "        \"\"\"Log completado de stage\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"stage_completed\",\n",
    "            stage=stage,\n",
    "            records_processed=records_processed,\n",
    "            duration_sec=duration_sec,\n",
    "            throughput_rps=records_processed / duration_sec\n",
    "        )\n",
    "    \n",
    "    def log_data_quality_check(self, check_name: str, passed: bool, details: dict):\n",
    "        \"\"\"Log data quality validation\"\"\"\n",
    "        level = \"info\" if passed else \"warning\"\n",
    "        getattr(self.logger, level)(\n",
    "            f\"quality_check\",\n",
    "            check_name=check_name,\n",
    "            passed=passed,\n",
    "            **details\n",
    "        )\n",
    "    \n",
    "    def log_error(self, stage: str, error: Exception, **kwargs):\n",
    "        \"\"\"Log error con stacktrace\"\"\"\n",
    "        self.logger.error(\n",
    "            f\"stage_failed\",\n",
    "            stage=stage,\n",
    "            error_type=type(error).__name__,\n",
    "            error_message=str(error),\n",
    "            stacktrace=traceback.format_exc(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Decorator para auto-logging\n",
    "def log_execution(stage_name: str):\n",
    "    \"\"\"Decorator to log function execution\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            pipeline_logger = kwargs.get('logger')\n",
    "            \n",
    "            if pipeline_logger:\n",
    "                pipeline_logger.log_stage_start(stage_name)\n",
    "            \n",
    "            start = datetime.now()\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                if pipeline_logger:\n",
    "                    duration = (datetime.now() - start).total_seconds()\n",
    "                    records = result.get('records_processed', 0) if isinstance(result, dict) else 0\n",
    "                    pipeline_logger.log_stage_complete(stage_name, records, duration)\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                if pipeline_logger:\n",
    "                    pipeline_logger.log_error(stage_name, e)\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage\n",
    "logger = DataPipelineLogger(\"ventas_etl\", run_id=\"run_20251030_100000\")\n",
    "\n",
    "@log_execution(\"extract\")\n",
    "def extract_data(logger=None):\n",
    "    # Extraction logic\n",
    "    return {\"records_processed\": 10000}\n",
    "\n",
    "@log_execution(\"transform\")\n",
    "def transform_data(df, logger=None):\n",
    "    # Transformation logic\n",
    "    \n",
    "    # Log data quality check\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    logger.log_data_quality_check(\n",
    "        \"null_check\",\n",
    "        passed=null_count == 0,\n",
    "        details={\"null_count\": null_count}\n",
    "    )\n",
    "    \n",
    "    return {\"records_processed\": len(df)}\n",
    "\n",
    "# Execute\n",
    "extract_data(logger=logger)\n",
    "transform_data(df, logger=logger)\n",
    "```\n",
    "\n",
    "**Correlation IDs: Tracing Across Services**\n",
    "\n",
    "```python\n",
    "import uuid\n",
    "from contextvars import ContextVar\n",
    "\n",
    "# Thread-safe context variable\n",
    "trace_id_var: ContextVar[str] = ContextVar('trace_id', default=None)\n",
    "\n",
    "def generate_trace_id() -> str:\n",
    "    \"\"\"Generate unique trace ID\"\"\"\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def set_trace_id(trace_id: str = None):\n",
    "    \"\"\"Set trace ID for current context\"\"\"\n",
    "    if trace_id is None:\n",
    "        trace_id = generate_trace_id()\n",
    "    trace_id_var.set(trace_id)\n",
    "    return trace_id\n",
    "\n",
    "def get_trace_id() -> str:\n",
    "    \"\"\"Get trace ID from current context\"\"\"\n",
    "    return trace_id_var.get()\n",
    "\n",
    "# FastAPI middleware to propagate trace ID\n",
    "from fastapi import Request\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "\n",
    "class TraceIdMiddleware(BaseHTTPMiddleware):\n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        # Extract trace_id from header or generate\n",
    "        trace_id = request.headers.get('X-Trace-Id', generate_trace_id())\n",
    "        set_trace_id(trace_id)\n",
    "        \n",
    "        # Add to logger context\n",
    "        logger.bind(trace_id=trace_id)\n",
    "        \n",
    "        response = await call_next(request)\n",
    "        \n",
    "        # Add to response header\n",
    "        response.headers['X-Trace-Id'] = trace_id\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Propagate to downstream services\n",
    "import httpx\n",
    "\n",
    "async def call_downstream_service(url: str):\n",
    "    \"\"\"Call otro servicio con trace ID\"\"\"\n",
    "    headers = {\n",
    "        'X-Trace-Id': get_trace_id()\n",
    "    }\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, headers=headers)\n",
    "        return response.json()\n",
    "\n",
    "# Now all logs across services share same trace_id → easy debugging!\n",
    "```\n",
    "\n",
    "**Caso Real: Uber's Observability Platform**\n",
    "\n",
    "Uber procesa **10M+ eventos/segundo** con:\n",
    "- **Logs**: JSON structured logs → Kafka → HDFS (long-term) + Elasticsearch (search)\n",
    "- **Metrics**: Prometheus (4M+ time series), M3DB (distributed storage)\n",
    "- **Traces**: Jaeger (100K+ traces/sec), custom sampling\n",
    "- **Cost**: $50M/year en infraestructura de observability\n",
    "- **ROI**: Reduce MTTR (Mean Time To Recovery) de 2h → 15min\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaab56e",
   "metadata": {},
   "source": [
    "### 📊 **Metrics & Prometheus: Measure What Matters**\n",
    "\n",
    "**Metric Types**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│  COUNTER (monotonic, only increases)                   │\n",
    "│  • events_processed_total                              │\n",
    "│  • errors_total                                        │\n",
    "│  • bytes_transferred_total                             │\n",
    "│  Query: rate(events_processed_total[5m])               │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  GAUGE (can go up or down)                             │\n",
    "│  • active_connections                                  │\n",
    "│  • queue_size                                          │\n",
    "│  • memory_usage_bytes                                  │\n",
    "│  Query: avg_over_time(queue_size[1h])                  │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  HISTOGRAM (distribution of values)                    │\n",
    "│  • request_duration_seconds                            │\n",
    "│  • record_size_bytes                                   │\n",
    "│  Query: histogram_quantile(0.99, ...)  # p99           │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  SUMMARY (client-side percentiles)                     │\n",
    "│  • processing_time_seconds                             │\n",
    "│  Query: processing_time_seconds{quantile=\"0.99\"}       │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Prometheus Client Implementation**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Gauge, Histogram, Summary\n",
    "from prometheus_client import start_http_server, CollectorRegistry\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Create registry (allows multiple apps in same process)\n",
    "registry = CollectorRegistry()\n",
    "\n",
    "# 1. COUNTER: Monotonic increasing\n",
    "events_processed = Counter(\n",
    "    'events_processed_total',\n",
    "    'Total number of events processed',\n",
    "    ['pipeline', 'stage'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "errors_total = Counter(\n",
    "    'errors_total',\n",
    "    'Total number of errors',\n",
    "    ['pipeline', 'stage', 'error_type'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 2. GAUGE: Current value\n",
    "active_jobs = Gauge(\n",
    "    'active_jobs',\n",
    "    'Number of active jobs',\n",
    "    ['pipeline'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "queue_size = Gauge(\n",
    "    'queue_size',\n",
    "    'Number of items in queue',\n",
    "    ['queue_name'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 3. HISTOGRAM: Distribution with buckets\n",
    "processing_duration = Histogram(\n",
    "    'processing_duration_seconds',\n",
    "    'Time spent processing records',\n",
    "    ['pipeline', 'stage'],\n",
    "    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0),\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "record_size = Histogram(\n",
    "    'record_size_bytes',\n",
    "    'Size of processed records',\n",
    "    ['pipeline'],\n",
    "    buckets=(100, 1000, 10000, 100000, 1000000),\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 4. SUMMARY: Client-side percentiles\n",
    "processing_time = Summary(\n",
    "    'processing_time_seconds',\n",
    "    'Processing time summary',\n",
    "    ['pipeline'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# Instrumented data pipeline\n",
    "class InstrumentedPipeline:\n",
    "    \"\"\"Data pipeline with Prometheus metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "    \n",
    "    def run_stage(self, stage_name: str, records: list):\n",
    "        \"\"\"Execute stage with metrics\"\"\"\n",
    "        \n",
    "        # Track active jobs\n",
    "        active_jobs.labels(pipeline=self.pipeline_name).inc()\n",
    "        \n",
    "        try:\n",
    "            # Time execution\n",
    "            with processing_duration.labels(\n",
    "                pipeline=self.pipeline_name,\n",
    "                stage=stage_name\n",
    "            ).time():\n",
    "                \n",
    "                # Process records\n",
    "                for record in records:\n",
    "                    # Increment counter\n",
    "                    events_processed.labels(\n",
    "                        pipeline=self.pipeline_name,\n",
    "                        stage=stage_name\n",
    "                    ).inc()\n",
    "                    \n",
    "                    # Record size\n",
    "                    record_size.labels(pipeline=self.pipeline_name).observe(\n",
    "                        len(str(record))\n",
    "                    )\n",
    "                    \n",
    "                    # Simulate processing\n",
    "                    time.sleep(random.uniform(0.001, 0.01))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Track errors\n",
    "            errors_total.labels(\n",
    "                pipeline=self.pipeline_name,\n",
    "                stage=stage_name,\n",
    "                error_type=type(e).__name__\n",
    "            ).inc()\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            # Decrease active jobs\n",
    "            active_jobs.labels(pipeline=self.pipeline_name).dec()\n",
    "\n",
    "# Start metrics server\n",
    "start_http_server(8000, registry=registry)\n",
    "# Metrics available at: http://localhost:8000/metrics\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = InstrumentedPipeline(\"ventas_etl\")\n",
    "pipeline.run_stage(\"extract\", [{\"id\": i} for i in range(1000)])\n",
    "pipeline.run_stage(\"transform\", [{\"id\": i} for i in range(1000)])\n",
    "\n",
    "# Prometheus scrapes /metrics every 15s\n",
    "\"\"\"\n",
    "# HELP events_processed_total Total number of events processed\n",
    "# TYPE events_processed_total counter\n",
    "events_processed_total{pipeline=\"ventas_etl\",stage=\"extract\"} 1000.0\n",
    "events_processed_total{pipeline=\"ventas_etl\",stage=\"transform\"} 1000.0\n",
    "\n",
    "# HELP processing_duration_seconds Time spent processing records\n",
    "# TYPE processing_duration_seconds histogram\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"0.1\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"0.5\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"1.0\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"2.5\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"5.0\"} 1.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"+Inf\"} 1.0\n",
    "processing_duration_seconds_sum{pipeline=\"ventas_etl\",stage=\"extract\"} 4.523\n",
    "processing_duration_seconds_count{pipeline=\"ventas_etl\",stage=\"extract\"} 1.0\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**PromQL Queries for Data Engineers**\n",
    "\n",
    "```python\n",
    "# Common queries\n",
    "promql_queries = {\n",
    "    # Throughput (events/second)\n",
    "    \"throughput\": \"\"\"\n",
    "        rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Error rate (%)\n",
    "    \"error_rate\": \"\"\"\n",
    "        (\n",
    "            rate(errors_total{pipeline=\"ventas_etl\"}[5m])\n",
    "            / \n",
    "            rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "        ) * 100\n",
    "    \"\"\",\n",
    "    \n",
    "    # P99 latency\n",
    "    \"p99_latency\": \"\"\"\n",
    "        histogram_quantile(0.99, \n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "        )\n",
    "    \"\"\",\n",
    "    \n",
    "    # P50 (median) latency\n",
    "    \"p50_latency\": \"\"\"\n",
    "        histogram_quantile(0.50, \n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "        )\n",
    "    \"\"\",\n",
    "    \n",
    "    # Total errors last hour\n",
    "    \"errors_1h\": \"\"\"\n",
    "        increase(errors_total{pipeline=\"ventas_etl\"}[1h])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Average queue size\n",
    "    \"avg_queue_size\": \"\"\"\n",
    "        avg_over_time(queue_size{queue_name=\"events\"}[5m])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Success rate (inverse of error rate)\n",
    "    \"success_rate\": \"\"\"\n",
    "        (1 - (\n",
    "            sum(rate(errors_total[5m]))\n",
    "            /\n",
    "            sum(rate(events_processed_total[5m]))\n",
    "        )) * 100\n",
    "    \"\"\",\n",
    "    \n",
    "    # Top 5 slowest pipelines\n",
    "    \"slowest_pipelines\": \"\"\"\n",
    "        topk(5, \n",
    "            avg by (pipeline) (\n",
    "                rate(processing_duration_seconds_sum[5m])\n",
    "                /\n",
    "                rate(processing_duration_seconds_count[5m])\n",
    "            )\n",
    "        )\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Alerting Rules (Prometheus Alertmanager)**\n",
    "\n",
    "```yaml\n",
    "# prometheus_rules.yml\n",
    "groups:\n",
    "  - name: data_pipeline_alerts\n",
    "    interval: 30s\n",
    "    rules:\n",
    "      # High error rate\n",
    "      - alert: HighErrorRate\n",
    "        expr: |\n",
    "          (\n",
    "            rate(errors_total{pipeline=\"ventas_etl\"}[5m])\n",
    "            /\n",
    "            rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "          ) > 0.05\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: data-engineering\n",
    "        annotations:\n",
    "          summary: \"High error rate in {{ $labels.pipeline }}\"\n",
    "          description: \"Error rate is {{ $value | humanizePercentage }}\"\n",
    "      \n",
    "      # High latency\n",
    "      - alert: HighLatency\n",
    "        expr: |\n",
    "          histogram_quantile(0.99,\n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "          ) > 30\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High P99 latency in {{ $labels.pipeline }}\"\n",
    "          description: \"P99 latency is {{ $value }}s (threshold: 30s)\"\n",
    "      \n",
    "      # Pipeline stuck (no events processed)\n",
    "      - alert: PipelineStuck\n",
    "        expr: |\n",
    "          rate(events_processed_total{pipeline=\"ventas_etl\"}[5m]) == 0\n",
    "        for: 15m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"Pipeline {{ $labels.pipeline }} is stuck\"\n",
    "          description: \"No events processed in last 15 minutes\"\n",
    "      \n",
    "      # Queue too large\n",
    "      - alert: QueueTooLarge\n",
    "        expr: queue_size{queue_name=\"events\"} > 10000\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Queue {{ $labels.queue_name }} is too large\"\n",
    "          description: \"Queue size: {{ $value }}\"\n",
    "```\n",
    "\n",
    "**Grafana Dashboard (JSON)**\n",
    "\n",
    "```python\n",
    "grafana_dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Data Pipeline Observability\",\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Throughput (events/sec)\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(rate(events_processed_total[5m])) by (pipeline)',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Error Rate (%)\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": '''\n",
    "                            (\n",
    "                                sum(rate(errors_total[5m])) by (pipeline)\n",
    "                                /\n",
    "                                sum(rate(events_processed_total[5m])) by (pipeline)\n",
    "                            ) * 100\n",
    "                        ''',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\",\n",
    "                \"alert\": {\n",
    "                    \"conditions\": [\n",
    "                        {\n",
    "                            \"evaluator\": {\"type\": \"gt\", \"params\": [5]},\n",
    "                            \"query\": {\"params\": [\"A\", \"5m\", \"now\"]}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"P50, P95, P99 Latency\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.50, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P50\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.95, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P95\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.99, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P99\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Active Jobs\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(active_jobs) by (pipeline)',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"gauge\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom Metrics for Data Quality**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Gauge\n",
    "\n",
    "# Data quality metrics\n",
    "data_quality_score = Gauge(\n",
    "    'data_quality_score',\n",
    "    'Data quality score (0-100)',\n",
    "    ['pipeline', 'table', 'check_type']\n",
    ")\n",
    "\n",
    "null_percentage = Gauge(\n",
    "    'null_percentage',\n",
    "    'Percentage of null values',\n",
    "    ['pipeline', 'table', 'column']\n",
    ")\n",
    "\n",
    "duplicate_count = Gauge(\n",
    "    'duplicate_count',\n",
    "    'Number of duplicate records',\n",
    "    ['pipeline', 'table']\n",
    ")\n",
    "\n",
    "# Update metrics\n",
    "def run_quality_checks(df, pipeline_name, table_name):\n",
    "    \"\"\"Run data quality checks and emit metrics\"\"\"\n",
    "    \n",
    "    # Null check\n",
    "    for col in df.columns:\n",
    "        null_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "        null_percentage.labels(\n",
    "            pipeline=pipeline_name,\n",
    "            table=table_name,\n",
    "            column=col\n",
    "        ).set(null_pct)\n",
    "    \n",
    "    # Duplicate check\n",
    "    dup_count = df.duplicated().sum()\n",
    "    duplicate_count.labels(\n",
    "        pipeline=pipeline_name,\n",
    "        table=table_name\n",
    "    ).set(dup_count)\n",
    "    \n",
    "    # Overall quality score\n",
    "    score = 100 - (null_pct + (dup_count / len(df) * 100))\n",
    "    data_quality_score.labels(\n",
    "        pipeline=pipeline_name,\n",
    "        table=table_name,\n",
    "        check_type='overall'\n",
    "    ).set(max(0, score))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc72b7",
   "metadata": {},
   "source": [
    "### 🔍 **Distributed Tracing: OpenTelemetry**\n",
    "\n",
    "**Why Distributed Tracing?**\n",
    "\n",
    "```\n",
    "Traditional Logs (per service):\n",
    "Service A: [10:00:01] Request received\n",
    "Service B: [10:00:02] Processing data\n",
    "Service C: [10:00:05] Database query\n",
    "❌ Can't connect these logs across services!\n",
    "\n",
    "Distributed Tracing:\n",
    "Trace ID: abc123\n",
    "├─ Span: API Request (Service A) [200ms]\n",
    "│  ├─ Span: Extract Data (Service B) [150ms]\n",
    "│  │  └─ Span: S3 Download [100ms]\n",
    "│  └─ Span: Transform Data (Service C) [50ms]\n",
    "│     └─ Span: DB Query [30ms]\n",
    "✅ Full request flow visualization!\n",
    "```\n",
    "\n",
    "**OpenTelemetry Architecture**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│  Application Code                                    │\n",
    "│  • Instrumented with OpenTelemetry SDK               │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  OpenTelemetry Collector                             │\n",
    "│  • Receives traces from apps                         │\n",
    "│  • Processes (sampling, filtering)                   │\n",
    "│  • Exports to backends                               │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  Backend Storage                                     │\n",
    "│  • Jaeger (open source)                              │\n",
    "│  • Tempo (Grafana)                                   │\n",
    "│  • Datadog, New Relic, etc.                          │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**OpenTelemetry Implementation**\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.instrumentation.requests import RequestsInstrumentor\n",
    "from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 1. SETUP TRACER\n",
    "def setup_tracing(service_name: str):\n",
    "    \"\"\"Initialize OpenTelemetry tracing\"\"\"\n",
    "    \n",
    "    # Create resource (service metadata)\n",
    "    resource = Resource.create({\n",
    "        \"service.name\": service_name,\n",
    "        \"service.version\": \"1.2.3\",\n",
    "        \"deployment.environment\": \"production\"\n",
    "    })\n",
    "    \n",
    "    # Create tracer provider\n",
    "    tracer_provider = TracerProvider(resource=resource)\n",
    "    \n",
    "    # Configure Jaeger exporter\n",
    "    jaeger_exporter = JaegerExporter(\n",
    "        agent_host_name=\"localhost\",\n",
    "        agent_port=6831,\n",
    "    )\n",
    "    \n",
    "    # Add span processor\n",
    "    span_processor = BatchSpanProcessor(jaeger_exporter)\n",
    "    tracer_provider.add_span_processor(span_processor)\n",
    "    \n",
    "    # Set global tracer provider\n",
    "    trace.set_tracer_provider(tracer_provider)\n",
    "    \n",
    "    # Auto-instrument libraries\n",
    "    RequestsInstrumentor().instrument()  # HTTP calls\n",
    "    SQLAlchemyInstrumentor().instrument()  # DB queries\n",
    "    \n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "# Initialize\n",
    "tracer = setup_tracing(\"ventas_etl_pipeline\")\n",
    "\n",
    "# 2. MANUAL INSTRUMENTATION\n",
    "def process_data_with_tracing(data):\n",
    "    \"\"\"Pipeline stage with tracing\"\"\"\n",
    "    \n",
    "    # Create parent span\n",
    "    with tracer.start_as_current_span(\"process_data\") as span:\n",
    "        # Add attributes\n",
    "        span.set_attribute(\"data.size\", len(data))\n",
    "        span.set_attribute(\"pipeline.name\", \"ventas_etl\")\n",
    "        \n",
    "        # Extract\n",
    "        with tracer.start_as_current_span(\"extract\") as extract_span:\n",
    "            extract_span.set_attribute(\"source\", \"s3\")\n",
    "            raw_data = extract_from_s3(data)\n",
    "            extract_span.set_attribute(\"records.count\", len(raw_data))\n",
    "        \n",
    "        # Transform\n",
    "        with tracer.start_as_current_span(\"transform\") as transform_span:\n",
    "            transform_span.set_attribute(\"transformation.type\", \"cleansing\")\n",
    "            \n",
    "            try:\n",
    "                cleaned_data = transform(raw_data)\n",
    "                transform_span.set_status(trace.Status(trace.StatusCode.OK))\n",
    "            except Exception as e:\n",
    "                transform_span.set_status(\n",
    "                    trace.Status(trace.StatusCode.ERROR, str(e))\n",
    "                )\n",
    "                transform_span.record_exception(e)\n",
    "                raise\n",
    "        \n",
    "        # Load\n",
    "        with tracer.start_as_current_span(\"load\") as load_span:\n",
    "            load_span.set_attribute(\"destination\", \"postgres\")\n",
    "            load_to_database(cleaned_data)\n",
    "        \n",
    "        # Add event\n",
    "        span.add_event(\"processing_completed\", {\n",
    "            \"records_processed\": len(cleaned_data)\n",
    "        })\n",
    "        \n",
    "        return cleaned_data\n",
    "\n",
    "# 3. CONTEXT PROPAGATION (across services)\n",
    "def call_downstream_service(url: str, trace_id: str):\n",
    "    \"\"\"Propagate trace context to downstream service\"\"\"\n",
    "    \n",
    "    from opentelemetry.propagate import inject\n",
    "    \n",
    "    # Create child span\n",
    "    with tracer.start_as_current_span(\"downstream_call\") as span:\n",
    "        span.set_attribute(\"http.url\", url)\n",
    "        \n",
    "        # Inject trace context into headers\n",
    "        headers = {}\n",
    "        inject(headers)\n",
    "        \n",
    "        # Make HTTP call (trace context propagated)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        span.set_attribute(\"http.status_code\", response.status_code)\n",
    "        \n",
    "        return response.json()\n",
    "\n",
    "# 4. BAGGAGE (cross-cutting context)\n",
    "from opentelemetry.baggage import set_baggage, get_baggage\n",
    "\n",
    "# Set baggage (propagates with trace)\n",
    "set_baggage(\"user.id\", \"user123\")\n",
    "set_baggage(\"tenant.id\", \"tenant456\")\n",
    "\n",
    "# Access in downstream services\n",
    "user_id = get_baggage(\"user.id\")\n",
    "tenant_id = get_baggage(\"tenant.id\")\n",
    "```\n",
    "\n",
    "**Sampling Strategies**\n",
    "\n",
    "```python\n",
    "from opentelemetry.sdk.trace.sampling import (\n",
    "    TraceIdRatioBased,\n",
    "    ParentBased,\n",
    "    StaticSampler,\n",
    "    ALWAYS_ON,\n",
    "    ALWAYS_OFF\n",
    ")\n",
    "\n",
    "# 1. ALWAYS ON (dev/staging)\n",
    "sampler = StaticSampler(ALWAYS_ON)\n",
    "\n",
    "# 2. PROBABILITY-BASED (10% of traces)\n",
    "sampler = TraceIdRatioBased(0.1)\n",
    "\n",
    "# 3. PARENT-BASED (follow parent decision)\n",
    "sampler = ParentBased(\n",
    "    root=TraceIdRatioBased(0.1),  # 10% for root spans\n",
    "    remote_parent_sampled=ALWAYS_ON,  # If parent sampled, sample\n",
    "    remote_parent_not_sampled=ALWAYS_OFF  # Else, don't sample\n",
    ")\n",
    "\n",
    "# 4. CUSTOM SAMPLER (sample errors always)\n",
    "class ErrorSampler(Sampler):\n",
    "    def should_sample(self, context, trace_id, name, kind, attributes, links):\n",
    "        # Always sample if error\n",
    "        if attributes and attributes.get(\"error\"):\n",
    "            return SamplingResult(Decision.RECORD_AND_SAMPLE)\n",
    "        \n",
    "        # 1% for normal traces\n",
    "        if random.random() < 0.01:\n",
    "            return SamplingResult(Decision.RECORD_AND_SAMPLE)\n",
    "        \n",
    "        return SamplingResult(Decision.DROP)\n",
    "\n",
    "# Apply to tracer provider\n",
    "tracer_provider = TracerProvider(sampler=ErrorSampler())\n",
    "```\n",
    "\n",
    "**Spark Instrumentation**\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "def spark_job_with_tracing():\n",
    "    \"\"\"Spark job con OpenTelemetry\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"spark_job\") as job_span:\n",
    "        job_span.set_attribute(\"spark.app.name\", \"ventas_etl\")\n",
    "        \n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"ventas_etl\").getOrCreate()\n",
    "        \n",
    "        # Read\n",
    "        with tracer.start_as_current_span(\"spark.read\") as read_span:\n",
    "            df = spark.read.parquet(\"s3://bucket/raw/\")\n",
    "            read_span.set_attribute(\"records.count\", df.count())\n",
    "        \n",
    "        # Transform\n",
    "        with tracer.start_as_current_span(\"spark.transform\") as transform_span:\n",
    "            df_transformed = df.filter(df['amount'] > 0)\n",
    "            transform_span.set_attribute(\"transformation\", \"filter_positive\")\n",
    "        \n",
    "        # Write\n",
    "        with tracer.start_as_current_span(\"spark.write\") as write_span:\n",
    "            df_transformed.write.mode(\"overwrite\").parquet(\"s3://bucket/curated/\")\n",
    "            write_span.set_attribute(\"output.path\", \"s3://bucket/curated/\")\n",
    "\n",
    "# Airflow instrumentation\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def task_with_tracing(**context):\n",
    "    \"\"\"Airflow task con tracing\"\"\"\n",
    "    \n",
    "    # Extract trace context from Airflow\n",
    "    dag_id = context['dag'].dag_id\n",
    "    task_id = context['task'].task_id\n",
    "    run_id = context['run_id']\n",
    "    \n",
    "    with tracer.start_as_current_span(f\"airflow.task.{task_id}\") as span:\n",
    "        span.set_attribute(\"airflow.dag_id\", dag_id)\n",
    "        span.set_attribute(\"airflow.run_id\", run_id)\n",
    "        \n",
    "        # Task logic\n",
    "        result = process_data()\n",
    "        \n",
    "        span.add_event(\"task_completed\", {\"result\": result})\n",
    "\n",
    "dag = DAG('ventas_etl', schedule_interval='@daily')\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='process',\n",
    "    python_callable=task_with_tracing,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "**Jaeger Query Examples**\n",
    "\n",
    "```bash\n",
    "# Find traces with errors\n",
    "service=ventas_etl error=true\n",
    "\n",
    "# Find slow traces (>5s duration)\n",
    "service=ventas_etl minDuration=5s\n",
    "\n",
    "# Find traces for specific user\n",
    "service=ventas_etl baggage.user.id=user123\n",
    "\n",
    "# Find traces with high DB latency\n",
    "service=ventas_etl operation=db.query minDuration=1s\n",
    "```\n",
    "\n",
    "**Trace Analysis Patterns**\n",
    "\n",
    "```python\n",
    "# Pattern 1: Find bottlenecks\n",
    "def analyze_trace_bottlenecks(trace_data):\n",
    "    \"\"\"Identify slowest spans in trace\"\"\"\n",
    "    \n",
    "    spans = sorted(trace_data['spans'], key=lambda x: x['duration'], reverse=True)\n",
    "    \n",
    "    bottlenecks = []\n",
    "    for span in spans[:5]:\n",
    "        bottlenecks.append({\n",
    "            'operation': span['operationName'],\n",
    "            'duration_ms': span['duration'] / 1000,\n",
    "            'percentage': (span['duration'] / trace_data['duration']) * 100\n",
    "        })\n",
    "    \n",
    "    return bottlenecks\n",
    "\n",
    "# Pattern 2: Critical path\n",
    "def find_critical_path(trace_data):\n",
    "    \"\"\"Find longest path through trace\"\"\"\n",
    "    \n",
    "    def build_span_tree(spans):\n",
    "        tree = {}\n",
    "        for span in spans:\n",
    "            parent_id = span.get('references', [{}])[0].get('spanID')\n",
    "            if parent_id not in tree:\n",
    "                tree[parent_id] = []\n",
    "            tree[parent_id].append(span)\n",
    "        return tree\n",
    "    \n",
    "    tree = build_span_tree(trace_data['spans'])\n",
    "    \n",
    "    def longest_path(span_id):\n",
    "        children = tree.get(span_id, [])\n",
    "        if not children:\n",
    "            return 0\n",
    "        return max(child['duration'] + longest_path(child['spanID']) for child in children)\n",
    "    \n",
    "    root = trace_data['spans'][0]\n",
    "    critical_path_duration = longest_path(root['spanID'])\n",
    "    \n",
    "    return critical_path_duration\n",
    "```\n",
    "\n",
    "**Caso Real: Netflix Tracing at Scale**\n",
    "\n",
    "Netflix usa tracing para debug de:\n",
    "- **15,000+ microservicios**\n",
    "- **1M+ requests/segundo**\n",
    "- **Sampling**: 0.1% (1 in 1000 traces)\n",
    "- **Retención**: 7 días (hot), 90 días (cold)\n",
    "- **Storage**: 5 PB de trace data\n",
    "- **Cost savings**: Reduce MTTR de horas → minutos, evita $10M+ en downtime/año\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edf070",
   "metadata": {},
   "source": [
    "### 🔗 **Data Lineage: OpenLineage, DataHub & Compliance**\n",
    "\n",
    "**Why Data Lineage?**\n",
    "\n",
    "```\n",
    "Without Lineage:\n",
    "❌ \"Where does this metric come from?\"\n",
    "❌ \"What breaks if I change this table?\"\n",
    "❌ \"Who uses this dataset?\"\n",
    "❌ \"How did this bad data get here?\"\n",
    "\n",
    "With Lineage:\n",
    "✅ Trace data from source → transformations → destination\n",
    "✅ Impact analysis (downstream dependencies)\n",
    "✅ Root cause analysis (upstream issues)\n",
    "✅ Compliance (GDPR data discovery)\n",
    "```\n",
    "\n",
    "**Lineage Architecture**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│  DATA SOURCES                                        │\n",
    "│  • Databases (Postgres, MySQL, Redshift)             │\n",
    "│  • Data Lakes (S3, ADLS, GCS)                        │\n",
    "│  • Data Warehouses (Snowflake, BigQuery)             │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  PROCESSING ENGINES (emit lineage events)            │\n",
    "│  • Airflow (OpenLineage plugin)                      │\n",
    "│  • Spark (OpenLineage listener)                      │\n",
    "│  • dbt (metadata API)                                │\n",
    "│  • Flink, Kafka, etc.                                │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  LINEAGE BACKENDS (store & query)                    │\n",
    "│  • OpenLineage → Marquez (open source)               │\n",
    "│  • DataHub (LinkedIn, open source)                   │\n",
    "│  • Amundsen (Lyft, open source)                      │\n",
    "│  • Commercial: Collibra, Alation, Atlan              │\n",
    "├──────────────────────────────────────────────────────┤\n",
    "│  VISUALIZATION & QUERY                               │\n",
    "│  • Graph UI (interactive lineage exploration)        │\n",
    "│  • API (programmatic queries)                        │\n",
    "│  • Search (find datasets, columns)                   │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**OpenLineage Standard**\n",
    "\n",
    "```python\n",
    "# OpenLineage Event Structure\n",
    "{\n",
    "    \"eventType\": \"START\",  # START, RUNNING, COMPLETE, FAIL, ABORT\n",
    "    \"eventTime\": \"2025-10-30T10:00:00.000Z\",\n",
    "    \"run\": {\n",
    "        \"runId\": \"abc123-def456-ghi789\"\n",
    "    },\n",
    "    \"job\": {\n",
    "        \"namespace\": \"data-platform\",\n",
    "        \"name\": \"ventas_etl\",\n",
    "        \"facets\": {\n",
    "            \"documentation\": {\n",
    "                \"description\": \"Daily sales ETL pipeline\"\n",
    "            },\n",
    "            \"sourceCode\": {\n",
    "                \"language\": \"python\",\n",
    "                \"sourceCode\": \"def process_sales()...\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"namespace\": \"s3://data-lake\",\n",
    "            \"name\": \"raw/sales.csv\",\n",
    "            \"facets\": {\n",
    "                \"schema\": {\n",
    "                    \"fields\": [\n",
    "                        {\"name\": \"order_id\", \"type\": \"INTEGER\"},\n",
    "                        {\"name\": \"amount\", \"type\": \"FLOAT\"},\n",
    "                        {\"name\": \"customer_id\", \"type\": \"INTEGER\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"dataSource\": {\n",
    "                    \"name\": \"s3\",\n",
    "                    \"uri\": \"s3://data-lake/raw/sales.csv\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {\n",
    "            \"namespace\": \"postgres://warehouse\",\n",
    "            \"name\": \"analytics.sales_fact\",\n",
    "            \"facets\": {\n",
    "                \"schema\": {\n",
    "                    \"fields\": [\n",
    "                        {\"name\": \"sale_id\", \"type\": \"INTEGER\"},\n",
    "                        {\"name\": \"total\", \"type\": \"DECIMAL\"},\n",
    "                        {\"name\": \"customer_key\", \"type\": \"INTEGER\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"dataQuality\": {\n",
    "                    \"rowCount\": 10000,\n",
    "                    \"bytes\": 500000,\n",
    "                    \"columnMetrics\": {\n",
    "                        \"total\": {\n",
    "                            \"nullCount\": 0,\n",
    "                            \"min\": 1.50,\n",
    "                            \"max\": 9999.99\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Airflow + OpenLineage Integration**\n",
    "\n",
    "```python\n",
    "# Install: pip install openlineage-airflow\n",
    "# airflow.cfg\n",
    "\"\"\"\n",
    "[openlineage]\n",
    "transport = {\"type\": \"http\", \"url\": \"http://marquez:5000\"}\n",
    "namespace = data-platform\n",
    "\"\"\"\n",
    "\n",
    "# DAG automatically emits lineage\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_sales(**context):\n",
    "    \"\"\"Extract sales from S3\"\"\"\n",
    "    # OpenLineage auto-captures:\n",
    "    # - Input: s3://data-lake/raw/sales.csv\n",
    "    # - Output: XCom (internal)\n",
    "    pass\n",
    "\n",
    "def transform_sales(**context):\n",
    "    \"\"\"Transform sales data\"\"\"\n",
    "    # OpenLineage auto-captures:\n",
    "    # - Input: XCom from extract_sales\n",
    "    # - Output: XCom to load_sales\n",
    "    pass\n",
    "\n",
    "with DAG('sales_etl', start_date=datetime(2025, 1, 1)) as dag:\n",
    "    \n",
    "    # Lineage captured automatically\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        python_callable=extract_sales\n",
    "    )\n",
    "    \n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        python_callable=transform_sales\n",
    "    )\n",
    "    \n",
    "    # SQL lineage captured from query\n",
    "    load = PostgresOperator(\n",
    "        task_id='load',\n",
    "        postgres_conn_id='warehouse',\n",
    "        sql=\"\"\"\n",
    "            INSERT INTO analytics.sales_fact\n",
    "            SELECT order_id, amount, customer_id\n",
    "            FROM staging.sales_raw\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    extract >> transform >> load\n",
    "\n",
    "# View lineage in Marquez UI:\n",
    "# http://marquez:3000/lineage/data-platform/sales_etl\n",
    "```\n",
    "\n",
    "**Spark + OpenLineage**\n",
    "\n",
    "```python\n",
    "# Configure Spark with OpenLineage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sales_etl\")\n",
    "    .config(\"spark.extraListeners\", \"io.openlineage.spark.agent.OpenLineageSparkListener\")\n",
    "    .config(\"spark.openlineage.transport.type\", \"http\")\n",
    "    .config(\"spark.openlineage.transport.url\", \"http://marquez:5000\")\n",
    "    .config(\"spark.openlineage.namespace\", \"data-platform\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Read (input lineage captured)\n",
    "df = spark.read.parquet(\"s3://data-lake/raw/sales/\")\n",
    "\n",
    "# Transform (column-level lineage)\n",
    "df_transformed = (\n",
    "    df\n",
    "    .filter(df['amount'] > 0)\n",
    "    .withColumn('total', df['amount'] * df['quantity'])\n",
    "    .select('order_id', 'total', 'customer_id')\n",
    ")\n",
    "\n",
    "# Write (output lineage captured)\n",
    "df_transformed.write.mode(\"overwrite\").parquet(\"s3://data-lake/curated/sales/\")\n",
    "\n",
    "# OpenLineage automatically emits:\n",
    "# - Input: s3://data-lake/raw/sales/\n",
    "# - Output: s3://data-lake/curated/sales/\n",
    "# - Column lineage: total = amount * quantity\n",
    "```\n",
    "\n",
    "**DataHub API Usage**\n",
    "\n",
    "```python\n",
    "from datahub.emitter.mce_builder import make_dataset_urn, make_data_platform_urn\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import (\n",
    "    DatasetPropertiesClass,\n",
    "    UpstreamLineageClass,\n",
    "    UpstreamClass\n",
    ")\n",
    "\n",
    "# Initialize emitter\n",
    "emitter = DatahubRestEmitter(\"http://datahub:8080\")\n",
    "\n",
    "# 1. REGISTER DATASET\n",
    "dataset_urn = make_dataset_urn(\n",
    "    platform=\"postgres\",\n",
    "    name=\"analytics.sales_fact\",\n",
    "    env=\"PROD\"\n",
    ")\n",
    "\n",
    "dataset_properties = DatasetPropertiesClass(\n",
    "    description=\"Daily sales fact table\",\n",
    "    customProperties={\n",
    "        \"owner\": \"data-team@company.com\",\n",
    "        \"sla\": \"daily by 9am\",\n",
    "        \"retention\": \"7 years\"\n",
    "    }\n",
    ")\n",
    "\n",
    "emitter.emit_mcp(\n",
    "    entity_urn=dataset_urn,\n",
    "    aspect=dataset_properties\n",
    ")\n",
    "\n",
    "# 2. EMIT LINEAGE\n",
    "upstream_sales = make_dataset_urn(\"s3\", \"data-lake/raw/sales\", \"PROD\")\n",
    "upstream_customers = make_dataset_urn(\"postgres\", \"crm.customers\", \"PROD\")\n",
    "\n",
    "lineage = UpstreamLineageClass(\n",
    "    upstreams=[\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_sales,\n",
    "            type=\"TRANSFORMED\"\n",
    "        ),\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_customers,\n",
    "            type=\"TRANSFORMED\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "emitter.emit_mcp(\n",
    "    entity_urn=dataset_urn,\n",
    "    aspect=lineage\n",
    ")\n",
    "\n",
    "# 3. QUERY LINEAGE (REST API)\n",
    "import requests\n",
    "\n",
    "def get_downstream_tables(table_urn: str):\n",
    "    \"\"\"Find all tables that depend on this table\"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    {\n",
    "      dataset(urn: \"%s\") {\n",
    "        downstreams(input: {query: \"*\", start: 0, count: 100}) {\n",
    "          total\n",
    "          relationships {\n",
    "            entity {\n",
    "              urn\n",
    "              properties {\n",
    "                name\n",
    "                description\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % table_urn\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://datahub:8080/api/graphql\",\n",
    "        json={\"query\": query}\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Find impact of changing sales_raw\n",
    "downstreams = get_downstream_tables(\"urn:li:dataset:(urn:li:dataPlatform:s3,data-lake/raw/sales,PROD)\")\n",
    "\n",
    "print(f\"This table impacts {len(downstreams['data']['dataset']['downstreams']['relationships'])} downstream tables\")\n",
    "```\n",
    "\n",
    "**Column-Level Lineage**\n",
    "\n",
    "```python\n",
    "from datahub.metadata.schema_classes import FineGrainedLineageClass\n",
    "\n",
    "# Define column-level transformations\n",
    "column_lineage = FineGrainedLineageClass(\n",
    "    upstreamType=\"FIELD_SET\",\n",
    "    upstreams=[\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:s3,raw.sales,PROD),amount)\",\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:s3,raw.sales,PROD),quantity)\"\n",
    "    ],\n",
    "    downstreamType=\"FIELD\",\n",
    "    downstreams=[\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,analytics.sales_fact,PROD),total)\"\n",
    "    ],\n",
    "    transformOperation=\"MULTIPLY\"\n",
    ")\n",
    "\n",
    "# Query: \"Where does column 'total' come from?\"\n",
    "# Answer: total = raw.sales.amount * raw.sales.quantity\n",
    "```\n",
    "\n",
    "**Lineage for Compliance (GDPR)**\n",
    "\n",
    "```python\n",
    "def find_pii_usage(column_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Find all downstream uses of PII column\n",
    "    (for GDPR right to erasure)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start from source\n",
    "    source_urn = make_dataset_urn(\"postgres\", table_name, \"PROD\")\n",
    "    \n",
    "    # BFS traversal of lineage graph\n",
    "    visited = set()\n",
    "    queue = [source_urn]\n",
    "    pii_locations = []\n",
    "    \n",
    "    while queue:\n",
    "        current_urn = queue.pop(0)\n",
    "        if current_urn in visited:\n",
    "            continue\n",
    "        \n",
    "        visited.add(current_urn)\n",
    "        \n",
    "        # Get downstream tables\n",
    "        downstreams = get_downstream_tables(current_urn)\n",
    "        \n",
    "        for downstream in downstreams:\n",
    "            table = downstream['entity']['urn']\n",
    "            \n",
    "            # Check if PII column exists\n",
    "            schema = get_table_schema(table)\n",
    "            if any(col['name'] == column_name for col in schema['fields']):\n",
    "                pii_locations.append({\n",
    "                    'table': table,\n",
    "                    'column': column_name,\n",
    "                    'lineage_path': get_path_from_source(source_urn, table)\n",
    "                })\n",
    "            \n",
    "            queue.append(table)\n",
    "    \n",
    "    return pii_locations\n",
    "\n",
    "# Example: Find all uses of customer email\n",
    "pii_usage = find_pii_usage('email', 'customers.users')\n",
    "\n",
    "\"\"\"\n",
    "Result:\n",
    "[\n",
    "  {\n",
    "    'table': 'analytics.user_events',\n",
    "    'column': 'email',\n",
    "    'lineage_path': ['customers.users', 'staging.users', 'analytics.user_events']\n",
    "  },\n",
    "  {\n",
    "    'table': 'ml.customer_features',\n",
    "    'column': 'email',\n",
    "    'lineage_path': ['customers.users', 'ml.customer_features']\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Now can delete email from all locations for GDPR compliance\n",
    "```\n",
    "\n",
    "**Impact Analysis**\n",
    "\n",
    "```python\n",
    "def analyze_breaking_change(table_name: str, column_to_remove: str):\n",
    "    \"\"\"\n",
    "    Analyze impact of removing a column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all downstream consumers\n",
    "    downstreams = get_downstream_tables(table_name)\n",
    "    \n",
    "    impact_report = {\n",
    "        'affected_pipelines': [],\n",
    "        'affected_dashboards': [],\n",
    "        'affected_ml_models': []\n",
    "    }\n",
    "    \n",
    "    for downstream in downstreams:\n",
    "        # Check if column is used\n",
    "        lineage = get_column_lineage(downstream['urn'])\n",
    "        \n",
    "        if column_to_remove in lineage['upstream_columns']:\n",
    "            if 'pipeline' in downstream['type']:\n",
    "                impact_report['affected_pipelines'].append(downstream)\n",
    "            elif 'dashboard' in downstream['type']:\n",
    "                impact_report['affected_dashboards'].append(downstream)\n",
    "            elif 'ml_model' in downstream['type']:\n",
    "                impact_report['affected_ml_models'].append(downstream)\n",
    "    \n",
    "    return impact_report\n",
    "\n",
    "# Before dropping column\n",
    "impact = analyze_breaking_change('sales_fact', 'legacy_id')\n",
    "\n",
    "print(f\"⚠️ Removing 'legacy_id' will break:\")\n",
    "print(f\"  - {len(impact['affected_pipelines'])} pipelines\")\n",
    "print(f\"  - {len(impact['affected_dashboards'])} dashboards\")\n",
    "print(f\"  - {len(impact['affected_ml_models'])} ML models\")\n",
    "```\n",
    "\n",
    "**Caso Real: LinkedIn DataHub**\n",
    "\n",
    "LinkedIn usa DataHub para rastrear:\n",
    "- **100,000+ datasets** (Hadoop, Kafka, databases)\n",
    "- **10,000+ pipelines** (Airflow, Azkaban)\n",
    "- **5,000+ dashboards** (Superset, Tableau)\n",
    "- **Column-level lineage**: 1M+ column relationships\n",
    "- **Impact**: Reduce data incidents de 50/mes → 5/mes\n",
    "- **Compliance**: GDPR data discovery en minutos vs semanas\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f4fa7",
   "metadata": {},
   "source": [
    "## 1. Los tres pilares de observabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cacde",
   "metadata": {},
   "source": [
    "- **Logs**: eventos estructurados (JSON) con contexto (trace_id, user, pipeline).\n",
    "- **Metrics**: contadores, gauges, histogramas (latencia, throughput, errores).\n",
    "- **Traces**: flujo de ejecución distribuido (OpenTelemetry)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3e707",
   "metadata": {},
   "source": [
    "## 2. Logs estructurados con loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys, json\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, serialize=True)  # JSON output\n",
    "\n",
    "logger.info('pipeline_started', pipeline='ventas_etl', version='v1.2.3')\n",
    "logger.error('validation_failed', pipeline='ventas_etl', error_count=5, sample_ids=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5547e2",
   "metadata": {},
   "source": [
    "## 3. Métricas con Prometheus (cliente Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prometheus_client import Counter, Histogram, start_http_server\n",
    "# import time, random\n",
    "# \n",
    "# EVENTS_PROCESSED = Counter('events_processed_total', 'Total de eventos procesados', ['pipeline'])\n",
    "# LATENCY = Histogram('processing_latency_seconds', 'Latencia de procesamiento', ['pipeline'])\n",
    "# \n",
    "# start_http_server(8000)  # exponer métricas en :8000/metrics\n",
    "# \n",
    "# while True:\n",
    "#     with LATENCY.labels(pipeline='ventas_etl').time():\n",
    "#         time.sleep(random.uniform(0.01, 0.1))\n",
    "#         EVENTS_PROCESSED.labels(pipeline='ventas_etl').inc()\n",
    "print('Ejemplo comentado; descomenta si tienes prometheus_client instalado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04949d8",
   "metadata": {},
   "source": [
    "## 4. Linaje de datos con OpenLineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02ec08",
   "metadata": {},
   "source": [
    "- Estándar abierto para rastrear de dónde vienen los datos y a dónde van.\n",
    "- Integración con Airflow, Spark, dbt, Flink.\n",
    "- Backend: Marquez, DataHub, Egeria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e212bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openlineage_example = r'''\n",
    "# Integración con Airflow (plugin OpenLineage)\n",
    "# airflow.cfg\n",
    "[openlineage]\n",
    "transport = {\"type\": \"http\", \"url\": \"http://marquez:5000\"}\n",
    "\n",
    "# En cada DAG Run, Airflow emite eventos de linaje:\n",
    "{\n",
    "  \"eventType\": \"START\",\n",
    "  \"job\": {\"namespace\": \"data-platform\", \"name\": \"ventas_etl\"},\n",
    "  \"inputs\": [{\"namespace\": \"s3\", \"name\": \"raw/ventas.csv\"}],\n",
    "  \"outputs\": [{\"namespace\": \"s3\", \"name\": \"curated/ventas.parquet\"}]\n",
    "}\n",
    "'''\n",
    "print(openlineage_example.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6ab61",
   "metadata": {},
   "source": [
    "## 5. SLOs (Service Level Objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454620e",
   "metadata": {},
   "source": [
    "- Definir objetivos medibles: ej. \"99.9% de los pipelines completan < 30 min\".\n",
    "- Alertar si SLO en riesgo (error budget casi agotado).\n",
    "- Revisar SLOs mensualmente y ajustar según necesidades del negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ead7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "slo_example = pd.DataFrame([\n",
    "  {'Pipeline':'ventas_etl', 'SLO':'Latencia p99 < 30min', 'Actual':'28min', 'Status':'✅'},\n",
    "  {'Pipeline':'streaming_kafka', 'SLO':'Errores < 0.1%', 'Actual':'0.05%', 'Status':'✅'},\n",
    "  {'Pipeline':'ml_training', 'SLO':'Disponibilidad > 99.5%', 'Actual':'99.2%', 'Status':'⚠️'},\n",
    "])\n",
    "slo_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622a0b8",
   "metadata": {},
   "source": [
    "## 6. Dashboards y alertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fc8d4",
   "metadata": {},
   "source": [
    "- Grafana para visualizar métricas de Prometheus.\n",
    "- Alertmanager para notificar en Slack/PagerDuty.\n",
    "- DataHub/Marquez UI para explorar linaje interactivo.\n",
    "- Logs centralizados en ELK/Loki para troubleshooting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
