{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19a6223",
   "metadata": {},
   "source": [
    "# \ud83d\udcca Observabilidad y Linaje de Datos\n",
    "\n",
    "Objetivo: implementar observabilidad end-to-end con logs estructurados, m\u00e9tricas (Prometheus), trazas (OpenTelemetry), linaje de datos (OpenLineage/DataHub) y SLOs.\n",
    "\n",
    "- Duraci\u00f3n: 120\u2013150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Senior 01 (Governance), pipelines en producci\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e040c57",
   "metadata": {},
   "source": [
    "### \ud83d\udce1 **Observability: From Monitoring to Full Visibility**\n",
    "\n",
    "**Evolution of Monitoring**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  TRADITIONAL MONITORING (2000s)                        \u2502\n",
    "\u2502  \u2022 Metrics: CPU, memory, disk                          \u2502\n",
    "\u2502  \u2022 Dashboards: Static graphs                           \u2502\n",
    "\u2502  \u2022 Alerts: Threshold-based (CPU > 80%)                 \u2502\n",
    "\u2502  \u2022 Problem: \"What is broken?\" \u274c                       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  OBSERVABILITY (2020s)                                 \u2502\n",
    "\u2502  \u2022 Logs + Metrics + Traces (3 pillars)                 \u2502\n",
    "\u2502  \u2022 Distributed tracing                                 \u2502\n",
    "\u2502  \u2022 High-cardinality queries                            \u2502\n",
    "\u2502  \u2022 Problem: \"Why is it broken?\" \u2705                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Three Pillars of Observability**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  1. LOGS (Events)                                       \u2502\n",
    "\u2502  \u2022 What: Discrete events (errors, warnings, debug)      \u2502\n",
    "\u2502  \u2022 Format: Structured JSON with context                 \u2502\n",
    "\u2502  \u2022 Tools: ELK, Loki, CloudWatch Logs                    \u2502\n",
    "\u2502  \u2022 Query: \"Show me all errors in pipeline X last hour\"  \u2502\n",
    "\u2502                                                          \u2502\n",
    "\u2502  2. METRICS (Aggregates)                                \u2502\n",
    "\u2502  \u2022 What: Numeric measurements over time                 \u2502\n",
    "\u2502  \u2022 Types: Counter, Gauge, Histogram, Summary            \u2502\n",
    "\u2502  \u2022 Tools: Prometheus, InfluxDB, Datadog                 \u2502\n",
    "\u2502  \u2022 Query: \"What's p99 latency for pipeline X?\"          \u2502\n",
    "\u2502                                                          \u2502\n",
    "\u2502  3. TRACES (Flows)                                      \u2502\n",
    "\u2502  \u2022 What: Request flow through distributed system        \u2502\n",
    "\u2502  \u2022 Standard: OpenTelemetry                              \u2502\n",
    "\u2502  \u2022 Tools: Jaeger, Zipkin, Tempo                         \u2502\n",
    "\u2502  \u2022 Query: \"Why is this request slow? Where?\"            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Structured Logging: From Strings to Context**\n",
    "\n",
    "```python\n",
    "# \u274c BAD: String logging\n",
    "import logging\n",
    "logging.info(\"Pipeline ventas_etl started\")\n",
    "logging.error(\"Validation failed with 5 errors: [1,2,3,4,5]\")\n",
    "\n",
    "# Output: Unparseable strings\n",
    "# 2025-10-30 10:00:00 INFO Pipeline ventas_etl started\n",
    "# 2025-10-30 10:01:00 ERROR Validation failed with 5 errors: [1,2,3,4,5]\n",
    "\n",
    "# \u2705 GOOD: Structured logging\n",
    "from loguru import logger\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Configure JSON output\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"{time} {level} {message}\",\n",
    "    serialize=True  # JSON output\n",
    ")\n",
    "\n",
    "# Add context\n",
    "logger.bind(\n",
    "    pipeline=\"ventas_etl\",\n",
    "    version=\"v1.2.3\",\n",
    "    environment=\"production\"\n",
    ").info(\"pipeline_started\")\n",
    "\n",
    "logger.bind(\n",
    "    pipeline=\"ventas_etl\",\n",
    "    error_count=5,\n",
    "    sample_ids=[1, 2, 3, 4, 5],\n",
    "    validation_type=\"schema\"\n",
    ").error(\"validation_failed\")\n",
    "\n",
    "# Output: Queryable JSON\n",
    "\"\"\"\n",
    "{\n",
    "  \"text\": \"pipeline_started\",\n",
    "  \"record\": {\n",
    "    \"time\": {\"timestamp\": 1730286000.123},\n",
    "    \"level\": {\"name\": \"INFO\"},\n",
    "    \"message\": \"pipeline_started\",\n",
    "    \"extra\": {\n",
    "      \"pipeline\": \"ventas_etl\",\n",
    "      \"version\": \"v1.2.3\",\n",
    "      \"environment\": \"production\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Now can query: \"Show all errors where pipeline=ventas_etl AND error_count > 3\"\n",
    "```\n",
    "\n",
    "**Logging Best Practices for Data Pipelines**\n",
    "\n",
    "```python\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from functools import wraps\n",
    "\n",
    "class DataPipelineLogger:\n",
    "    \"\"\"Logging wrapper para data pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str, run_id: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # Configure logger\n",
    "        logger.remove()\n",
    "        logger.add(\n",
    "            f\"logs/{pipeline_name}/{{time}}.log\",\n",
    "            rotation=\"500 MB\",\n",
    "            retention=\"30 days\",\n",
    "            serialize=True,\n",
    "            format=\"{time} {level} {message}\"\n",
    "        )\n",
    "        \n",
    "        # Add common context\n",
    "        self.logger = logger.bind(\n",
    "            pipeline=pipeline_name,\n",
    "            run_id=run_id,\n",
    "            environment=\"production\"\n",
    "        )\n",
    "    \n",
    "    def log_stage_start(self, stage: str, **kwargs):\n",
    "        \"\"\"Log inicio de stage\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"stage_started\",\n",
    "            stage=stage,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def log_stage_complete(self, stage: str, records_processed: int, duration_sec: float):\n",
    "        \"\"\"Log completado de stage\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"stage_completed\",\n",
    "            stage=stage,\n",
    "            records_processed=records_processed,\n",
    "            duration_sec=duration_sec,\n",
    "            throughput_rps=records_processed / duration_sec\n",
    "        )\n",
    "    \n",
    "    def log_data_quality_check(self, check_name: str, passed: bool, details: dict):\n",
    "        \"\"\"Log data quality validation\"\"\"\n",
    "        level = \"info\" if passed else \"warning\"\n",
    "        getattr(self.logger, level)(\n",
    "            f\"quality_check\",\n",
    "            check_name=check_name,\n",
    "            passed=passed,\n",
    "            **details\n",
    "        )\n",
    "    \n",
    "    def log_error(self, stage: str, error: Exception, **kwargs):\n",
    "        \"\"\"Log error con stacktrace\"\"\"\n",
    "        self.logger.error(\n",
    "            f\"stage_failed\",\n",
    "            stage=stage,\n",
    "            error_type=type(error).__name__,\n",
    "            error_message=str(error),\n",
    "            stacktrace=traceback.format_exc(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Decorator para auto-logging\n",
    "def log_execution(stage_name: str):\n",
    "    \"\"\"Decorator to log function execution\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            pipeline_logger = kwargs.get('logger')\n",
    "            \n",
    "            if pipeline_logger:\n",
    "                pipeline_logger.log_stage_start(stage_name)\n",
    "            \n",
    "            start = datetime.now()\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                if pipeline_logger:\n",
    "                    duration = (datetime.now() - start).total_seconds()\n",
    "                    records = result.get('records_processed', 0) if isinstance(result, dict) else 0\n",
    "                    pipeline_logger.log_stage_complete(stage_name, records, duration)\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                if pipeline_logger:\n",
    "                    pipeline_logger.log_error(stage_name, e)\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage\n",
    "logger = DataPipelineLogger(\"ventas_etl\", run_id=\"run_20251030_100000\")\n",
    "\n",
    "@log_execution(\"extract\")\n",
    "def extract_data(logger=None):\n",
    "    # Extraction logic\n",
    "    return {\"records_processed\": 10000}\n",
    "\n",
    "@log_execution(\"transform\")\n",
    "def transform_data(df, logger=None):\n",
    "    # Transformation logic\n",
    "    \n",
    "    # Log data quality check\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    logger.log_data_quality_check(\n",
    "        \"null_check\",\n",
    "        passed=null_count == 0,\n",
    "        details={\"null_count\": null_count}\n",
    "    )\n",
    "    \n",
    "    return {\"records_processed\": len(df)}\n",
    "\n",
    "# Execute\n",
    "extract_data(logger=logger)\n",
    "transform_data(df, logger=logger)\n",
    "```\n",
    "\n",
    "**Correlation IDs: Tracing Across Services**\n",
    "\n",
    "```python\n",
    "import uuid\n",
    "from contextvars import ContextVar\n",
    "\n",
    "# Thread-safe context variable\n",
    "trace_id_var: ContextVar[str] = ContextVar('trace_id', default=None)\n",
    "\n",
    "def generate_trace_id() -> str:\n",
    "    \"\"\"Generate unique trace ID\"\"\"\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def set_trace_id(trace_id: str = None):\n",
    "    \"\"\"Set trace ID for current context\"\"\"\n",
    "    if trace_id is None:\n",
    "        trace_id = generate_trace_id()\n",
    "    trace_id_var.set(trace_id)\n",
    "    return trace_id\n",
    "\n",
    "def get_trace_id() -> str:\n",
    "    \"\"\"Get trace ID from current context\"\"\"\n",
    "    return trace_id_var.get()\n",
    "\n",
    "# FastAPI middleware to propagate trace ID\n",
    "from fastapi import Request\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "\n",
    "class TraceIdMiddleware(BaseHTTPMiddleware):\n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        # Extract trace_id from header or generate\n",
    "        trace_id = request.headers.get('X-Trace-Id', generate_trace_id())\n",
    "        set_trace_id(trace_id)\n",
    "        \n",
    "        # Add to logger context\n",
    "        logger.bind(trace_id=trace_id)\n",
    "        \n",
    "        response = await call_next(request)\n",
    "        \n",
    "        # Add to response header\n",
    "        response.headers['X-Trace-Id'] = trace_id\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Propagate to downstream services\n",
    "import httpx\n",
    "\n",
    "async def call_downstream_service(url: str):\n",
    "    \"\"\"Call otro servicio con trace ID\"\"\"\n",
    "    headers = {\n",
    "        'X-Trace-Id': get_trace_id()\n",
    "    }\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, headers=headers)\n",
    "        return response.json()\n",
    "\n",
    "# Now all logs across services share same trace_id \u2192 easy debugging!\n",
    "```\n",
    "\n",
    "**Caso Real: Uber's Observability Platform**\n",
    "\n",
    "Uber procesa **10M+ eventos/segundo** con:\n",
    "- **Logs**: JSON structured logs \u2192 Kafka \u2192 HDFS (long-term) + Elasticsearch (search)\n",
    "- **Metrics**: Prometheus (4M+ time series), M3DB (distributed storage)\n",
    "- **Traces**: Jaeger (100K+ traces/sec), custom sampling\n",
    "- **Cost**: $50M/year en infraestructura de observability\n",
    "- **ROI**: Reduce MTTR (Mean Time To Recovery) de 2h \u2192 15min\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaab56e",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **Metrics & Prometheus: Measure What Matters**\n",
    "\n",
    "**Metric Types**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  COUNTER (monotonic, only increases)                   \u2502\n",
    "\u2502  \u2022 events_processed_total                              \u2502\n",
    "\u2502  \u2022 errors_total                                        \u2502\n",
    "\u2502  \u2022 bytes_transferred_total                             \u2502\n",
    "\u2502  Query: rate(events_processed_total[5m])               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  GAUGE (can go up or down)                             \u2502\n",
    "\u2502  \u2022 active_connections                                  \u2502\n",
    "\u2502  \u2022 queue_size                                          \u2502\n",
    "\u2502  \u2022 memory_usage_bytes                                  \u2502\n",
    "\u2502  Query: avg_over_time(queue_size[1h])                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  HISTOGRAM (distribution of values)                    \u2502\n",
    "\u2502  \u2022 request_duration_seconds                            \u2502\n",
    "\u2502  \u2022 record_size_bytes                                   \u2502\n",
    "\u2502  Query: histogram_quantile(0.99, ...)  # p99           \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  SUMMARY (client-side percentiles)                     \u2502\n",
    "\u2502  \u2022 processing_time_seconds                             \u2502\n",
    "\u2502  Query: processing_time_seconds{quantile=\"0.99\"}       \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Prometheus Client Implementation**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Gauge, Histogram, Summary\n",
    "from prometheus_client import start_http_server, CollectorRegistry\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Create registry (allows multiple apps in same process)\n",
    "registry = CollectorRegistry()\n",
    "\n",
    "# 1. COUNTER: Monotonic increasing\n",
    "events_processed = Counter(\n",
    "    'events_processed_total',\n",
    "    'Total number of events processed',\n",
    "    ['pipeline', 'stage'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "errors_total = Counter(\n",
    "    'errors_total',\n",
    "    'Total number of errors',\n",
    "    ['pipeline', 'stage', 'error_type'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 2. GAUGE: Current value\n",
    "active_jobs = Gauge(\n",
    "    'active_jobs',\n",
    "    'Number of active jobs',\n",
    "    ['pipeline'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "queue_size = Gauge(\n",
    "    'queue_size',\n",
    "    'Number of items in queue',\n",
    "    ['queue_name'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 3. HISTOGRAM: Distribution with buckets\n",
    "processing_duration = Histogram(\n",
    "    'processing_duration_seconds',\n",
    "    'Time spent processing records',\n",
    "    ['pipeline', 'stage'],\n",
    "    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0),\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "record_size = Histogram(\n",
    "    'record_size_bytes',\n",
    "    'Size of processed records',\n",
    "    ['pipeline'],\n",
    "    buckets=(100, 1000, 10000, 100000, 1000000),\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# 4. SUMMARY: Client-side percentiles\n",
    "processing_time = Summary(\n",
    "    'processing_time_seconds',\n",
    "    'Processing time summary',\n",
    "    ['pipeline'],\n",
    "    registry=registry\n",
    ")\n",
    "\n",
    "# Instrumented data pipeline\n",
    "class InstrumentedPipeline:\n",
    "    \"\"\"Data pipeline with Prometheus metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "    \n",
    "    def run_stage(self, stage_name: str, records: list):\n",
    "        \"\"\"Execute stage with metrics\"\"\"\n",
    "        \n",
    "        # Track active jobs\n",
    "        active_jobs.labels(pipeline=self.pipeline_name).inc()\n",
    "        \n",
    "        try:\n",
    "            # Time execution\n",
    "            with processing_duration.labels(\n",
    "                pipeline=self.pipeline_name,\n",
    "                stage=stage_name\n",
    "            ).time():\n",
    "                \n",
    "                # Process records\n",
    "                for record in records:\n",
    "                    # Increment counter\n",
    "                    events_processed.labels(\n",
    "                        pipeline=self.pipeline_name,\n",
    "                        stage=stage_name\n",
    "                    ).inc()\n",
    "                    \n",
    "                    # Record size\n",
    "                    record_size.labels(pipeline=self.pipeline_name).observe(\n",
    "                        len(str(record))\n",
    "                    )\n",
    "                    \n",
    "                    # Simulate processing\n",
    "                    time.sleep(random.uniform(0.001, 0.01))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Track errors\n",
    "            errors_total.labels(\n",
    "                pipeline=self.pipeline_name,\n",
    "                stage=stage_name,\n",
    "                error_type=type(e).__name__\n",
    "            ).inc()\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            # Decrease active jobs\n",
    "            active_jobs.labels(pipeline=self.pipeline_name).dec()\n",
    "\n",
    "# Start metrics server\n",
    "start_http_server(8000, registry=registry)\n",
    "# Metrics available at: http://localhost:8000/metrics\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = InstrumentedPipeline(\"ventas_etl\")\n",
    "pipeline.run_stage(\"extract\", [{\"id\": i} for i in range(1000)])\n",
    "pipeline.run_stage(\"transform\", [{\"id\": i} for i in range(1000)])\n",
    "\n",
    "# Prometheus scrapes /metrics every 15s\n",
    "\"\"\"\n",
    "# HELP events_processed_total Total number of events processed\n",
    "# TYPE events_processed_total counter\n",
    "events_processed_total{pipeline=\"ventas_etl\",stage=\"extract\"} 1000.0\n",
    "events_processed_total{pipeline=\"ventas_etl\",stage=\"transform\"} 1000.0\n",
    "\n",
    "# HELP processing_duration_seconds Time spent processing records\n",
    "# TYPE processing_duration_seconds histogram\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"0.1\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"0.5\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"1.0\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"2.5\"} 0.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"5.0\"} 1.0\n",
    "processing_duration_seconds_bucket{pipeline=\"ventas_etl\",stage=\"extract\",le=\"+Inf\"} 1.0\n",
    "processing_duration_seconds_sum{pipeline=\"ventas_etl\",stage=\"extract\"} 4.523\n",
    "processing_duration_seconds_count{pipeline=\"ventas_etl\",stage=\"extract\"} 1.0\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**PromQL Queries for Data Engineers**\n",
    "\n",
    "```python\n",
    "# Common queries\n",
    "promql_queries = {\n",
    "    # Throughput (events/second)\n",
    "    \"throughput\": \"\"\"\n",
    "        rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Error rate (%)\n",
    "    \"error_rate\": \"\"\"\n",
    "        (\n",
    "            rate(errors_total{pipeline=\"ventas_etl\"}[5m])\n",
    "            / \n",
    "            rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "        ) * 100\n",
    "    \"\"\",\n",
    "    \n",
    "    # P99 latency\n",
    "    \"p99_latency\": \"\"\"\n",
    "        histogram_quantile(0.99, \n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "        )\n",
    "    \"\"\",\n",
    "    \n",
    "    # P50 (median) latency\n",
    "    \"p50_latency\": \"\"\"\n",
    "        histogram_quantile(0.50, \n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "        )\n",
    "    \"\"\",\n",
    "    \n",
    "    # Total errors last hour\n",
    "    \"errors_1h\": \"\"\"\n",
    "        increase(errors_total{pipeline=\"ventas_etl\"}[1h])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Average queue size\n",
    "    \"avg_queue_size\": \"\"\"\n",
    "        avg_over_time(queue_size{queue_name=\"events\"}[5m])\n",
    "    \"\"\",\n",
    "    \n",
    "    # Success rate (inverse of error rate)\n",
    "    \"success_rate\": \"\"\"\n",
    "        (1 - (\n",
    "            sum(rate(errors_total[5m]))\n",
    "            /\n",
    "            sum(rate(events_processed_total[5m]))\n",
    "        )) * 100\n",
    "    \"\"\",\n",
    "    \n",
    "    # Top 5 slowest pipelines\n",
    "    \"slowest_pipelines\": \"\"\"\n",
    "        topk(5, \n",
    "            avg by (pipeline) (\n",
    "                rate(processing_duration_seconds_sum[5m])\n",
    "                /\n",
    "                rate(processing_duration_seconds_count[5m])\n",
    "            )\n",
    "        )\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Alerting Rules (Prometheus Alertmanager)**\n",
    "\n",
    "```yaml\n",
    "# prometheus_rules.yml\n",
    "groups:\n",
    "  - name: data_pipeline_alerts\n",
    "    interval: 30s\n",
    "    rules:\n",
    "      # High error rate\n",
    "      - alert: HighErrorRate\n",
    "        expr: |\n",
    "          (\n",
    "            rate(errors_total{pipeline=\"ventas_etl\"}[5m])\n",
    "            /\n",
    "            rate(events_processed_total{pipeline=\"ventas_etl\"}[5m])\n",
    "          ) > 0.05\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: critical\n",
    "          team: data-engineering\n",
    "        annotations:\n",
    "          summary: \"High error rate in {{ $labels.pipeline }}\"\n",
    "          description: \"Error rate is {{ $value | humanizePercentage }}\"\n",
    "      \n",
    "      # High latency\n",
    "      - alert: HighLatency\n",
    "        expr: |\n",
    "          histogram_quantile(0.99,\n",
    "            rate(processing_duration_seconds_bucket{pipeline=\"ventas_etl\"}[5m])\n",
    "          ) > 30\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High P99 latency in {{ $labels.pipeline }}\"\n",
    "          description: \"P99 latency is {{ $value }}s (threshold: 30s)\"\n",
    "      \n",
    "      # Pipeline stuck (no events processed)\n",
    "      - alert: PipelineStuck\n",
    "        expr: |\n",
    "          rate(events_processed_total{pipeline=\"ventas_etl\"}[5m]) == 0\n",
    "        for: 15m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"Pipeline {{ $labels.pipeline }} is stuck\"\n",
    "          description: \"No events processed in last 15 minutes\"\n",
    "      \n",
    "      # Queue too large\n",
    "      - alert: QueueTooLarge\n",
    "        expr: queue_size{queue_name=\"events\"} > 10000\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Queue {{ $labels.queue_name }} is too large\"\n",
    "          description: \"Queue size: {{ $value }}\"\n",
    "```\n",
    "\n",
    "**Grafana Dashboard (JSON)**\n",
    "\n",
    "```python\n",
    "grafana_dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Data Pipeline Observability\",\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Throughput (events/sec)\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(rate(events_processed_total[5m])) by (pipeline)',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Error Rate (%)\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": '''\n",
    "                            (\n",
    "                                sum(rate(errors_total[5m])) by (pipeline)\n",
    "                                /\n",
    "                                sum(rate(events_processed_total[5m])) by (pipeline)\n",
    "                            ) * 100\n",
    "                        ''',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\",\n",
    "                \"alert\": {\n",
    "                    \"conditions\": [\n",
    "                        {\n",
    "                            \"evaluator\": {\"type\": \"gt\", \"params\": [5]},\n",
    "                            \"query\": {\"params\": [\"A\", \"5m\", \"now\"]}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"P50, P95, P99 Latency\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.50, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P50\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.95, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P95\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.99, rate(processing_duration_seconds_bucket[5m]))',\n",
    "                        \"legendFormat\": \"P99\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"graph\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Active Jobs\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(active_jobs) by (pipeline)',\n",
    "                        \"legendFormat\": \"{{ pipeline }}\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"gauge\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom Metrics for Data Quality**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Gauge\n",
    "\n",
    "# Data quality metrics\n",
    "data_quality_score = Gauge(\n",
    "    'data_quality_score',\n",
    "    'Data quality score (0-100)',\n",
    "    ['pipeline', 'table', 'check_type']\n",
    ")\n",
    "\n",
    "null_percentage = Gauge(\n",
    "    'null_percentage',\n",
    "    'Percentage of null values',\n",
    "    ['pipeline', 'table', 'column']\n",
    ")\n",
    "\n",
    "duplicate_count = Gauge(\n",
    "    'duplicate_count',\n",
    "    'Number of duplicate records',\n",
    "    ['pipeline', 'table']\n",
    ")\n",
    "\n",
    "# Update metrics\n",
    "def run_quality_checks(df, pipeline_name, table_name):\n",
    "    \"\"\"Run data quality checks and emit metrics\"\"\"\n",
    "    \n",
    "    # Null check\n",
    "    for col in df.columns:\n",
    "        null_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "        null_percentage.labels(\n",
    "            pipeline=pipeline_name,\n",
    "            table=table_name,\n",
    "            column=col\n",
    "        ).set(null_pct)\n",
    "    \n",
    "    # Duplicate check\n",
    "    dup_count = df.duplicated().sum()\n",
    "    duplicate_count.labels(\n",
    "        pipeline=pipeline_name,\n",
    "        table=table_name\n",
    "    ).set(dup_count)\n",
    "    \n",
    "    # Overall quality score\n",
    "    score = 100 - (null_pct + (dup_count / len(df) * 100))\n",
    "    data_quality_score.labels(\n",
    "        pipeline=pipeline_name,\n",
    "        table=table_name,\n",
    "        check_type='overall'\n",
    "    ).set(max(0, score))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc72b7",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d **Distributed Tracing: OpenTelemetry**\n",
    "\n",
    "**Why Distributed Tracing?**\n",
    "\n",
    "```\n",
    "Traditional Logs (per service):\n",
    "Service A: [10:00:01] Request received\n",
    "Service B: [10:00:02] Processing data\n",
    "Service C: [10:00:05] Database query\n",
    "\u274c Can't connect these logs across services!\n",
    "\n",
    "Distributed Tracing:\n",
    "Trace ID: abc123\n",
    "\u251c\u2500 Span: API Request (Service A) [200ms]\n",
    "\u2502  \u251c\u2500 Span: Extract Data (Service B) [150ms]\n",
    "\u2502  \u2502  \u2514\u2500 Span: S3 Download [100ms]\n",
    "\u2502  \u2514\u2500 Span: Transform Data (Service C) [50ms]\n",
    "\u2502     \u2514\u2500 Span: DB Query [30ms]\n",
    "\u2705 Full request flow visualization!\n",
    "```\n",
    "\n",
    "**OpenTelemetry Architecture**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Application Code                                    \u2502\n",
    "\u2502  \u2022 Instrumented with OpenTelemetry SDK               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  OpenTelemetry Collector                             \u2502\n",
    "\u2502  \u2022 Receives traces from apps                         \u2502\n",
    "\u2502  \u2022 Processes (sampling, filtering)                   \u2502\n",
    "\u2502  \u2022 Exports to backends                               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  Backend Storage                                     \u2502\n",
    "\u2502  \u2022 Jaeger (open source)                              \u2502\n",
    "\u2502  \u2022 Tempo (Grafana)                                   \u2502\n",
    "\u2502  \u2022 Datadog, New Relic, etc.                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**OpenTelemetry Implementation**\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.instrumentation.requests import RequestsInstrumentor\n",
    "from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 1. SETUP TRACER\n",
    "def setup_tracing(service_name: str):\n",
    "    \"\"\"Initialize OpenTelemetry tracing\"\"\"\n",
    "    \n",
    "    # Create resource (service metadata)\n",
    "    resource = Resource.create({\n",
    "        \"service.name\": service_name,\n",
    "        \"service.version\": \"1.2.3\",\n",
    "        \"deployment.environment\": \"production\"\n",
    "    })\n",
    "    \n",
    "    # Create tracer provider\n",
    "    tracer_provider = TracerProvider(resource=resource)\n",
    "    \n",
    "    # Configure Jaeger exporter\n",
    "    jaeger_exporter = JaegerExporter(\n",
    "        agent_host_name=\"localhost\",\n",
    "        agent_port=6831,\n",
    "    )\n",
    "    \n",
    "    # Add span processor\n",
    "    span_processor = BatchSpanProcessor(jaeger_exporter)\n",
    "    tracer_provider.add_span_processor(span_processor)\n",
    "    \n",
    "    # Set global tracer provider\n",
    "    trace.set_tracer_provider(tracer_provider)\n",
    "    \n",
    "    # Auto-instrument libraries\n",
    "    RequestsInstrumentor().instrument()  # HTTP calls\n",
    "    SQLAlchemyInstrumentor().instrument()  # DB queries\n",
    "    \n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "# Initialize\n",
    "tracer = setup_tracing(\"ventas_etl_pipeline\")\n",
    "\n",
    "# 2. MANUAL INSTRUMENTATION\n",
    "def process_data_with_tracing(data):\n",
    "    \"\"\"Pipeline stage with tracing\"\"\"\n",
    "    \n",
    "    # Create parent span\n",
    "    with tracer.start_as_current_span(\"process_data\") as span:\n",
    "        # Add attributes\n",
    "        span.set_attribute(\"data.size\", len(data))\n",
    "        span.set_attribute(\"pipeline.name\", \"ventas_etl\")\n",
    "        \n",
    "        # Extract\n",
    "        with tracer.start_as_current_span(\"extract\") as extract_span:\n",
    "            extract_span.set_attribute(\"source\", \"s3\")\n",
    "            raw_data = extract_from_s3(data)\n",
    "            extract_span.set_attribute(\"records.count\", len(raw_data))\n",
    "        \n",
    "        # Transform\n",
    "        with tracer.start_as_current_span(\"transform\") as transform_span:\n",
    "            transform_span.set_attribute(\"transformation.type\", \"cleansing\")\n",
    "            \n",
    "            try:\n",
    "                cleaned_data = transform(raw_data)\n",
    "                transform_span.set_status(trace.Status(trace.StatusCode.OK))\n",
    "            except Exception as e:\n",
    "                transform_span.set_status(\n",
    "                    trace.Status(trace.StatusCode.ERROR, str(e))\n",
    "                )\n",
    "                transform_span.record_exception(e)\n",
    "                raise\n",
    "        \n",
    "        # Load\n",
    "        with tracer.start_as_current_span(\"load\") as load_span:\n",
    "            load_span.set_attribute(\"destination\", \"postgres\")\n",
    "            load_to_database(cleaned_data)\n",
    "        \n",
    "        # Add event\n",
    "        span.add_event(\"processing_completed\", {\n",
    "            \"records_processed\": len(cleaned_data)\n",
    "        })\n",
    "        \n",
    "        return cleaned_data\n",
    "\n",
    "# 3. CONTEXT PROPAGATION (across services)\n",
    "def call_downstream_service(url: str, trace_id: str):\n",
    "    \"\"\"Propagate trace context to downstream service\"\"\"\n",
    "    \n",
    "    from opentelemetry.propagate import inject\n",
    "    \n",
    "    # Create child span\n",
    "    with tracer.start_as_current_span(\"downstream_call\") as span:\n",
    "        span.set_attribute(\"http.url\", url)\n",
    "        \n",
    "        # Inject trace context into headers\n",
    "        headers = {}\n",
    "        inject(headers)\n",
    "        \n",
    "        # Make HTTP call (trace context propagated)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        span.set_attribute(\"http.status_code\", response.status_code)\n",
    "        \n",
    "        return response.json()\n",
    "\n",
    "# 4. BAGGAGE (cross-cutting context)\n",
    "from opentelemetry.baggage import set_baggage, get_baggage\n",
    "\n",
    "# Set baggage (propagates with trace)\n",
    "set_baggage(\"user.id\", \"user123\")\n",
    "set_baggage(\"tenant.id\", \"tenant456\")\n",
    "\n",
    "# Access in downstream services\n",
    "user_id = get_baggage(\"user.id\")\n",
    "tenant_id = get_baggage(\"tenant.id\")\n",
    "```\n",
    "\n",
    "**Sampling Strategies**\n",
    "\n",
    "```python\n",
    "from opentelemetry.sdk.trace.sampling import (\n",
    "    TraceIdRatioBased,\n",
    "    ParentBased,\n",
    "    StaticSampler,\n",
    "    ALWAYS_ON,\n",
    "    ALWAYS_OFF\n",
    ")\n",
    "\n",
    "# 1. ALWAYS ON (dev/staging)\n",
    "sampler = StaticSampler(ALWAYS_ON)\n",
    "\n",
    "# 2. PROBABILITY-BASED (10% of traces)\n",
    "sampler = TraceIdRatioBased(0.1)\n",
    "\n",
    "# 3. PARENT-BASED (follow parent decision)\n",
    "sampler = ParentBased(\n",
    "    root=TraceIdRatioBased(0.1),  # 10% for root spans\n",
    "    remote_parent_sampled=ALWAYS_ON,  # If parent sampled, sample\n",
    "    remote_parent_not_sampled=ALWAYS_OFF  # Else, don't sample\n",
    ")\n",
    "\n",
    "# 4. CUSTOM SAMPLER (sample errors always)\n",
    "class ErrorSampler(Sampler):\n",
    "    def should_sample(self, context, trace_id, name, kind, attributes, links):\n",
    "        # Always sample if error\n",
    "        if attributes and attributes.get(\"error\"):\n",
    "            return SamplingResult(Decision.RECORD_AND_SAMPLE)\n",
    "        \n",
    "        # 1% for normal traces\n",
    "        if random.random() < 0.01:\n",
    "            return SamplingResult(Decision.RECORD_AND_SAMPLE)\n",
    "        \n",
    "        return SamplingResult(Decision.DROP)\n",
    "\n",
    "# Apply to tracer provider\n",
    "tracer_provider = TracerProvider(sampler=ErrorSampler())\n",
    "```\n",
    "\n",
    "**Spark Instrumentation**\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "def spark_job_with_tracing():\n",
    "    \"\"\"Spark job con OpenTelemetry\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"spark_job\") as job_span:\n",
    "        job_span.set_attribute(\"spark.app.name\", \"ventas_etl\")\n",
    "        \n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"ventas_etl\").getOrCreate()\n",
    "        \n",
    "        # Read\n",
    "        with tracer.start_as_current_span(\"spark.read\") as read_span:\n",
    "            df = spark.read.parquet(\"s3://bucket/raw/\")\n",
    "            read_span.set_attribute(\"records.count\", df.count())\n",
    "        \n",
    "        # Transform\n",
    "        with tracer.start_as_current_span(\"spark.transform\") as transform_span:\n",
    "            df_transformed = df.filter(df['amount'] > 0)\n",
    "            transform_span.set_attribute(\"transformation\", \"filter_positive\")\n",
    "        \n",
    "        # Write\n",
    "        with tracer.start_as_current_span(\"spark.write\") as write_span:\n",
    "            df_transformed.write.mode(\"overwrite\").parquet(\"s3://bucket/curated/\")\n",
    "            write_span.set_attribute(\"output.path\", \"s3://bucket/curated/\")\n",
    "\n",
    "# Airflow instrumentation\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def task_with_tracing(**context):\n",
    "    \"\"\"Airflow task con tracing\"\"\"\n",
    "    \n",
    "    # Extract trace context from Airflow\n",
    "    dag_id = context['dag'].dag_id\n",
    "    task_id = context['task'].task_id\n",
    "    run_id = context['run_id']\n",
    "    \n",
    "    with tracer.start_as_current_span(f\"airflow.task.{task_id}\") as span:\n",
    "        span.set_attribute(\"airflow.dag_id\", dag_id)\n",
    "        span.set_attribute(\"airflow.run_id\", run_id)\n",
    "        \n",
    "        # Task logic\n",
    "        result = process_data()\n",
    "        \n",
    "        span.add_event(\"task_completed\", {\"result\": result})\n",
    "\n",
    "dag = DAG('ventas_etl', schedule_interval='@daily')\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='process',\n",
    "    python_callable=task_with_tracing,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "**Jaeger Query Examples**\n",
    "\n",
    "```bash\n",
    "# Find traces with errors\n",
    "service=ventas_etl error=true\n",
    "\n",
    "# Find slow traces (>5s duration)\n",
    "service=ventas_etl minDuration=5s\n",
    "\n",
    "# Find traces for specific user\n",
    "service=ventas_etl baggage.user.id=user123\n",
    "\n",
    "# Find traces with high DB latency\n",
    "service=ventas_etl operation=db.query minDuration=1s\n",
    "```\n",
    "\n",
    "**Trace Analysis Patterns**\n",
    "\n",
    "```python\n",
    "# Pattern 1: Find bottlenecks\n",
    "def analyze_trace_bottlenecks(trace_data):\n",
    "    \"\"\"Identify slowest spans in trace\"\"\"\n",
    "    \n",
    "    spans = sorted(trace_data['spans'], key=lambda x: x['duration'], reverse=True)\n",
    "    \n",
    "    bottlenecks = []\n",
    "    for span in spans[:5]:\n",
    "        bottlenecks.append({\n",
    "            'operation': span['operationName'],\n",
    "            'duration_ms': span['duration'] / 1000,\n",
    "            'percentage': (span['duration'] / trace_data['duration']) * 100\n",
    "        })\n",
    "    \n",
    "    return bottlenecks\n",
    "\n",
    "# Pattern 2: Critical path\n",
    "def find_critical_path(trace_data):\n",
    "    \"\"\"Find longest path through trace\"\"\"\n",
    "    \n",
    "    def build_span_tree(spans):\n",
    "        tree = {}\n",
    "        for span in spans:\n",
    "            parent_id = span.get('references', [{}])[0].get('spanID')\n",
    "            if parent_id not in tree:\n",
    "                tree[parent_id] = []\n",
    "            tree[parent_id].append(span)\n",
    "        return tree\n",
    "    \n",
    "    tree = build_span_tree(trace_data['spans'])\n",
    "    \n",
    "    def longest_path(span_id):\n",
    "        children = tree.get(span_id, [])\n",
    "        if not children:\n",
    "            return 0\n",
    "        return max(child['duration'] + longest_path(child['spanID']) for child in children)\n",
    "    \n",
    "    root = trace_data['spans'][0]\n",
    "    critical_path_duration = longest_path(root['spanID'])\n",
    "    \n",
    "    return critical_path_duration\n",
    "```\n",
    "\n",
    "**Caso Real: Netflix Tracing at Scale**\n",
    "\n",
    "Netflix usa tracing para debug de:\n",
    "- **15,000+ microservicios**\n",
    "- **1M+ requests/segundo**\n",
    "- **Sampling**: 0.1% (1 in 1000 traces)\n",
    "- **Retenci\u00f3n**: 7 d\u00edas (hot), 90 d\u00edas (cold)\n",
    "- **Storage**: 5 PB de trace data\n",
    "- **Cost savings**: Reduce MTTR de horas \u2192 minutos, evita $10M+ en downtime/a\u00f1o\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edf070",
   "metadata": {},
   "source": [
    "### \ud83d\udd17 **Data Lineage: OpenLineage, DataHub & Compliance**\n",
    "\n",
    "**Why Data Lineage?**\n",
    "\n",
    "```\n",
    "Without Lineage:\n",
    "\u274c \"Where does this metric come from?\"\n",
    "\u274c \"What breaks if I change this table?\"\n",
    "\u274c \"Who uses this dataset?\"\n",
    "\u274c \"How did this bad data get here?\"\n",
    "\n",
    "With Lineage:\n",
    "\u2705 Trace data from source \u2192 transformations \u2192 destination\n",
    "\u2705 Impact analysis (downstream dependencies)\n",
    "\u2705 Root cause analysis (upstream issues)\n",
    "\u2705 Compliance (GDPR data discovery)\n",
    "```\n",
    "\n",
    "**Lineage Architecture**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  DATA SOURCES                                        \u2502\n",
    "\u2502  \u2022 Databases (Postgres, MySQL, Redshift)             \u2502\n",
    "\u2502  \u2022 Data Lakes (S3, ADLS, GCS)                        \u2502\n",
    "\u2502  \u2022 Data Warehouses (Snowflake, BigQuery)             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  PROCESSING ENGINES (emit lineage events)            \u2502\n",
    "\u2502  \u2022 Airflow (OpenLineage plugin)                      \u2502\n",
    "\u2502  \u2022 Spark (OpenLineage listener)                      \u2502\n",
    "\u2502  \u2022 dbt (metadata API)                                \u2502\n",
    "\u2502  \u2022 Flink, Kafka, etc.                                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  LINEAGE BACKENDS (store & query)                    \u2502\n",
    "\u2502  \u2022 OpenLineage \u2192 Marquez (open source)               \u2502\n",
    "\u2502  \u2022 DataHub (LinkedIn, open source)                   \u2502\n",
    "\u2502  \u2022 Amundsen (Lyft, open source)                      \u2502\n",
    "\u2502  \u2022 Commercial: Collibra, Alation, Atlan              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  VISUALIZATION & QUERY                               \u2502\n",
    "\u2502  \u2022 Graph UI (interactive lineage exploration)        \u2502\n",
    "\u2502  \u2022 API (programmatic queries)                        \u2502\n",
    "\u2502  \u2022 Search (find datasets, columns)                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**OpenLineage Standard**\n",
    "\n",
    "```python\n",
    "# OpenLineage Event Structure\n",
    "{\n",
    "    \"eventType\": \"START\",  # START, RUNNING, COMPLETE, FAIL, ABORT\n",
    "    \"eventTime\": \"2025-10-30T10:00:00.000Z\",\n",
    "    \"run\": {\n",
    "        \"runId\": \"abc123-def456-ghi789\"\n",
    "    },\n",
    "    \"job\": {\n",
    "        \"namespace\": \"data-platform\",\n",
    "        \"name\": \"ventas_etl\",\n",
    "        \"facets\": {\n",
    "            \"documentation\": {\n",
    "                \"description\": \"Daily sales ETL pipeline\"\n",
    "            },\n",
    "            \"sourceCode\": {\n",
    "                \"language\": \"python\",\n",
    "                \"sourceCode\": \"def process_sales()...\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"namespace\": \"s3://data-lake\",\n",
    "            \"name\": \"raw/sales.csv\",\n",
    "            \"facets\": {\n",
    "                \"schema\": {\n",
    "                    \"fields\": [\n",
    "                        {\"name\": \"order_id\", \"type\": \"INTEGER\"},\n",
    "                        {\"name\": \"amount\", \"type\": \"FLOAT\"},\n",
    "                        {\"name\": \"customer_id\", \"type\": \"INTEGER\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"dataSource\": {\n",
    "                    \"name\": \"s3\",\n",
    "                    \"uri\": \"s3://data-lake/raw/sales.csv\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {\n",
    "            \"namespace\": \"postgres://warehouse\",\n",
    "            \"name\": \"analytics.sales_fact\",\n",
    "            \"facets\": {\n",
    "                \"schema\": {\n",
    "                    \"fields\": [\n",
    "                        {\"name\": \"sale_id\", \"type\": \"INTEGER\"},\n",
    "                        {\"name\": \"total\", \"type\": \"DECIMAL\"},\n",
    "                        {\"name\": \"customer_key\", \"type\": \"INTEGER\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"dataQuality\": {\n",
    "                    \"rowCount\": 10000,\n",
    "                    \"bytes\": 500000,\n",
    "                    \"columnMetrics\": {\n",
    "                        \"total\": {\n",
    "                            \"nullCount\": 0,\n",
    "                            \"min\": 1.50,\n",
    "                            \"max\": 9999.99\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Airflow + OpenLineage Integration**\n",
    "\n",
    "```python\n",
    "# Install: pip install openlineage-airflow\n",
    "# airflow.cfg\n",
    "\"\"\"\n",
    "[openlineage]\n",
    "transport = {\"type\": \"http\", \"url\": \"http://marquez:5000\"}\n",
    "namespace = data-platform\n",
    "\"\"\"\n",
    "\n",
    "# DAG automatically emits lineage\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_sales(**context):\n",
    "    \"\"\"Extract sales from S3\"\"\"\n",
    "    # OpenLineage auto-captures:\n",
    "    # - Input: s3://data-lake/raw/sales.csv\n",
    "    # - Output: XCom (internal)\n",
    "    pass\n",
    "\n",
    "def transform_sales(**context):\n",
    "    \"\"\"Transform sales data\"\"\"\n",
    "    # OpenLineage auto-captures:\n",
    "    # - Input: XCom from extract_sales\n",
    "    # - Output: XCom to load_sales\n",
    "    pass\n",
    "\n",
    "with DAG('sales_etl', start_date=datetime(2025, 1, 1)) as dag:\n",
    "    \n",
    "    # Lineage captured automatically\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        python_callable=extract_sales\n",
    "    )\n",
    "    \n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        python_callable=transform_sales\n",
    "    )\n",
    "    \n",
    "    # SQL lineage captured from query\n",
    "    load = PostgresOperator(\n",
    "        task_id='load',\n",
    "        postgres_conn_id='warehouse',\n",
    "        sql=\"\"\"\n",
    "            INSERT INTO analytics.sales_fact\n",
    "            SELECT order_id, amount, customer_id\n",
    "            FROM staging.sales_raw\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    extract >> transform >> load\n",
    "\n",
    "# View lineage in Marquez UI:\n",
    "# http://marquez:3000/lineage/data-platform/sales_etl\n",
    "```\n",
    "\n",
    "**Spark + OpenLineage**\n",
    "\n",
    "```python\n",
    "# Configure Spark with OpenLineage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sales_etl\")\n",
    "    .config(\"spark.extraListeners\", \"io.openlineage.spark.agent.OpenLineageSparkListener\")\n",
    "    .config(\"spark.openlineage.transport.type\", \"http\")\n",
    "    .config(\"spark.openlineage.transport.url\", \"http://marquez:5000\")\n",
    "    .config(\"spark.openlineage.namespace\", \"data-platform\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Read (input lineage captured)\n",
    "df = spark.read.parquet(\"s3://data-lake/raw/sales/\")\n",
    "\n",
    "# Transform (column-level lineage)\n",
    "df_transformed = (\n",
    "    df\n",
    "    .filter(df['amount'] > 0)\n",
    "    .withColumn('total', df['amount'] * df['quantity'])\n",
    "    .select('order_id', 'total', 'customer_id')\n",
    ")\n",
    "\n",
    "# Write (output lineage captured)\n",
    "df_transformed.write.mode(\"overwrite\").parquet(\"s3://data-lake/curated/sales/\")\n",
    "\n",
    "# OpenLineage automatically emits:\n",
    "# - Input: s3://data-lake/raw/sales/\n",
    "# - Output: s3://data-lake/curated/sales/\n",
    "# - Column lineage: total = amount * quantity\n",
    "```\n",
    "\n",
    "**DataHub API Usage**\n",
    "\n",
    "```python\n",
    "from datahub.emitter.mce_builder import make_dataset_urn, make_data_platform_urn\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.metadata.schema_classes import (\n",
    "    DatasetPropertiesClass,\n",
    "    UpstreamLineageClass,\n",
    "    UpstreamClass\n",
    ")\n",
    "\n",
    "# Initialize emitter\n",
    "emitter = DatahubRestEmitter(\"http://datahub:8080\")\n",
    "\n",
    "# 1. REGISTER DATASET\n",
    "dataset_urn = make_dataset_urn(\n",
    "    platform=\"postgres\",\n",
    "    name=\"analytics.sales_fact\",\n",
    "    env=\"PROD\"\n",
    ")\n",
    "\n",
    "dataset_properties = DatasetPropertiesClass(\n",
    "    description=\"Daily sales fact table\",\n",
    "    customProperties={\n",
    "        \"owner\": \"data-team@company.com\",\n",
    "        \"sla\": \"daily by 9am\",\n",
    "        \"retention\": \"7 years\"\n",
    "    }\n",
    ")\n",
    "\n",
    "emitter.emit_mcp(\n",
    "    entity_urn=dataset_urn,\n",
    "    aspect=dataset_properties\n",
    ")\n",
    "\n",
    "# 2. EMIT LINEAGE\n",
    "upstream_sales = make_dataset_urn(\"s3\", \"data-lake/raw/sales\", \"PROD\")\n",
    "upstream_customers = make_dataset_urn(\"postgres\", \"crm.customers\", \"PROD\")\n",
    "\n",
    "lineage = UpstreamLineageClass(\n",
    "    upstreams=[\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_sales,\n",
    "            type=\"TRANSFORMED\"\n",
    "        ),\n",
    "        UpstreamClass(\n",
    "            dataset=upstream_customers,\n",
    "            type=\"TRANSFORMED\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "emitter.emit_mcp(\n",
    "    entity_urn=dataset_urn,\n",
    "    aspect=lineage\n",
    ")\n",
    "\n",
    "# 3. QUERY LINEAGE (REST API)\n",
    "import requests\n",
    "\n",
    "def get_downstream_tables(table_urn: str):\n",
    "    \"\"\"Find all tables that depend on this table\"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    {\n",
    "      dataset(urn: \"%s\") {\n",
    "        downstreams(input: {query: \"*\", start: 0, count: 100}) {\n",
    "          total\n",
    "          relationships {\n",
    "            entity {\n",
    "              urn\n",
    "              properties {\n",
    "                name\n",
    "                description\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % table_urn\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://datahub:8080/api/graphql\",\n",
    "        json={\"query\": query}\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Find impact of changing sales_raw\n",
    "downstreams = get_downstream_tables(\"urn:li:dataset:(urn:li:dataPlatform:s3,data-lake/raw/sales,PROD)\")\n",
    "\n",
    "print(f\"This table impacts {len(downstreams['data']['dataset']['downstreams']['relationships'])} downstream tables\")\n",
    "```\n",
    "\n",
    "**Column-Level Lineage**\n",
    "\n",
    "```python\n",
    "from datahub.metadata.schema_classes import FineGrainedLineageClass\n",
    "\n",
    "# Define column-level transformations\n",
    "column_lineage = FineGrainedLineageClass(\n",
    "    upstreamType=\"FIELD_SET\",\n",
    "    upstreams=[\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:s3,raw.sales,PROD),amount)\",\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:s3,raw.sales,PROD),quantity)\"\n",
    "    ],\n",
    "    downstreamType=\"FIELD\",\n",
    "    downstreams=[\n",
    "        \"urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,analytics.sales_fact,PROD),total)\"\n",
    "    ],\n",
    "    transformOperation=\"MULTIPLY\"\n",
    ")\n",
    "\n",
    "# Query: \"Where does column 'total' come from?\"\n",
    "# Answer: total = raw.sales.amount * raw.sales.quantity\n",
    "```\n",
    "\n",
    "**Lineage for Compliance (GDPR)**\n",
    "\n",
    "```python\n",
    "def find_pii_usage(column_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Find all downstream uses of PII column\n",
    "    (for GDPR right to erasure)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start from source\n",
    "    source_urn = make_dataset_urn(\"postgres\", table_name, \"PROD\")\n",
    "    \n",
    "    # BFS traversal of lineage graph\n",
    "    visited = set()\n",
    "    queue = [source_urn]\n",
    "    pii_locations = []\n",
    "    \n",
    "    while queue:\n",
    "        current_urn = queue.pop(0)\n",
    "        if current_urn in visited:\n",
    "            continue\n",
    "        \n",
    "        visited.add(current_urn)\n",
    "        \n",
    "        # Get downstream tables\n",
    "        downstreams = get_downstream_tables(current_urn)\n",
    "        \n",
    "        for downstream in downstreams:\n",
    "            table = downstream['entity']['urn']\n",
    "            \n",
    "            # Check if PII column exists\n",
    "            schema = get_table_schema(table)\n",
    "            if any(col['name'] == column_name for col in schema['fields']):\n",
    "                pii_locations.append({\n",
    "                    'table': table,\n",
    "                    'column': column_name,\n",
    "                    'lineage_path': get_path_from_source(source_urn, table)\n",
    "                })\n",
    "            \n",
    "            queue.append(table)\n",
    "    \n",
    "    return pii_locations\n",
    "\n",
    "# Example: Find all uses of customer email\n",
    "pii_usage = find_pii_usage('email', 'customers.users')\n",
    "\n",
    "\"\"\"\n",
    "Result:\n",
    "[\n",
    "  {\n",
    "    'table': 'analytics.user_events',\n",
    "    'column': 'email',\n",
    "    'lineage_path': ['customers.users', 'staging.users', 'analytics.user_events']\n",
    "  },\n",
    "  {\n",
    "    'table': 'ml.customer_features',\n",
    "    'column': 'email',\n",
    "    'lineage_path': ['customers.users', 'ml.customer_features']\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Now can delete email from all locations for GDPR compliance\n",
    "```\n",
    "\n",
    "**Impact Analysis**\n",
    "\n",
    "```python\n",
    "def analyze_breaking_change(table_name: str, column_to_remove: str):\n",
    "    \"\"\"\n",
    "    Analyze impact of removing a column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all downstream consumers\n",
    "    downstreams = get_downstream_tables(table_name)\n",
    "    \n",
    "    impact_report = {\n",
    "        'affected_pipelines': [],\n",
    "        'affected_dashboards': [],\n",
    "        'affected_ml_models': []\n",
    "    }\n",
    "    \n",
    "    for downstream in downstreams:\n",
    "        # Check if column is used\n",
    "        lineage = get_column_lineage(downstream['urn'])\n",
    "        \n",
    "        if column_to_remove in lineage['upstream_columns']:\n",
    "            if 'pipeline' in downstream['type']:\n",
    "                impact_report['affected_pipelines'].append(downstream)\n",
    "            elif 'dashboard' in downstream['type']:\n",
    "                impact_report['affected_dashboards'].append(downstream)\n",
    "            elif 'ml_model' in downstream['type']:\n",
    "                impact_report['affected_ml_models'].append(downstream)\n",
    "    \n",
    "    return impact_report\n",
    "\n",
    "# Before dropping column\n",
    "impact = analyze_breaking_change('sales_fact', 'legacy_id')\n",
    "\n",
    "print(f\"\u26a0\ufe0f Removing 'legacy_id' will break:\")\n",
    "print(f\"  - {len(impact['affected_pipelines'])} pipelines\")\n",
    "print(f\"  - {len(impact['affected_dashboards'])} dashboards\")\n",
    "print(f\"  - {len(impact['affected_ml_models'])} ML models\")\n",
    "```\n",
    "\n",
    "**Caso Real: LinkedIn DataHub**\n",
    "\n",
    "LinkedIn usa DataHub para rastrear:\n",
    "- **100,000+ datasets** (Hadoop, Kafka, databases)\n",
    "- **10,000+ pipelines** (Airflow, Azkaban)\n",
    "- **5,000+ dashboards** (Superset, Tableau)\n",
    "- **Column-level lineage**: 1M+ column relationships\n",
    "- **Impact**: Reduce data incidents de 50/mes \u2192 5/mes\n",
    "- **Compliance**: GDPR data discovery en minutos vs semanas\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f4fa7",
   "metadata": {},
   "source": [
    "## 1. Los tres pilares de observabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cacde",
   "metadata": {},
   "source": [
    "- **Logs**: eventos estructurados (JSON) con contexto (trace_id, user, pipeline).\n",
    "- **Metrics**: contadores, gauges, histogramas (latencia, throughput, errores).\n",
    "- **Traces**: flujo de ejecuci\u00f3n distribuido (OpenTelemetry)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3e707",
   "metadata": {},
   "source": [
    "## 2. Logs estructurados con loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys, json\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, serialize=True)  # JSON output\n",
    "\n",
    "logger.info('pipeline_started', pipeline='ventas_etl', version='v1.2.3')\n",
    "logger.error('validation_failed', pipeline='ventas_etl', error_count=5, sample_ids=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5547e2",
   "metadata": {},
   "source": [
    "## 3. M\u00e9tricas con Prometheus (cliente Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prometheus_client import Counter, Histogram, start_http_server\n",
    "# import time, random\n",
    "# \n",
    "# EVENTS_PROCESSED = Counter('events_processed_total', 'Total de eventos procesados', ['pipeline'])\n",
    "# LATENCY = Histogram('processing_latency_seconds', 'Latencia de procesamiento', ['pipeline'])\n",
    "# \n",
    "# start_http_server(8000)  # exponer m\u00e9tricas en :8000/metrics\n",
    "# \n",
    "# while True:\n",
    "#     with LATENCY.labels(pipeline='ventas_etl').time():\n",
    "#         time.sleep(random.uniform(0.01, 0.1))\n",
    "#         EVENTS_PROCESSED.labels(pipeline='ventas_etl').inc()\n",
    "print('Ejemplo comentado; descomenta si tienes prometheus_client instalado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04949d8",
   "metadata": {},
   "source": [
    "## 4. Linaje de datos con OpenLineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02ec08",
   "metadata": {},
   "source": [
    "- Est\u00e1ndar abierto para rastrear de d\u00f3nde vienen los datos y a d\u00f3nde van.\n",
    "- Integraci\u00f3n con Airflow, Spark, dbt, Flink.\n",
    "- Backend: Marquez, DataHub, Egeria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e212bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openlineage_example = r'''\n",
    "# Integraci\u00f3n con Airflow (plugin OpenLineage)\n",
    "# airflow.cfg\n",
    "[openlineage]\n",
    "transport = {\"type\": \"http\", \"url\": \"http://marquez:5000\"}\n",
    "\n",
    "# En cada DAG Run, Airflow emite eventos de linaje:\n",
    "{\n",
    "  \"eventType\": \"START\",\n",
    "  \"job\": {\"namespace\": \"data-platform\", \"name\": \"ventas_etl\"},\n",
    "  \"inputs\": [{\"namespace\": \"s3\", \"name\": \"raw/ventas.csv\"}],\n",
    "  \"outputs\": [{\"namespace\": \"s3\", \"name\": \"curated/ventas.parquet\"}]\n",
    "}\n",
    "'''\n",
    "print(openlineage_example.splitlines()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6ab61",
   "metadata": {},
   "source": [
    "## 5. SLOs (Service Level Objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454620e",
   "metadata": {},
   "source": [
    "- Definir objetivos medibles: ej. \"99.9% de los pipelines completan < 30 min\".\n",
    "- Alertar si SLO en riesgo (error budget casi agotado).\n",
    "- Revisar SLOs mensualmente y ajustar seg\u00fan necesidades del negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ead7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "slo_example = pd.DataFrame([\n",
    "  {'Pipeline':'ventas_etl', 'SLO':'Latencia p99 < 30min', 'Actual':'28min', 'Status':'\u2705'},\n",
    "  {'Pipeline':'streaming_kafka', 'SLO':'Errores < 0.1%', 'Actual':'0.05%', 'Status':'\u2705'},\n",
    "  {'Pipeline':'ml_training', 'SLO':'Disponibilidad > 99.5%', 'Actual':'99.2%', 'Status':'\u26a0\ufe0f'},\n",
    "])\n",
    "slo_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622a0b8",
   "metadata": {},
   "source": [
    "## 6. Dashboards y alertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fc8d4",
   "metadata": {},
   "source": [
    "- Grafana para visualizar m\u00e9tricas de Prometheus.\n",
    "- Alertmanager para notificar en Slack/PagerDuty.\n",
    "- DataHub/Marquez UI para explorar linaje interactivo.\n",
    "- Logs centralizados en ELK/Loki para troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa \u2192](09_proyecto_integrador_1.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb)\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
