{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb7bbce",
   "metadata": {},
   "source": [
    "# \ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh\n",
    "\n",
    "Objetivo: comprender y contrastar arquitecturas de referencia (Lambda, Kappa, Delta) y patrones organizacionales (Data Mesh), con ejemplos de dise\u00f1o y trade-offs.\n",
    "\n",
    "- Duraci\u00f3n: 120\u2013150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Mid completo, experiencia con batch y streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2042cc",
   "metadata": {},
   "source": [
    "## 1. Arquitectura Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a450ff1",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Lambda Architecture: Batch + Speed Layer**\n",
    "\n",
    "**Origen y Motivaci\u00f3n (Nathan Marz, 2011):**\n",
    "\n",
    "```\n",
    "Problema cl\u00e1sico:\n",
    "- Batch processing: Preciso pero lento (horas/d\u00edas)\n",
    "- Stream processing: R\u00e1pido pero complejo (estado, fallos)\n",
    "\n",
    "Soluci\u00f3n Lambda:\n",
    "- Batch Layer: Verdad absoluta, inmutable, reprocessable\n",
    "- Speed Layer: Aproximaci\u00f3n r\u00e1pida, eventualmente consistente\n",
    "- Serving Layer: Merge de ambas vistas\n",
    "```\n",
    "\n",
    "**Arquitectura Completa:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     DATA SOURCES                             \u2502\n",
    "\u2502  \u2022 Kafka Topics                                              \u2502\n",
    "\u2502  \u2022 Database CDC (Debezium)                                   \u2502\n",
    "\u2502  \u2022 Application Logs                                          \u2502\n",
    "\u2502  \u2022 IoT Streams                                               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                  \u2502\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                    \u2502\n",
    "        \u25bc                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 BATCH LAYER  \u2502    \u2502 SPEED LAYER  \u2502\n",
    "\u2502              \u2502    \u2502              \u2502\n",
    "\u2502 \u2022 Spark Batch\u2502    \u2502 \u2022 Flink      \u2502\n",
    "\u2502 \u2022 Hadoop MR  \u2502    \u2502 \u2022 Spark SS   \u2502\n",
    "\u2502 \u2022 Hive       \u2502    \u2502 \u2022 Storm      \u2502\n",
    "\u2502              \u2502    \u2502              \u2502\n",
    "\u2502 Runs: Daily  \u2502    \u2502 Runs: Real-  \u2502\n",
    "\u2502       @2AM   \u2502    \u2502       time   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502                   \u2502\n",
    "       \u2502 Master Dataset    \u2502 Delta Views\n",
    "       \u2502 (Immutable)       \u2502 (Mutable)\n",
    "       \u2502                   \u2502\n",
    "       \u25bc                   \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502      SERVING LAYER               \u2502\n",
    "\u2502                                  \u2502\n",
    "\u2502  Query(t) = Batch(0\u2192t-1h) +     \u2502\n",
    "\u2502             Speed(t-1h\u2192t)        \u2502\n",
    "\u2502                                  \u2502\n",
    "\u2502  \u2022 Druid                         \u2502\n",
    "\u2502  \u2022 Cassandra                     \u2502\n",
    "\u2502  \u2022 ElasticSearch                 \u2502\n",
    "\u2502  \u2022 BigQuery                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "              \u25bc\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502   API    \u2502\n",
    "        \u2502Dashboard \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Componentes Detallados:**\n",
    "\n",
    "**1. Batch Layer (Verdad Inmutable):**\n",
    "\n",
    "```python\n",
    "# Caracter\u00edsticas:\n",
    "# - Procesamiento completo del hist\u00f3rico\n",
    "# - Recalculable desde el inicio\n",
    "# - Optimizado para throughput (no latencia)\n",
    "# - Tolerancia a fallos simple (restart)\n",
    "\n",
    "# Ejemplo: Agregaciones diarias con Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchLayer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer ALL hist\u00f3rico (full recompute)\n",
    "raw_events = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"s3://datalake/raw/events/\")\n",
    "\n",
    "# Agregaciones pesadas\n",
    "user_stats_batch = raw_events \\\n",
    "    .groupBy(\"user_id\", \"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer\n",
    "user_stats_batch.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"s3://datalake/serving/user_stats_batch\")\n",
    "\n",
    "# Schedule: Daily @2AM (Airflow DAG)\n",
    "# Duration: 2-4 horas para procesar 1 a\u00f1o de datos\n",
    "# Cost: $$$ (large cluster, pero solo 1x/d\u00eda)\n",
    "```\n",
    "\n",
    "**2. Speed Layer (Low Latency Incremental):**\n",
    "\n",
    "```python\n",
    "# Caracter\u00edsticas:\n",
    "# - Solo datos recientes (\u00faltimas horas)\n",
    "# - Baja latencia (<1 min)\n",
    "# - Estado mutable (agregaciones incrementales)\n",
    "# - Complejidad de exactly-once\n",
    "\n",
    "# Ejemplo: Streaming con Spark Structured Streaming\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Leer solo NUEVOS eventos\n",
    "events_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Misma l\u00f3gica que batch (pero incremental)\n",
    "user_stats_speed = events_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer (diferentes tablas)\n",
    "query = user_stats_speed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/speed\") \\\n",
    "    .start(\"s3://datalake/serving/user_stats_speed\")\n",
    "\n",
    "# Runs: Continuously\n",
    "# Latency: 30s - 2 min\n",
    "# Cost: $$ (small cluster, pero 24/7)\n",
    "```\n",
    "\n",
    "**3. Serving Layer (Query Merge):**\n",
    "\n",
    "```python\n",
    "# Merge batch + speed en query time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_user_stats(user_id: str, as_of: datetime = None):\n",
    "    \"\"\"\n",
    "    Query que merge batch + speed layers\n",
    "    \"\"\"\n",
    "    if as_of is None:\n",
    "        as_of = datetime.now()\n",
    "    \n",
    "    # Batch boundary (t\u00edpicamente hace 1-2 horas)\n",
    "    batch_cutoff = as_of - timedelta(hours=2)\n",
    "    \n",
    "    # Query batch layer (hist\u00f3rico hasta cutoff)\n",
    "    batch_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_batch\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND date < '{batch_cutoff.date()}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Query speed layer (reciente desde cutoff)\n",
    "    speed_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_speed\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND window.start >= '{batch_cutoff}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Merge results\n",
    "    total_events = batch_stats[0]['events'] + speed_stats[0]['events']\n",
    "    total_revenue = batch_stats[0]['revenue'] + speed_stats[0]['revenue']\n",
    "    \n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'total_events': total_events,\n",
    "        'total_revenue': total_revenue,\n",
    "        'as_of': as_of,\n",
    "        'batch_cutoff': batch_cutoff\n",
    "    }\n",
    "```\n",
    "\n",
    "**Ventajas de Lambda:**\n",
    "\n",
    "```python\n",
    "ventajas = {\n",
    "    \"1. Robustez\": \"\"\"\n",
    "        Batch layer es inmutable \u2192 f\u00e1cil debugging\n",
    "        Speed layer falla \u2192 a\u00fan tienes batch como fallback\n",
    "        Disaster recovery: recompute desde raw data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Precisi\u00f3n Garantizada\": \"\"\"\n",
    "        Batch garantiza exactitud (full recompute)\n",
    "        Speed solo para latencia, batch corrige errores\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Separation of Concerns\": \"\"\"\n",
    "        Batch: optimiza throughput (large partitions, columnar)\n",
    "        Speed: optimiza latencia (small batches, in-memory)\n",
    "        Cada uno usa tecnolog\u00eda ideal para su caso\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Auditor\u00eda\": \"\"\"\n",
    "        Raw data inmutable \u2192 compliance (GDPR, SOX)\n",
    "        Puedes recompute hist\u00f3rico para auditor\u00edas\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas (\u00bfPor qu\u00e9 Lambda est\u00e1 en declive?):**\n",
    "\n",
    "```python\n",
    "desventajas = {\n",
    "    \"1. C\u00f3digo Duplicado\": \"\"\"\n",
    "        MISMA l\u00f3gica implementada 2 veces:\n",
    "        - batch_aggregations.py (Spark Batch)\n",
    "        - speed_aggregations.py (Spark Streaming)\n",
    "        \n",
    "        Cambio de l\u00f3gica \u2192 actualizar AMBOS\n",
    "        Testing \u2192 doble esfuerzo\n",
    "        Bugs \u2192 pueden divergir silenciosamente\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Complejidad Operacional\": \"\"\"\n",
    "        Mantener 2 pipelines:\n",
    "        - Batch: Airflow DAG, cluster management, retry logic\n",
    "        - Speed: Streaming query monitoring, checkpoint management\n",
    "        \n",
    "        2x infraestructura, 2x alertas, 2x oncall\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Eventual Consistency\": \"\"\"\n",
    "        Periodo de gracia donde batch/speed no alineados\n",
    "        \n",
    "        Ejemplo:\n",
    "        10:00 AM: Evento llega\n",
    "        10:01 AM: Speed layer procesa \u2192 visible en dashboard\n",
    "        02:00 AM: Batch recompute \u2192 corrige peque\u00f1os errores\n",
    "        \n",
    "        Usuario puede ver n\u00fameros ligeramente diferentes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Costos\": \"\"\"\n",
    "        Batch cluster: Large (100 nodes) @ 2-4h/d\u00eda\n",
    "        Speed cluster: Medium (20 nodes) @ 24/7\n",
    "        \n",
    "        Total: ~$50K/mes para org mediana\n",
    "        \n",
    "        vs Lakehouse unificado: ~$30K/mes\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales (Cu\u00e1ndo Lambda Tiene Sentido):**\n",
    "\n",
    "```python\n",
    "# 1. LEGACY SYSTEMS con batch existente\n",
    "\"\"\"\n",
    "Empresa: Retail con 20 a\u00f1os de Hadoop\n",
    "Situaci\u00f3n: 500 Hive tables, cientos de Spark jobs\n",
    "Necesidad: Agregar real-time sin reescribir todo\n",
    "\n",
    "Soluci\u00f3n: Lambda\n",
    "- Mantener batch layer existente (no tocas legacy)\n",
    "- Agregar speed layer con Flink para casos cr\u00edticos\n",
    "- Migraci\u00f3n gradual dominio por dominio\n",
    "\"\"\"\n",
    "\n",
    "# 2. ALTA PRECISI\u00d3N requerida\n",
    "\"\"\"\n",
    "Empresa: Financial trading\n",
    "Situaci\u00f3n: Regulaciones requieren recompute exacto\n",
    "Necesidad: Auditor\u00edas pueden pedir recalcular 5 a\u00f1os atr\u00e1s\n",
    "\n",
    "Soluci\u00f3n: Lambda\n",
    "- Batch layer garantiza reproducibilidad exacta\n",
    "- Speed layer para trading real-time\n",
    "- Eventual consistency aceptable (batch corrige)\n",
    "\"\"\"\n",
    "\n",
    "# 3. MUY DIFERENTES SLAs batch vs stream\n",
    "\"\"\"\n",
    "Empresa: IoT con billones de sensores\n",
    "Situaci\u00f3n: \n",
    "  - Alertas cr\u00edticas: <100ms (anomal\u00edas)\n",
    "  - An\u00e1lisis hist\u00f3rico: d\u00edas OK (tendencias)\n",
    "\n",
    "Soluci\u00f3n: Lambda\n",
    "- Speed layer: Flink para alertas ultra-r\u00e1pidas\n",
    "- Batch layer: Hadoop/Hive para an\u00e1lisis pesados\n",
    "- Tecnolog\u00edas muy diferentes, dif\u00edcil unificar\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migraci\u00f3n de Lambda \u2192 Modern Stack:**\n",
    "\n",
    "```python\n",
    "# Estrategia: Unified Batch+Stream con Delta Lake\n",
    "\n",
    "# ANTES (Lambda):\n",
    "# 1. Batch: Spark job diario (4h, 100 nodes)\n",
    "# 2. Speed: Flink continuous (24/7, 20 nodes)\n",
    "# 3. Serving: Druid (merge queries)\n",
    "\n",
    "# DESPU\u00c9S (Delta/Lakehouse):\n",
    "# 1. Unified: Spark Structured Streaming (24/7, 30 nodes)\n",
    "# 2. Micro-batches cada 5 min\n",
    "# 3. Delta Lake: ACID transactions, no merge needed\n",
    "\n",
    "# Migration timeline:\n",
    "\"\"\"\n",
    "Month 1-2: Setup Delta Lake infrastructure\n",
    "Month 3-4: Migrate batch pipelines to Delta\n",
    "Month 5-6: Migrate speed pipelines to Structured Streaming\n",
    "Month 7: Dual-run (Lambda + Delta) for validation\n",
    "Month 8: Cutover to Delta, decommission Lambda\n",
    "Month 9: Cleanup, optimize\n",
    "\n",
    "Total: 9 months\n",
    "Cost: $200K engineering + $50K infra\n",
    "Savings: $20K/mes ongoing (ROI: 2.5 years)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: LinkedIn (Inventor de Lambda):**\n",
    "\n",
    "```python\n",
    "# LinkedIn invent\u00f3 Lambda en 2011\n",
    "# Usaron Lambda 2011-2018 (7 a\u00f1os)\n",
    "\n",
    "arquitectura_linkedin = {\n",
    "    \"Batch Layer\": {\n",
    "        \"Tecnolog\u00eda\": \"Hadoop MapReduce \u2192 Spark\",\n",
    "        \"Datos\": \"Kafka topics replicados a HDFS\",\n",
    "        \"Frecuencia\": \"Daily @midnight\",\n",
    "        \"Output\": \"Hive tables (member profiles, connections, jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Speed Layer\": {\n",
    "        \"Tecnolog\u00eda\": \"Storm \u2192 Samza \u2192 Kafka Streams\",\n",
    "        \"Datos\": \"Kafka topics directamente\",\n",
    "        \"Latencia\": \"<1 second\",\n",
    "        \"Output\": \"Espresso (NoSQL) para low-latency reads\"\n",
    "    },\n",
    "    \n",
    "    \"Serving Layer\": {\n",
    "        \"Tecnolog\u00eda\": \"Espresso + Voldemort (key-value stores)\",\n",
    "        \"Pattern\": \"API merge batch + speed en read time\"\n",
    "    },\n",
    "    \n",
    "    \"2018 Migration\": \"\"\"\n",
    "        LinkedIn migr\u00f3 a Unified Streaming (Samza + Kafka)\n",
    "        Raz\u00f3n: Complejidad de mantener doble l\u00f3gica\n",
    "        Resultado: -30% c\u00f3digo, -40% operaciones\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd386004",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Kappa Architecture: Stream-Only Simplification**\n",
    "\n",
    "**Origen (Jay Kreps, LinkedIn/Confluent, 2014):**\n",
    "\n",
    "```\n",
    "Cr\u00edtica a Lambda:\n",
    "\"\u00bfPor qu\u00e9 mantener 2 sistemas si streaming puede hacer todo?\"\n",
    "\n",
    "Propuesta Kappa:\n",
    "- Eliminar batch layer completamente\n",
    "- Todo es streaming (batch = replay del log)\n",
    "- Inmutable log como single source of truth\n",
    "```\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         DATA SOURCES                     \u2502\n",
    "\u2502  \u2022 Applications                          \u2502\n",
    "\u2502  \u2022 Databases (CDC)                       \u2502\n",
    "\u2502  \u2022 IoT Devices                           \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "              \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    KAFKA (Immutable Log)                \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  \u2022 Infinite retention (or very long)    \u2502\n",
    "\u2502  \u2022 Partitioned & Replicated             \u2502\n",
    "\u2502  \u2022 Serves as \"Master Dataset\"           \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  Topics:                                 \u2502\n",
    "\u2502  \u251c\u2500 events (retention: 1 year)          \u2502\n",
    "\u2502  \u251c\u2500 transactions (retention: 5 years)   \u2502\n",
    "\u2502  \u2514\u2500 user_actions (retention: 90 days)   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "              \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   STREAM PROCESSING                     \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  Version 1: Flink Job                   \u2502\n",
    "\u2502  \u251c\u2500 Aggregations                        \u2502\n",
    "\u2502  \u251c\u2500 Joins                               \u2502\n",
    "\u2502  \u2514\u2500 Output \u2192 Cassandra                  \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  Version 2: New logic needed            \u2502\n",
    "\u2502  \u251c\u2500 Deploy new Flink job                \u2502\n",
    "\u2502  \u251c\u2500 Replay from offset 0                \u2502\n",
    "\u2502  \u2514\u2500 Output \u2192 Cassandra v2               \u2502\n",
    "\u2502                                          \u2502\n",
    "\u2502  \ud83d\udd04 Reprocessing = Replay log           \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "              \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502       SERVING LAYER                     \u2502\n",
    "\u2502  \u2022 Cassandra                            \u2502\n",
    "\u2502  \u2022 ElasticSearch                        \u2502\n",
    "\u2502  \u2022 Redis                                \u2502\n",
    "\u2502  \u2022 PostgreSQL                           \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Principios Clave:**\n",
    "\n",
    "```python\n",
    "principios_kappa = {\n",
    "    \"1. Everything is a Stream\": \"\"\"\n",
    "        No distinci\u00f3n entre batch y stream\n",
    "        \n",
    "        Batch tradicional:\n",
    "        SELECT SUM(revenue) FROM sales WHERE date = '2025-10-30'\n",
    "        \n",
    "        Kappa:\n",
    "        Consume Kafka topic 'sales', aggregate, done\n",
    "        (No diferencia conceptual con real-time)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Immutable Log as Source of Truth\": \"\"\"\n",
    "        Kafka = Database of record\n",
    "        \n",
    "        Ventajas:\n",
    "        - Time travel: replay desde cualquier offset\n",
    "        - Debugging: reproduce bug con datos exactos\n",
    "        - Migration: deploy nueva versi\u00f3n, replay, compare\n",
    "        \n",
    "        Ejemplo:\n",
    "        offset 0 \u2192 offset 1M (1 a\u00f1o de datos)\n",
    "        Reprocessing: ~4 horas con 50 partitions\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution in Log\": \"\"\"\n",
    "        Log contiene todos los schemas hist\u00f3ricos\n",
    "        \n",
    "        V1: {\"user_id\": 123, \"action\": \"click\"}\n",
    "        V2: {\"user_id\": 123, \"action\": \"click\", \"device\": \"mobile\"}\n",
    "        V3: {\"user_id\": 123, \"event_type\": \"click\", \"metadata\": {...}}\n",
    "        \n",
    "        Consumer handle all versions gracefully\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Reprocessing for Code Changes\": \"\"\"\n",
    "        Cambio de l\u00f3gica \u2192 no reescribir batch\n",
    "        \n",
    "        Workflow:\n",
    "        1. Deploy new stream processor (version 2)\n",
    "        2. Replay desde offset 0 (parallel con v1)\n",
    "        3. Validate output v2 matches expected\n",
    "        4. Cutover traffic to v2\n",
    "        5. Decommission v1\n",
    "        \n",
    "        Duration: horas/d\u00edas (no semanas de reescribir batch)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Implementaci\u00f3n Real con Kafka + Flink:**\n",
    "\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.table.descriptors import Kafka, Schema, Json\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Configuraci\u00f3n Kafka source con retenci\u00f3n larga\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        product_id STRING,\n",
    "        revenue DOUBLE,\n",
    "        event_timestamp TIMESTAMP(3),\n",
    "        WATERMARK FOR event_timestamp AS event_timestamp - INTERVAL '10' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'properties.group.id' = 'user-stats-processor-v2',\n",
    "        'scan.startup.mode' = 'earliest-offset',  -- Replay desde inicio\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Stream processing (unificado batch+stream)\n",
    "user_stats = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        TUMBLE_START(event_timestamp, INTERVAL '1' DAY) as day,\n",
    "        COUNT(*) as total_events,\n",
    "        SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,\n",
    "        COUNT(DISTINCT product_id) as unique_products\n",
    "    FROM events\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_timestamp, INTERVAL '1' DAY)\n",
    "\"\"\")\n",
    "\n",
    "# Output a Cassandra\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_stats_cassandra (\n",
    "        user_id STRING,\n",
    "        day TIMESTAMP(3),\n",
    "        total_events BIGINT,\n",
    "        total_revenue DOUBLE,\n",
    "        unique_products BIGINT,\n",
    "        PRIMARY KEY (user_id, day) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'cassandra',\n",
    "        'host' = 'cassandra',\n",
    "        'table-name' = 'user_stats'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "user_stats.execute_insert('user_stats_cassandra').wait()\n",
    "\n",
    "# Reprocessing:\n",
    "# 1. Stop old job\n",
    "# 2. Deploy this code (nueva l\u00f3gica)\n",
    "# 3. Kafka consumer group lee desde offset 0\n",
    "# 4. Reprocesa 1 a\u00f1o de datos en ~4 horas\n",
    "# 5. Nueva tabla user_stats_cassandra tiene datos corregidos\n",
    "```\n",
    "\n",
    "**Comparaci\u00f3n Kafka Retention:**\n",
    "\n",
    "```python\n",
    "# Configuraci\u00f3n t\u00edpica para Kappa\n",
    "kafka_retention = {\n",
    "    \"Short Retention (Anti-Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 604800000  # 7 d\u00edas\",\n",
    "        \"Disk\": \"1 TB / partition\",\n",
    "        \"Cost\": \"$500/mes\",\n",
    "        \"Reprocessing\": \"\u274c Solo \u00faltimos 7 d\u00edas disponibles\",\n",
    "        \"Use Case\": \"Lambda architecture (Kafka solo buffer)\"\n",
    "    },\n",
    "    \n",
    "    \"Long Retention (Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 31536000000  # 1 a\u00f1o\",\n",
    "        \"Disk\": \"52 TB / partition (con compaction)\",\n",
    "        \"Cost\": \"$2,600/mes (S3 tiered storage)\",\n",
    "        \"Reprocessing\": \"\u2705 1 a\u00f1o completo disponible\",\n",
    "        \"Use Case\": \"Kappa architecture (Kafka es source of truth)\"\n",
    "    },\n",
    "    \n",
    "    \"Infinite Retention (Extreme Kappa)\": {\n",
    "        \"Config\": \"retention.ms = -1  # Forever\",\n",
    "        \"Disk\": \"Ilimitado (requiere tiered storage)\",\n",
    "        \"Cost\": \"$5K/mes (S3 archival)\",\n",
    "        \"Reprocessing\": \"\u2705 Hist\u00f3rico completo\",\n",
    "        \"Use Case\": \"Event sourcing, auditabilidad estricta\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Kafka Tiered Storage (KIP-405, Kafka 3.0+)\n",
    "# Archiva logs antiguos a S3, mantiene recientes en SSD\n",
    "\"\"\"\n",
    "kafka-storage.properties:\n",
    "  remote.log.storage.enable=true\n",
    "  remote.log.storage.manager.class=org.apache.kafka.server.log.remote.storage.RemoteLogStorageManager\n",
    "  \n",
    "Estructura:\n",
    "  Local SSD: \u00daltimos 7 d\u00edas (hot data)\n",
    "  S3 Bucket: >7 d\u00edas (cold data, comprimido)\n",
    "  \n",
    "Cost reduction: 70% vs all-SSD\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_kappa = {\n",
    "    \"1. Simplicidad Operacional\": \"\"\"\n",
    "        Un solo pipeline \u2192 un solo sistema para monitorear\n",
    "        Un solo lenguaje/framework (Flink o Spark)\n",
    "        Un solo cluster para operar\n",
    "        \n",
    "        On-call rotation:\n",
    "        Lambda: 2 sistemas \u00d7 2 equipos = 4 rotaciones/semana\n",
    "        Kappa: 1 sistema \u00d7 1 equipo = 2 rotaciones/semana\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. No C\u00f3digo Duplicado\": \"\"\"\n",
    "        Misma l\u00f3gica para batch y stream\n",
    "        \n",
    "        Lambda: 2000 l\u00edneas batch.py + 2000 l\u00edneas stream.py = 4000\n",
    "        Kappa: 2000 l\u00edneas unified.py = 2000 (50% menos c\u00f3digo)\n",
    "        \n",
    "        Bug fix: 1 cambio vs 2 cambios\n",
    "        Testing: 1 suite vs 2 suites\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Reprocessing R\u00e1pido\": \"\"\"\n",
    "        Cambio de l\u00f3gica \u2192 replay log en horas\n",
    "        \n",
    "        Lambda:\n",
    "        - Reescribir batch job (d\u00edas de dev)\n",
    "        - Ejecutar en cluster grande (4h)\n",
    "        - Validar, debug, iterar\n",
    "        Total: 1-2 semanas\n",
    "        \n",
    "        Kappa:\n",
    "        - Modificar stream job (horas de dev)\n",
    "        - Replay desde offset 0 (4h)\n",
    "        - Compare old vs new output\n",
    "        Total: 1 d\u00eda\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Exactly-Once M\u00e1s Simple\": \"\"\"\n",
    "        Kafka transactions + idempotent producers\n",
    "        \n",
    "        Flink + Kafka:\n",
    "        - Checkpointing en Kafka offsets\n",
    "        - Transactional writes a sinks\n",
    "        - No necesidad de merge batch+speed\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch tiene at-least-once (reintentos)\n",
    "        - Speed tiene exactly-once (complejo)\n",
    "        - Merge puede duplicar sin deduplication\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "desventajas_kappa = {\n",
    "    \"1. Costo de Almacenamiento\": \"\"\"\n",
    "        Kafka con 1 a\u00f1o de retenci\u00f3n = caro\n",
    "        \n",
    "        Ejemplo: 10K events/s \u00d7 10 KB/event \u00d7 1 a\u00f1o\n",
    "        = 3.15 PB/a\u00f1o crudo\n",
    "        Con compaction + tiered storage: 500 TB\n",
    "        Cost: $5K/mes S3 + $10K/mes Kafka brokers\n",
    "        \n",
    "        vs Hadoop HDFS: $2K/mes (pero sin streaming)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Reprocessing Largo\": \"\"\"\n",
    "        Replay 1 a\u00f1o de datos = horas/d\u00edas\n",
    "        \n",
    "        1 TB data @ 100 MB/s = 2.8 horas ideal\n",
    "        Real (with processing): 6-12 horas\n",
    "        \n",
    "        Durante reprocessing:\n",
    "        - Nuevo c\u00f3digo corre en paralelo (doble costo)\n",
    "        - Output dual (old version + new version)\n",
    "        - Validaci\u00f3n manual necesaria\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch recompute overnight (no impact users)\n",
    "        - Speed layer sigue sirviendo tr\u00e1fico\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. State Management Complejo\": \"\"\"\n",
    "        Streaming state para 1 a\u00f1o de datos = grande\n",
    "        \n",
    "        Ejemplo: User profiles con aggregations\n",
    "        10M users \u00d7 1 KB state = 10 GB state\n",
    "        \n",
    "        Flink RocksDB:\n",
    "        - State en disco (slower)\n",
    "        - Checkpointing largo (minutes)\n",
    "        - Recovery largo si crash (restore state)\n",
    "        \n",
    "        Lambda batch:\n",
    "        - No state (stateless join con full tables)\n",
    "        - Simpler, pero m\u00e1s lento\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. No Optimizaci\u00f3n por Caso\": \"\"\"\n",
    "        Streaming debe servir TODOS los casos\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch: Columnar Parquet, predicate pushdown, Z-order\n",
    "        - Speed: Row-based, in-memory, indexes\n",
    "        \n",
    "        Kappa:\n",
    "        - Un formato compromiso (no \u00f3ptimo para ninguno)\n",
    "        - Queries complejos (joins 5 tables) lentos en stream\n",
    "        - Queries simples (count) overkill con stream\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Ideales para Kappa:**\n",
    "\n",
    "```python\n",
    "# 1. REAL-TIME FIRST con poco hist\u00f3rico\n",
    "\"\"\"\n",
    "Empresa: Fintech fraud detection\n",
    "Datos: 90 d\u00edas de transacciones\n",
    "Procesamiento: <100ms latency requerido\n",
    "Reprocessing: Semanal (ajustar modelos ML)\n",
    "\n",
    "Kappa perfecto:\n",
    "- 90 d\u00edas en Kafka (100 TB, manejable)\n",
    "- Flink streaming para detecci\u00f3n real-time\n",
    "- Replay semanal para ajustar modelos (4h)\n",
    "\"\"\"\n",
    "\n",
    "# 2. EVENT SOURCING puro\n",
    "\"\"\"\n",
    "Empresa: Booking platform\n",
    "Patr\u00f3n: Event sourcing (todo es evento inmutable)\n",
    "Datos: bookings, cancellations, modifications\n",
    "Queries: Reconstruir estado actual desde eventos\n",
    "\n",
    "Kappa natural:\n",
    "- Kafka como event store (immutable log)\n",
    "- Flink materializa vistas (current bookings)\n",
    "- Replay para nuevas vistas (add \"bookings by hotel\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. HIGH CHANGE FREQUENCY en l\u00f3gica\n",
    "\"\"\"\n",
    "Empresa: Ad-tech con modelos A/B testing constante\n",
    "Situaci\u00f3n: Cambio de l\u00f3gica 2-3x/semana\n",
    "Necesidad: Deploy r\u00e1pido, validar, iterar\n",
    "\n",
    "Kappa ventaja:\n",
    "- Modify stream job (1h dev)\n",
    "- Deploy v2 parallel v1 (30 min)\n",
    "- Replay \u00faltimo d\u00eda (30 min)\n",
    "- Compare outputs, cutover (1h)\n",
    "Total: 3 horas por cambio vs 3 d\u00edas Lambda\n",
    "\"\"\"\n",
    "\n",
    "# 4. SMALL-MEDIUM data scale\n",
    "\"\"\"\n",
    "Empresa: SaaS startup\n",
    "Datos: <100 TB total\n",
    "Users: <1M\n",
    "Traffic: <1K events/s\n",
    "\n",
    "Kappa ventaja:\n",
    "- Un cluster Kafka+Flink (simple)\n",
    "- Retenci\u00f3n 1 a\u00f1o f\u00e1cil (10 TB en Kafka)\n",
    "- Equipo peque\u00f1o puede mantener\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Anti-Patterns (Cu\u00e1ndo NO usar Kappa):**\n",
    "\n",
    "```python\n",
    "# \u274c 1. HUGE data scale con queries complejos\n",
    "\"\"\"\n",
    "Empresa: Retail con 10+ a\u00f1os de hist\u00f3rico\n",
    "Datos: 100 PB en HDFS\n",
    "Queries: SQL complejo (20+ joins, window functions)\n",
    "\n",
    "Problema Kappa:\n",
    "- 100 PB en Kafka = $500K/mes (vs $50K HDFS)\n",
    "- Streaming joins lentos vs batch columnar\n",
    "- Replay 10 a\u00f1os = semanas (inviable)\n",
    "\n",
    "Better: Lambda o Delta Lakehouse\n",
    "\"\"\"\n",
    "\n",
    "# \u274c 2. BATCH-FIRST workloads\n",
    "\"\"\"\n",
    "Empresa: Data warehouse con reportes nocturnos\n",
    "Queries: ETL batch 90% workload, streaming 10%\n",
    "Latency: Horas OK para reportes\n",
    "\n",
    "Problema Kappa:\n",
    "- Overhead streaming para batch workloads\n",
    "- Kafka infraestructura innecesaria\n",
    "- Spark batch m\u00e1s eficiente que Spark Streaming para batch\n",
    "\n",
    "Better: Tradicional batch (Airflow + Spark)\n",
    "\"\"\"\n",
    "\n",
    "# \u274c 3. ESTRICTA compliance con reprocessing prohibido\n",
    "\"\"\"\n",
    "Empresa: Banco con regulaciones estrictas\n",
    "Requerimiento: Auditor\u00edas requieren datos exactos \"as-of\"\n",
    "Prohibici\u00f3n: No se puede \"rehacer\" c\u00e1lculos pasados\n",
    "\n",
    "Problema Kappa:\n",
    "- Reprocessing es core feature (pero regulador dice no)\n",
    "- Inmutabilidad requiere batch layer aparte\n",
    "\n",
    "Better: Lambda con batch layer auditado/certificado\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: Uber (Kappa en Producci\u00f3n):**\n",
    "\n",
    "```python\n",
    "uber_kappa = {\n",
    "    \"Caso\": \"Surge pricing (precios din\u00e1micos)\",\n",
    "    \n",
    "    \"Arquitectura\": {\n",
    "        \"Eventos\": \"ride_requests, driver_locations, ride_completions\",\n",
    "        \"Kafka\": \"500K events/s, retenci\u00f3n 7 d\u00edas (suficiente)\",\n",
    "        \"Flink\": \"Windowed aggregations (supply/demand por \u00e1rea)\",\n",
    "        \"Output\": \"Redis (precios actuales), Cassandra (hist\u00f3rico)\",\n",
    "        \"Latency\": \"<200ms end-to-end\"\n",
    "    },\n",
    "    \n",
    "    \"Kappa Benefits\": \"\"\"\n",
    "        1. Cambio frecuente de algoritmo pricing (weekly)\n",
    "        2. Replay 7 d\u00edas para validar nuevo algoritmo (2h)\n",
    "        3. A/B testing: correr 2 versiones paralelo, compare\n",
    "        4. No batch layer needed (solo \u00faltimos d\u00edas relevantes)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Evolution\": \"\"\"\n",
    "        2015: Lambda (Spark batch + Storm streaming)\n",
    "        2016: Migrated to Kappa (Samza streaming only)\n",
    "        2018: Moved to Flink (better stateful processing)\n",
    "        2020: Hybrid (Kappa para real-time, Hudi para analytics)\n",
    "        \n",
    "        Raz\u00f3n hybrid: Analytics largo plazo necesita batch optimizations\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d7c4",
   "metadata": {},
   "source": [
    "### \ud83c\udfe0 **Delta Architecture (Lakehouse): Unified Batch+Stream**\n",
    "\n",
    "**Evoluci\u00f3n Natural (2020+):**\n",
    "\n",
    "```\n",
    "Lambda (2011): Batch + Speed layers separados\n",
    "   \u2193\n",
    "Kappa (2014): Solo streaming (pero caro, complejo)\n",
    "   \u2193\n",
    "Delta (2020): Unified sobre transactional storage\n",
    "```\n",
    "\n",
    "**Arquitectura Lakehouse:**\n",
    "\n",
    "```python\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    DATA SOURCES                              \u2502\n",
    "\u2502  \u2022 Applications \u2192 Kafka                                      \u2502\n",
    "\u2502  \u2022 Databases \u2192 CDC (Debezium)                                \u2502\n",
    "\u2502  \u2022 Batch files \u2192 S3/ADLS                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "                     \u25bc\n",
    "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502  INGESTION LAYER \u2502\n",
    "          \u2502                  \u2502\n",
    "          \u2502  Spark Streaming \u2502 \u2190 Unified engine!\n",
    "          \u2502  (micro-batches) \u2502\n",
    "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                   \u2502\n",
    "                   \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              DELTA LAKE / ICEBERG STORAGE                     \u2502\n",
    "\u2502                                                               \u2502\n",
    "\u2502  Bronze (Raw)           Silver (Cleaned)      Gold (Curated) \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502 Append-only\u2502        \u2502 Deduplicated\u2502      \u2502Aggregations\u2502  \u2502\n",
    "\u2502  \u2502 Parquet +  \u2502   \u2192    \u2502 Validated   \u2502  \u2192   \u2502Star Schema \u2502  \u2502\n",
    "\u2502  \u2502 Delta Log  \u2502        \u2502 Enriched    \u2502      \u2502Business    \u2502  \u2502\n",
    "\u2502  \u2502            \u2502        \u2502             \u2502      \u2502Logic       \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                                                               \u2502\n",
    "\u2502  Features:                                                    \u2502\n",
    "\u2502  \u2705 ACID Transactions                                        \u2502\n",
    "\u2502  \u2705 Time Travel (rollback, audit)                           \u2502\n",
    "\u2502  \u2705 Schema Evolution                                         \u2502\n",
    "\u2502  \u2705 Upserts/Deletes                                          \u2502\n",
    "\u2502  \u2705 Unified Batch+Stream reads                               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502                    \u2502\n",
    "          \u25bc                    \u25bc\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502 BATCH QUERIES \u2502    \u2502  STREAMING   \u2502\n",
    "  \u2502               \u2502    \u2502   QUERIES    \u2502\n",
    "  \u2502 Spark SQL     \u2502    \u2502 Spark SS     \u2502\n",
    "  \u2502 Presto/Trino  \u2502    \u2502 Flink        \u2502\n",
    "  \u2502 Athena        \u2502    \u2502 Real-time    \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502                    \u2502\n",
    "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u25bc\n",
    "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "            \u2502   CONSUMPTION   \u2502\n",
    "            \u2502                 \u2502\n",
    "            \u2502  \u2022 BI Tools     \u2502\n",
    "            \u2502  \u2022 ML Training  \u2502\n",
    "            \u2502  \u2022 APIs         \u2502\n",
    "            \u2502  \u2022 Dashboards   \u2502\n",
    "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Caracter\u00edsticas Clave:**\n",
    "\n",
    "```python\n",
    "delta_features = {\n",
    "    \"1. ACID Transactions en Object Storage\": \"\"\"\n",
    "        Problema tradicional:\n",
    "        S3/ADLS no tienen transacciones\n",
    "        \u2192 Race conditions, partial writes\n",
    "        \n",
    "        Delta Lake solution:\n",
    "        _delta_log/00000000000000000123.json\n",
    "        {\n",
    "          \"commitInfo\": {\"timestamp\": \"2025-10-30T10:30:00\"},\n",
    "          \"add\": [\n",
    "            {\"path\": \"part-00000.parquet\", \"size\": 1024000},\n",
    "            {\"path\": \"part-00001.parquet\", \"size\": 1024000}\n",
    "          ],\n",
    "          \"remove\": []\n",
    "        }\n",
    "        \n",
    "        Atomic commit: Write log entry = commit completo\n",
    "        Readers ven versi\u00f3n consistente (snapshot isolation)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Unified Batch + Stream\": \"\"\"\n",
    "        SAME TABLE para batch y streaming\n",
    "        \n",
    "        Escritura streaming:\n",
    "        df.writeStream\n",
    "          .format(\"delta\")\n",
    "          .outputMode(\"append\")\n",
    "          .start(\"/delta/events\")\n",
    "        \n",
    "        Lectura batch:\n",
    "        spark.read\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")\n",
    "          .where(\"date = '2025-10-30'\")\n",
    "        \n",
    "        Lectura streaming:\n",
    "        spark.readStream\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")  # Lee cambios incrementales\n",
    "        \n",
    "        NO necesitas batch layer separada!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Time Travel\": \"\"\"\n",
    "        Acceso a versiones hist\u00f3ricas\n",
    "        \n",
    "        # Leer versi\u00f3n espec\u00edfica\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"versionAsOf\", 42)\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        # Leer timestamp espec\u00edfico\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"timestampAsOf\", \"2025-10-29 00:00:00\")\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        Casos de uso:\n",
    "        - Rollback: Deployment malo \u2192 revert\n",
    "        - Audit: \"\u00bfQu\u00e9 datos vio el modelo ayer?\"\n",
    "        - Debugging: \"Reproduce bug con datos exactos\"\n",
    "        - Compliance: \"Muestra datos as-of Q3 2025\"\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Schema Evolution\": \"\"\"\n",
    "        Agregar columnas sin downtime\n",
    "        \n",
    "        V1: {user_id, action, timestamp}\n",
    "        V2: {user_id, action, timestamp, device}  \u2190 Add column\n",
    "        \n",
    "        Delta:\n",
    "        df_v2.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .save(\"/delta/events\")\n",
    "        \n",
    "        Lectores antiguos: device = null\n",
    "        Lectores nuevos: device le\u00eddo correctamente\n",
    "        \n",
    "        NO necesitas backfill!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Upserts (MERGE)\": \"\"\"\n",
    "        CDC y SCD Type 2 nativos\n",
    "        \n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            updates.alias(\"source\"),\n",
    "            \"target.user_id = source.user_id\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"email\": \"source.email\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).whenNotMatchedInsert(values={\n",
    "            \"user_id\": \"source.user_id\",\n",
    "            \"email\": \"source.email\",\n",
    "            \"created_at\": \"source.created_at\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).execute()\n",
    "        \n",
    "        Streaming UPSERT:\n",
    "        updates.writeStream\n",
    "          .foreachBatch(lambda df, _: upsert_function(df))\n",
    "          .start()\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Medallion Architecture (Bronze/Silver/Gold):**\n",
    "\n",
    "```python\n",
    "# BRONZE: Raw data, append-only\n",
    "bronze_events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").cast(\"string\").alias(\"raw_json\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "bronze_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(\"/delta/bronze/events\")\n",
    "\n",
    "# SILVER: Cleaned, validated, deduplicated\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "silver_events = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/bronze/events\") \\\n",
    "    .select(\n",
    "        from_json(col(\"raw_json\"), event_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\  # Validaci\u00f3n\n",
    "    .dropDuplicates([\"user_id\", \"event_timestamp\"])  # Dedup\n",
    "\n",
    "silver_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver\") \\\n",
    "    .start(\"/delta/silver/events\")\n",
    "\n",
    "# GOLD: Business aggregations\n",
    "gold_user_stats = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/silver/events\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 day\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"daily_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"event_types\")\n",
    "    )\n",
    "\n",
    "gold_user_stats.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/gold\") \\\n",
    "    .start(\"/delta/gold/user_daily_stats\")\n",
    "\n",
    "# BI Tool lee desde Gold\n",
    "df_dashboard = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/gold/user_daily_stats\") \\\n",
    "    .where(\"window.start >= current_date() - interval 7 days\")\n",
    "```\n",
    "\n",
    "**Ventajas sobre Lambda/Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_delta = {\n",
    "    \"vs Lambda\": {\n",
    "        \"C\u00f3digo Unificado\": \"\"\"\n",
    "            Lambda: 2 pipelines (batch.py + stream.py)\n",
    "            Delta: 1 pipeline (unified.py)\n",
    "            \n",
    "            Reduction: 50% c\u00f3digo, 50% tests, 50% bugs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Sin Merge Layer\": \"\"\"\n",
    "            Lambda: Query = merge(batch, speed) en serving\n",
    "            Delta: Query directo a Delta (single source)\n",
    "            \n",
    "            Latency: -100ms (no merge overhead)\n",
    "            Consistency: Guaranteed (ACID transactions)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Operaciones Simples\": \"\"\"\n",
    "            Lambda: Mantener 2 clusters + serving layer\n",
    "            Delta: 1 cluster Spark + Delta storage\n",
    "            \n",
    "            On-call: -50% alerts, -40% incidents\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Kappa\": {\n",
    "        \"Costo Storage\": \"\"\"\n",
    "            Kappa: Kafka 1 a\u00f1o = $15K/mes\n",
    "            Delta: S3 object storage = $2K/mes\n",
    "            \n",
    "            Savings: 85% storage costs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Batch Optimized\": \"\"\"\n",
    "            Kappa: Streaming para todo (no \u00f3ptimo)\n",
    "            Delta: Columnar Parquet + statistics\n",
    "            \n",
    "            Query performance: 10x faster para analytics\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Reprocessing Flexible\": \"\"\"\n",
    "            Kappa: Replay Kafka (limited retention)\n",
    "            Delta: Time travel infinito\n",
    "            \n",
    "            Auditabilidad: 5+ a\u00f1os hist\u00f3rico disponible\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Traditional Warehouse\": {\n",
    "        \"Cost\": \"\"\"\n",
    "            Warehouse: $25/TB/month (Snowflake)\n",
    "            Delta: $0.023/GB/month (S3) = $23/TB\n",
    "            \n",
    "            Savings: ~50% at petabyte scale\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Flexibility\": \"\"\"\n",
    "            Warehouse: Vendor lock-in, SQL only\n",
    "            Delta: Open format, Spark/Presto/Flink/Python\n",
    "            \n",
    "            ML Integration: Native (same storage)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Scalability\": \"\"\"\n",
    "            Warehouse: Scale up (expensive)\n",
    "            Delta: Scale out (S3 infinite)\n",
    "            \n",
    "            Growth: No limits, pay-as-you-go\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "```python\n",
    "# 1. Databricks - Inventor de Delta Lake\n",
    "\"\"\"\n",
    "Cliente: Comcast (Telecomunicaciones)\n",
    "Datos: 2 PB diarios (network logs, streaming video, IoT)\n",
    "Arquitectura anterior: Lambda (Hadoop batch + Kafka streaming)\n",
    "\n",
    "Migration a Delta:\n",
    "- 2018: Pilot con 10 TB (1 mes)\n",
    "- 2019: Migration 500 TB (6 meses)\n",
    "- 2020: Full adoption 2 PB/d\u00eda\n",
    "\n",
    "Resultados:\n",
    "- Cost: -30% ($2M/a\u00f1o savings)\n",
    "- Latency: 4h \u2192 15 min para dashboards\n",
    "- Code: -50% (unified batch+stream)\n",
    "- Incidents: -60% (ACID transactions)\n",
    "\n",
    "Stack:\n",
    "- Ingestion: Spark Streaming\n",
    "- Storage: Delta Lake en S3\n",
    "- Compute: Databricks clusters\n",
    "- Consumption: Tableau + ML models\n",
    "\"\"\"\n",
    "\n",
    "# 2. Riot Games (League of Legends)\n",
    "\"\"\"\n",
    "Caso: Game analytics + Player behavior\n",
    "Datos: 100M+ games/day, petabytes de events\n",
    "Latency: <5 min para anti-cheat, <1 hour para balance\n",
    "\n",
    "Delta Architecture:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Game Servers \u2502 \u2192 Kafka (1M events/s)\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2193\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Spark Stream \u2502 \u2192 Delta Bronze (raw events)\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2193\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Enrichment   \u2502 \u2192 Delta Silver (validated)\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2193\n",
    "    \u250c\u2500\u2500\u2534\u2500\u2500\u2510\n",
    "    \u2502     \u2502\n",
    "    \u25bc     \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 ML \u2502  \u2502 BI \u2502 \u2192 Delta Gold (aggregations)\n",
    "\u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Benefits:\n",
    "- Unified: Game designers query same data ML uses\n",
    "- Time travel: \"Replay match from 2 days ago\"\n",
    "- Schema evolution: Add features without downtime\n",
    "\"\"\"\n",
    "\n",
    "# 3. Adobe Experience Platform\n",
    "\"\"\"\n",
    "Challenge: 100s of tenants, isolated data, compliance\n",
    "Solution: Delta Lake multi-tenant\n",
    "\n",
    "Architecture:\n",
    "/delta/tenant_001/events/\n",
    "/delta/tenant_002/events/\n",
    "...\n",
    "/delta/tenant_500/events/\n",
    "\n",
    "Features:\n",
    "- Row-level security (GDPR compliance)\n",
    "- Tenant isolation (Delta sharing)\n",
    "- Unified operations (single platform)\n",
    "- Time travel (data recovery per tenant)\n",
    "\n",
    "Scale:\n",
    "- 500+ tenants\n",
    "- 10 PB total data\n",
    "- 100K writes/s\n",
    "- <100ms p99 latency\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migraci\u00f3n Lambda \u2192 Delta:**\n",
    "\n",
    "```python\n",
    "migration_strategy = {\n",
    "    \"Phase 1: Setup (Month 1-2)\": \"\"\"\n",
    "        1. Provision Delta Lake infrastructure\n",
    "           - S3 bucket con versioning\n",
    "           - Unity Catalog setup\n",
    "           - Spark clusters (Databricks/EMR)\n",
    "        \n",
    "        2. Crear medallion structure\n",
    "           /delta/bronze/\n",
    "           /delta/silver/\n",
    "           /delta/gold/\n",
    "        \n",
    "        3. Setup CI/CD pipelines\n",
    "           - GitHub Actions para deploy\n",
    "           - Testing framework (pytest)\n",
    "           - Monitoring (Datadog/Prometheus)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 2: Bronze Layer (Month 3-4)\": \"\"\"\n",
    "        1. Migrate raw ingestion\n",
    "           Kafka \u2192 Delta Bronze (append-only)\n",
    "           \n",
    "        2. Dual-write period\n",
    "           - Write to both Lambda batch + Delta\n",
    "           - Compare row counts, checksums\n",
    "           \n",
    "        3. Backfill hist\u00f3rico\n",
    "           HDFS/Hive \u2192 Delta Bronze\n",
    "           (1-time copy, then delete HDFS)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 3: Silver Layer (Month 5-6)\": \"\"\"\n",
    "        1. Migrate transformation logic\n",
    "           Batch Spark jobs \u2192 Delta pipelines\n",
    "           \n",
    "        2. Unified batch+stream\n",
    "           - Remove duplicate code\n",
    "           - Single pipeline para ambos\n",
    "           \n",
    "        3. Data quality checks\n",
    "           Great Expectations integration\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 4: Gold Layer (Month 7-8)\": \"\"\"\n",
    "        1. Migrate aggregations\n",
    "           Speed layer \u2192 Delta Gold micro-batches\n",
    "           \n",
    "        2. Decommission serving layer\n",
    "           Druid/Cassandra \u2192 query Delta directly\n",
    "           \n",
    "        3. BI tools reconfigure\n",
    "           Point Tableau/Power BI to Delta\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 5: Validation (Month 9)\": \"\"\"\n",
    "        1. Parallel run\n",
    "           - Lambda still running (backup)\n",
    "           - Delta serving production traffic\n",
    "           \n",
    "        2. Compare metrics\n",
    "           - Latency, accuracy, cost\n",
    "           - User feedback (BI teams)\n",
    "        \n",
    "        3. Fix edge cases\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 6: Cutover (Month 10)\": \"\"\"\n",
    "        1. Decommission Lambda\n",
    "           - Stop batch jobs\n",
    "           - Stop speed layer\n",
    "           - Delete Druid/Cassandra\n",
    "        \n",
    "        2. Cleanup\n",
    "           - Delete HDFS clusters\n",
    "           - Reclaim savings\n",
    "        \n",
    "        3. Training\n",
    "           - Team training on Delta\n",
    "           - Documentation update\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total\": \"\"\"\n",
    "        Duration: 10 months\n",
    "        Cost: $300K engineering + $100K infra\n",
    "        Ongoing savings: $50K/mes\n",
    "        ROI: 8 months\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Partition Strategy\": \"\"\"\n",
    "        \u2705 Date partitioning (daily/hourly)\n",
    "        partitionBy(\"date\")\n",
    "        \n",
    "        \u26a0\ufe0f Z-ordering para high-cardinality\n",
    "        OPTIMIZE events ZORDER BY (user_id, product_id)\n",
    "        \n",
    "        \u274c Over-partitioning\n",
    "        partitionBy(\"date\", \"hour\", \"user_id\")  # Millones de particiones!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Compaction\": \"\"\"\n",
    "        Auto-compact para streaming:\n",
    "        .option(\"autoCompact\", \"true\")\n",
    "        \n",
    "        Scheduled OPTIMIZE:\n",
    "        OPTIMIZE events WHERE date >= current_date() - 7\n",
    "        \n",
    "        VACUUM para cleanup:\n",
    "        VACUUM events RETAIN 168 HOURS  # 7 d\u00edas\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution\": \"\"\"\n",
    "        Always mergeSchema para backward compatibility:\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        Schema validation:\n",
    "        df.write.format(\"delta\")\n",
    "          .option(\"enforceSchema\", \"true\")\n",
    "          .save()\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Monitoring\": \"\"\"\n",
    "        Metrics to track:\n",
    "        - Write throughput (records/s)\n",
    "        - Read latency (p50, p95, p99)\n",
    "        - File count (small files problem)\n",
    "        - Version count (retention policy)\n",
    "        - Transaction log size\n",
    "        \n",
    "        Alerts:\n",
    "        - File count >100K per partition\n",
    "        - Transaction log >10 MB\n",
    "        - Write latency p99 >5s\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fb62d",
   "metadata": {},
   "source": [
    "### \ud83c\udf10 **Data Mesh: Paradigma Organizacional Descentralizado**\n",
    "\n",
    "**Problema: Data Platform Centralizado (Monolito)**\n",
    "\n",
    "```\n",
    "Organizaci\u00f3n tradicional:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         CENTRAL DATA TEAM (20 personas)        \u2502\n",
    "\u2502                                                 \u2502\n",
    "\u2502  Responsable de:                                \u2502\n",
    "\u2502  \u2022 Todos los pipelines                         \u2502\n",
    "\u2502  \u2022 Todos los data models                       \u2502\n",
    "\u2502  \u2022 Todos los dashboards                        \u2502\n",
    "\u2502  \u2022 Todos los ML models                         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  DATA WAREHOUSE   \u2502\n",
    "   \u2502   (Single DB)     \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502                 \u2502\n",
    "    \u25bc                 \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Ventas \u2502      \u2502Log\u00edstica \u2502 ... (10+ dominios)\n",
    "\u2502 Team   \u2502      \u2502 Team     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "   \u2193                 \u2193\n",
    "Request            Request\n",
    "pipeline          pipeline\n",
    "   \u2193                 \u2193\n",
    "  \u23f0 3 meses        \u23f0 3 meses\n",
    "  \ud83d\udc1b Bugs           \ud83d\udc1b Bugs\n",
    "  \ud83d\udcc9 Low quality    \ud83d\udcc9 Low quality\n",
    "\n",
    "Problemas:\n",
    "\u274c Bottleneck: Central team sobrecargado (backlog 6+ meses)\n",
    "\u274c Context loss: Data team no entiende dominio de negocio\n",
    "\u274c Scalability: Imposible crecer linearmente\n",
    "\u274c Ownership: Nadie responsable de calidad de datos\n",
    "```\n",
    "\n",
    "**Soluci\u00f3n: Data Mesh (Zhamak Dehghani, 2019)**\n",
    "\n",
    "```\n",
    "Data Mesh: Descentralizaci\u00f3n por dominio\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502        FEDERATED GOVERNANCE                  \u2502\n",
    "\u2502  (Pol\u00edticas centrales, enforcement local)    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     \u2502              \u2502              \u2502\n",
    "\u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 VENTAS   \u2502  \u2502LOG\u00cdSTICA \u2502  \u2502 FINANZAS \u2502\n",
    "\u2502 DOMAIN   \u2502  \u2502 DOMAIN   \u2502  \u2502 DOMAIN   \u2502\n",
    "\u2502          \u2502  \u2502          \u2502  \u2502          \u2502\n",
    "\u2502 Data     \u2502  \u2502 Data     \u2502  \u2502 Data     \u2502\n",
    "\u2502 Product  \u2502  \u2502 Product  \u2502  \u2502 Product  \u2502\n",
    "\u2502 Owner    \u2502  \u2502 Owner    \u2502  \u2502 Owner    \u2502\n",
    "\u2502          \u2502  \u2502          \u2502  \u2502          \u2502\n",
    "\u2502 \u2022 ETL    \u2502  \u2502 \u2022 ETL    \u2502  \u2502 \u2022 ETL    \u2502\n",
    "\u2502 \u2022 Quality\u2502  \u2502 \u2022 Quality\u2502  \u2502 \u2022 Quality\u2502\n",
    "\u2502 \u2022 API    \u2502  \u2502 \u2022 API    \u2502  \u2502 \u2022 API    \u2502\n",
    "\u2502 \u2022 Docs   \u2502  \u2502 \u2022 Docs   \u2502  \u2502 \u2022 Docs   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     \u2502              \u2502              \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                    \u2502\n",
    "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502 SELF-SERVE PLATFORM \u2502\n",
    "         \u2502 (Infra com\u00fan)       \u2502\n",
    "         \u2502 \u2022 Spark/Airflow     \u2502\n",
    "         \u2502 \u2022 Monitoring        \u2502\n",
    "         \u2502 \u2022 Data Catalog      \u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**4 Principios Fundamentales:**\n",
    "\n",
    "```python\n",
    "principios_data_mesh = {\n",
    "    \"1. Domain-Oriented Decentralization\": \"\"\"\n",
    "        Datos pertenecen al dominio de negocio\n",
    "        \n",
    "        Ejemplo: E-commerce\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 DOMAIN: Ventas                          \u2502\n",
    "        \u2502 Owner: VP of Sales                      \u2502\n",
    "        \u2502 Data Products:                          \u2502\n",
    "        \u2502  \u2022 orders (transaccional)               \u2502\n",
    "        \u2502  \u2022 orders_aggregated (anal\u00edtico)        \u2502\n",
    "        \u2502  \u2022 customer_segments (ML)               \u2502\n",
    "        \u2502                                         \u2502\n",
    "        \u2502 Team composition:                       \u2502\n",
    "        \u2502  \u2022 Product Manager (prioridades)        \u2502\n",
    "        \u2502  \u2022 Data Engineer (pipelines)            \u2502\n",
    "        \u2502  \u2022 Analytics Engineer (dbt models)      \u2502\n",
    "        \u2502  \u2022 Data Analyst (consumers)             \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        Ventajas:\n",
    "        \u2705 Domain expertise: Equipo entiende negocio\n",
    "        \u2705 Velocity: No esperar central team\n",
    "        \u2705 Accountability: Owner claro de calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Data as a Product\": \"\"\"\n",
    "        Cada dataset es un producto con:\n",
    "        \n",
    "        \ud83d\udccb SLA (Service Level Agreement):\n",
    "        - Latency: <15 min para orders_aggregated\n",
    "        - Availability: 99.9% uptime\n",
    "        - Freshness: Updated cada 5 min\n",
    "        - Quality: >95% completeness\n",
    "        \n",
    "        \ud83d\udcd6 Documentation:\n",
    "        - Schema: Columnas, tipos, constraints\n",
    "        - Lineage: De d\u00f3nde viene cada campo\n",
    "        - Examples: Query samples\n",
    "        - Contact: Slack channel, owner email\n",
    "        \n",
    "        \ud83d\udd12 Access Control:\n",
    "        - Public: Todos pueden leer\n",
    "        - Private: Solo equipo ventas\n",
    "        - PII: Masked para no-autorizados\n",
    "        \n",
    "        \ud83d\udcca Observability:\n",
    "        - Metrics: Row count, size, update time\n",
    "        - Alerts: SLA violations\n",
    "        - Changelog: Version history\n",
    "        \n",
    "        Ejemplo manifest.yaml:\n",
    "        ```\n",
    "        product_name: orders_aggregated\n",
    "        owner: ventas-data@company.com\n",
    "        sla:\n",
    "          latency_minutes: 15\n",
    "          availability_percent: 99.9\n",
    "          quality_score: 0.95\n",
    "        schema_url: s3://catalog/ventas/orders_agg/schema.json\n",
    "        lineage:\n",
    "          sources:\n",
    "            - orders_raw\n",
    "            - customers\n",
    "            - products\n",
    "        access:\n",
    "          read: [sales_team, finance_team, executives]\n",
    "          write: [sales_data_engineers]\n",
    "        ```\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Self-Serve Data Platform\": \"\"\"\n",
    "        Infraestructura com\u00fan para todos los dominios\n",
    "        \n",
    "        Platform Components:\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 COMPUTE                              \u2502\n",
    "        \u2502  \u2022 Spark clusters (on-demand)       \u2502\n",
    "        \u2502  \u2022 Airflow (orchestration)          \u2502\n",
    "        \u2502  \u2022 dbt (transformations)            \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 STORAGE                              \u2502\n",
    "        \u2502  \u2022 Delta Lake (lakehouse)           \u2502\n",
    "        \u2502  \u2022 S3 buckets (per-domain)          \u2502\n",
    "        \u2502  \u2022 Unity Catalog (metadata)         \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 OBSERVABILITY                        \u2502\n",
    "        \u2502  \u2022 Datadog (metrics, logs)          \u2502\n",
    "        \u2502  \u2022 Great Expectations (quality)     \u2502\n",
    "        \u2502  \u2022 OpenLineage (lineage tracking)   \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 GOVERNANCE                           \u2502\n",
    "        \u2502  \u2022 Data Catalog (Amundsen/DataHub)  \u2502\n",
    "        \u2502  \u2022 Access Control (Ranger/Unity)    \u2502\n",
    "        \u2502  \u2022 Policy Engine (OPA)              \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        Self-service workflow:\n",
    "        1. Team crea repo: github.com/company/ventas-data\n",
    "        2. Define product: manifest.yaml + dbt models\n",
    "        3. CI/CD deploy: Tests \u2192 Stage \u2192 Prod\n",
    "        4. Auto-register: Catalog actualizado\n",
    "        5. Monitoring: Dashboards auto-generated\n",
    "        \n",
    "        Platform team provee:\n",
    "        - Templates (cookiecutter)\n",
    "        - Best practices docs\n",
    "        - On-call support\n",
    "        - Cost monitoring\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Federated Computational Governance\": \"\"\"\n",
    "        Pol\u00edticas globales + Autonom\u00eda local\n",
    "        \n",
    "        GLOBAL POLICIES (Central governance):\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 1. Security                         \u2502\n",
    "        \u2502    \u2022 PII must be encrypted at rest \u2502\n",
    "        \u2502    \u2022 Access logs retained 1 year   \u2502\n",
    "        \u2502    \u2022 MFA required for production   \u2502\n",
    "        \u2502                                     \u2502\n",
    "        \u2502 2. Compliance (GDPR, CCPA)         \u2502\n",
    "        \u2502    \u2022 Right to be forgotten         \u2502\n",
    "        \u2502    \u2022 Data retention max 7 years    \u2502\n",
    "        \u2502    \u2022 Audit trail required          \u2502\n",
    "        \u2502                                     \u2502\n",
    "        \u2502 3. Quality                          \u2502\n",
    "        \u2502    \u2022 All products have tests       \u2502\n",
    "        \u2502    \u2022 SLA violations reported       \u2502\n",
    "        \u2502    \u2022 Schema changes versioned      \u2502\n",
    "        \u2502                                     \u2502\n",
    "        \u2502 4. Interoperability                \u2502\n",
    "        \u2502    \u2022 Standard formats (Parquet)    \u2502\n",
    "        \u2502    \u2022 Common IDs (user_id format)   \u2502\n",
    "        \u2502    \u2022 Shared ontology (terms)       \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        LOCAL AUTONOMY (Domain teams):\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 \u2022 Choose transformation tools       \u2502\n",
    "        \u2502   (dbt, Spark, Python)             \u2502\n",
    "        \u2502 \u2022 Define data models               \u2502\n",
    "        \u2502 \u2022 Set refresh frequency            \u2502\n",
    "        \u2502 \u2022 Manage access within domain      \u2502\n",
    "        \u2502 \u2022 Optimize costs                   \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        Enforcement:\n",
    "        - Automated checks in CI/CD\n",
    "        - Policy-as-code (OPA policies)\n",
    "        - Failing checks block deploy\n",
    "        - Dashboards show compliance\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Implementaci\u00f3n Real: Data Mesh Stack**\n",
    "\n",
    "```python\n",
    "# Estructura organizacional\n",
    "data_mesh_org = {\n",
    "    \"Domain Teams (Aut\u00f3nomos)\": {\n",
    "        \"Ventas Domain\": {\n",
    "            \"Members\": \"8 personas\",\n",
    "            \"Products\": [\n",
    "                \"orders (transactional)\",\n",
    "                \"orders_daily_agg (analytical)\",\n",
    "                \"customer_rfm_segments (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Spark, dbt, Delta Lake\",\n",
    "            \"Budget\": \"$50K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Log\u00edstica Domain\": {\n",
    "            \"Members\": \"6 personas\",\n",
    "            \"Products\": [\n",
    "                \"shipments (operational)\",\n",
    "                \"delivery_performance (metrics)\",\n",
    "                \"route_optimization (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Flink, Kafka, Iceberg\",\n",
    "            \"Budget\": \"$40K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Marketing Domain\": {\n",
    "            \"Members\": \"5 personas\",\n",
    "            \"Products\": [\n",
    "                \"campaigns (metadata)\",\n",
    "                \"attribution (analytics)\",\n",
    "                \"propensity_scores (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Airflow, Python, Delta Lake\",\n",
    "            \"Budget\": \"$30K/mes\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Platform Team (Enabling)\": {\n",
    "        \"Members\": \"10 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Maintain Spark/Airflow infrastructure\",\n",
    "            \"Operate Data Catalog\",\n",
    "            \"Enforce governance policies\",\n",
    "            \"Cost optimization\",\n",
    "            \"Training & documentation\"\n",
    "        ],\n",
    "        \"Budget\": \"$100K/mes\",\n",
    "        \"Ratio\": \"1 platform engineer per 20 domain engineers\"\n",
    "    },\n",
    "    \n",
    "    \"Governance Team (Federated)\": {\n",
    "        \"Members\": \"3 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Define global policies\",\n",
    "            \"Compliance (GDPR, SOX)\",\n",
    "            \"Audit & reporting\",\n",
    "            \"Cross-domain standards\"\n",
    "        ],\n",
    "        \"Budget\": \"$20K/mes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data Product Example\n",
    "data_product_spec = '''\n",
    "# orders_daily_agg Data Product\n",
    "\n",
    "## Overview\n",
    "Daily aggregated orders for BI dashboards and reporting.\n",
    "\n",
    "## Owner\n",
    "- Team: Ventas Data Engineering\n",
    "- Contact: ventas-data@company.com\n",
    "- Slack: #ventas-data\n",
    "\n",
    "## SLA\n",
    "- Freshness: Updated daily @6AM UTC\n",
    "- Latency: <30 minutes processing time\n",
    "- Availability: 99.9% (max 8.76h downtime/year)\n",
    "- Quality: >99% completeness, >98% accuracy\n",
    "\n",
    "## Schema\n",
    "| Column | Type | Description | PII |\n",
    "|--------|------|-------------|-----|\n",
    "| date | DATE | Order date | No |\n",
    "| country | STRING | Country code (ISO) | No |\n",
    "| total_orders | BIGINT | Count of orders | No |\n",
    "| total_revenue | DECIMAL(10,2) | Sum of revenue (USD) | No |\n",
    "| avg_order_value | DECIMAL(10,2) | Average order value | No |\n",
    "\n",
    "## Lineage\n",
    "```\n",
    "orders_raw (Bronze)\n",
    "    \u2193\n",
    "orders_validated (Silver)\n",
    "    \u2193\n",
    "orders_daily_agg (Gold) \u2190 THIS PRODUCT\n",
    "```\n",
    "\n",
    "## Access\n",
    "- Read: sales_team, finance_team, executives\n",
    "- Write: sales_data_engineers\n",
    "- Admin: sales_domain_owner\n",
    "\n",
    "## Usage Examples\n",
    "```sql\n",
    "-- Get last 7 days\n",
    "SELECT * FROM orders_daily_agg\n",
    "WHERE date >= CURRENT_DATE - INTERVAL 7 DAYS\n",
    "ORDER BY date DESC\n",
    "\n",
    "-- YoY comparison\n",
    "SELECT \n",
    "    date,\n",
    "    total_revenue,\n",
    "    LAG(total_revenue, 365) OVER (ORDER BY date) as prev_year_revenue\n",
    "FROM orders_daily_agg\n",
    "WHERE date >= '2024-01-01'\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "- Dashboard: https://monitoring.company.com/ventas/orders_agg\n",
    "- Alerts: PagerDuty team \"sales-data-oncall\"\n",
    "\n",
    "## Changelog\n",
    "- 2025-10-01: Added avg_order_value column\n",
    "- 2025-09-15: Changed country to ISO codes\n",
    "- 2025-08-01: Initial release\n",
    "'''\n",
    "```\n",
    "\n",
    "**Ventajas de Data Mesh:**\n",
    "\n",
    "```python\n",
    "ventajas_mesh = {\n",
    "    \"1. Scalability\": \"\"\"\n",
    "        Tradicional: Central team \u2192 linear growth impossible\n",
    "        10 domains \u00d7 1 central team = bottleneck\n",
    "        \n",
    "        Data Mesh: Domain teams \u2192 linear scaling\n",
    "        10 domains \u00d7 10 teams = 100 engineers productive\n",
    "        \n",
    "        Growth: Agregar dominio no afecta otros\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Domain Expertise\": \"\"\"\n",
    "        Central team:\n",
    "        \"\u00bfQu\u00e9 significa 'adjusted_revenue'?\"\n",
    "        \u2192 Ask business (delays)\n",
    "        \n",
    "        Domain team:\n",
    "        Engineers trabajan con business daily\n",
    "        \u2192 Deep understanding, mejor calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Velocity\": \"\"\"\n",
    "        Backlog central: 6 meses\n",
    "        Domain ownership: 2 semanas\n",
    "        \n",
    "        Feature request \u2192 deploy:\n",
    "        Central: 3-6 meses\n",
    "        Mesh: 1-2 sprints\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Quality\": \"\"\"\n",
    "        Central team: Best effort (no ownership)\n",
    "        Domain team: Accountable (SLAs, metrics)\n",
    "        \n",
    "        Data quality:\n",
    "        Central: 85% average\n",
    "        Mesh: 95% average (incentivized)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desaf\u00edos de Data Mesh:**\n",
    "\n",
    "```python\n",
    "desafios_mesh = {\n",
    "    \"1. Organizational Maturity\": \"\"\"\n",
    "        Requiere:\n",
    "        - DevOps culture (CI/CD, testing)\n",
    "        - Data literacy en equipos de negocio\n",
    "        - Executive buy-in (budget descentralizado)\n",
    "        \n",
    "        NO funciona si:\n",
    "        - Organizaci\u00f3n muy jer\u00e1rquica\n",
    "        - Equipos no aut\u00f3nomos\n",
    "        - Sin cultura de ownership\n",
    "        \n",
    "        Ejemplo fracaso:\n",
    "        Company X intent\u00f3 mesh sin DevOps\n",
    "        \u2192 Dominios no saben deploy pipelines\n",
    "        \u2192 Rollback a central team en 6 meses\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Platform Complexity\": \"\"\"\n",
    "        Self-serve platform = sophisticated\n",
    "        \n",
    "        Requiere:\n",
    "        - Multi-tenancy (aislamiento dominios)\n",
    "        - Cost tracking per domain\n",
    "        - Automated provisioning\n",
    "        - Standardized observability\n",
    "        \n",
    "        Platform team: 10-15 personas m\u00ednimo\n",
    "        Cost: $1M+/a\u00f1o (salaries + infra)\n",
    "        \n",
    "        Break-even: ~100+ data engineers total\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Duplication Risk\": \"\"\"\n",
    "        Sin governance fuerte:\n",
    "        - 3 equipos crean \"customer_dim\" diferente\n",
    "        - M\u00e9tricas inconsistentes cross-domain\n",
    "        - Redundant data copies (cost)\n",
    "        \n",
    "        Mitigaci\u00f3n:\n",
    "        - Data Catalog mandatory\n",
    "        - Shared domain for common entities\n",
    "        - Regular cross-domain sync meetings\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Skillset Gap\": \"\"\"\n",
    "        Domain engineers necesitan:\n",
    "        - Data engineering (ETL)\n",
    "        - Analytics engineering (dbt)\n",
    "        - DevOps (CI/CD)\n",
    "        - Domain knowledge (business)\n",
    "        \n",
    "        Hiring difficult: Full-stack data engineers\n",
    "        Training: 6-12 meses ramp-up\n",
    "        \n",
    "        Alternative: Embedded specialists\n",
    "        - 1 data engineer per 2 domains\n",
    "        - Rotates between teams\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso (Cu\u00e1ndo Data Mesh Funciona):**\n",
    "\n",
    "```python\n",
    "# \u2705 1. LARGE ORG con m\u00faltiples dominios\n",
    "\"\"\"\n",
    "Empresa: Zalando (E-commerce Europeo)\n",
    "Tama\u00f1o: 10,000+ empleados, 20+ pa\u00edses\n",
    "Dominios: 50+ (Fashion, Logistics, Payments, ...)\n",
    "\n",
    "Data Mesh adoption (2020):\n",
    "- 50 domain teams aut\u00f3nomos\n",
    "- Platform team: 25 personas\n",
    "- 200+ data products published\n",
    "- Self-serve: 90% requests no need central\n",
    "\n",
    "Results:\n",
    "- Time-to-market: 6 meses \u2192 2 semanas\n",
    "- Data quality: 80% \u2192 95%\n",
    "- Team satisfaction: +40%\n",
    "\"\"\"\n",
    "\n",
    "# \u2705 2. DISTRIBUTED company\n",
    "\"\"\"\n",
    "Empresa: Confluent (remote-first)\n",
    "Dominios: Customer Success, Engineering, Sales\n",
    "Challenge: Central team = timezone hell\n",
    "\n",
    "Data Mesh benefits:\n",
    "- Teams own data in their timezone\n",
    "- Async collaboration (via catalog)\n",
    "- No central bottleneck\n",
    "\"\"\"\n",
    "\n",
    "# \u274c 3. SMALL STARTUP (<100 personas)\n",
    "\"\"\"\n",
    "Mesh overhead > benefits\n",
    "Better: Small central team (5 personas)\n",
    "Mesh when: Reach 200+ engineers\n",
    "\"\"\"\n",
    "\n",
    "# \u274c 4. HIGHLY REGULATED (sin autonom\u00eda)\n",
    "\"\"\"\n",
    "Banking: Regulators require central control\n",
    "Mesh autonomy conflicts with compliance\n",
    "Better: Centralized with strong governance\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migration Path: Central \u2192 Data Mesh**\n",
    "\n",
    "```python\n",
    "migration_roadmap = {\n",
    "    \"Year 1: Foundation\": \"\"\"\n",
    "        Q1: Executive alignment\n",
    "        - Present mesh vision\n",
    "        - Secure budget\n",
    "        - Identify pilot domain\n",
    "        \n",
    "        Q2: Platform MVP\n",
    "        - Self-serve Spark/Airflow\n",
    "        - Data Catalog (Amundsen)\n",
    "        - Basic CI/CD templates\n",
    "        \n",
    "        Q3-Q4: Pilot domain\n",
    "        - Choose 1 domain (e.g., Sales)\n",
    "        - Migrate 3 products\n",
    "        - Learn lessons, iterate\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 2: Scale\": \"\"\"\n",
    "        Q1-Q2: Onboard 5 domains\n",
    "        - Training programs\n",
    "        - Platform improvements\n",
    "        - Governance policies drafted\n",
    "        \n",
    "        Q3-Q4: Federated governance\n",
    "        - Policy engine (OPA)\n",
    "        - Compliance automation\n",
    "        - Cross-domain standards\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 3: Maturity\": \"\"\"\n",
    "        Q1-Q2: Onboard remaining domains\n",
    "        - 100% domains on mesh\n",
    "        - Decommission central team legacy\n",
    "        \n",
    "        Q3-Q4: Optimization\n",
    "        - Cost optimization\n",
    "        - Advanced features (ML platform)\n",
    "        - Community of practice\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total Investment\": \"\"\"\n",
    "        Platform team: 10 personas \u00d7 $150K \u00d7 3 a\u00f1os = $4.5M\n",
    "        Training: $500K\n",
    "        Tooling: $1M\n",
    "        Total: $6M\n",
    "        \n",
    "        Break-even: Year 4 (velocity gains)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0bd31",
   "metadata": {},
   "source": [
    "- Batch layer: hist\u00f3rico completo, recalculable, alta latencia (Spark/Hadoop).\n",
    "- Speed layer: streaming incremental, baja latencia (Kafka/Flink/Spark Streaming).\n",
    "- Serving layer: merge de vistas batch + speed para consultas (Druid/Cassandra/BigQuery).\n",
    "- Trade-offs: doble l\u00f3gica (batch + stream), complejidad operacional, eventual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_diagram = '''\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   Data     \u2502\n",
    "\u2502  Sources   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "      \u2502\n",
    "   \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n",
    "   \u2502Queue \u2502 (Kafka)\n",
    "   \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n",
    "      \u2502\n",
    " \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n",
    " \u2502  Batch   \u2502  (Spark jobs nocturnos)\n",
    " \u2502  Layer   \u2502\n",
    " \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "      \u2502\n",
    "   Master\n",
    "   Dataset\n",
    "      \u2502\n",
    "   \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n",
    "   \u2502Serving\u2502 (Druid/BigQuery)\n",
    "   \u2502 Layer \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "      \u2191\n",
    "   \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n",
    "   \u2502Speed \u2502 (Flink/Spark Streaming)\n",
    "   \u2502Layer \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "'''\n",
    "print(lambda_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a99dd",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9117d8",
   "metadata": {},
   "source": [
    "- Un solo pipeline streaming para todo (batch = replay del log).\n",
    "- Simplifica operaciones y l\u00f3gica unificada.\n",
    "- Requiere log infinito (Kafka con retenci\u00f3n larga) y capacidad de reprocessing.\n",
    "- Trade-offs: coste de almacenamiento del log, complejidad del estado, menor optimizaci\u00f3n batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a06c00",
   "metadata": {},
   "source": [
    "## 3. Arquitectura Delta (Lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11be385",
   "metadata": {},
   "source": [
    "- Unifica batch y streaming sobre un \u00fanico storage transaccional (Delta/Iceberg).\n",
    "- Streaming escribe micro-batches transaccionales; batch lee hist\u00f3rico con time travel.\n",
    "- Elimina duplicaci\u00f3n de c\u00f3digo y simplifica gobernanza (un cat\u00e1logo, un linaje).\n",
    "- Trade-offs: requiere adopci\u00f3n de formato y motor compatible (Spark/Trino)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab620719",
   "metadata": {},
   "source": [
    "## 4. Data Mesh: paradigma organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851967",
   "metadata": {},
   "source": [
    "- Descentralizaci\u00f3n de la propiedad de datos por dominio de negocio.\n",
    "- Data products: APIs/tablas con SLOs, documentaci\u00f3n, linaje y contratos.\n",
    "- Plataforma self-service: herramientas comunes (CI/CD, cat\u00e1logo, observabilidad).\n",
    "- Gobernanza federada: pol\u00edticas globales + autonom\u00eda local.\n",
    "- Trade-offs: requiere madurez organizacional, infraestructura s\u00f3lida, cambio cultural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0232c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_principles = '''\n",
    "1. Domain-oriented decentralization: equipos de dominio (ventas, log\u00edstica, finanzas) son due\u00f1os de sus datos.\n",
    "2. Data as a product: cada dataset tiene owner, SLO, versionado, documentaci\u00f3n y calidad garantizada.\n",
    "3. Self-serve platform: infraestructura com\u00fan (orquestaci\u00f3n, observabilidad, cat\u00e1logo) sin silos.\n",
    "4. Federated governance: pol\u00edticas de seguridad, privacidad y calidad definidas centralmente, aplicadas localmente.\n",
    "'''\n",
    "print(mesh_principles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5c55f",
   "metadata": {},
   "source": [
    "## 5. Comparaci\u00f3n y cu\u00e1ndo elegir cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comp = pd.DataFrame([\n",
    "  {'Arquitectura':'Lambda', 'Latencia':'Batch: horas, Speed: segundos', 'Complejidad':'Alta (doble l\u00f3gica)', 'Casos':'Legacy con requerimientos mixtos'},\n",
    "  {'Arquitectura':'Kappa', 'Latencia':'Baja (streaming)', 'Complejidad':'Media (un pipeline)', 'Casos':'Todo es streaming, reprocessing factible'},\n",
    "  {'Arquitectura':'Delta', 'Latencia':'Baja-Media (micro-batch)', 'Complejidad':'Media (un storage)', 'Casos':'Lakehouse, unificaci\u00f3n batch/stream'},\n",
    "  {'Arquitectura':'Data Mesh', 'Latencia':'Variable (por dominio)', 'Complejidad':'Alta (organizacional)', 'Casos':'Grandes org con m\u00faltiples dominios'}\n",
    "])\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cc56e",
   "metadata": {},
   "source": [
    "## 6. Ejercicio de dise\u00f1o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b272baf",
   "metadata": {},
   "source": [
    "Dise\u00f1a la arquitectura de datos para un e-commerce con:\n",
    "- Dominio Ventas: transacciones en tiempo real (Kafka).\n",
    "- Dominio Log\u00edstica: batch nocturno (inventario/env\u00edos).\n",
    "- Dominio Anal\u00edtica: dashboards con latencia < 5 min.\n",
    "- Requisitos: auditabilidad, GDPR, linaje, costos optimizados.\n",
    "\n",
    "Responde: \u00bfLambda, Kappa, Delta o h\u00edbrida? \u00bfData Mesh aplicable? Justifica con trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83e\udd16 ML Pipelines y Feature Stores \u2192](05_ml_pipelines_feature_stores.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Senior:**\n",
    "- [\ud83c\udfdb\ufe0f Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y pr\u00e1ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [\ud83c\udfdb\ufe0f Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83e\udd16 ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [\ud83d\udcb0 Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [\ud83d\udd10 Seguridad, Compliance y Auditor\u00eda de Datos](07_seguridad_compliance.ipynb)\n",
    "- [\ud83d\udcca Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [\ud83c\udfc6 Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udf10 Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
