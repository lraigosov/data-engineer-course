{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb7bbce",
   "metadata": {},
   "source": [
    "# 🏛️ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh\n",
    "\n",
    "Objetivo: comprender y contrastar arquitecturas de referencia (Lambda, Kappa, Delta) y patrones organizacionales (Data Mesh), con ejemplos de diseño y trade-offs.\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Mid completo, experiencia con batch y streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2042cc",
   "metadata": {},
   "source": [
    "## 1. Arquitectura Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a450ff1",
   "metadata": {},
   "source": [
    "### 🏗️ **Lambda Architecture: Batch + Speed Layer**\n",
    "\n",
    "**Origen y Motivación (Nathan Marz, 2011):**\n",
    "\n",
    "```\n",
    "Problema clásico:\n",
    "- Batch processing: Preciso pero lento (horas/días)\n",
    "- Stream processing: Rápido pero complejo (estado, fallos)\n",
    "\n",
    "Solución Lambda:\n",
    "- Batch Layer: Verdad absoluta, inmutable, reprocessable\n",
    "- Speed Layer: Aproximación rápida, eventualmente consistente\n",
    "- Serving Layer: Merge de ambas vistas\n",
    "```\n",
    "\n",
    "**Arquitectura Completa:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     DATA SOURCES                             │\n",
    "│  • Kafka Topics                                              │\n",
    "│  • Database CDC (Debezium)                                   │\n",
    "│  • Application Logs                                          │\n",
    "│  • IoT Streams                                               │\n",
    "└─────────────────┬───────────────────────────────────────────┘\n",
    "                  │\n",
    "        ┌─────────┴──────────┐\n",
    "        │                    │\n",
    "        ▼                    ▼\n",
    "┌──────────────┐    ┌──────────────┐\n",
    "│ BATCH LAYER  │    │ SPEED LAYER  │\n",
    "│              │    │              │\n",
    "│ • Spark Batch│    │ • Flink      │\n",
    "│ • Hadoop MR  │    │ • Spark SS   │\n",
    "│ • Hive       │    │ • Storm      │\n",
    "│              │    │              │\n",
    "│ Runs: Daily  │    │ Runs: Real-  │\n",
    "│       @2AM   │    │       time   │\n",
    "└──────┬───────┘    └──────┬───────┘\n",
    "       │                   │\n",
    "       │ Master Dataset    │ Delta Views\n",
    "       │ (Immutable)       │ (Mutable)\n",
    "       │                   │\n",
    "       ▼                   ▼\n",
    "┌─────────────────────────────────┐\n",
    "│      SERVING LAYER               │\n",
    "│                                  │\n",
    "│  Query(t) = Batch(0→t-1h) +     │\n",
    "│             Speed(t-1h→t)        │\n",
    "│                                  │\n",
    "│  • Druid                         │\n",
    "│  • Cassandra                     │\n",
    "│  • ElasticSearch                 │\n",
    "│  • BigQuery                      │\n",
    "└─────────────┬───────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "        ┌──────────┐\n",
    "        │   API    │\n",
    "        │Dashboard │\n",
    "        └──────────┘\n",
    "```\n",
    "\n",
    "**Componentes Detallados:**\n",
    "\n",
    "**1. Batch Layer (Verdad Inmutable):**\n",
    "\n",
    "```python\n",
    "# Características:\n",
    "# - Procesamiento completo del histórico\n",
    "# - Recalculable desde el inicio\n",
    "# - Optimizado para throughput (no latencia)\n",
    "# - Tolerancia a fallos simple (restart)\n",
    "\n",
    "# Ejemplo: Agregaciones diarias con Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchLayer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer ALL histórico (full recompute)\n",
    "raw_events = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"s3://datalake/raw/events/\")\n",
    "\n",
    "# Agregaciones pesadas\n",
    "user_stats_batch = raw_events \\\n",
    "    .groupBy(\"user_id\", \"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer\n",
    "user_stats_batch.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"s3://datalake/serving/user_stats_batch\")\n",
    "\n",
    "# Schedule: Daily @2AM (Airflow DAG)\n",
    "# Duration: 2-4 horas para procesar 1 año de datos\n",
    "# Cost: $$$ (large cluster, pero solo 1x/día)\n",
    "```\n",
    "\n",
    "**2. Speed Layer (Low Latency Incremental):**\n",
    "\n",
    "```python\n",
    "# Características:\n",
    "# - Solo datos recientes (últimas horas)\n",
    "# - Baja latencia (<1 min)\n",
    "# - Estado mutable (agregaciones incrementales)\n",
    "# - Complejidad de exactly-once\n",
    "\n",
    "# Ejemplo: Streaming con Spark Structured Streaming\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Leer solo NUEVOS eventos\n",
    "events_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Misma lógica que batch (pero incremental)\n",
    "user_stats_speed = events_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer (diferentes tablas)\n",
    "query = user_stats_speed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/speed\") \\\n",
    "    .start(\"s3://datalake/serving/user_stats_speed\")\n",
    "\n",
    "# Runs: Continuously\n",
    "# Latency: 30s - 2 min\n",
    "# Cost: $$ (small cluster, pero 24/7)\n",
    "```\n",
    "\n",
    "**3. Serving Layer (Query Merge):**\n",
    "\n",
    "```python\n",
    "# Merge batch + speed en query time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_user_stats(user_id: str, as_of: datetime = None):\n",
    "    \"\"\"\n",
    "    Query que merge batch + speed layers\n",
    "    \"\"\"\n",
    "    if as_of is None:\n",
    "        as_of = datetime.now()\n",
    "    \n",
    "    # Batch boundary (típicamente hace 1-2 horas)\n",
    "    batch_cutoff = as_of - timedelta(hours=2)\n",
    "    \n",
    "    # Query batch layer (histórico hasta cutoff)\n",
    "    batch_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_batch\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND date < '{batch_cutoff.date()}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Query speed layer (reciente desde cutoff)\n",
    "    speed_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_speed\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND window.start >= '{batch_cutoff}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Merge results\n",
    "    total_events = batch_stats[0]['events'] + speed_stats[0]['events']\n",
    "    total_revenue = batch_stats[0]['revenue'] + speed_stats[0]['revenue']\n",
    "    \n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'total_events': total_events,\n",
    "        'total_revenue': total_revenue,\n",
    "        'as_of': as_of,\n",
    "        'batch_cutoff': batch_cutoff\n",
    "    }\n",
    "```\n",
    "\n",
    "**Ventajas de Lambda:**\n",
    "\n",
    "```python\n",
    "ventajas = {\n",
    "    \"1. Robustez\": \"\"\"\n",
    "        Batch layer es inmutable → fácil debugging\n",
    "        Speed layer falla → aún tienes batch como fallback\n",
    "        Disaster recovery: recompute desde raw data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Precisión Garantizada\": \"\"\"\n",
    "        Batch garantiza exactitud (full recompute)\n",
    "        Speed solo para latencia, batch corrige errores\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Separation of Concerns\": \"\"\"\n",
    "        Batch: optimiza throughput (large partitions, columnar)\n",
    "        Speed: optimiza latencia (small batches, in-memory)\n",
    "        Cada uno usa tecnología ideal para su caso\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Auditoría\": \"\"\"\n",
    "        Raw data inmutable → compliance (GDPR, SOX)\n",
    "        Puedes recompute histórico para auditorías\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas (¿Por qué Lambda está en declive?):**\n",
    "\n",
    "```python\n",
    "desventajas = {\n",
    "    \"1. Código Duplicado\": \"\"\"\n",
    "        MISMA lógica implementada 2 veces:\n",
    "        - batch_aggregations.py (Spark Batch)\n",
    "        - speed_aggregations.py (Spark Streaming)\n",
    "        \n",
    "        Cambio de lógica → actualizar AMBOS\n",
    "        Testing → doble esfuerzo\n",
    "        Bugs → pueden divergir silenciosamente\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Complejidad Operacional\": \"\"\"\n",
    "        Mantener 2 pipelines:\n",
    "        - Batch: Airflow DAG, cluster management, retry logic\n",
    "        - Speed: Streaming query monitoring, checkpoint management\n",
    "        \n",
    "        2x infraestructura, 2x alertas, 2x oncall\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Eventual Consistency\": \"\"\"\n",
    "        Periodo de gracia donde batch/speed no alineados\n",
    "        \n",
    "        Ejemplo:\n",
    "        10:00 AM: Evento llega\n",
    "        10:01 AM: Speed layer procesa → visible en dashboard\n",
    "        02:00 AM: Batch recompute → corrige pequeños errores\n",
    "        \n",
    "        Usuario puede ver números ligeramente diferentes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Costos\": \"\"\"\n",
    "        Batch cluster: Large (100 nodes) @ 2-4h/día\n",
    "        Speed cluster: Medium (20 nodes) @ 24/7\n",
    "        \n",
    "        Total: ~$50K/mes para org mediana\n",
    "        \n",
    "        vs Lakehouse unificado: ~$30K/mes\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales (Cuándo Lambda Tiene Sentido):**\n",
    "\n",
    "```python\n",
    "# 1. LEGACY SYSTEMS con batch existente\n",
    "\"\"\"\n",
    "Empresa: Retail con 20 años de Hadoop\n",
    "Situación: 500 Hive tables, cientos de Spark jobs\n",
    "Necesidad: Agregar real-time sin reescribir todo\n",
    "\n",
    "Solución: Lambda\n",
    "- Mantener batch layer existente (no tocas legacy)\n",
    "- Agregar speed layer con Flink para casos críticos\n",
    "- Migración gradual dominio por dominio\n",
    "\"\"\"\n",
    "\n",
    "# 2. ALTA PRECISIÓN requerida\n",
    "\"\"\"\n",
    "Empresa: Financial trading\n",
    "Situación: Regulaciones requieren recompute exacto\n",
    "Necesidad: Auditorías pueden pedir recalcular 5 años atrás\n",
    "\n",
    "Solución: Lambda\n",
    "- Batch layer garantiza reproducibilidad exacta\n",
    "- Speed layer para trading real-time\n",
    "- Eventual consistency aceptable (batch corrige)\n",
    "\"\"\"\n",
    "\n",
    "# 3. MUY DIFERENTES SLAs batch vs stream\n",
    "\"\"\"\n",
    "Empresa: IoT con billones de sensores\n",
    "Situación: \n",
    "  - Alertas críticas: <100ms (anomalías)\n",
    "  - Análisis histórico: días OK (tendencias)\n",
    "\n",
    "Solución: Lambda\n",
    "- Speed layer: Flink para alertas ultra-rápidas\n",
    "- Batch layer: Hadoop/Hive para análisis pesados\n",
    "- Tecnologías muy diferentes, difícil unificar\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migración de Lambda → Modern Stack:**\n",
    "\n",
    "```python\n",
    "# Estrategia: Unified Batch+Stream con Delta Lake\n",
    "\n",
    "# ANTES (Lambda):\n",
    "# 1. Batch: Spark job diario (4h, 100 nodes)\n",
    "# 2. Speed: Flink continuous (24/7, 20 nodes)\n",
    "# 3. Serving: Druid (merge queries)\n",
    "\n",
    "# DESPUÉS (Delta/Lakehouse):\n",
    "# 1. Unified: Spark Structured Streaming (24/7, 30 nodes)\n",
    "# 2. Micro-batches cada 5 min\n",
    "# 3. Delta Lake: ACID transactions, no merge needed\n",
    "\n",
    "# Migration timeline:\n",
    "\"\"\"\n",
    "Month 1-2: Setup Delta Lake infrastructure\n",
    "Month 3-4: Migrate batch pipelines to Delta\n",
    "Month 5-6: Migrate speed pipelines to Structured Streaming\n",
    "Month 7: Dual-run (Lambda + Delta) for validation\n",
    "Month 8: Cutover to Delta, decommission Lambda\n",
    "Month 9: Cleanup, optimize\n",
    "\n",
    "Total: 9 months\n",
    "Cost: $200K engineering + $50K infra\n",
    "Savings: $20K/mes ongoing (ROI: 2.5 years)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: LinkedIn (Inventor de Lambda):**\n",
    "\n",
    "```python\n",
    "# LinkedIn inventó Lambda en 2011\n",
    "# Usaron Lambda 2011-2018 (7 años)\n",
    "\n",
    "arquitectura_linkedin = {\n",
    "    \"Batch Layer\": {\n",
    "        \"Tecnología\": \"Hadoop MapReduce → Spark\",\n",
    "        \"Datos\": \"Kafka topics replicados a HDFS\",\n",
    "        \"Frecuencia\": \"Daily @midnight\",\n",
    "        \"Output\": \"Hive tables (member profiles, connections, jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Speed Layer\": {\n",
    "        \"Tecnología\": \"Storm → Samza → Kafka Streams\",\n",
    "        \"Datos\": \"Kafka topics directamente\",\n",
    "        \"Latencia\": \"<1 second\",\n",
    "        \"Output\": \"Espresso (NoSQL) para low-latency reads\"\n",
    "    },\n",
    "    \n",
    "    \"Serving Layer\": {\n",
    "        \"Tecnología\": \"Espresso + Voldemort (key-value stores)\",\n",
    "        \"Pattern\": \"API merge batch + speed en read time\"\n",
    "    },\n",
    "    \n",
    "    \"2018 Migration\": \"\"\"\n",
    "        LinkedIn migró a Unified Streaming (Samza + Kafka)\n",
    "        Razón: Complejidad de mantener doble lógica\n",
    "        Resultado: -30% código, -40% operaciones\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd386004",
   "metadata": {},
   "source": [
    "### 🔄 **Kappa Architecture: Stream-Only Simplification**\n",
    "\n",
    "**Origen (Jay Kreps, LinkedIn/Confluent, 2014):**\n",
    "\n",
    "```\n",
    "Crítica a Lambda:\n",
    "\"¿Por qué mantener 2 sistemas si streaming puede hacer todo?\"\n",
    "\n",
    "Propuesta Kappa:\n",
    "- Eliminar batch layer completamente\n",
    "- Todo es streaming (batch = replay del log)\n",
    "- Inmutable log como single source of truth\n",
    "```\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────┐\n",
    "│         DATA SOURCES                     │\n",
    "│  • Applications                          │\n",
    "│  • Databases (CDC)                       │\n",
    "│  • IoT Devices                           │\n",
    "└─────────────┬───────────────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│    KAFKA (Immutable Log)                │\n",
    "│                                          │\n",
    "│  • Infinite retention (or very long)    │\n",
    "│  • Partitioned & Replicated             │\n",
    "│  • Serves as \"Master Dataset\"           │\n",
    "│                                          │\n",
    "│  Topics:                                 │\n",
    "│  ├─ events (retention: 1 year)          │\n",
    "│  ├─ transactions (retention: 5 years)   │\n",
    "│  └─ user_actions (retention: 90 days)   │\n",
    "└─────────────┬───────────────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│   STREAM PROCESSING                     │\n",
    "│                                          │\n",
    "│  Version 1: Flink Job                   │\n",
    "│  ├─ Aggregations                        │\n",
    "│  ├─ Joins                               │\n",
    "│  └─ Output → Cassandra                  │\n",
    "│                                          │\n",
    "│  Version 2: New logic needed            │\n",
    "│  ├─ Deploy new Flink job                │\n",
    "│  ├─ Replay from offset 0                │\n",
    "│  └─ Output → Cassandra v2               │\n",
    "│                                          │\n",
    "│  🔄 Reprocessing = Replay log           │\n",
    "└─────────────┬───────────────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│       SERVING LAYER                     │\n",
    "│  • Cassandra                            │\n",
    "│  • ElasticSearch                        │\n",
    "│  • Redis                                │\n",
    "│  • PostgreSQL                           │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Principios Clave:**\n",
    "\n",
    "```python\n",
    "principios_kappa = {\n",
    "    \"1. Everything is a Stream\": \"\"\"\n",
    "        No distinción entre batch y stream\n",
    "        \n",
    "        Batch tradicional:\n",
    "        SELECT SUM(revenue) FROM sales WHERE date = '2025-10-30'\n",
    "        \n",
    "        Kappa:\n",
    "        Consume Kafka topic 'sales', aggregate, done\n",
    "        (No diferencia conceptual con real-time)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Immutable Log as Source of Truth\": \"\"\"\n",
    "        Kafka = Database of record\n",
    "        \n",
    "        Ventajas:\n",
    "        - Time travel: replay desde cualquier offset\n",
    "        - Debugging: reproduce bug con datos exactos\n",
    "        - Migration: deploy nueva versión, replay, compare\n",
    "        \n",
    "        Ejemplo:\n",
    "        offset 0 → offset 1M (1 año de datos)\n",
    "        Reprocessing: ~4 horas con 50 partitions\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution in Log\": \"\"\"\n",
    "        Log contiene todos los schemas históricos\n",
    "        \n",
    "        V1: {\"user_id\": 123, \"action\": \"click\"}\n",
    "        V2: {\"user_id\": 123, \"action\": \"click\", \"device\": \"mobile\"}\n",
    "        V3: {\"user_id\": 123, \"event_type\": \"click\", \"metadata\": {...}}\n",
    "        \n",
    "        Consumer handle all versions gracefully\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Reprocessing for Code Changes\": \"\"\"\n",
    "        Cambio de lógica → no reescribir batch\n",
    "        \n",
    "        Workflow:\n",
    "        1. Deploy new stream processor (version 2)\n",
    "        2. Replay desde offset 0 (parallel con v1)\n",
    "        3. Validate output v2 matches expected\n",
    "        4. Cutover traffic to v2\n",
    "        5. Decommission v1\n",
    "        \n",
    "        Duration: horas/días (no semanas de reescribir batch)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Implementación Real con Kafka + Flink:**\n",
    "\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.table.descriptors import Kafka, Schema, Json\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Configuración Kafka source con retención larga\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        product_id STRING,\n",
    "        revenue DOUBLE,\n",
    "        event_timestamp TIMESTAMP(3),\n",
    "        WATERMARK FOR event_timestamp AS event_timestamp - INTERVAL '10' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'properties.group.id' = 'user-stats-processor-v2',\n",
    "        'scan.startup.mode' = 'earliest-offset',  -- Replay desde inicio\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Stream processing (unificado batch+stream)\n",
    "user_stats = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        TUMBLE_START(event_timestamp, INTERVAL '1' DAY) as day,\n",
    "        COUNT(*) as total_events,\n",
    "        SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,\n",
    "        COUNT(DISTINCT product_id) as unique_products\n",
    "    FROM events\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_timestamp, INTERVAL '1' DAY)\n",
    "\"\"\")\n",
    "\n",
    "# Output a Cassandra\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_stats_cassandra (\n",
    "        user_id STRING,\n",
    "        day TIMESTAMP(3),\n",
    "        total_events BIGINT,\n",
    "        total_revenue DOUBLE,\n",
    "        unique_products BIGINT,\n",
    "        PRIMARY KEY (user_id, day) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'cassandra',\n",
    "        'host' = 'cassandra',\n",
    "        'table-name' = 'user_stats'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "user_stats.execute_insert('user_stats_cassandra').wait()\n",
    "\n",
    "# Reprocessing:\n",
    "# 1. Stop old job\n",
    "# 2. Deploy this code (nueva lógica)\n",
    "# 3. Kafka consumer group lee desde offset 0\n",
    "# 4. Reprocesa 1 año de datos en ~4 horas\n",
    "# 5. Nueva tabla user_stats_cassandra tiene datos corregidos\n",
    "```\n",
    "\n",
    "**Comparación Kafka Retention:**\n",
    "\n",
    "```python\n",
    "# Configuración típica para Kappa\n",
    "kafka_retention = {\n",
    "    \"Short Retention (Anti-Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 604800000  # 7 días\",\n",
    "        \"Disk\": \"1 TB / partition\",\n",
    "        \"Cost\": \"$500/mes\",\n",
    "        \"Reprocessing\": \"❌ Solo últimos 7 días disponibles\",\n",
    "        \"Use Case\": \"Lambda architecture (Kafka solo buffer)\"\n",
    "    },\n",
    "    \n",
    "    \"Long Retention (Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 31536000000  # 1 año\",\n",
    "        \"Disk\": \"52 TB / partition (con compaction)\",\n",
    "        \"Cost\": \"$2,600/mes (S3 tiered storage)\",\n",
    "        \"Reprocessing\": \"✅ 1 año completo disponible\",\n",
    "        \"Use Case\": \"Kappa architecture (Kafka es source of truth)\"\n",
    "    },\n",
    "    \n",
    "    \"Infinite Retention (Extreme Kappa)\": {\n",
    "        \"Config\": \"retention.ms = -1  # Forever\",\n",
    "        \"Disk\": \"Ilimitado (requiere tiered storage)\",\n",
    "        \"Cost\": \"$5K/mes (S3 archival)\",\n",
    "        \"Reprocessing\": \"✅ Histórico completo\",\n",
    "        \"Use Case\": \"Event sourcing, auditabilidad estricta\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Kafka Tiered Storage (KIP-405, Kafka 3.0+)\n",
    "# Archiva logs antiguos a S3, mantiene recientes en SSD\n",
    "\"\"\"\n",
    "kafka-storage.properties:\n",
    "  remote.log.storage.enable=true\n",
    "  remote.log.storage.manager.class=org.apache.kafka.server.log.remote.storage.RemoteLogStorageManager\n",
    "  \n",
    "Estructura:\n",
    "  Local SSD: Últimos 7 días (hot data)\n",
    "  S3 Bucket: >7 días (cold data, comprimido)\n",
    "  \n",
    "Cost reduction: 70% vs all-SSD\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_kappa = {\n",
    "    \"1. Simplicidad Operacional\": \"\"\"\n",
    "        Un solo pipeline → un solo sistema para monitorear\n",
    "        Un solo lenguaje/framework (Flink o Spark)\n",
    "        Un solo cluster para operar\n",
    "        \n",
    "        On-call rotation:\n",
    "        Lambda: 2 sistemas × 2 equipos = 4 rotaciones/semana\n",
    "        Kappa: 1 sistema × 1 equipo = 2 rotaciones/semana\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. No Código Duplicado\": \"\"\"\n",
    "        Misma lógica para batch y stream\n",
    "        \n",
    "        Lambda: 2000 líneas batch.py + 2000 líneas stream.py = 4000\n",
    "        Kappa: 2000 líneas unified.py = 2000 (50% menos código)\n",
    "        \n",
    "        Bug fix: 1 cambio vs 2 cambios\n",
    "        Testing: 1 suite vs 2 suites\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Reprocessing Rápido\": \"\"\"\n",
    "        Cambio de lógica → replay log en horas\n",
    "        \n",
    "        Lambda:\n",
    "        - Reescribir batch job (días de dev)\n",
    "        - Ejecutar en cluster grande (4h)\n",
    "        - Validar, debug, iterar\n",
    "        Total: 1-2 semanas\n",
    "        \n",
    "        Kappa:\n",
    "        - Modificar stream job (horas de dev)\n",
    "        - Replay desde offset 0 (4h)\n",
    "        - Compare old vs new output\n",
    "        Total: 1 día\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Exactly-Once Más Simple\": \"\"\"\n",
    "        Kafka transactions + idempotent producers\n",
    "        \n",
    "        Flink + Kafka:\n",
    "        - Checkpointing en Kafka offsets\n",
    "        - Transactional writes a sinks\n",
    "        - No necesidad de merge batch+speed\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch tiene at-least-once (reintentos)\n",
    "        - Speed tiene exactly-once (complejo)\n",
    "        - Merge puede duplicar sin deduplication\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "desventajas_kappa = {\n",
    "    \"1. Costo de Almacenamiento\": \"\"\"\n",
    "        Kafka con 1 año de retención = caro\n",
    "        \n",
    "        Ejemplo: 10K events/s × 10 KB/event × 1 año\n",
    "        = 3.15 PB/año crudo\n",
    "        Con compaction + tiered storage: 500 TB\n",
    "        Cost: $5K/mes S3 + $10K/mes Kafka brokers\n",
    "        \n",
    "        vs Hadoop HDFS: $2K/mes (pero sin streaming)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Reprocessing Largo\": \"\"\"\n",
    "        Replay 1 año de datos = horas/días\n",
    "        \n",
    "        1 TB data @ 100 MB/s = 2.8 horas ideal\n",
    "        Real (with processing): 6-12 horas\n",
    "        \n",
    "        Durante reprocessing:\n",
    "        - Nuevo código corre en paralelo (doble costo)\n",
    "        - Output dual (old version + new version)\n",
    "        - Validación manual necesaria\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch recompute overnight (no impact users)\n",
    "        - Speed layer sigue sirviendo tráfico\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. State Management Complejo\": \"\"\"\n",
    "        Streaming state para 1 año de datos = grande\n",
    "        \n",
    "        Ejemplo: User profiles con aggregations\n",
    "        10M users × 1 KB state = 10 GB state\n",
    "        \n",
    "        Flink RocksDB:\n",
    "        - State en disco (slower)\n",
    "        - Checkpointing largo (minutes)\n",
    "        - Recovery largo si crash (restore state)\n",
    "        \n",
    "        Lambda batch:\n",
    "        - No state (stateless join con full tables)\n",
    "        - Simpler, pero más lento\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. No Optimización por Caso\": \"\"\"\n",
    "        Streaming debe servir TODOS los casos\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch: Columnar Parquet, predicate pushdown, Z-order\n",
    "        - Speed: Row-based, in-memory, indexes\n",
    "        \n",
    "        Kappa:\n",
    "        - Un formato compromiso (no óptimo para ninguno)\n",
    "        - Queries complejos (joins 5 tables) lentos en stream\n",
    "        - Queries simples (count) overkill con stream\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Ideales para Kappa:**\n",
    "\n",
    "```python\n",
    "# 1. REAL-TIME FIRST con poco histórico\n",
    "\"\"\"\n",
    "Empresa: Fintech fraud detection\n",
    "Datos: 90 días de transacciones\n",
    "Procesamiento: <100ms latency requerido\n",
    "Reprocessing: Semanal (ajustar modelos ML)\n",
    "\n",
    "Kappa perfecto:\n",
    "- 90 días en Kafka (100 TB, manejable)\n",
    "- Flink streaming para detección real-time\n",
    "- Replay semanal para ajustar modelos (4h)\n",
    "\"\"\"\n",
    "\n",
    "# 2. EVENT SOURCING puro\n",
    "\"\"\"\n",
    "Empresa: Booking platform\n",
    "Patrón: Event sourcing (todo es evento inmutable)\n",
    "Datos: bookings, cancellations, modifications\n",
    "Queries: Reconstruir estado actual desde eventos\n",
    "\n",
    "Kappa natural:\n",
    "- Kafka como event store (immutable log)\n",
    "- Flink materializa vistas (current bookings)\n",
    "- Replay para nuevas vistas (add \"bookings by hotel\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. HIGH CHANGE FREQUENCY en lógica\n",
    "\"\"\"\n",
    "Empresa: Ad-tech con modelos A/B testing constante\n",
    "Situación: Cambio de lógica 2-3x/semana\n",
    "Necesidad: Deploy rápido, validar, iterar\n",
    "\n",
    "Kappa ventaja:\n",
    "- Modify stream job (1h dev)\n",
    "- Deploy v2 parallel v1 (30 min)\n",
    "- Replay último día (30 min)\n",
    "- Compare outputs, cutover (1h)\n",
    "Total: 3 horas por cambio vs 3 días Lambda\n",
    "\"\"\"\n",
    "\n",
    "# 4. SMALL-MEDIUM data scale\n",
    "\"\"\"\n",
    "Empresa: SaaS startup\n",
    "Datos: <100 TB total\n",
    "Users: <1M\n",
    "Traffic: <1K events/s\n",
    "\n",
    "Kappa ventaja:\n",
    "- Un cluster Kafka+Flink (simple)\n",
    "- Retención 1 año fácil (10 TB en Kafka)\n",
    "- Equipo pequeño puede mantener\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Anti-Patterns (Cuándo NO usar Kappa):**\n",
    "\n",
    "```python\n",
    "# ❌ 1. HUGE data scale con queries complejos\n",
    "\"\"\"\n",
    "Empresa: Retail con 10+ años de histórico\n",
    "Datos: 100 PB en HDFS\n",
    "Queries: SQL complejo (20+ joins, window functions)\n",
    "\n",
    "Problema Kappa:\n",
    "- 100 PB en Kafka = $500K/mes (vs $50K HDFS)\n",
    "- Streaming joins lentos vs batch columnar\n",
    "- Replay 10 años = semanas (inviable)\n",
    "\n",
    "Better: Lambda o Delta Lakehouse\n",
    "\"\"\"\n",
    "\n",
    "# ❌ 2. BATCH-FIRST workloads\n",
    "\"\"\"\n",
    "Empresa: Data warehouse con reportes nocturnos\n",
    "Queries: ETL batch 90% workload, streaming 10%\n",
    "Latency: Horas OK para reportes\n",
    "\n",
    "Problema Kappa:\n",
    "- Overhead streaming para batch workloads\n",
    "- Kafka infraestructura innecesaria\n",
    "- Spark batch más eficiente que Spark Streaming para batch\n",
    "\n",
    "Better: Tradicional batch (Airflow + Spark)\n",
    "\"\"\"\n",
    "\n",
    "# ❌ 3. ESTRICTA compliance con reprocessing prohibido\n",
    "\"\"\"\n",
    "Empresa: Banco con regulaciones estrictas\n",
    "Requerimiento: Auditorías requieren datos exactos \"as-of\"\n",
    "Prohibición: No se puede \"rehacer\" cálculos pasados\n",
    "\n",
    "Problema Kappa:\n",
    "- Reprocessing es core feature (pero regulador dice no)\n",
    "- Inmutabilidad requiere batch layer aparte\n",
    "\n",
    "Better: Lambda con batch layer auditado/certificado\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: Uber (Kappa en Producción):**\n",
    "\n",
    "```python\n",
    "uber_kappa = {\n",
    "    \"Caso\": \"Surge pricing (precios dinámicos)\",\n",
    "    \n",
    "    \"Arquitectura\": {\n",
    "        \"Eventos\": \"ride_requests, driver_locations, ride_completions\",\n",
    "        \"Kafka\": \"500K events/s, retención 7 días (suficiente)\",\n",
    "        \"Flink\": \"Windowed aggregations (supply/demand por área)\",\n",
    "        \"Output\": \"Redis (precios actuales), Cassandra (histórico)\",\n",
    "        \"Latency\": \"<200ms end-to-end\"\n",
    "    },\n",
    "    \n",
    "    \"Kappa Benefits\": \"\"\"\n",
    "        1. Cambio frecuente de algoritmo pricing (weekly)\n",
    "        2. Replay 7 días para validar nuevo algoritmo (2h)\n",
    "        3. A/B testing: correr 2 versiones paralelo, compare\n",
    "        4. No batch layer needed (solo últimos días relevantes)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Evolution\": \"\"\"\n",
    "        2015: Lambda (Spark batch + Storm streaming)\n",
    "        2016: Migrated to Kappa (Samza streaming only)\n",
    "        2018: Moved to Flink (better stateful processing)\n",
    "        2020: Hybrid (Kappa para real-time, Hudi para analytics)\n",
    "        \n",
    "        Razón hybrid: Analytics largo plazo necesita batch optimizations\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d7c4",
   "metadata": {},
   "source": [
    "### 🏠 **Delta Architecture (Lakehouse): Unified Batch+Stream**\n",
    "\n",
    "**Evolución Natural (2020+):**\n",
    "\n",
    "```\n",
    "Lambda (2011): Batch + Speed layers separados\n",
    "   ↓\n",
    "Kappa (2014): Solo streaming (pero caro, complejo)\n",
    "   ↓\n",
    "Delta (2020): Unified sobre transactional storage\n",
    "```\n",
    "\n",
    "**Arquitectura Lakehouse:**\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    DATA SOURCES                              │\n",
    "│  • Applications → Kafka                                      │\n",
    "│  • Databases → CDC (Debezium)                                │\n",
    "│  • Batch files → S3/ADLS                                     │\n",
    "└────────────────────┬────────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "          ┌──────────────────┐\n",
    "          │  INGESTION LAYER │\n",
    "          │                  │\n",
    "          │  Spark Streaming │ ← Unified engine!\n",
    "          │  (micro-batches) │\n",
    "          └────────┬─────────┘\n",
    "                   │\n",
    "                   ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│              DELTA LAKE / ICEBERG STORAGE                     │\n",
    "│                                                               │\n",
    "│  Bronze (Raw)           Silver (Cleaned)      Gold (Curated) │\n",
    "│  ┌────────────┐        ┌────────────┐       ┌────────────┐  │\n",
    "│  │ Append-only│        │ Deduplicated│      │Aggregations│  │\n",
    "│  │ Parquet +  │   →    │ Validated   │  →   │Star Schema │  │\n",
    "│  │ Delta Log  │        │ Enriched    │      │Business    │  │\n",
    "│  │            │        │             │      │Logic       │  │\n",
    "│  └────────────┘        └────────────┘       └────────────┘  │\n",
    "│                                                               │\n",
    "│  Features:                                                    │\n",
    "│  ✅ ACID Transactions                                        │\n",
    "│  ✅ Time Travel (rollback, audit)                           │\n",
    "│  ✅ Schema Evolution                                         │\n",
    "│  ✅ Upserts/Deletes                                          │\n",
    "│  ✅ Unified Batch+Stream reads                               │\n",
    "└────────────────────┬──────────────────────────────────────────┘\n",
    "                     │\n",
    "          ┌──────────┴─────────┐\n",
    "          │                    │\n",
    "          ▼                    ▼\n",
    "  ┌───────────────┐    ┌──────────────┐\n",
    "  │ BATCH QUERIES │    │  STREAMING   │\n",
    "  │               │    │   QUERIES    │\n",
    "  │ Spark SQL     │    │ Spark SS     │\n",
    "  │ Presto/Trino  │    │ Flink        │\n",
    "  │ Athena        │    │ Real-time    │\n",
    "  └───────────────┘    └──────────────┘\n",
    "          │                    │\n",
    "          └──────────┬─────────┘\n",
    "                     ▼\n",
    "            ┌─────────────────┐\n",
    "            │   CONSUMPTION   │\n",
    "            │                 │\n",
    "            │  • BI Tools     │\n",
    "            │  • ML Training  │\n",
    "            │  • APIs         │\n",
    "            │  • Dashboards   │\n",
    "            └─────────────────┘\n",
    "```\n",
    "\n",
    "**Características Clave:**\n",
    "\n",
    "```python\n",
    "delta_features = {\n",
    "    \"1. ACID Transactions en Object Storage\": \"\"\"\n",
    "        Problema tradicional:\n",
    "        S3/ADLS no tienen transacciones\n",
    "        → Race conditions, partial writes\n",
    "        \n",
    "        Delta Lake solution:\n",
    "        _delta_log/00000000000000000123.json\n",
    "        {\n",
    "          \"commitInfo\": {\"timestamp\": \"2025-10-30T10:30:00\"},\n",
    "          \"add\": [\n",
    "            {\"path\": \"part-00000.parquet\", \"size\": 1024000},\n",
    "            {\"path\": \"part-00001.parquet\", \"size\": 1024000}\n",
    "          ],\n",
    "          \"remove\": []\n",
    "        }\n",
    "        \n",
    "        Atomic commit: Write log entry = commit completo\n",
    "        Readers ven versión consistente (snapshot isolation)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Unified Batch + Stream\": \"\"\"\n",
    "        SAME TABLE para batch y streaming\n",
    "        \n",
    "        Escritura streaming:\n",
    "        df.writeStream\n",
    "          .format(\"delta\")\n",
    "          .outputMode(\"append\")\n",
    "          .start(\"/delta/events\")\n",
    "        \n",
    "        Lectura batch:\n",
    "        spark.read\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")\n",
    "          .where(\"date = '2025-10-30'\")\n",
    "        \n",
    "        Lectura streaming:\n",
    "        spark.readStream\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")  # Lee cambios incrementales\n",
    "        \n",
    "        NO necesitas batch layer separada!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Time Travel\": \"\"\"\n",
    "        Acceso a versiones históricas\n",
    "        \n",
    "        # Leer versión específica\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"versionAsOf\", 42)\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        # Leer timestamp específico\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"timestampAsOf\", \"2025-10-29 00:00:00\")\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        Casos de uso:\n",
    "        - Rollback: Deployment malo → revert\n",
    "        - Audit: \"¿Qué datos vio el modelo ayer?\"\n",
    "        - Debugging: \"Reproduce bug con datos exactos\"\n",
    "        - Compliance: \"Muestra datos as-of Q3 2025\"\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Schema Evolution\": \"\"\"\n",
    "        Agregar columnas sin downtime\n",
    "        \n",
    "        V1: {user_id, action, timestamp}\n",
    "        V2: {user_id, action, timestamp, device}  ← Add column\n",
    "        \n",
    "        Delta:\n",
    "        df_v2.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .save(\"/delta/events\")\n",
    "        \n",
    "        Lectores antiguos: device = null\n",
    "        Lectores nuevos: device leído correctamente\n",
    "        \n",
    "        NO necesitas backfill!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Upserts (MERGE)\": \"\"\"\n",
    "        CDC y SCD Type 2 nativos\n",
    "        \n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            updates.alias(\"source\"),\n",
    "            \"target.user_id = source.user_id\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"email\": \"source.email\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).whenNotMatchedInsert(values={\n",
    "            \"user_id\": \"source.user_id\",\n",
    "            \"email\": \"source.email\",\n",
    "            \"created_at\": \"source.created_at\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).execute()\n",
    "        \n",
    "        Streaming UPSERT:\n",
    "        updates.writeStream\n",
    "          .foreachBatch(lambda df, _: upsert_function(df))\n",
    "          .start()\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Medallion Architecture (Bronze/Silver/Gold):**\n",
    "\n",
    "```python\n",
    "# BRONZE: Raw data, append-only\n",
    "bronze_events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").cast(\"string\").alias(\"raw_json\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "bronze_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(\"/delta/bronze/events\")\n",
    "\n",
    "# SILVER: Cleaned, validated, deduplicated\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "silver_events = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/bronze/events\") \\\n",
    "    .select(\n",
    "        from_json(col(\"raw_json\"), event_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\  # Validación\n",
    "    .dropDuplicates([\"user_id\", \"event_timestamp\"])  # Dedup\n",
    "\n",
    "silver_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver\") \\\n",
    "    .start(\"/delta/silver/events\")\n",
    "\n",
    "# GOLD: Business aggregations\n",
    "gold_user_stats = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/silver/events\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 day\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"daily_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"event_types\")\n",
    "    )\n",
    "\n",
    "gold_user_stats.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/gold\") \\\n",
    "    .start(\"/delta/gold/user_daily_stats\")\n",
    "\n",
    "# BI Tool lee desde Gold\n",
    "df_dashboard = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/gold/user_daily_stats\") \\\n",
    "    .where(\"window.start >= current_date() - interval 7 days\")\n",
    "```\n",
    "\n",
    "**Ventajas sobre Lambda/Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_delta = {\n",
    "    \"vs Lambda\": {\n",
    "        \"Código Unificado\": \"\"\"\n",
    "            Lambda: 2 pipelines (batch.py + stream.py)\n",
    "            Delta: 1 pipeline (unified.py)\n",
    "            \n",
    "            Reduction: 50% código, 50% tests, 50% bugs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Sin Merge Layer\": \"\"\"\n",
    "            Lambda: Query = merge(batch, speed) en serving\n",
    "            Delta: Query directo a Delta (single source)\n",
    "            \n",
    "            Latency: -100ms (no merge overhead)\n",
    "            Consistency: Guaranteed (ACID transactions)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Operaciones Simples\": \"\"\"\n",
    "            Lambda: Mantener 2 clusters + serving layer\n",
    "            Delta: 1 cluster Spark + Delta storage\n",
    "            \n",
    "            On-call: -50% alerts, -40% incidents\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Kappa\": {\n",
    "        \"Costo Storage\": \"\"\"\n",
    "            Kappa: Kafka 1 año = $15K/mes\n",
    "            Delta: S3 object storage = $2K/mes\n",
    "            \n",
    "            Savings: 85% storage costs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Batch Optimized\": \"\"\"\n",
    "            Kappa: Streaming para todo (no óptimo)\n",
    "            Delta: Columnar Parquet + statistics\n",
    "            \n",
    "            Query performance: 10x faster para analytics\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Reprocessing Flexible\": \"\"\"\n",
    "            Kappa: Replay Kafka (limited retention)\n",
    "            Delta: Time travel infinito\n",
    "            \n",
    "            Auditabilidad: 5+ años histórico disponible\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Traditional Warehouse\": {\n",
    "        \"Cost\": \"\"\"\n",
    "            Warehouse: $25/TB/month (Snowflake)\n",
    "            Delta: $0.023/GB/month (S3) = $23/TB\n",
    "            \n",
    "            Savings: ~50% at petabyte scale\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Flexibility\": \"\"\"\n",
    "            Warehouse: Vendor lock-in, SQL only\n",
    "            Delta: Open format, Spark/Presto/Flink/Python\n",
    "            \n",
    "            ML Integration: Native (same storage)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Scalability\": \"\"\"\n",
    "            Warehouse: Scale up (expensive)\n",
    "            Delta: Scale out (S3 infinite)\n",
    "            \n",
    "            Growth: No limits, pay-as-you-go\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "```python\n",
    "# 1. Databricks - Inventor de Delta Lake\n",
    "\"\"\"\n",
    "Cliente: Comcast (Telecomunicaciones)\n",
    "Datos: 2 PB diarios (network logs, streaming video, IoT)\n",
    "Arquitectura anterior: Lambda (Hadoop batch + Kafka streaming)\n",
    "\n",
    "Migration a Delta:\n",
    "- 2018: Pilot con 10 TB (1 mes)\n",
    "- 2019: Migration 500 TB (6 meses)\n",
    "- 2020: Full adoption 2 PB/día\n",
    "\n",
    "Resultados:\n",
    "- Cost: -30% ($2M/año savings)\n",
    "- Latency: 4h → 15 min para dashboards\n",
    "- Code: -50% (unified batch+stream)\n",
    "- Incidents: -60% (ACID transactions)\n",
    "\n",
    "Stack:\n",
    "- Ingestion: Spark Streaming\n",
    "- Storage: Delta Lake en S3\n",
    "- Compute: Databricks clusters\n",
    "- Consumption: Tableau + ML models\n",
    "\"\"\"\n",
    "\n",
    "# 2. Riot Games (League of Legends)\n",
    "\"\"\"\n",
    "Caso: Game analytics + Player behavior\n",
    "Datos: 100M+ games/day, petabytes de events\n",
    "Latency: <5 min para anti-cheat, <1 hour para balance\n",
    "\n",
    "Delta Architecture:\n",
    "┌──────────────┐\n",
    "│ Game Servers │ → Kafka (1M events/s)\n",
    "└──────────────┘\n",
    "       ↓\n",
    "┌──────────────┐\n",
    "│ Spark Stream │ → Delta Bronze (raw events)\n",
    "└──────────────┘\n",
    "       ↓\n",
    "┌──────────────┐\n",
    "│ Enrichment   │ → Delta Silver (validated)\n",
    "└──────────────┘\n",
    "       ↓\n",
    "    ┌──┴──┐\n",
    "    │     │\n",
    "    ▼     ▼\n",
    "┌────┐  ┌────┐\n",
    "│ ML │  │ BI │ → Delta Gold (aggregations)\n",
    "└────┘  └────┘\n",
    "\n",
    "Benefits:\n",
    "- Unified: Game designers query same data ML uses\n",
    "- Time travel: \"Replay match from 2 days ago\"\n",
    "- Schema evolution: Add features without downtime\n",
    "\"\"\"\n",
    "\n",
    "# 3. Adobe Experience Platform\n",
    "\"\"\"\n",
    "Challenge: 100s of tenants, isolated data, compliance\n",
    "Solution: Delta Lake multi-tenant\n",
    "\n",
    "Architecture:\n",
    "/delta/tenant_001/events/\n",
    "/delta/tenant_002/events/\n",
    "...\n",
    "/delta/tenant_500/events/\n",
    "\n",
    "Features:\n",
    "- Row-level security (GDPR compliance)\n",
    "- Tenant isolation (Delta sharing)\n",
    "- Unified operations (single platform)\n",
    "- Time travel (data recovery per tenant)\n",
    "\n",
    "Scale:\n",
    "- 500+ tenants\n",
    "- 10 PB total data\n",
    "- 100K writes/s\n",
    "- <100ms p99 latency\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migración Lambda → Delta:**\n",
    "\n",
    "```python\n",
    "migration_strategy = {\n",
    "    \"Phase 1: Setup (Month 1-2)\": \"\"\"\n",
    "        1. Provision Delta Lake infrastructure\n",
    "           - S3 bucket con versioning\n",
    "           - Unity Catalog setup\n",
    "           - Spark clusters (Databricks/EMR)\n",
    "        \n",
    "        2. Crear medallion structure\n",
    "           /delta/bronze/\n",
    "           /delta/silver/\n",
    "           /delta/gold/\n",
    "        \n",
    "        3. Setup CI/CD pipelines\n",
    "           - GitHub Actions para deploy\n",
    "           - Testing framework (pytest)\n",
    "           - Monitoring (Datadog/Prometheus)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 2: Bronze Layer (Month 3-4)\": \"\"\"\n",
    "        1. Migrate raw ingestion\n",
    "           Kafka → Delta Bronze (append-only)\n",
    "           \n",
    "        2. Dual-write period\n",
    "           - Write to both Lambda batch + Delta\n",
    "           - Compare row counts, checksums\n",
    "           \n",
    "        3. Backfill histórico\n",
    "           HDFS/Hive → Delta Bronze\n",
    "           (1-time copy, then delete HDFS)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 3: Silver Layer (Month 5-6)\": \"\"\"\n",
    "        1. Migrate transformation logic\n",
    "           Batch Spark jobs → Delta pipelines\n",
    "           \n",
    "        2. Unified batch+stream\n",
    "           - Remove duplicate code\n",
    "           - Single pipeline para ambos\n",
    "           \n",
    "        3. Data quality checks\n",
    "           Great Expectations integration\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 4: Gold Layer (Month 7-8)\": \"\"\"\n",
    "        1. Migrate aggregations\n",
    "           Speed layer → Delta Gold micro-batches\n",
    "           \n",
    "        2. Decommission serving layer\n",
    "           Druid/Cassandra → query Delta directly\n",
    "           \n",
    "        3. BI tools reconfigure\n",
    "           Point Tableau/Power BI to Delta\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 5: Validation (Month 9)\": \"\"\"\n",
    "        1. Parallel run\n",
    "           - Lambda still running (backup)\n",
    "           - Delta serving production traffic\n",
    "           \n",
    "        2. Compare metrics\n",
    "           - Latency, accuracy, cost\n",
    "           - User feedback (BI teams)\n",
    "        \n",
    "        3. Fix edge cases\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 6: Cutover (Month 10)\": \"\"\"\n",
    "        1. Decommission Lambda\n",
    "           - Stop batch jobs\n",
    "           - Stop speed layer\n",
    "           - Delete Druid/Cassandra\n",
    "        \n",
    "        2. Cleanup\n",
    "           - Delete HDFS clusters\n",
    "           - Reclaim savings\n",
    "        \n",
    "        3. Training\n",
    "           - Team training on Delta\n",
    "           - Documentation update\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total\": \"\"\"\n",
    "        Duration: 10 months\n",
    "        Cost: $300K engineering + $100K infra\n",
    "        Ongoing savings: $50K/mes\n",
    "        ROI: 8 months\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Partition Strategy\": \"\"\"\n",
    "        ✅ Date partitioning (daily/hourly)\n",
    "        partitionBy(\"date\")\n",
    "        \n",
    "        ⚠️ Z-ordering para high-cardinality\n",
    "        OPTIMIZE events ZORDER BY (user_id, product_id)\n",
    "        \n",
    "        ❌ Over-partitioning\n",
    "        partitionBy(\"date\", \"hour\", \"user_id\")  # Millones de particiones!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Compaction\": \"\"\"\n",
    "        Auto-compact para streaming:\n",
    "        .option(\"autoCompact\", \"true\")\n",
    "        \n",
    "        Scheduled OPTIMIZE:\n",
    "        OPTIMIZE events WHERE date >= current_date() - 7\n",
    "        \n",
    "        VACUUM para cleanup:\n",
    "        VACUUM events RETAIN 168 HOURS  # 7 días\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution\": \"\"\"\n",
    "        Always mergeSchema para backward compatibility:\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        Schema validation:\n",
    "        df.write.format(\"delta\")\n",
    "          .option(\"enforceSchema\", \"true\")\n",
    "          .save()\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Monitoring\": \"\"\"\n",
    "        Metrics to track:\n",
    "        - Write throughput (records/s)\n",
    "        - Read latency (p50, p95, p99)\n",
    "        - File count (small files problem)\n",
    "        - Version count (retention policy)\n",
    "        - Transaction log size\n",
    "        \n",
    "        Alerts:\n",
    "        - File count >100K per partition\n",
    "        - Transaction log >10 MB\n",
    "        - Write latency p99 >5s\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fb62d",
   "metadata": {},
   "source": [
    "### 🌐 **Data Mesh: Paradigma Organizacional Descentralizado**\n",
    "\n",
    "**Problema: Data Platform Centralizado (Monolito)**\n",
    "\n",
    "```\n",
    "Organización tradicional:\n",
    "┌────────────────────────────────────────────────┐\n",
    "│         CENTRAL DATA TEAM (20 personas)        │\n",
    "│                                                 │\n",
    "│  Responsable de:                                │\n",
    "│  • Todos los pipelines                         │\n",
    "│  • Todos los data models                       │\n",
    "│  • Todos los dashboards                        │\n",
    "│  • Todos los ML models                         │\n",
    "└────────────┬───────────────────────────────────┘\n",
    "             │\n",
    "   ┌─────────┴─────────┐\n",
    "   │  DATA WAREHOUSE   │\n",
    "   │   (Single DB)     │\n",
    "   └─────────┬─────────┘\n",
    "             │\n",
    "    ┌────────┴────────┐\n",
    "    │                 │\n",
    "    ▼                 ▼\n",
    "┌────────┐      ┌──────────┐\n",
    "│ Ventas │      │Logística │ ... (10+ dominios)\n",
    "│ Team   │      │ Team     │\n",
    "└────────┘      └──────────┘\n",
    "   ↓                 ↓\n",
    "Request            Request\n",
    "pipeline          pipeline\n",
    "   ↓                 ↓\n",
    "  ⏰ 3 meses        ⏰ 3 meses\n",
    "  🐛 Bugs           🐛 Bugs\n",
    "  📉 Low quality    📉 Low quality\n",
    "\n",
    "Problemas:\n",
    "❌ Bottleneck: Central team sobrecargado (backlog 6+ meses)\n",
    "❌ Context loss: Data team no entiende dominio de negocio\n",
    "❌ Scalability: Imposible crecer linearmente\n",
    "❌ Ownership: Nadie responsable de calidad de datos\n",
    "```\n",
    "\n",
    "**Solución: Data Mesh (Zhamak Dehghani, 2019)**\n",
    "\n",
    "```\n",
    "Data Mesh: Descentralización por dominio\n",
    "┌─────────────────────────────────────────────┐\n",
    "│        FEDERATED GOVERNANCE                  │\n",
    "│  (Políticas centrales, enforcement local)    │\n",
    "└────┬──────────────┬──────────────┬───────────┘\n",
    "     │              │              │\n",
    "┌────▼─────┐  ┌────▼─────┐  ┌────▼─────┐\n",
    "│ VENTAS   │  │LOGÍSTICA │  │ FINANZAS │\n",
    "│ DOMAIN   │  │ DOMAIN   │  │ DOMAIN   │\n",
    "│          │  │          │  │          │\n",
    "│ Data     │  │ Data     │  │ Data     │\n",
    "│ Product  │  │ Product  │  │ Product  │\n",
    "│ Owner    │  │ Owner    │  │ Owner    │\n",
    "│          │  │          │  │          │\n",
    "│ • ETL    │  │ • ETL    │  │ • ETL    │\n",
    "│ • Quality│  │ • Quality│  │ • Quality│\n",
    "│ • API    │  │ • API    │  │ • API    │\n",
    "│ • Docs   │  │ • Docs   │  │ • Docs   │\n",
    "└────┬─────┘  └────┬─────┘  └────┬─────┘\n",
    "     │              │              │\n",
    "     └──────────────┴──────────────┘\n",
    "                    │\n",
    "         ┌──────────▼──────────┐\n",
    "         │ SELF-SERVE PLATFORM │\n",
    "         │ (Infra común)       │\n",
    "         │ • Spark/Airflow     │\n",
    "         │ • Monitoring        │\n",
    "         │ • Data Catalog      │\n",
    "         └─────────────────────┘\n",
    "```\n",
    "\n",
    "**4 Principios Fundamentales:**\n",
    "\n",
    "```python\n",
    "principios_data_mesh = {\n",
    "    \"1. Domain-Oriented Decentralization\": \"\"\"\n",
    "        Datos pertenecen al dominio de negocio\n",
    "        \n",
    "        Ejemplo: E-commerce\n",
    "        ┌────────────────────────────────────────┐\n",
    "        │ DOMAIN: Ventas                          │\n",
    "        │ Owner: VP of Sales                      │\n",
    "        │ Data Products:                          │\n",
    "        │  • orders (transaccional)               │\n",
    "        │  • orders_aggregated (analítico)        │\n",
    "        │  • customer_segments (ML)               │\n",
    "        │                                         │\n",
    "        │ Team composition:                       │\n",
    "        │  • Product Manager (prioridades)        │\n",
    "        │  • Data Engineer (pipelines)            │\n",
    "        │  • Analytics Engineer (dbt models)      │\n",
    "        │  • Data Analyst (consumers)             │\n",
    "        └────────────────────────────────────────┘\n",
    "        \n",
    "        Ventajas:\n",
    "        ✅ Domain expertise: Equipo entiende negocio\n",
    "        ✅ Velocity: No esperar central team\n",
    "        ✅ Accountability: Owner claro de calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Data as a Product\": \"\"\"\n",
    "        Cada dataset es un producto con:\n",
    "        \n",
    "        📋 SLA (Service Level Agreement):\n",
    "        - Latency: <15 min para orders_aggregated\n",
    "        - Availability: 99.9% uptime\n",
    "        - Freshness: Updated cada 5 min\n",
    "        - Quality: >95% completeness\n",
    "        \n",
    "        📖 Documentation:\n",
    "        - Schema: Columnas, tipos, constraints\n",
    "        - Lineage: De dónde viene cada campo\n",
    "        - Examples: Query samples\n",
    "        - Contact: Slack channel, owner email\n",
    "        \n",
    "        🔒 Access Control:\n",
    "        - Public: Todos pueden leer\n",
    "        - Private: Solo equipo ventas\n",
    "        - PII: Masked para no-autorizados\n",
    "        \n",
    "        📊 Observability:\n",
    "        - Metrics: Row count, size, update time\n",
    "        - Alerts: SLA violations\n",
    "        - Changelog: Version history\n",
    "        \n",
    "        Ejemplo manifest.yaml:\n",
    "        ```\n",
    "        product_name: orders_aggregated\n",
    "        owner: ventas-data@company.com\n",
    "        sla:\n",
    "          latency_minutes: 15\n",
    "          availability_percent: 99.9\n",
    "          quality_score: 0.95\n",
    "        schema_url: s3://catalog/ventas/orders_agg/schema.json\n",
    "        lineage:\n",
    "          sources:\n",
    "            - orders_raw\n",
    "            - customers\n",
    "            - products\n",
    "        access:\n",
    "          read: [sales_team, finance_team, executives]\n",
    "          write: [sales_data_engineers]\n",
    "        ```\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Self-Serve Data Platform\": \"\"\"\n",
    "        Infraestructura común para todos los dominios\n",
    "        \n",
    "        Platform Components:\n",
    "        ┌─────────────────────────────────────┐\n",
    "        │ COMPUTE                              │\n",
    "        │  • Spark clusters (on-demand)       │\n",
    "        │  • Airflow (orchestration)          │\n",
    "        │  • dbt (transformations)            │\n",
    "        └─────────────────────────────────────┘\n",
    "        \n",
    "        ┌─────────────────────────────────────┐\n",
    "        │ STORAGE                              │\n",
    "        │  • Delta Lake (lakehouse)           │\n",
    "        │  • S3 buckets (per-domain)          │\n",
    "        │  • Unity Catalog (metadata)         │\n",
    "        └─────────────────────────────────────┘\n",
    "        \n",
    "        ┌─────────────────────────────────────┐\n",
    "        │ OBSERVABILITY                        │\n",
    "        │  • Datadog (metrics, logs)          │\n",
    "        │  • Great Expectations (quality)     │\n",
    "        │  • OpenLineage (lineage tracking)   │\n",
    "        └─────────────────────────────────────┘\n",
    "        \n",
    "        ┌─────────────────────────────────────┐\n",
    "        │ GOVERNANCE                           │\n",
    "        │  • Data Catalog (Amundsen/DataHub)  │\n",
    "        │  • Access Control (Ranger/Unity)    │\n",
    "        │  • Policy Engine (OPA)              │\n",
    "        └─────────────────────────────────────┘\n",
    "        \n",
    "        Self-service workflow:\n",
    "        1. Team crea repo: github.com/company/ventas-data\n",
    "        2. Define product: manifest.yaml + dbt models\n",
    "        3. CI/CD deploy: Tests → Stage → Prod\n",
    "        4. Auto-register: Catalog actualizado\n",
    "        5. Monitoring: Dashboards auto-generated\n",
    "        \n",
    "        Platform team provee:\n",
    "        - Templates (cookiecutter)\n",
    "        - Best practices docs\n",
    "        - On-call support\n",
    "        - Cost monitoring\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Federated Computational Governance\": \"\"\"\n",
    "        Políticas globales + Autonomía local\n",
    "        \n",
    "        GLOBAL POLICIES (Central governance):\n",
    "        ┌────────────────────────────────────┐\n",
    "        │ 1. Security                         │\n",
    "        │    • PII must be encrypted at rest │\n",
    "        │    • Access logs retained 1 year   │\n",
    "        │    • MFA required for production   │\n",
    "        │                                     │\n",
    "        │ 2. Compliance (GDPR, CCPA)         │\n",
    "        │    • Right to be forgotten         │\n",
    "        │    • Data retention max 7 years    │\n",
    "        │    • Audit trail required          │\n",
    "        │                                     │\n",
    "        │ 3. Quality                          │\n",
    "        │    • All products have tests       │\n",
    "        │    • SLA violations reported       │\n",
    "        │    • Schema changes versioned      │\n",
    "        │                                     │\n",
    "        │ 4. Interoperability                │\n",
    "        │    • Standard formats (Parquet)    │\n",
    "        │    • Common IDs (user_id format)   │\n",
    "        │    • Shared ontology (terms)       │\n",
    "        └────────────────────────────────────┘\n",
    "        \n",
    "        LOCAL AUTONOMY (Domain teams):\n",
    "        ┌────────────────────────────────────┐\n",
    "        │ • Choose transformation tools       │\n",
    "        │   (dbt, Spark, Python)             │\n",
    "        │ • Define data models               │\n",
    "        │ • Set refresh frequency            │\n",
    "        │ • Manage access within domain      │\n",
    "        │ • Optimize costs                   │\n",
    "        └────────────────────────────────────┘\n",
    "        \n",
    "        Enforcement:\n",
    "        - Automated checks in CI/CD\n",
    "        - Policy-as-code (OPA policies)\n",
    "        - Failing checks block deploy\n",
    "        - Dashboards show compliance\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Implementación Real: Data Mesh Stack**\n",
    "\n",
    "```python\n",
    "# Estructura organizacional\n",
    "data_mesh_org = {\n",
    "    \"Domain Teams (Autónomos)\": {\n",
    "        \"Ventas Domain\": {\n",
    "            \"Members\": \"8 personas\",\n",
    "            \"Products\": [\n",
    "                \"orders (transactional)\",\n",
    "                \"orders_daily_agg (analytical)\",\n",
    "                \"customer_rfm_segments (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Spark, dbt, Delta Lake\",\n",
    "            \"Budget\": \"$50K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Logística Domain\": {\n",
    "            \"Members\": \"6 personas\",\n",
    "            \"Products\": [\n",
    "                \"shipments (operational)\",\n",
    "                \"delivery_performance (metrics)\",\n",
    "                \"route_optimization (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Flink, Kafka, Iceberg\",\n",
    "            \"Budget\": \"$40K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Marketing Domain\": {\n",
    "            \"Members\": \"5 personas\",\n",
    "            \"Products\": [\n",
    "                \"campaigns (metadata)\",\n",
    "                \"attribution (analytics)\",\n",
    "                \"propensity_scores (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Airflow, Python, Delta Lake\",\n",
    "            \"Budget\": \"$30K/mes\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Platform Team (Enabling)\": {\n",
    "        \"Members\": \"10 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Maintain Spark/Airflow infrastructure\",\n",
    "            \"Operate Data Catalog\",\n",
    "            \"Enforce governance policies\",\n",
    "            \"Cost optimization\",\n",
    "            \"Training & documentation\"\n",
    "        ],\n",
    "        \"Budget\": \"$100K/mes\",\n",
    "        \"Ratio\": \"1 platform engineer per 20 domain engineers\"\n",
    "    },\n",
    "    \n",
    "    \"Governance Team (Federated)\": {\n",
    "        \"Members\": \"3 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Define global policies\",\n",
    "            \"Compliance (GDPR, SOX)\",\n",
    "            \"Audit & reporting\",\n",
    "            \"Cross-domain standards\"\n",
    "        ],\n",
    "        \"Budget\": \"$20K/mes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data Product Example\n",
    "data_product_spec = '''\n",
    "# orders_daily_agg Data Product\n",
    "\n",
    "## Overview\n",
    "Daily aggregated orders for BI dashboards and reporting.\n",
    "\n",
    "## Owner\n",
    "- Team: Ventas Data Engineering\n",
    "- Contact: ventas-data@company.com\n",
    "- Slack: #ventas-data\n",
    "\n",
    "## SLA\n",
    "- Freshness: Updated daily @6AM UTC\n",
    "- Latency: <30 minutes processing time\n",
    "- Availability: 99.9% (max 8.76h downtime/year)\n",
    "- Quality: >99% completeness, >98% accuracy\n",
    "\n",
    "## Schema\n",
    "| Column | Type | Description | PII |\n",
    "|--------|------|-------------|-----|\n",
    "| date | DATE | Order date | No |\n",
    "| country | STRING | Country code (ISO) | No |\n",
    "| total_orders | BIGINT | Count of orders | No |\n",
    "| total_revenue | DECIMAL(10,2) | Sum of revenue (USD) | No |\n",
    "| avg_order_value | DECIMAL(10,2) | Average order value | No |\n",
    "\n",
    "## Lineage\n",
    "```\n",
    "orders_raw (Bronze)\n",
    "    ↓\n",
    "orders_validated (Silver)\n",
    "    ↓\n",
    "orders_daily_agg (Gold) ← THIS PRODUCT\n",
    "```\n",
    "\n",
    "## Access\n",
    "- Read: sales_team, finance_team, executives\n",
    "- Write: sales_data_engineers\n",
    "- Admin: sales_domain_owner\n",
    "\n",
    "## Usage Examples\n",
    "```sql\n",
    "-- Get last 7 days\n",
    "SELECT * FROM orders_daily_agg\n",
    "WHERE date >= CURRENT_DATE - INTERVAL 7 DAYS\n",
    "ORDER BY date DESC\n",
    "\n",
    "-- YoY comparison\n",
    "SELECT \n",
    "    date,\n",
    "    total_revenue,\n",
    "    LAG(total_revenue, 365) OVER (ORDER BY date) as prev_year_revenue\n",
    "FROM orders_daily_agg\n",
    "WHERE date >= '2024-01-01'\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "- Dashboard: https://monitoring.company.com/ventas/orders_agg\n",
    "- Alerts: PagerDuty team \"sales-data-oncall\"\n",
    "\n",
    "## Changelog\n",
    "- 2025-10-01: Added avg_order_value column\n",
    "- 2025-09-15: Changed country to ISO codes\n",
    "- 2025-08-01: Initial release\n",
    "'''\n",
    "```\n",
    "\n",
    "**Ventajas de Data Mesh:**\n",
    "\n",
    "```python\n",
    "ventajas_mesh = {\n",
    "    \"1. Scalability\": \"\"\"\n",
    "        Tradicional: Central team → linear growth impossible\n",
    "        10 domains × 1 central team = bottleneck\n",
    "        \n",
    "        Data Mesh: Domain teams → linear scaling\n",
    "        10 domains × 10 teams = 100 engineers productive\n",
    "        \n",
    "        Growth: Agregar dominio no afecta otros\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Domain Expertise\": \"\"\"\n",
    "        Central team:\n",
    "        \"¿Qué significa 'adjusted_revenue'?\"\n",
    "        → Ask business (delays)\n",
    "        \n",
    "        Domain team:\n",
    "        Engineers trabajan con business daily\n",
    "        → Deep understanding, mejor calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Velocity\": \"\"\"\n",
    "        Backlog central: 6 meses\n",
    "        Domain ownership: 2 semanas\n",
    "        \n",
    "        Feature request → deploy:\n",
    "        Central: 3-6 meses\n",
    "        Mesh: 1-2 sprints\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Quality\": \"\"\"\n",
    "        Central team: Best effort (no ownership)\n",
    "        Domain team: Accountable (SLAs, metrics)\n",
    "        \n",
    "        Data quality:\n",
    "        Central: 85% average\n",
    "        Mesh: 95% average (incentivized)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desafíos de Data Mesh:**\n",
    "\n",
    "```python\n",
    "desafios_mesh = {\n",
    "    \"1. Organizational Maturity\": \"\"\"\n",
    "        Requiere:\n",
    "        - DevOps culture (CI/CD, testing)\n",
    "        - Data literacy en equipos de negocio\n",
    "        - Executive buy-in (budget descentralizado)\n",
    "        \n",
    "        NO funciona si:\n",
    "        - Organización muy jerárquica\n",
    "        - Equipos no autónomos\n",
    "        - Sin cultura de ownership\n",
    "        \n",
    "        Ejemplo fracaso:\n",
    "        Company X intentó mesh sin DevOps\n",
    "        → Dominios no saben deploy pipelines\n",
    "        → Rollback a central team en 6 meses\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Platform Complexity\": \"\"\"\n",
    "        Self-serve platform = sophisticated\n",
    "        \n",
    "        Requiere:\n",
    "        - Multi-tenancy (aislamiento dominios)\n",
    "        - Cost tracking per domain\n",
    "        - Automated provisioning\n",
    "        - Standardized observability\n",
    "        \n",
    "        Platform team: 10-15 personas mínimo\n",
    "        Cost: $1M+/año (salaries + infra)\n",
    "        \n",
    "        Break-even: ~100+ data engineers total\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Duplication Risk\": \"\"\"\n",
    "        Sin governance fuerte:\n",
    "        - 3 equipos crean \"customer_dim\" diferente\n",
    "        - Métricas inconsistentes cross-domain\n",
    "        - Redundant data copies (cost)\n",
    "        \n",
    "        Mitigación:\n",
    "        - Data Catalog mandatory\n",
    "        - Shared domain for common entities\n",
    "        - Regular cross-domain sync meetings\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Skillset Gap\": \"\"\"\n",
    "        Domain engineers necesitan:\n",
    "        - Data engineering (ETL)\n",
    "        - Analytics engineering (dbt)\n",
    "        - DevOps (CI/CD)\n",
    "        - Domain knowledge (business)\n",
    "        \n",
    "        Hiring difficult: Full-stack data engineers\n",
    "        Training: 6-12 meses ramp-up\n",
    "        \n",
    "        Alternative: Embedded specialists\n",
    "        - 1 data engineer per 2 domains\n",
    "        - Rotates between teams\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso (Cuándo Data Mesh Funciona):**\n",
    "\n",
    "```python\n",
    "# ✅ 1. LARGE ORG con múltiples dominios\n",
    "\"\"\"\n",
    "Empresa: Zalando (E-commerce Europeo)\n",
    "Tamaño: 10,000+ empleados, 20+ países\n",
    "Dominios: 50+ (Fashion, Logistics, Payments, ...)\n",
    "\n",
    "Data Mesh adoption (2020):\n",
    "- 50 domain teams autónomos\n",
    "- Platform team: 25 personas\n",
    "- 200+ data products published\n",
    "- Self-serve: 90% requests no need central\n",
    "\n",
    "Results:\n",
    "- Time-to-market: 6 meses → 2 semanas\n",
    "- Data quality: 80% → 95%\n",
    "- Team satisfaction: +40%\n",
    "\"\"\"\n",
    "\n",
    "# ✅ 2. DISTRIBUTED company\n",
    "\"\"\"\n",
    "Empresa: Confluent (remote-first)\n",
    "Dominios: Customer Success, Engineering, Sales\n",
    "Challenge: Central team = timezone hell\n",
    "\n",
    "Data Mesh benefits:\n",
    "- Teams own data in their timezone\n",
    "- Async collaboration (via catalog)\n",
    "- No central bottleneck\n",
    "\"\"\"\n",
    "\n",
    "# ❌ 3. SMALL STARTUP (<100 personas)\n",
    "\"\"\"\n",
    "Mesh overhead > benefits\n",
    "Better: Small central team (5 personas)\n",
    "Mesh when: Reach 200+ engineers\n",
    "\"\"\"\n",
    "\n",
    "# ❌ 4. HIGHLY REGULATED (sin autonomía)\n",
    "\"\"\"\n",
    "Banking: Regulators require central control\n",
    "Mesh autonomy conflicts with compliance\n",
    "Better: Centralized with strong governance\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migration Path: Central → Data Mesh**\n",
    "\n",
    "```python\n",
    "migration_roadmap = {\n",
    "    \"Year 1: Foundation\": \"\"\"\n",
    "        Q1: Executive alignment\n",
    "        - Present mesh vision\n",
    "        - Secure budget\n",
    "        - Identify pilot domain\n",
    "        \n",
    "        Q2: Platform MVP\n",
    "        - Self-serve Spark/Airflow\n",
    "        - Data Catalog (Amundsen)\n",
    "        - Basic CI/CD templates\n",
    "        \n",
    "        Q3-Q4: Pilot domain\n",
    "        - Choose 1 domain (e.g., Sales)\n",
    "        - Migrate 3 products\n",
    "        - Learn lessons, iterate\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 2: Scale\": \"\"\"\n",
    "        Q1-Q2: Onboard 5 domains\n",
    "        - Training programs\n",
    "        - Platform improvements\n",
    "        - Governance policies drafted\n",
    "        \n",
    "        Q3-Q4: Federated governance\n",
    "        - Policy engine (OPA)\n",
    "        - Compliance automation\n",
    "        - Cross-domain standards\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 3: Maturity\": \"\"\"\n",
    "        Q1-Q2: Onboard remaining domains\n",
    "        - 100% domains on mesh\n",
    "        - Decommission central team legacy\n",
    "        \n",
    "        Q3-Q4: Optimization\n",
    "        - Cost optimization\n",
    "        - Advanced features (ML platform)\n",
    "        - Community of practice\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total Investment\": \"\"\"\n",
    "        Platform team: 10 personas × $150K × 3 años = $4.5M\n",
    "        Training: $500K\n",
    "        Tooling: $1M\n",
    "        Total: $6M\n",
    "        \n",
    "        Break-even: Year 4 (velocity gains)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0bd31",
   "metadata": {},
   "source": [
    "- Batch layer: histórico completo, recalculable, alta latencia (Spark/Hadoop).\n",
    "- Speed layer: streaming incremental, baja latencia (Kafka/Flink/Spark Streaming).\n",
    "- Serving layer: merge de vistas batch + speed para consultas (Druid/Cassandra/BigQuery).\n",
    "- Trade-offs: doble lógica (batch + stream), complejidad operacional, eventual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_diagram = '''\n",
    "┌────────────┐\n",
    "│   Data     │\n",
    "│  Sources   │\n",
    "└─────┬──────┘\n",
    "      │\n",
    "   ┌──┴───┐\n",
    "   │Queue │ (Kafka)\n",
    "   └──┬───┘\n",
    "      │\n",
    " ┌────┴─────┐\n",
    " │  Batch   │  (Spark jobs nocturnos)\n",
    " │  Layer   │\n",
    " └────┬─────┘\n",
    "      │\n",
    "   Master\n",
    "   Dataset\n",
    "      │\n",
    "   ┌──┴───┐\n",
    "   │Serving│ (Druid/BigQuery)\n",
    "   │ Layer │\n",
    "   └──────┘\n",
    "      ↑\n",
    "   ┌──┴───┐\n",
    "   │Speed │ (Flink/Spark Streaming)\n",
    "   │Layer │\n",
    "   └──────┘\n",
    "'''\n",
    "print(lambda_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a99dd",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9117d8",
   "metadata": {},
   "source": [
    "- Un solo pipeline streaming para todo (batch = replay del log).\n",
    "- Simplifica operaciones y lógica unificada.\n",
    "- Requiere log infinito (Kafka con retención larga) y capacidad de reprocessing.\n",
    "- Trade-offs: coste de almacenamiento del log, complejidad del estado, menor optimización batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a06c00",
   "metadata": {},
   "source": [
    "## 3. Arquitectura Delta (Lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11be385",
   "metadata": {},
   "source": [
    "- Unifica batch y streaming sobre un único storage transaccional (Delta/Iceberg).\n",
    "- Streaming escribe micro-batches transaccionales; batch lee histórico con time travel.\n",
    "- Elimina duplicación de código y simplifica gobernanza (un catálogo, un linaje).\n",
    "- Trade-offs: requiere adopción de formato y motor compatible (Spark/Trino)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab620719",
   "metadata": {},
   "source": [
    "## 4. Data Mesh: paradigma organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851967",
   "metadata": {},
   "source": [
    "- Descentralización de la propiedad de datos por dominio de negocio.\n",
    "- Data products: APIs/tablas con SLOs, documentación, linaje y contratos.\n",
    "- Plataforma self-service: herramientas comunes (CI/CD, catálogo, observabilidad).\n",
    "- Gobernanza federada: políticas globales + autonomía local.\n",
    "- Trade-offs: requiere madurez organizacional, infraestructura sólida, cambio cultural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0232c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_principles = '''\n",
    "1. Domain-oriented decentralization: equipos de dominio (ventas, logística, finanzas) son dueños de sus datos.\n",
    "2. Data as a product: cada dataset tiene owner, SLO, versionado, documentación y calidad garantizada.\n",
    "3. Self-serve platform: infraestructura común (orquestación, observabilidad, catálogo) sin silos.\n",
    "4. Federated governance: políticas de seguridad, privacidad y calidad definidas centralmente, aplicadas localmente.\n",
    "'''\n",
    "print(mesh_principles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5c55f",
   "metadata": {},
   "source": [
    "## 5. Comparación y cuándo elegir cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comp = pd.DataFrame([\n",
    "  {'Arquitectura':'Lambda', 'Latencia':'Batch: horas, Speed: segundos', 'Complejidad':'Alta (doble lógica)', 'Casos':'Legacy con requerimientos mixtos'},\n",
    "  {'Arquitectura':'Kappa', 'Latencia':'Baja (streaming)', 'Complejidad':'Media (un pipeline)', 'Casos':'Todo es streaming, reprocessing factible'},\n",
    "  {'Arquitectura':'Delta', 'Latencia':'Baja-Media (micro-batch)', 'Complejidad':'Media (un storage)', 'Casos':'Lakehouse, unificación batch/stream'},\n",
    "  {'Arquitectura':'Data Mesh', 'Latencia':'Variable (por dominio)', 'Complejidad':'Alta (organizacional)', 'Casos':'Grandes org con múltiples dominios'}\n",
    "])\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cc56e",
   "metadata": {},
   "source": [
    "## 6. Ejercicio de diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b272baf",
   "metadata": {},
   "source": [
    "Diseña la arquitectura de datos para un e-commerce con:\n",
    "- Dominio Ventas: transacciones en tiempo real (Kafka).\n",
    "- Dominio Logística: batch nocturno (inventario/envíos).\n",
    "- Dominio Analítica: dashboards con latencia < 5 min.\n",
    "- Requisitos: auditabilidad, GDPR, linaje, costos optimizados.\n",
    "\n",
    "Responde: ¿Lambda, Kappa, Delta o híbrida? ¿Data Mesh aplicable? Justifica con trade-offs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
