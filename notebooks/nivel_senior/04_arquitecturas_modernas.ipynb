{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb7bbce",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh\n",
    "\n",
    "Objetivo: comprender y contrastar arquitecturas de referencia (Lambda, Kappa, Delta) y patrones organizacionales (Data Mesh), con ejemplos de diseÃ±o y trade-offs.\n",
    "\n",
    "- DuraciÃ³n: 120â€“150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Mid completo, experiencia con batch y streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2042cc",
   "metadata": {},
   "source": [
    "## 1. Arquitectura Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a450ff1",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ **Lambda Architecture: Batch + Speed Layer**\n",
    "\n",
    "**Origen y MotivaciÃ³n (Nathan Marz, 2011):**\n",
    "\n",
    "```\n",
    "Problema clÃ¡sico:\n",
    "- Batch processing: Preciso pero lento (horas/dÃ­as)\n",
    "- Stream processing: RÃ¡pido pero complejo (estado, fallos)\n",
    "\n",
    "SoluciÃ³n Lambda:\n",
    "- Batch Layer: Verdad absoluta, inmutable, reprocessable\n",
    "- Speed Layer: AproximaciÃ³n rÃ¡pida, eventualmente consistente\n",
    "- Serving Layer: Merge de ambas vistas\n",
    "```\n",
    "\n",
    "**Arquitectura Completa:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     DATA SOURCES                             â”‚\n",
    "â”‚  â€¢ Kafka Topics                                              â”‚\n",
    "â”‚  â€¢ Database CDC (Debezium)                                   â”‚\n",
    "â”‚  â€¢ Application Logs                                          â”‚\n",
    "â”‚  â€¢ IoT Streams                                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                    â”‚\n",
    "        â–¼                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ BATCH LAYER  â”‚    â”‚ SPEED LAYER  â”‚\n",
    "â”‚              â”‚    â”‚              â”‚\n",
    "â”‚ â€¢ Spark Batchâ”‚    â”‚ â€¢ Flink      â”‚\n",
    "â”‚ â€¢ Hadoop MR  â”‚    â”‚ â€¢ Spark SS   â”‚\n",
    "â”‚ â€¢ Hive       â”‚    â”‚ â€¢ Storm      â”‚\n",
    "â”‚              â”‚    â”‚              â”‚\n",
    "â”‚ Runs: Daily  â”‚    â”‚ Runs: Real-  â”‚\n",
    "â”‚       @2AM   â”‚    â”‚       time   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                   â”‚\n",
    "       â”‚ Master Dataset    â”‚ Delta Views\n",
    "       â”‚ (Immutable)       â”‚ (Mutable)\n",
    "       â”‚                   â”‚\n",
    "       â–¼                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      SERVING LAYER               â”‚\n",
    "â”‚                                  â”‚\n",
    "â”‚  Query(t) = Batch(0â†’t-1h) +     â”‚\n",
    "â”‚             Speed(t-1hâ†’t)        â”‚\n",
    "â”‚                                  â”‚\n",
    "â”‚  â€¢ Druid                         â”‚\n",
    "â”‚  â€¢ Cassandra                     â”‚\n",
    "â”‚  â€¢ ElasticSearch                 â”‚\n",
    "â”‚  â€¢ BigQuery                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   API    â”‚\n",
    "        â”‚Dashboard â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Componentes Detallados:**\n",
    "\n",
    "**1. Batch Layer (Verdad Inmutable):**\n",
    "\n",
    "```python\n",
    "# CaracterÃ­sticas:\n",
    "# - Procesamiento completo del histÃ³rico\n",
    "# - Recalculable desde el inicio\n",
    "# - Optimizado para throughput (no latencia)\n",
    "# - Tolerancia a fallos simple (restart)\n",
    "\n",
    "# Ejemplo: Agregaciones diarias con Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchLayer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer ALL histÃ³rico (full recompute)\n",
    "raw_events = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"s3://datalake/raw/events/\")\n",
    "\n",
    "# Agregaciones pesadas\n",
    "user_stats_batch = raw_events \\\n",
    "    .groupBy(\"user_id\", \"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer\n",
    "user_stats_batch.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"s3://datalake/serving/user_stats_batch\")\n",
    "\n",
    "# Schedule: Daily @2AM (Airflow DAG)\n",
    "# Duration: 2-4 horas para procesar 1 aÃ±o de datos\n",
    "# Cost: $$$ (large cluster, pero solo 1x/dÃ­a)\n",
    "```\n",
    "\n",
    "**2. Speed Layer (Low Latency Incremental):**\n",
    "\n",
    "```python\n",
    "# CaracterÃ­sticas:\n",
    "# - Solo datos recientes (Ãºltimas horas)\n",
    "# - Baja latencia (<1 min)\n",
    "# - Estado mutable (agregaciones incrementales)\n",
    "# - Complejidad de exactly-once\n",
    "\n",
    "# Ejemplo: Streaming con Spark Structured Streaming\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Leer solo NUEVOS eventos\n",
    "events_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Misma lÃ³gica que batch (pero incremental)\n",
    "user_stats_speed = events_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer (diferentes tablas)\n",
    "query = user_stats_speed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/speed\") \\\n",
    "    .start(\"s3://datalake/serving/user_stats_speed\")\n",
    "\n",
    "# Runs: Continuously\n",
    "# Latency: 30s - 2 min\n",
    "# Cost: $$ (small cluster, pero 24/7)\n",
    "```\n",
    "\n",
    "**3. Serving Layer (Query Merge):**\n",
    "\n",
    "```python\n",
    "# Merge batch + speed en query time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_user_stats(user_id: str, as_of: datetime = None):\n",
    "    \"\"\"\n",
    "    Query que merge batch + speed layers\n",
    "    \"\"\"\n",
    "    if as_of is None:\n",
    "        as_of = datetime.now()\n",
    "    \n",
    "    # Batch boundary (tÃ­picamente hace 1-2 horas)\n",
    "    batch_cutoff = as_of - timedelta(hours=2)\n",
    "    \n",
    "    # Query batch layer (histÃ³rico hasta cutoff)\n",
    "    batch_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_batch\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND date < '{batch_cutoff.date()}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Query speed layer (reciente desde cutoff)\n",
    "    speed_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_speed\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND window.start >= '{batch_cutoff}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Merge results\n",
    "    total_events = batch_stats[0]['events'] + speed_stats[0]['events']\n",
    "    total_revenue = batch_stats[0]['revenue'] + speed_stats[0]['revenue']\n",
    "    \n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'total_events': total_events,\n",
    "        'total_revenue': total_revenue,\n",
    "        'as_of': as_of,\n",
    "        'batch_cutoff': batch_cutoff\n",
    "    }\n",
    "```\n",
    "\n",
    "**Ventajas de Lambda:**\n",
    "\n",
    "```python\n",
    "ventajas = {\n",
    "    \"1. Robustez\": \"\"\"\n",
    "        Batch layer es inmutable â†’ fÃ¡cil debugging\n",
    "        Speed layer falla â†’ aÃºn tienes batch como fallback\n",
    "        Disaster recovery: recompute desde raw data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. PrecisiÃ³n Garantizada\": \"\"\"\n",
    "        Batch garantiza exactitud (full recompute)\n",
    "        Speed solo para latencia, batch corrige errores\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Separation of Concerns\": \"\"\"\n",
    "        Batch: optimiza throughput (large partitions, columnar)\n",
    "        Speed: optimiza latencia (small batches, in-memory)\n",
    "        Cada uno usa tecnologÃ­a ideal para su caso\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. AuditorÃ­a\": \"\"\"\n",
    "        Raw data inmutable â†’ compliance (GDPR, SOX)\n",
    "        Puedes recompute histÃ³rico para auditorÃ­as\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas (Â¿Por quÃ© Lambda estÃ¡ en declive?):**\n",
    "\n",
    "```python\n",
    "desventajas = {\n",
    "    \"1. CÃ³digo Duplicado\": \"\"\"\n",
    "        MISMA lÃ³gica implementada 2 veces:\n",
    "        - batch_aggregations.py (Spark Batch)\n",
    "        - speed_aggregations.py (Spark Streaming)\n",
    "        \n",
    "        Cambio de lÃ³gica â†’ actualizar AMBOS\n",
    "        Testing â†’ doble esfuerzo\n",
    "        Bugs â†’ pueden divergir silenciosamente\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Complejidad Operacional\": \"\"\"\n",
    "        Mantener 2 pipelines:\n",
    "        - Batch: Airflow DAG, cluster management, retry logic\n",
    "        - Speed: Streaming query monitoring, checkpoint management\n",
    "        \n",
    "        2x infraestructura, 2x alertas, 2x oncall\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Eventual Consistency\": \"\"\"\n",
    "        Periodo de gracia donde batch/speed no alineados\n",
    "        \n",
    "        Ejemplo:\n",
    "        10:00 AM: Evento llega\n",
    "        10:01 AM: Speed layer procesa â†’ visible en dashboard\n",
    "        02:00 AM: Batch recompute â†’ corrige pequeÃ±os errores\n",
    "        \n",
    "        Usuario puede ver nÃºmeros ligeramente diferentes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Costos\": \"\"\"\n",
    "        Batch cluster: Large (100 nodes) @ 2-4h/dÃ­a\n",
    "        Speed cluster: Medium (20 nodes) @ 24/7\n",
    "        \n",
    "        Total: ~$50K/mes para org mediana\n",
    "        \n",
    "        vs Lakehouse unificado: ~$30K/mes\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales (CuÃ¡ndo Lambda Tiene Sentido):**\n",
    "\n",
    "```python\n",
    "# 1. LEGACY SYSTEMS con batch existente\n",
    "\"\"\"\n",
    "Empresa: Retail con 20 aÃ±os de Hadoop\n",
    "SituaciÃ³n: 500 Hive tables, cientos de Spark jobs\n",
    "Necesidad: Agregar real-time sin reescribir todo\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Mantener batch layer existente (no tocas legacy)\n",
    "- Agregar speed layer con Flink para casos crÃ­ticos\n",
    "- MigraciÃ³n gradual dominio por dominio\n",
    "\"\"\"\n",
    "\n",
    "# 2. ALTA PRECISIÃ“N requerida\n",
    "\"\"\"\n",
    "Empresa: Financial trading\n",
    "SituaciÃ³n: Regulaciones requieren recompute exacto\n",
    "Necesidad: AuditorÃ­as pueden pedir recalcular 5 aÃ±os atrÃ¡s\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Batch layer garantiza reproducibilidad exacta\n",
    "- Speed layer para trading real-time\n",
    "- Eventual consistency aceptable (batch corrige)\n",
    "\"\"\"\n",
    "\n",
    "# 3. MUY DIFERENTES SLAs batch vs stream\n",
    "\"\"\"\n",
    "Empresa: IoT con billones de sensores\n",
    "SituaciÃ³n: \n",
    "  - Alertas crÃ­ticas: <100ms (anomalÃ­as)\n",
    "  - AnÃ¡lisis histÃ³rico: dÃ­as OK (tendencias)\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Speed layer: Flink para alertas ultra-rÃ¡pidas\n",
    "- Batch layer: Hadoop/Hive para anÃ¡lisis pesados\n",
    "- TecnologÃ­as muy diferentes, difÃ­cil unificar\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**MigraciÃ³n de Lambda â†’ Modern Stack:**\n",
    "\n",
    "```python\n",
    "# Estrategia: Unified Batch+Stream con Delta Lake\n",
    "\n",
    "# ANTES (Lambda):\n",
    "# 1. Batch: Spark job diario (4h, 100 nodes)\n",
    "# 2. Speed: Flink continuous (24/7, 20 nodes)\n",
    "# 3. Serving: Druid (merge queries)\n",
    "\n",
    "# DESPUÃ‰S (Delta/Lakehouse):\n",
    "# 1. Unified: Spark Structured Streaming (24/7, 30 nodes)\n",
    "# 2. Micro-batches cada 5 min\n",
    "# 3. Delta Lake: ACID transactions, no merge needed\n",
    "\n",
    "# Migration timeline:\n",
    "\"\"\"\n",
    "Month 1-2: Setup Delta Lake infrastructure\n",
    "Month 3-4: Migrate batch pipelines to Delta\n",
    "Month 5-6: Migrate speed pipelines to Structured Streaming\n",
    "Month 7: Dual-run (Lambda + Delta) for validation\n",
    "Month 8: Cutover to Delta, decommission Lambda\n",
    "Month 9: Cleanup, optimize\n",
    "\n",
    "Total: 9 months\n",
    "Cost: $200K engineering + $50K infra\n",
    "Savings: $20K/mes ongoing (ROI: 2.5 years)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: LinkedIn (Inventor de Lambda):**\n",
    "\n",
    "```python\n",
    "# LinkedIn inventÃ³ Lambda en 2011\n",
    "# Usaron Lambda 2011-2018 (7 aÃ±os)\n",
    "\n",
    "arquitectura_linkedin = {\n",
    "    \"Batch Layer\": {\n",
    "        \"TecnologÃ­a\": \"Hadoop MapReduce â†’ Spark\",\n",
    "        \"Datos\": \"Kafka topics replicados a HDFS\",\n",
    "        \"Frecuencia\": \"Daily @midnight\",\n",
    "        \"Output\": \"Hive tables (member profiles, connections, jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Speed Layer\": {\n",
    "        \"TecnologÃ­a\": \"Storm â†’ Samza â†’ Kafka Streams\",\n",
    "        \"Datos\": \"Kafka topics directamente\",\n",
    "        \"Latencia\": \"<1 second\",\n",
    "        \"Output\": \"Espresso (NoSQL) para low-latency reads\"\n",
    "    },\n",
    "    \n",
    "    \"Serving Layer\": {\n",
    "        \"TecnologÃ­a\": \"Espresso + Voldemort (key-value stores)\",\n",
    "        \"Pattern\": \"API merge batch + speed en read time\"\n",
    "    },\n",
    "    \n",
    "    \"2018 Migration\": \"\"\"\n",
    "        LinkedIn migrÃ³ a Unified Streaming (Samza + Kafka)\n",
    "        RazÃ³n: Complejidad de mantener doble lÃ³gica\n",
    "        Resultado: -30% cÃ³digo, -40% operaciones\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd386004",
   "metadata": {},
   "source": [
    "### ğŸ”„ **Kappa Architecture: Stream-Only Simplification**\n",
    "\n",
    "**Origen (Jay Kreps, LinkedIn/Confluent, 2014):**\n",
    "\n",
    "```\n",
    "CrÃ­tica a Lambda:\n",
    "\"Â¿Por quÃ© mantener 2 sistemas si streaming puede hacer todo?\"\n",
    "\n",
    "Propuesta Kappa:\n",
    "- Eliminar batch layer completamente\n",
    "- Todo es streaming (batch = replay del log)\n",
    "- Inmutable log como single source of truth\n",
    "```\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         DATA SOURCES                     â”‚\n",
    "â”‚  â€¢ Applications                          â”‚\n",
    "â”‚  â€¢ Databases (CDC)                       â”‚\n",
    "â”‚  â€¢ IoT Devices                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    KAFKA (Immutable Log)                â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  â€¢ Infinite retention (or very long)    â”‚\n",
    "â”‚  â€¢ Partitioned & Replicated             â”‚\n",
    "â”‚  â€¢ Serves as \"Master Dataset\"           â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Topics:                                 â”‚\n",
    "â”‚  â”œâ”€ events (retention: 1 year)          â”‚\n",
    "â”‚  â”œâ”€ transactions (retention: 5 years)   â”‚\n",
    "â”‚  â””â”€ user_actions (retention: 90 days)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   STREAM PROCESSING                     â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Version 1: Flink Job                   â”‚\n",
    "â”‚  â”œâ”€ Aggregations                        â”‚\n",
    "â”‚  â”œâ”€ Joins                               â”‚\n",
    "â”‚  â””â”€ Output â†’ Cassandra                  â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Version 2: New logic needed            â”‚\n",
    "â”‚  â”œâ”€ Deploy new Flink job                â”‚\n",
    "â”‚  â”œâ”€ Replay from offset 0                â”‚\n",
    "â”‚  â””â”€ Output â†’ Cassandra v2               â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  ğŸ”„ Reprocessing = Replay log           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       SERVING LAYER                     â”‚\n",
    "â”‚  â€¢ Cassandra                            â”‚\n",
    "â”‚  â€¢ ElasticSearch                        â”‚\n",
    "â”‚  â€¢ Redis                                â”‚\n",
    "â”‚  â€¢ PostgreSQL                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Principios Clave:**\n",
    "\n",
    "```python\n",
    "principios_kappa = {\n",
    "    \"1. Everything is a Stream\": \"\"\"\n",
    "        No distinciÃ³n entre batch y stream\n",
    "        \n",
    "        Batch tradicional:\n",
    "        SELECT SUM(revenue) FROM sales WHERE date = '2025-10-30'\n",
    "        \n",
    "        Kappa:\n",
    "        Consume Kafka topic 'sales', aggregate, done\n",
    "        (No diferencia conceptual con real-time)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Immutable Log as Source of Truth\": \"\"\"\n",
    "        Kafka = Database of record\n",
    "        \n",
    "        Ventajas:\n",
    "        - Time travel: replay desde cualquier offset\n",
    "        - Debugging: reproduce bug con datos exactos\n",
    "        - Migration: deploy nueva versiÃ³n, replay, compare\n",
    "        \n",
    "        Ejemplo:\n",
    "        offset 0 â†’ offset 1M (1 aÃ±o de datos)\n",
    "        Reprocessing: ~4 horas con 50 partitions\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution in Log\": \"\"\"\n",
    "        Log contiene todos los schemas histÃ³ricos\n",
    "        \n",
    "        V1: {\"user_id\": 123, \"action\": \"click\"}\n",
    "        V2: {\"user_id\": 123, \"action\": \"click\", \"device\": \"mobile\"}\n",
    "        V3: {\"user_id\": 123, \"event_type\": \"click\", \"metadata\": {...}}\n",
    "        \n",
    "        Consumer handle all versions gracefully\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Reprocessing for Code Changes\": \"\"\"\n",
    "        Cambio de lÃ³gica â†’ no reescribir batch\n",
    "        \n",
    "        Workflow:\n",
    "        1. Deploy new stream processor (version 2)\n",
    "        2. Replay desde offset 0 (parallel con v1)\n",
    "        3. Validate output v2 matches expected\n",
    "        4. Cutover traffic to v2\n",
    "        5. Decommission v1\n",
    "        \n",
    "        Duration: horas/dÃ­as (no semanas de reescribir batch)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**ImplementaciÃ³n Real con Kafka + Flink:**\n",
    "\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.table.descriptors import Kafka, Schema, Json\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# ConfiguraciÃ³n Kafka source con retenciÃ³n larga\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        product_id STRING,\n",
    "        revenue DOUBLE,\n",
    "        event_timestamp TIMESTAMP(3),\n",
    "        WATERMARK FOR event_timestamp AS event_timestamp - INTERVAL '10' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'properties.group.id' = 'user-stats-processor-v2',\n",
    "        'scan.startup.mode' = 'earliest-offset',  -- Replay desde inicio\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Stream processing (unificado batch+stream)\n",
    "user_stats = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        TUMBLE_START(event_timestamp, INTERVAL '1' DAY) as day,\n",
    "        COUNT(*) as total_events,\n",
    "        SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,\n",
    "        COUNT(DISTINCT product_id) as unique_products\n",
    "    FROM events\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_timestamp, INTERVAL '1' DAY)\n",
    "\"\"\")\n",
    "\n",
    "# Output a Cassandra\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_stats_cassandra (\n",
    "        user_id STRING,\n",
    "        day TIMESTAMP(3),\n",
    "        total_events BIGINT,\n",
    "        total_revenue DOUBLE,\n",
    "        unique_products BIGINT,\n",
    "        PRIMARY KEY (user_id, day) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'cassandra',\n",
    "        'host' = 'cassandra',\n",
    "        'table-name' = 'user_stats'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "user_stats.execute_insert('user_stats_cassandra').wait()\n",
    "\n",
    "# Reprocessing:\n",
    "# 1. Stop old job\n",
    "# 2. Deploy this code (nueva lÃ³gica)\n",
    "# 3. Kafka consumer group lee desde offset 0\n",
    "# 4. Reprocesa 1 aÃ±o de datos en ~4 horas\n",
    "# 5. Nueva tabla user_stats_cassandra tiene datos corregidos\n",
    "```\n",
    "\n",
    "**ComparaciÃ³n Kafka Retention:**\n",
    "\n",
    "```python\n",
    "# ConfiguraciÃ³n tÃ­pica para Kappa\n",
    "kafka_retention = {\n",
    "    \"Short Retention (Anti-Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 604800000  # 7 dÃ­as\",\n",
    "        \"Disk\": \"1 TB / partition\",\n",
    "        \"Cost\": \"$500/mes\",\n",
    "        \"Reprocessing\": \"âŒ Solo Ãºltimos 7 dÃ­as disponibles\",\n",
    "        \"Use Case\": \"Lambda architecture (Kafka solo buffer)\"\n",
    "    },\n",
    "    \n",
    "    \"Long Retention (Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 31536000000  # 1 aÃ±o\",\n",
    "        \"Disk\": \"52 TB / partition (con compaction)\",\n",
    "        \"Cost\": \"$2,600/mes (S3 tiered storage)\",\n",
    "        \"Reprocessing\": \"âœ… 1 aÃ±o completo disponible\",\n",
    "        \"Use Case\": \"Kappa architecture (Kafka es source of truth)\"\n",
    "    },\n",
    "    \n",
    "    \"Infinite Retention (Extreme Kappa)\": {\n",
    "        \"Config\": \"retention.ms = -1  # Forever\",\n",
    "        \"Disk\": \"Ilimitado (requiere tiered storage)\",\n",
    "        \"Cost\": \"$5K/mes (S3 archival)\",\n",
    "        \"Reprocessing\": \"âœ… HistÃ³rico completo\",\n",
    "        \"Use Case\": \"Event sourcing, auditabilidad estricta\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Kafka Tiered Storage (KIP-405, Kafka 3.0+)\n",
    "# Archiva logs antiguos a S3, mantiene recientes en SSD\n",
    "\"\"\"\n",
    "kafka-storage.properties:\n",
    "  remote.log.storage.enable=true\n",
    "  remote.log.storage.manager.class=org.apache.kafka.server.log.remote.storage.RemoteLogStorageManager\n",
    "  \n",
    "Estructura:\n",
    "  Local SSD: Ãšltimos 7 dÃ­as (hot data)\n",
    "  S3 Bucket: >7 dÃ­as (cold data, comprimido)\n",
    "  \n",
    "Cost reduction: 70% vs all-SSD\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_kappa = {\n",
    "    \"1. Simplicidad Operacional\": \"\"\"\n",
    "        Un solo pipeline â†’ un solo sistema para monitorear\n",
    "        Un solo lenguaje/framework (Flink o Spark)\n",
    "        Un solo cluster para operar\n",
    "        \n",
    "        On-call rotation:\n",
    "        Lambda: 2 sistemas Ã— 2 equipos = 4 rotaciones/semana\n",
    "        Kappa: 1 sistema Ã— 1 equipo = 2 rotaciones/semana\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. No CÃ³digo Duplicado\": \"\"\"\n",
    "        Misma lÃ³gica para batch y stream\n",
    "        \n",
    "        Lambda: 2000 lÃ­neas batch.py + 2000 lÃ­neas stream.py = 4000\n",
    "        Kappa: 2000 lÃ­neas unified.py = 2000 (50% menos cÃ³digo)\n",
    "        \n",
    "        Bug fix: 1 cambio vs 2 cambios\n",
    "        Testing: 1 suite vs 2 suites\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Reprocessing RÃ¡pido\": \"\"\"\n",
    "        Cambio de lÃ³gica â†’ replay log en horas\n",
    "        \n",
    "        Lambda:\n",
    "        - Reescribir batch job (dÃ­as de dev)\n",
    "        - Ejecutar en cluster grande (4h)\n",
    "        - Validar, debug, iterar\n",
    "        Total: 1-2 semanas\n",
    "        \n",
    "        Kappa:\n",
    "        - Modificar stream job (horas de dev)\n",
    "        - Replay desde offset 0 (4h)\n",
    "        - Compare old vs new output\n",
    "        Total: 1 dÃ­a\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Exactly-Once MÃ¡s Simple\": \"\"\"\n",
    "        Kafka transactions + idempotent producers\n",
    "        \n",
    "        Flink + Kafka:\n",
    "        - Checkpointing en Kafka offsets\n",
    "        - Transactional writes a sinks\n",
    "        - No necesidad de merge batch+speed\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch tiene at-least-once (reintentos)\n",
    "        - Speed tiene exactly-once (complejo)\n",
    "        - Merge puede duplicar sin deduplication\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "desventajas_kappa = {\n",
    "    \"1. Costo de Almacenamiento\": \"\"\"\n",
    "        Kafka con 1 aÃ±o de retenciÃ³n = caro\n",
    "        \n",
    "        Ejemplo: 10K events/s Ã— 10 KB/event Ã— 1 aÃ±o\n",
    "        = 3.15 PB/aÃ±o crudo\n",
    "        Con compaction + tiered storage: 500 TB\n",
    "        Cost: $5K/mes S3 + $10K/mes Kafka brokers\n",
    "        \n",
    "        vs Hadoop HDFS: $2K/mes (pero sin streaming)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Reprocessing Largo\": \"\"\"\n",
    "        Replay 1 aÃ±o de datos = horas/dÃ­as\n",
    "        \n",
    "        1 TB data @ 100 MB/s = 2.8 horas ideal\n",
    "        Real (with processing): 6-12 horas\n",
    "        \n",
    "        Durante reprocessing:\n",
    "        - Nuevo cÃ³digo corre en paralelo (doble costo)\n",
    "        - Output dual (old version + new version)\n",
    "        - ValidaciÃ³n manual necesaria\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch recompute overnight (no impact users)\n",
    "        - Speed layer sigue sirviendo trÃ¡fico\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. State Management Complejo\": \"\"\"\n",
    "        Streaming state para 1 aÃ±o de datos = grande\n",
    "        \n",
    "        Ejemplo: User profiles con aggregations\n",
    "        10M users Ã— 1 KB state = 10 GB state\n",
    "        \n",
    "        Flink RocksDB:\n",
    "        - State en disco (slower)\n",
    "        - Checkpointing largo (minutes)\n",
    "        - Recovery largo si crash (restore state)\n",
    "        \n",
    "        Lambda batch:\n",
    "        - No state (stateless join con full tables)\n",
    "        - Simpler, pero mÃ¡s lento\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. No OptimizaciÃ³n por Caso\": \"\"\"\n",
    "        Streaming debe servir TODOS los casos\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch: Columnar Parquet, predicate pushdown, Z-order\n",
    "        - Speed: Row-based, in-memory, indexes\n",
    "        \n",
    "        Kappa:\n",
    "        - Un formato compromiso (no Ã³ptimo para ninguno)\n",
    "        - Queries complejos (joins 5 tables) lentos en stream\n",
    "        - Queries simples (count) overkill con stream\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Ideales para Kappa:**\n",
    "\n",
    "```python\n",
    "# 1. REAL-TIME FIRST con poco histÃ³rico\n",
    "\"\"\"\n",
    "Empresa: Fintech fraud detection\n",
    "Datos: 90 dÃ­as de transacciones\n",
    "Procesamiento: <100ms latency requerido\n",
    "Reprocessing: Semanal (ajustar modelos ML)\n",
    "\n",
    "Kappa perfecto:\n",
    "- 90 dÃ­as en Kafka (100 TB, manejable)\n",
    "- Flink streaming para detecciÃ³n real-time\n",
    "- Replay semanal para ajustar modelos (4h)\n",
    "\"\"\"\n",
    "\n",
    "# 2. EVENT SOURCING puro\n",
    "\"\"\"\n",
    "Empresa: Booking platform\n",
    "PatrÃ³n: Event sourcing (todo es evento inmutable)\n",
    "Datos: bookings, cancellations, modifications\n",
    "Queries: Reconstruir estado actual desde eventos\n",
    "\n",
    "Kappa natural:\n",
    "- Kafka como event store (immutable log)\n",
    "- Flink materializa vistas (current bookings)\n",
    "- Replay para nuevas vistas (add \"bookings by hotel\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. HIGH CHANGE FREQUENCY en lÃ³gica\n",
    "\"\"\"\n",
    "Empresa: Ad-tech con modelos A/B testing constante\n",
    "SituaciÃ³n: Cambio de lÃ³gica 2-3x/semana\n",
    "Necesidad: Deploy rÃ¡pido, validar, iterar\n",
    "\n",
    "Kappa ventaja:\n",
    "- Modify stream job (1h dev)\n",
    "- Deploy v2 parallel v1 (30 min)\n",
    "- Replay Ãºltimo dÃ­a (30 min)\n",
    "- Compare outputs, cutover (1h)\n",
    "Total: 3 horas por cambio vs 3 dÃ­as Lambda\n",
    "\"\"\"\n",
    "\n",
    "# 4. SMALL-MEDIUM data scale\n",
    "\"\"\"\n",
    "Empresa: SaaS startup\n",
    "Datos: <100 TB total\n",
    "Users: <1M\n",
    "Traffic: <1K events/s\n",
    "\n",
    "Kappa ventaja:\n",
    "- Un cluster Kafka+Flink (simple)\n",
    "- RetenciÃ³n 1 aÃ±o fÃ¡cil (10 TB en Kafka)\n",
    "- Equipo pequeÃ±o puede mantener\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Anti-Patterns (CuÃ¡ndo NO usar Kappa):**\n",
    "\n",
    "```python\n",
    "# âŒ 1. HUGE data scale con queries complejos\n",
    "\"\"\"\n",
    "Empresa: Retail con 10+ aÃ±os de histÃ³rico\n",
    "Datos: 100 PB en HDFS\n",
    "Queries: SQL complejo (20+ joins, window functions)\n",
    "\n",
    "Problema Kappa:\n",
    "- 100 PB en Kafka = $500K/mes (vs $50K HDFS)\n",
    "- Streaming joins lentos vs batch columnar\n",
    "- Replay 10 aÃ±os = semanas (inviable)\n",
    "\n",
    "Better: Lambda o Delta Lakehouse\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 2. BATCH-FIRST workloads\n",
    "\"\"\"\n",
    "Empresa: Data warehouse con reportes nocturnos\n",
    "Queries: ETL batch 90% workload, streaming 10%\n",
    "Latency: Horas OK para reportes\n",
    "\n",
    "Problema Kappa:\n",
    "- Overhead streaming para batch workloads\n",
    "- Kafka infraestructura innecesaria\n",
    "- Spark batch mÃ¡s eficiente que Spark Streaming para batch\n",
    "\n",
    "Better: Tradicional batch (Airflow + Spark)\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 3. ESTRICTA compliance con reprocessing prohibido\n",
    "\"\"\"\n",
    "Empresa: Banco con regulaciones estrictas\n",
    "Requerimiento: AuditorÃ­as requieren datos exactos \"as-of\"\n",
    "ProhibiciÃ³n: No se puede \"rehacer\" cÃ¡lculos pasados\n",
    "\n",
    "Problema Kappa:\n",
    "- Reprocessing es core feature (pero regulador dice no)\n",
    "- Inmutabilidad requiere batch layer aparte\n",
    "\n",
    "Better: Lambda con batch layer auditado/certificado\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: Uber (Kappa en ProducciÃ³n):**\n",
    "\n",
    "```python\n",
    "uber_kappa = {\n",
    "    \"Caso\": \"Surge pricing (precios dinÃ¡micos)\",\n",
    "    \n",
    "    \"Arquitectura\": {\n",
    "        \"Eventos\": \"ride_requests, driver_locations, ride_completions\",\n",
    "        \"Kafka\": \"500K events/s, retenciÃ³n 7 dÃ­as (suficiente)\",\n",
    "        \"Flink\": \"Windowed aggregations (supply/demand por Ã¡rea)\",\n",
    "        \"Output\": \"Redis (precios actuales), Cassandra (histÃ³rico)\",\n",
    "        \"Latency\": \"<200ms end-to-end\"\n",
    "    },\n",
    "    \n",
    "    \"Kappa Benefits\": \"\"\"\n",
    "        1. Cambio frecuente de algoritmo pricing (weekly)\n",
    "        2. Replay 7 dÃ­as para validar nuevo algoritmo (2h)\n",
    "        3. A/B testing: correr 2 versiones paralelo, compare\n",
    "        4. No batch layer needed (solo Ãºltimos dÃ­as relevantes)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Evolution\": \"\"\"\n",
    "        2015: Lambda (Spark batch + Storm streaming)\n",
    "        2016: Migrated to Kappa (Samza streaming only)\n",
    "        2018: Moved to Flink (better stateful processing)\n",
    "        2020: Hybrid (Kappa para real-time, Hudi para analytics)\n",
    "        \n",
    "        RazÃ³n hybrid: Analytics largo plazo necesita batch optimizations\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d7c4",
   "metadata": {},
   "source": [
    "### ğŸ  **Delta Architecture (Lakehouse): Unified Batch+Stream**\n",
    "\n",
    "**EvoluciÃ³n Natural (2020+):**\n",
    "\n",
    "```\n",
    "Lambda (2011): Batch + Speed layers separados\n",
    "   â†“\n",
    "Kappa (2014): Solo streaming (pero caro, complejo)\n",
    "   â†“\n",
    "Delta (2020): Unified sobre transactional storage\n",
    "```\n",
    "\n",
    "**Arquitectura Lakehouse:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATA SOURCES                              â”‚\n",
    "â”‚  â€¢ Applications â†’ Kafka                                      â”‚\n",
    "â”‚  â€¢ Databases â†’ CDC (Debezium)                                â”‚\n",
    "â”‚  â€¢ Batch files â†’ S3/ADLS                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚  INGESTION LAYER â”‚\n",
    "          â”‚                  â”‚\n",
    "          â”‚  Spark Streaming â”‚ â† Unified engine!\n",
    "          â”‚  (micro-batches) â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              DELTA LAKE / ICEBERG STORAGE                     â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Bronze (Raw)           Silver (Cleaned)      Gold (Curated) â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Append-onlyâ”‚        â”‚ Deduplicatedâ”‚      â”‚Aggregationsâ”‚  â”‚\n",
    "â”‚  â”‚ Parquet +  â”‚   â†’    â”‚ Validated   â”‚  â†’   â”‚Star Schema â”‚  â”‚\n",
    "â”‚  â”‚ Delta Log  â”‚        â”‚ Enriched    â”‚      â”‚Business    â”‚  â”‚\n",
    "â”‚  â”‚            â”‚        â”‚             â”‚      â”‚Logic       â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Features:                                                    â”‚\n",
    "â”‚  âœ… ACID Transactions                                        â”‚\n",
    "â”‚  âœ… Time Travel (rollback, audit)                           â”‚\n",
    "â”‚  âœ… Schema Evolution                                         â”‚\n",
    "â”‚  âœ… Upserts/Deletes                                          â”‚\n",
    "â”‚  âœ… Unified Batch+Stream reads                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚                    â”‚\n",
    "          â–¼                    â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ BATCH QUERIES â”‚    â”‚  STREAMING   â”‚\n",
    "  â”‚               â”‚    â”‚   QUERIES    â”‚\n",
    "  â”‚ Spark SQL     â”‚    â”‚ Spark SS     â”‚\n",
    "  â”‚ Presto/Trino  â”‚    â”‚ Flink        â”‚\n",
    "  â”‚ Athena        â”‚    â”‚ Real-time    â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                    â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   CONSUMPTION   â”‚\n",
    "            â”‚                 â”‚\n",
    "            â”‚  â€¢ BI Tools     â”‚\n",
    "            â”‚  â€¢ ML Training  â”‚\n",
    "            â”‚  â€¢ APIs         â”‚\n",
    "            â”‚  â€¢ Dashboards   â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**CaracterÃ­sticas Clave:**\n",
    "\n",
    "```python\n",
    "delta_features = {\n",
    "    \"1. ACID Transactions en Object Storage\": \"\"\"\n",
    "        Problema tradicional:\n",
    "        S3/ADLS no tienen transacciones\n",
    "        â†’ Race conditions, partial writes\n",
    "        \n",
    "        Delta Lake solution:\n",
    "        _delta_log/00000000000000000123.json\n",
    "        {\n",
    "          \"commitInfo\": {\"timestamp\": \"2025-10-30T10:30:00\"},\n",
    "          \"add\": [\n",
    "            {\"path\": \"part-00000.parquet\", \"size\": 1024000},\n",
    "            {\"path\": \"part-00001.parquet\", \"size\": 1024000}\n",
    "          ],\n",
    "          \"remove\": []\n",
    "        }\n",
    "        \n",
    "        Atomic commit: Write log entry = commit completo\n",
    "        Readers ven versiÃ³n consistente (snapshot isolation)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Unified Batch + Stream\": \"\"\"\n",
    "        SAME TABLE para batch y streaming\n",
    "        \n",
    "        Escritura streaming:\n",
    "        df.writeStream\n",
    "          .format(\"delta\")\n",
    "          .outputMode(\"append\")\n",
    "          .start(\"/delta/events\")\n",
    "        \n",
    "        Lectura batch:\n",
    "        spark.read\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")\n",
    "          .where(\"date = '2025-10-30'\")\n",
    "        \n",
    "        Lectura streaming:\n",
    "        spark.readStream\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")  # Lee cambios incrementales\n",
    "        \n",
    "        NO necesitas batch layer separada!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Time Travel\": \"\"\"\n",
    "        Acceso a versiones histÃ³ricas\n",
    "        \n",
    "        # Leer versiÃ³n especÃ­fica\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"versionAsOf\", 42)\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        # Leer timestamp especÃ­fico\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"timestampAsOf\", \"2025-10-29 00:00:00\")\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        Casos de uso:\n",
    "        - Rollback: Deployment malo â†’ revert\n",
    "        - Audit: \"Â¿QuÃ© datos vio el modelo ayer?\"\n",
    "        - Debugging: \"Reproduce bug con datos exactos\"\n",
    "        - Compliance: \"Muestra datos as-of Q3 2025\"\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Schema Evolution\": \"\"\"\n",
    "        Agregar columnas sin downtime\n",
    "        \n",
    "        V1: {user_id, action, timestamp}\n",
    "        V2: {user_id, action, timestamp, device}  â† Add column\n",
    "        \n",
    "        Delta:\n",
    "        df_v2.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .save(\"/delta/events\")\n",
    "        \n",
    "        Lectores antiguos: device = null\n",
    "        Lectores nuevos: device leÃ­do correctamente\n",
    "        \n",
    "        NO necesitas backfill!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Upserts (MERGE)\": \"\"\"\n",
    "        CDC y SCD Type 2 nativos\n",
    "        \n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            updates.alias(\"source\"),\n",
    "            \"target.user_id = source.user_id\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"email\": \"source.email\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).whenNotMatchedInsert(values={\n",
    "            \"user_id\": \"source.user_id\",\n",
    "            \"email\": \"source.email\",\n",
    "            \"created_at\": \"source.created_at\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).execute()\n",
    "        \n",
    "        Streaming UPSERT:\n",
    "        updates.writeStream\n",
    "          .foreachBatch(lambda df, _: upsert_function(df))\n",
    "          .start()\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Medallion Architecture (Bronze/Silver/Gold):**\n",
    "\n",
    "```python\n",
    "# BRONZE: Raw data, append-only\n",
    "bronze_events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").cast(\"string\").alias(\"raw_json\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "bronze_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(\"/delta/bronze/events\")\n",
    "\n",
    "# SILVER: Cleaned, validated, deduplicated\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "silver_events = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/bronze/events\") \\\n",
    "    .select(\n",
    "        from_json(col(\"raw_json\"), event_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\  # ValidaciÃ³n\n",
    "    .dropDuplicates([\"user_id\", \"event_timestamp\"])  # Dedup\n",
    "\n",
    "silver_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver\") \\\n",
    "    .start(\"/delta/silver/events\")\n",
    "\n",
    "# GOLD: Business aggregations\n",
    "gold_user_stats = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/silver/events\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 day\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"daily_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"event_types\")\n",
    "    )\n",
    "\n",
    "gold_user_stats.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/gold\") \\\n",
    "    .start(\"/delta/gold/user_daily_stats\")\n",
    "\n",
    "# BI Tool lee desde Gold\n",
    "df_dashboard = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/gold/user_daily_stats\") \\\n",
    "    .where(\"window.start >= current_date() - interval 7 days\")\n",
    "```\n",
    "\n",
    "**Ventajas sobre Lambda/Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_delta = {\n",
    "    \"vs Lambda\": {\n",
    "        \"CÃ³digo Unificado\": \"\"\"\n",
    "            Lambda: 2 pipelines (batch.py + stream.py)\n",
    "            Delta: 1 pipeline (unified.py)\n",
    "            \n",
    "            Reduction: 50% cÃ³digo, 50% tests, 50% bugs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Sin Merge Layer\": \"\"\"\n",
    "            Lambda: Query = merge(batch, speed) en serving\n",
    "            Delta: Query directo a Delta (single source)\n",
    "            \n",
    "            Latency: -100ms (no merge overhead)\n",
    "            Consistency: Guaranteed (ACID transactions)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Operaciones Simples\": \"\"\"\n",
    "            Lambda: Mantener 2 clusters + serving layer\n",
    "            Delta: 1 cluster Spark + Delta storage\n",
    "            \n",
    "            On-call: -50% alerts, -40% incidents\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Kappa\": {\n",
    "        \"Costo Storage\": \"\"\"\n",
    "            Kappa: Kafka 1 aÃ±o = $15K/mes\n",
    "            Delta: S3 object storage = $2K/mes\n",
    "            \n",
    "            Savings: 85% storage costs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Batch Optimized\": \"\"\"\n",
    "            Kappa: Streaming para todo (no Ã³ptimo)\n",
    "            Delta: Columnar Parquet + statistics\n",
    "            \n",
    "            Query performance: 10x faster para analytics\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Reprocessing Flexible\": \"\"\"\n",
    "            Kappa: Replay Kafka (limited retention)\n",
    "            Delta: Time travel infinito\n",
    "            \n",
    "            Auditabilidad: 5+ aÃ±os histÃ³rico disponible\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Traditional Warehouse\": {\n",
    "        \"Cost\": \"\"\"\n",
    "            Warehouse: $25/TB/month (Snowflake)\n",
    "            Delta: $0.023/GB/month (S3) = $23/TB\n",
    "            \n",
    "            Savings: ~50% at petabyte scale\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Flexibility\": \"\"\"\n",
    "            Warehouse: Vendor lock-in, SQL only\n",
    "            Delta: Open format, Spark/Presto/Flink/Python\n",
    "            \n",
    "            ML Integration: Native (same storage)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Scalability\": \"\"\"\n",
    "            Warehouse: Scale up (expensive)\n",
    "            Delta: Scale out (S3 infinite)\n",
    "            \n",
    "            Growth: No limits, pay-as-you-go\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "```python\n",
    "# 1. Databricks - Inventor de Delta Lake\n",
    "\"\"\"\n",
    "Cliente: Comcast (Telecomunicaciones)\n",
    "Datos: 2 PB diarios (network logs, streaming video, IoT)\n",
    "Arquitectura anterior: Lambda (Hadoop batch + Kafka streaming)\n",
    "\n",
    "Migration a Delta:\n",
    "- 2018: Pilot con 10 TB (1 mes)\n",
    "- 2019: Migration 500 TB (6 meses)\n",
    "- 2020: Full adoption 2 PB/dÃ­a\n",
    "\n",
    "Resultados:\n",
    "- Cost: -30% ($2M/aÃ±o savings)\n",
    "- Latency: 4h â†’ 15 min para dashboards\n",
    "- Code: -50% (unified batch+stream)\n",
    "- Incidents: -60% (ACID transactions)\n",
    "\n",
    "Stack:\n",
    "- Ingestion: Spark Streaming\n",
    "- Storage: Delta Lake en S3\n",
    "- Compute: Databricks clusters\n",
    "- Consumption: Tableau + ML models\n",
    "\"\"\"\n",
    "\n",
    "# 2. Riot Games (League of Legends)\n",
    "\"\"\"\n",
    "Caso: Game analytics + Player behavior\n",
    "Datos: 100M+ games/day, petabytes de events\n",
    "Latency: <5 min para anti-cheat, <1 hour para balance\n",
    "\n",
    "Delta Architecture:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Game Servers â”‚ â†’ Kafka (1M events/s)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Spark Stream â”‚ â†’ Delta Bronze (raw events)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Enrichment   â”‚ â†’ Delta Silver (validated)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "    â”Œâ”€â”€â”´â”€â”€â”\n",
    "    â”‚     â”‚\n",
    "    â–¼     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”\n",
    "â”‚ ML â”‚  â”‚ BI â”‚ â†’ Delta Gold (aggregations)\n",
    "â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜\n",
    "\n",
    "Benefits:\n",
    "- Unified: Game designers query same data ML uses\n",
    "- Time travel: \"Replay match from 2 days ago\"\n",
    "- Schema evolution: Add features without downtime\n",
    "\"\"\"\n",
    "\n",
    "# 3. Adobe Experience Platform\n",
    "\"\"\"\n",
    "Challenge: 100s of tenants, isolated data, compliance\n",
    "Solution: Delta Lake multi-tenant\n",
    "\n",
    "Architecture:\n",
    "/delta/tenant_001/events/\n",
    "/delta/tenant_002/events/\n",
    "...\n",
    "/delta/tenant_500/events/\n",
    "\n",
    "Features:\n",
    "- Row-level security (GDPR compliance)\n",
    "- Tenant isolation (Delta sharing)\n",
    "- Unified operations (single platform)\n",
    "- Time travel (data recovery per tenant)\n",
    "\n",
    "Scale:\n",
    "- 500+ tenants\n",
    "- 10 PB total data\n",
    "- 100K writes/s\n",
    "- <100ms p99 latency\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**MigraciÃ³n Lambda â†’ Delta:**\n",
    "\n",
    "```python\n",
    "migration_strategy = {\n",
    "    \"Phase 1: Setup (Month 1-2)\": \"\"\"\n",
    "        1. Provision Delta Lake infrastructure\n",
    "           - S3 bucket con versioning\n",
    "           - Unity Catalog setup\n",
    "           - Spark clusters (Databricks/EMR)\n",
    "        \n",
    "        2. Crear medallion structure\n",
    "           /delta/bronze/\n",
    "           /delta/silver/\n",
    "           /delta/gold/\n",
    "        \n",
    "        3. Setup CI/CD pipelines\n",
    "           - GitHub Actions para deploy\n",
    "           - Testing framework (pytest)\n",
    "           - Monitoring (Datadog/Prometheus)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 2: Bronze Layer (Month 3-4)\": \"\"\"\n",
    "        1. Migrate raw ingestion\n",
    "           Kafka â†’ Delta Bronze (append-only)\n",
    "           \n",
    "        2. Dual-write period\n",
    "           - Write to both Lambda batch + Delta\n",
    "           - Compare row counts, checksums\n",
    "           \n",
    "        3. Backfill histÃ³rico\n",
    "           HDFS/Hive â†’ Delta Bronze\n",
    "           (1-time copy, then delete HDFS)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 3: Silver Layer (Month 5-6)\": \"\"\"\n",
    "        1. Migrate transformation logic\n",
    "           Batch Spark jobs â†’ Delta pipelines\n",
    "           \n",
    "        2. Unified batch+stream\n",
    "           - Remove duplicate code\n",
    "           - Single pipeline para ambos\n",
    "           \n",
    "        3. Data quality checks\n",
    "           Great Expectations integration\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 4: Gold Layer (Month 7-8)\": \"\"\"\n",
    "        1. Migrate aggregations\n",
    "           Speed layer â†’ Delta Gold micro-batches\n",
    "           \n",
    "        2. Decommission serving layer\n",
    "           Druid/Cassandra â†’ query Delta directly\n",
    "           \n",
    "        3. BI tools reconfigure\n",
    "           Point Tableau/Power BI to Delta\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 5: Validation (Month 9)\": \"\"\"\n",
    "        1. Parallel run\n",
    "           - Lambda still running (backup)\n",
    "           - Delta serving production traffic\n",
    "           \n",
    "        2. Compare metrics\n",
    "           - Latency, accuracy, cost\n",
    "           - User feedback (BI teams)\n",
    "        \n",
    "        3. Fix edge cases\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 6: Cutover (Month 10)\": \"\"\"\n",
    "        1. Decommission Lambda\n",
    "           - Stop batch jobs\n",
    "           - Stop speed layer\n",
    "           - Delete Druid/Cassandra\n",
    "        \n",
    "        2. Cleanup\n",
    "           - Delete HDFS clusters\n",
    "           - Reclaim savings\n",
    "        \n",
    "        3. Training\n",
    "           - Team training on Delta\n",
    "           - Documentation update\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total\": \"\"\"\n",
    "        Duration: 10 months\n",
    "        Cost: $300K engineering + $100K infra\n",
    "        Ongoing savings: $50K/mes\n",
    "        ROI: 8 months\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Partition Strategy\": \"\"\"\n",
    "        âœ… Date partitioning (daily/hourly)\n",
    "        partitionBy(\"date\")\n",
    "        \n",
    "        âš ï¸ Z-ordering para high-cardinality\n",
    "        OPTIMIZE events ZORDER BY (user_id, product_id)\n",
    "        \n",
    "        âŒ Over-partitioning\n",
    "        partitionBy(\"date\", \"hour\", \"user_id\")  # Millones de particiones!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Compaction\": \"\"\"\n",
    "        Auto-compact para streaming:\n",
    "        .option(\"autoCompact\", \"true\")\n",
    "        \n",
    "        Scheduled OPTIMIZE:\n",
    "        OPTIMIZE events WHERE date >= current_date() - 7\n",
    "        \n",
    "        VACUUM para cleanup:\n",
    "        VACUUM events RETAIN 168 HOURS  # 7 dÃ­as\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution\": \"\"\"\n",
    "        Always mergeSchema para backward compatibility:\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        Schema validation:\n",
    "        df.write.format(\"delta\")\n",
    "          .option(\"enforceSchema\", \"true\")\n",
    "          .save()\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Monitoring\": \"\"\"\n",
    "        Metrics to track:\n",
    "        - Write throughput (records/s)\n",
    "        - Read latency (p50, p95, p99)\n",
    "        - File count (small files problem)\n",
    "        - Version count (retention policy)\n",
    "        - Transaction log size\n",
    "        \n",
    "        Alerts:\n",
    "        - File count >100K per partition\n",
    "        - Transaction log >10 MB\n",
    "        - Write latency p99 >5s\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fb62d",
   "metadata": {},
   "source": [
    "### ğŸŒ **Data Mesh: Paradigma Organizacional Descentralizado**\n",
    "\n",
    "**Problema: Data Platform Centralizado (Monolito)**\n",
    "\n",
    "```\n",
    "OrganizaciÃ³n tradicional:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         CENTRAL DATA TEAM (20 personas)        â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  Responsable de:                                â”‚\n",
    "â”‚  â€¢ Todos los pipelines                         â”‚\n",
    "â”‚  â€¢ Todos los data models                       â”‚\n",
    "â”‚  â€¢ Todos los dashboards                        â”‚\n",
    "â”‚  â€¢ Todos los ML models                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  DATA WAREHOUSE   â”‚\n",
    "   â”‚   (Single DB)     â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                 â”‚\n",
    "    â–¼                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Ventas â”‚      â”‚LogÃ­stica â”‚ ... (10+ dominios)\n",
    "â”‚ Team   â”‚      â”‚ Team     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“                 â†“\n",
    "Request            Request\n",
    "pipeline          pipeline\n",
    "   â†“                 â†“\n",
    "  â° 3 meses        â° 3 meses\n",
    "  ğŸ› Bugs           ğŸ› Bugs\n",
    "  ğŸ“‰ Low quality    ğŸ“‰ Low quality\n",
    "\n",
    "Problemas:\n",
    "âŒ Bottleneck: Central team sobrecargado (backlog 6+ meses)\n",
    "âŒ Context loss: Data team no entiende dominio de negocio\n",
    "âŒ Scalability: Imposible crecer linearmente\n",
    "âŒ Ownership: Nadie responsable de calidad de datos\n",
    "```\n",
    "\n",
    "**SoluciÃ³n: Data Mesh (Zhamak Dehghani, 2019)**\n",
    "\n",
    "```\n",
    "Data Mesh: DescentralizaciÃ³n por dominio\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        FEDERATED GOVERNANCE                  â”‚\n",
    "â”‚  (PolÃ­ticas centrales, enforcement local)    â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚              â”‚              â”‚\n",
    "â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "â”‚ VENTAS   â”‚  â”‚LOGÃSTICA â”‚  â”‚ FINANZAS â”‚\n",
    "â”‚ DOMAIN   â”‚  â”‚ DOMAIN   â”‚  â”‚ DOMAIN   â”‚\n",
    "â”‚          â”‚  â”‚          â”‚  â”‚          â”‚\n",
    "â”‚ Data     â”‚  â”‚ Data     â”‚  â”‚ Data     â”‚\n",
    "â”‚ Product  â”‚  â”‚ Product  â”‚  â”‚ Product  â”‚\n",
    "â”‚ Owner    â”‚  â”‚ Owner    â”‚  â”‚ Owner    â”‚\n",
    "â”‚          â”‚  â”‚          â”‚  â”‚          â”‚\n",
    "â”‚ â€¢ ETL    â”‚  â”‚ â€¢ ETL    â”‚  â”‚ â€¢ ETL    â”‚\n",
    "â”‚ â€¢ Qualityâ”‚  â”‚ â€¢ Qualityâ”‚  â”‚ â€¢ Qualityâ”‚\n",
    "â”‚ â€¢ API    â”‚  â”‚ â€¢ API    â”‚  â”‚ â€¢ API    â”‚\n",
    "â”‚ â€¢ Docs   â”‚  â”‚ â€¢ Docs   â”‚  â”‚ â€¢ Docs   â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚              â”‚              â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ SELF-SERVE PLATFORM â”‚\n",
    "         â”‚ (Infra comÃºn)       â”‚\n",
    "         â”‚ â€¢ Spark/Airflow     â”‚\n",
    "         â”‚ â€¢ Monitoring        â”‚\n",
    "         â”‚ â€¢ Data Catalog      â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**4 Principios Fundamentales:**\n",
    "\n",
    "```python\n",
    "principios_data_mesh = {\n",
    "    \"1. Domain-Oriented Decentralization\": \"\"\"\n",
    "        Datos pertenecen al dominio de negocio\n",
    "        \n",
    "        Ejemplo: E-commerce\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ DOMAIN: Ventas                          â”‚\n",
    "        â”‚ Owner: VP of Sales                      â”‚\n",
    "        â”‚ Data Products:                          â”‚\n",
    "        â”‚  â€¢ orders (transaccional)               â”‚\n",
    "        â”‚  â€¢ orders_aggregated (analÃ­tico)        â”‚\n",
    "        â”‚  â€¢ customer_segments (ML)               â”‚\n",
    "        â”‚                                         â”‚\n",
    "        â”‚ Team composition:                       â”‚\n",
    "        â”‚  â€¢ Product Manager (prioridades)        â”‚\n",
    "        â”‚  â€¢ Data Engineer (pipelines)            â”‚\n",
    "        â”‚  â€¢ Analytics Engineer (dbt models)      â”‚\n",
    "        â”‚  â€¢ Data Analyst (consumers)             â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Ventajas:\n",
    "        âœ… Domain expertise: Equipo entiende negocio\n",
    "        âœ… Velocity: No esperar central team\n",
    "        âœ… Accountability: Owner claro de calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Data as a Product\": \"\"\"\n",
    "        Cada dataset es un producto con:\n",
    "        \n",
    "        ğŸ“‹ SLA (Service Level Agreement):\n",
    "        - Latency: <15 min para orders_aggregated\n",
    "        - Availability: 99.9% uptime\n",
    "        - Freshness: Updated cada 5 min\n",
    "        - Quality: >95% completeness\n",
    "        \n",
    "        ğŸ“– Documentation:\n",
    "        - Schema: Columnas, tipos, constraints\n",
    "        - Lineage: De dÃ³nde viene cada campo\n",
    "        - Examples: Query samples\n",
    "        - Contact: Slack channel, owner email\n",
    "        \n",
    "        ğŸ”’ Access Control:\n",
    "        - Public: Todos pueden leer\n",
    "        - Private: Solo equipo ventas\n",
    "        - PII: Masked para no-autorizados\n",
    "        \n",
    "        ğŸ“Š Observability:\n",
    "        - Metrics: Row count, size, update time\n",
    "        - Alerts: SLA violations\n",
    "        - Changelog: Version history\n",
    "        \n",
    "        Ejemplo manifest.yaml:\n",
    "        ```\n",
    "        product_name: orders_aggregated\n",
    "        owner: ventas-data@company.com\n",
    "        sla:\n",
    "          latency_minutes: 15\n",
    "          availability_percent: 99.9\n",
    "          quality_score: 0.95\n",
    "        schema_url: s3://catalog/ventas/orders_agg/schema.json\n",
    "        lineage:\n",
    "          sources:\n",
    "            - orders_raw\n",
    "            - customers\n",
    "            - products\n",
    "        access:\n",
    "          read: [sales_team, finance_team, executives]\n",
    "          write: [sales_data_engineers]\n",
    "        ```\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Self-Serve Data Platform\": \"\"\"\n",
    "        Infraestructura comÃºn para todos los dominios\n",
    "        \n",
    "        Platform Components:\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ COMPUTE                              â”‚\n",
    "        â”‚  â€¢ Spark clusters (on-demand)       â”‚\n",
    "        â”‚  â€¢ Airflow (orchestration)          â”‚\n",
    "        â”‚  â€¢ dbt (transformations)            â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ STORAGE                              â”‚\n",
    "        â”‚  â€¢ Delta Lake (lakehouse)           â”‚\n",
    "        â”‚  â€¢ S3 buckets (per-domain)          â”‚\n",
    "        â”‚  â€¢ Unity Catalog (metadata)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ OBSERVABILITY                        â”‚\n",
    "        â”‚  â€¢ Datadog (metrics, logs)          â”‚\n",
    "        â”‚  â€¢ Great Expectations (quality)     â”‚\n",
    "        â”‚  â€¢ OpenLineage (lineage tracking)   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ GOVERNANCE                           â”‚\n",
    "        â”‚  â€¢ Data Catalog (Amundsen/DataHub)  â”‚\n",
    "        â”‚  â€¢ Access Control (Ranger/Unity)    â”‚\n",
    "        â”‚  â€¢ Policy Engine (OPA)              â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Self-service workflow:\n",
    "        1. Team crea repo: github.com/company/ventas-data\n",
    "        2. Define product: manifest.yaml + dbt models\n",
    "        3. CI/CD deploy: Tests â†’ Stage â†’ Prod\n",
    "        4. Auto-register: Catalog actualizado\n",
    "        5. Monitoring: Dashboards auto-generated\n",
    "        \n",
    "        Platform team provee:\n",
    "        - Templates (cookiecutter)\n",
    "        - Best practices docs\n",
    "        - On-call support\n",
    "        - Cost monitoring\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Federated Computational Governance\": \"\"\"\n",
    "        PolÃ­ticas globales + AutonomÃ­a local\n",
    "        \n",
    "        GLOBAL POLICIES (Central governance):\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ 1. Security                         â”‚\n",
    "        â”‚    â€¢ PII must be encrypted at rest â”‚\n",
    "        â”‚    â€¢ Access logs retained 1 year   â”‚\n",
    "        â”‚    â€¢ MFA required for production   â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 2. Compliance (GDPR, CCPA)         â”‚\n",
    "        â”‚    â€¢ Right to be forgotten         â”‚\n",
    "        â”‚    â€¢ Data retention max 7 years    â”‚\n",
    "        â”‚    â€¢ Audit trail required          â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 3. Quality                          â”‚\n",
    "        â”‚    â€¢ All products have tests       â”‚\n",
    "        â”‚    â€¢ SLA violations reported       â”‚\n",
    "        â”‚    â€¢ Schema changes versioned      â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 4. Interoperability                â”‚\n",
    "        â”‚    â€¢ Standard formats (Parquet)    â”‚\n",
    "        â”‚    â€¢ Common IDs (user_id format)   â”‚\n",
    "        â”‚    â€¢ Shared ontology (terms)       â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        LOCAL AUTONOMY (Domain teams):\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ â€¢ Choose transformation tools       â”‚\n",
    "        â”‚   (dbt, Spark, Python)             â”‚\n",
    "        â”‚ â€¢ Define data models               â”‚\n",
    "        â”‚ â€¢ Set refresh frequency            â”‚\n",
    "        â”‚ â€¢ Manage access within domain      â”‚\n",
    "        â”‚ â€¢ Optimize costs                   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Enforcement:\n",
    "        - Automated checks in CI/CD\n",
    "        - Policy-as-code (OPA policies)\n",
    "        - Failing checks block deploy\n",
    "        - Dashboards show compliance\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**ImplementaciÃ³n Real: Data Mesh Stack**\n",
    "\n",
    "```python\n",
    "# Estructura organizacional\n",
    "data_mesh_org = {\n",
    "    \"Domain Teams (AutÃ³nomos)\": {\n",
    "        \"Ventas Domain\": {\n",
    "            \"Members\": \"8 personas\",\n",
    "            \"Products\": [\n",
    "                \"orders (transactional)\",\n",
    "                \"orders_daily_agg (analytical)\",\n",
    "                \"customer_rfm_segments (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Spark, dbt, Delta Lake\",\n",
    "            \"Budget\": \"$50K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"LogÃ­stica Domain\": {\n",
    "            \"Members\": \"6 personas\",\n",
    "            \"Products\": [\n",
    "                \"shipments (operational)\",\n",
    "                \"delivery_performance (metrics)\",\n",
    "                \"route_optimization (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Flink, Kafka, Iceberg\",\n",
    "            \"Budget\": \"$40K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Marketing Domain\": {\n",
    "            \"Members\": \"5 personas\",\n",
    "            \"Products\": [\n",
    "                \"campaigns (metadata)\",\n",
    "                \"attribution (analytics)\",\n",
    "                \"propensity_scores (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Airflow, Python, Delta Lake\",\n",
    "            \"Budget\": \"$30K/mes\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Platform Team (Enabling)\": {\n",
    "        \"Members\": \"10 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Maintain Spark/Airflow infrastructure\",\n",
    "            \"Operate Data Catalog\",\n",
    "            \"Enforce governance policies\",\n",
    "            \"Cost optimization\",\n",
    "            \"Training & documentation\"\n",
    "        ],\n",
    "        \"Budget\": \"$100K/mes\",\n",
    "        \"Ratio\": \"1 platform engineer per 20 domain engineers\"\n",
    "    },\n",
    "    \n",
    "    \"Governance Team (Federated)\": {\n",
    "        \"Members\": \"3 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Define global policies\",\n",
    "            \"Compliance (GDPR, SOX)\",\n",
    "            \"Audit & reporting\",\n",
    "            \"Cross-domain standards\"\n",
    "        ],\n",
    "        \"Budget\": \"$20K/mes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data Product Example\n",
    "data_product_spec = '''\n",
    "# orders_daily_agg Data Product\n",
    "\n",
    "## Overview\n",
    "Daily aggregated orders for BI dashboards and reporting.\n",
    "\n",
    "## Owner\n",
    "- Team: Ventas Data Engineering\n",
    "- Contact: ventas-data@company.com\n",
    "- Slack: #ventas-data\n",
    "\n",
    "## SLA\n",
    "- Freshness: Updated daily @6AM UTC\n",
    "- Latency: <30 minutes processing time\n",
    "- Availability: 99.9% (max 8.76h downtime/year)\n",
    "- Quality: >99% completeness, >98% accuracy\n",
    "\n",
    "## Schema\n",
    "| Column | Type | Description | PII |\n",
    "|--------|------|-------------|-----|\n",
    "| date | DATE | Order date | No |\n",
    "| country | STRING | Country code (ISO) | No |\n",
    "| total_orders | BIGINT | Count of orders | No |\n",
    "| total_revenue | DECIMAL(10,2) | Sum of revenue (USD) | No |\n",
    "| avg_order_value | DECIMAL(10,2) | Average order value | No |\n",
    "\n",
    "## Lineage\n",
    "```\n",
    "orders_raw (Bronze)\n",
    "    â†“\n",
    "orders_validated (Silver)\n",
    "    â†“\n",
    "orders_daily_agg (Gold) â† THIS PRODUCT\n",
    "```\n",
    "\n",
    "## Access\n",
    "- Read: sales_team, finance_team, executives\n",
    "- Write: sales_data_engineers\n",
    "- Admin: sales_domain_owner\n",
    "\n",
    "## Usage Examples\n",
    "```sql\n",
    "-- Get last 7 days\n",
    "SELECT * FROM orders_daily_agg\n",
    "WHERE date >= CURRENT_DATE - INTERVAL 7 DAYS\n",
    "ORDER BY date DESC\n",
    "\n",
    "-- YoY comparison\n",
    "SELECT \n",
    "    date,\n",
    "    total_revenue,\n",
    "    LAG(total_revenue, 365) OVER (ORDER BY date) as prev_year_revenue\n",
    "FROM orders_daily_agg\n",
    "WHERE date >= '2024-01-01'\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "- Dashboard: https://monitoring.company.com/ventas/orders_agg\n",
    "- Alerts: PagerDuty team \"sales-data-oncall\"\n",
    "\n",
    "## Changelog\n",
    "- 2025-10-01: Added avg_order_value column\n",
    "- 2025-09-15: Changed country to ISO codes\n",
    "- 2025-08-01: Initial release\n",
    "'''\n",
    "```\n",
    "\n",
    "**Ventajas de Data Mesh:**\n",
    "\n",
    "```python\n",
    "ventajas_mesh = {\n",
    "    \"1. Scalability\": \"\"\"\n",
    "        Tradicional: Central team â†’ linear growth impossible\n",
    "        10 domains Ã— 1 central team = bottleneck\n",
    "        \n",
    "        Data Mesh: Domain teams â†’ linear scaling\n",
    "        10 domains Ã— 10 teams = 100 engineers productive\n",
    "        \n",
    "        Growth: Agregar dominio no afecta otros\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Domain Expertise\": \"\"\"\n",
    "        Central team:\n",
    "        \"Â¿QuÃ© significa 'adjusted_revenue'?\"\n",
    "        â†’ Ask business (delays)\n",
    "        \n",
    "        Domain team:\n",
    "        Engineers trabajan con business daily\n",
    "        â†’ Deep understanding, mejor calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Velocity\": \"\"\"\n",
    "        Backlog central: 6 meses\n",
    "        Domain ownership: 2 semanas\n",
    "        \n",
    "        Feature request â†’ deploy:\n",
    "        Central: 3-6 meses\n",
    "        Mesh: 1-2 sprints\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Quality\": \"\"\"\n",
    "        Central team: Best effort (no ownership)\n",
    "        Domain team: Accountable (SLAs, metrics)\n",
    "        \n",
    "        Data quality:\n",
    "        Central: 85% average\n",
    "        Mesh: 95% average (incentivized)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**DesafÃ­os de Data Mesh:**\n",
    "\n",
    "```python\n",
    "desafios_mesh = {\n",
    "    \"1. Organizational Maturity\": \"\"\"\n",
    "        Requiere:\n",
    "        - DevOps culture (CI/CD, testing)\n",
    "        - Data literacy en equipos de negocio\n",
    "        - Executive buy-in (budget descentralizado)\n",
    "        \n",
    "        NO funciona si:\n",
    "        - OrganizaciÃ³n muy jerÃ¡rquica\n",
    "        - Equipos no autÃ³nomos\n",
    "        - Sin cultura de ownership\n",
    "        \n",
    "        Ejemplo fracaso:\n",
    "        Company X intentÃ³ mesh sin DevOps\n",
    "        â†’ Dominios no saben deploy pipelines\n",
    "        â†’ Rollback a central team en 6 meses\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Platform Complexity\": \"\"\"\n",
    "        Self-serve platform = sophisticated\n",
    "        \n",
    "        Requiere:\n",
    "        - Multi-tenancy (aislamiento dominios)\n",
    "        - Cost tracking per domain\n",
    "        - Automated provisioning\n",
    "        - Standardized observability\n",
    "        \n",
    "        Platform team: 10-15 personas mÃ­nimo\n",
    "        Cost: $1M+/aÃ±o (salaries + infra)\n",
    "        \n",
    "        Break-even: ~100+ data engineers total\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Duplication Risk\": \"\"\"\n",
    "        Sin governance fuerte:\n",
    "        - 3 equipos crean \"customer_dim\" diferente\n",
    "        - MÃ©tricas inconsistentes cross-domain\n",
    "        - Redundant data copies (cost)\n",
    "        \n",
    "        MitigaciÃ³n:\n",
    "        - Data Catalog mandatory\n",
    "        - Shared domain for common entities\n",
    "        - Regular cross-domain sync meetings\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Skillset Gap\": \"\"\"\n",
    "        Domain engineers necesitan:\n",
    "        - Data engineering (ETL)\n",
    "        - Analytics engineering (dbt)\n",
    "        - DevOps (CI/CD)\n",
    "        - Domain knowledge (business)\n",
    "        \n",
    "        Hiring difficult: Full-stack data engineers\n",
    "        Training: 6-12 meses ramp-up\n",
    "        \n",
    "        Alternative: Embedded specialists\n",
    "        - 1 data engineer per 2 domains\n",
    "        - Rotates between teams\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso (CuÃ¡ndo Data Mesh Funciona):**\n",
    "\n",
    "```python\n",
    "# âœ… 1. LARGE ORG con mÃºltiples dominios\n",
    "\"\"\"\n",
    "Empresa: Zalando (E-commerce Europeo)\n",
    "TamaÃ±o: 10,000+ empleados, 20+ paÃ­ses\n",
    "Dominios: 50+ (Fashion, Logistics, Payments, ...)\n",
    "\n",
    "Data Mesh adoption (2020):\n",
    "- 50 domain teams autÃ³nomos\n",
    "- Platform team: 25 personas\n",
    "- 200+ data products published\n",
    "- Self-serve: 90% requests no need central\n",
    "\n",
    "Results:\n",
    "- Time-to-market: 6 meses â†’ 2 semanas\n",
    "- Data quality: 80% â†’ 95%\n",
    "- Team satisfaction: +40%\n",
    "\"\"\"\n",
    "\n",
    "# âœ… 2. DISTRIBUTED company\n",
    "\"\"\"\n",
    "Empresa: Confluent (remote-first)\n",
    "Dominios: Customer Success, Engineering, Sales\n",
    "Challenge: Central team = timezone hell\n",
    "\n",
    "Data Mesh benefits:\n",
    "- Teams own data in their timezone\n",
    "- Async collaboration (via catalog)\n",
    "- No central bottleneck\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 3. SMALL STARTUP (<100 personas)\n",
    "\"\"\"\n",
    "Mesh overhead > benefits\n",
    "Better: Small central team (5 personas)\n",
    "Mesh when: Reach 200+ engineers\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 4. HIGHLY REGULATED (sin autonomÃ­a)\n",
    "\"\"\"\n",
    "Banking: Regulators require central control\n",
    "Mesh autonomy conflicts with compliance\n",
    "Better: Centralized with strong governance\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migration Path: Central â†’ Data Mesh**\n",
    "\n",
    "```python\n",
    "migration_roadmap = {\n",
    "    \"Year 1: Foundation\": \"\"\"\n",
    "        Q1: Executive alignment\n",
    "        - Present mesh vision\n",
    "        - Secure budget\n",
    "        - Identify pilot domain\n",
    "        \n",
    "        Q2: Platform MVP\n",
    "        - Self-serve Spark/Airflow\n",
    "        - Data Catalog (Amundsen)\n",
    "        - Basic CI/CD templates\n",
    "        \n",
    "        Q3-Q4: Pilot domain\n",
    "        - Choose 1 domain (e.g., Sales)\n",
    "        - Migrate 3 products\n",
    "        - Learn lessons, iterate\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 2: Scale\": \"\"\"\n",
    "        Q1-Q2: Onboard 5 domains\n",
    "        - Training programs\n",
    "        - Platform improvements\n",
    "        - Governance policies drafted\n",
    "        \n",
    "        Q3-Q4: Federated governance\n",
    "        - Policy engine (OPA)\n",
    "        - Compliance automation\n",
    "        - Cross-domain standards\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 3: Maturity\": \"\"\"\n",
    "        Q1-Q2: Onboard remaining domains\n",
    "        - 100% domains on mesh\n",
    "        - Decommission central team legacy\n",
    "        \n",
    "        Q3-Q4: Optimization\n",
    "        - Cost optimization\n",
    "        - Advanced features (ML platform)\n",
    "        - Community of practice\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total Investment\": \"\"\"\n",
    "        Platform team: 10 personas Ã— $150K Ã— 3 aÃ±os = $4.5M\n",
    "        Training: $500K\n",
    "        Tooling: $1M\n",
    "        Total: $6M\n",
    "        \n",
    "        Break-even: Year 4 (velocity gains)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0bd31",
   "metadata": {},
   "source": [
    "- Batch layer: histÃ³rico completo, recalculable, alta latencia (Spark/Hadoop).\n",
    "- Speed layer: streaming incremental, baja latencia (Kafka/Flink/Spark Streaming).\n",
    "- Serving layer: merge de vistas batch + speed para consultas (Druid/Cassandra/BigQuery).\n",
    "- Trade-offs: doble lÃ³gica (batch + stream), complejidad operacional, eventual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_diagram = '''\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Data     â”‚\n",
    "â”‚  Sources   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Queue â”‚ (Kafka)\n",
    "   â””â”€â”€â”¬â”€â”€â”€â”˜\n",
    "      â”‚\n",
    " â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    " â”‚  Batch   â”‚  (Spark jobs nocturnos)\n",
    " â”‚  Layer   â”‚\n",
    " â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "   Master\n",
    "   Dataset\n",
    "      â”‚\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Servingâ”‚ (Druid/BigQuery)\n",
    "   â”‚ Layer â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â†‘\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Speed â”‚ (Flink/Spark Streaming)\n",
    "   â”‚Layer â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "'''\n",
    "print(lambda_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a99dd",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9117d8",
   "metadata": {},
   "source": [
    "- Un solo pipeline streaming para todo (batch = replay del log).\n",
    "- Simplifica operaciones y lÃ³gica unificada.\n",
    "- Requiere log infinito (Kafka con retenciÃ³n larga) y capacidad de reprocessing.\n",
    "- Trade-offs: coste de almacenamiento del log, complejidad del estado, menor optimizaciÃ³n batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a06c00",
   "metadata": {},
   "source": [
    "## 3. Arquitectura Delta (Lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11be385",
   "metadata": {},
   "source": [
    "- Unifica batch y streaming sobre un Ãºnico storage transaccional (Delta/Iceberg).\n",
    "- Streaming escribe micro-batches transaccionales; batch lee histÃ³rico con time travel.\n",
    "- Elimina duplicaciÃ³n de cÃ³digo y simplifica gobernanza (un catÃ¡logo, un linaje).\n",
    "- Trade-offs: requiere adopciÃ³n de formato y motor compatible (Spark/Trino)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab620719",
   "metadata": {},
   "source": [
    "## 4. Data Mesh: paradigma organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851967",
   "metadata": {},
   "source": [
    "- DescentralizaciÃ³n de la propiedad de datos por dominio de negocio.\n",
    "- Data products: APIs/tablas con SLOs, documentaciÃ³n, linaje y contratos.\n",
    "- Plataforma self-service: herramientas comunes (CI/CD, catÃ¡logo, observabilidad).\n",
    "- Gobernanza federada: polÃ­ticas globales + autonomÃ­a local.\n",
    "- Trade-offs: requiere madurez organizacional, infraestructura sÃ³lida, cambio cultural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0232c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_principles = '''\n",
    "1. Domain-oriented decentralization: equipos de dominio (ventas, logÃ­stica, finanzas) son dueÃ±os de sus datos.\n",
    "2. Data as a product: cada dataset tiene owner, SLO, versionado, documentaciÃ³n y calidad garantizada.\n",
    "3. Self-serve platform: infraestructura comÃºn (orquestaciÃ³n, observabilidad, catÃ¡logo) sin silos.\n",
    "4. Federated governance: polÃ­ticas de seguridad, privacidad y calidad definidas centralmente, aplicadas localmente.\n",
    "'''\n",
    "print(mesh_principles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5c55f",
   "metadata": {},
   "source": [
    "## 5. ComparaciÃ³n y cuÃ¡ndo elegir cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comp = pd.DataFrame([\n",
    "  {'Arquitectura':'Lambda', 'Latencia':'Batch: horas, Speed: segundos', 'Complejidad':'Alta (doble lÃ³gica)', 'Casos':'Legacy con requerimientos mixtos'},\n",
    "  {'Arquitectura':'Kappa', 'Latencia':'Baja (streaming)', 'Complejidad':'Media (un pipeline)', 'Casos':'Todo es streaming, reprocessing factible'},\n",
    "  {'Arquitectura':'Delta', 'Latencia':'Baja-Media (micro-batch)', 'Complejidad':'Media (un storage)', 'Casos':'Lakehouse, unificaciÃ³n batch/stream'},\n",
    "  {'Arquitectura':'Data Mesh', 'Latencia':'Variable (por dominio)', 'Complejidad':'Alta (organizacional)', 'Casos':'Grandes org con mÃºltiples dominios'}\n",
    "])\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cc56e",
   "metadata": {},
   "source": [
    "## 6. Ejercicio de diseÃ±o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b272baf",
   "metadata": {},
   "source": [
    "DiseÃ±a la arquitectura de datos para un e-commerce con:\n",
    "- Dominio Ventas: transacciones en tiempo real (Kafka).\n",
    "- Dominio LogÃ­stica: batch nocturno (inventario/envÃ­os).\n",
    "- Dominio AnalÃ­tica: dashboards con latencia < 5 min.\n",
    "- Requisitos: auditabilidad, GDPR, linaje, costos optimizados.\n",
    "\n",
    "Responde: Â¿Lambda, Kappa, Delta o hÃ­brida? Â¿Data Mesh aplicable? Justifica con trade-offs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
