{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb7bbce",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh\n",
    "\n",
    "Objetivo: comprender y contrastar arquitecturas de referencia (Lambda, Kappa, Delta) y patrones organizacionales (Data Mesh), con ejemplos de diseÃ±o y trade-offs.\n",
    "\n",
    "- DuraciÃ³n: 120â€“150 min\n",
    "- Dificultad: Alta\n",
    "- Prerrequisitos: Mid completo, experiencia con batch y streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2042cc",
   "metadata": {},
   "source": [
    "## 1. Arquitectura Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a450ff1",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ **Lambda Architecture: Batch + Speed Layer**\n",
    "\n",
    "**Origen y MotivaciÃ³n (Nathan Marz, 2011):**\n",
    "\n",
    "```\n",
    "Problema clÃ¡sico:\n",
    "- Batch processing: Preciso pero lento (horas/dÃ­as)\n",
    "- Stream processing: RÃ¡pido pero complejo (estado, fallos)\n",
    "\n",
    "SoluciÃ³n Lambda:\n",
    "- Batch Layer: Verdad absoluta, inmutable, reprocessable\n",
    "- Speed Layer: AproximaciÃ³n rÃ¡pida, eventualmente consistente\n",
    "- Serving Layer: Merge de ambas vistas\n",
    "```\n",
    "\n",
    "**Arquitectura Completa:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     DATA SOURCES                             â”‚\n",
    "â”‚  â€¢ Kafka Topics                                              â”‚\n",
    "â”‚  â€¢ Database CDC (Debezium)                                   â”‚\n",
    "â”‚  â€¢ Application Logs                                          â”‚\n",
    "â”‚  â€¢ IoT Streams                                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                    â”‚\n",
    "        â–¼                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ BATCH LAYER  â”‚    â”‚ SPEED LAYER  â”‚\n",
    "â”‚              â”‚    â”‚              â”‚\n",
    "â”‚ â€¢ Spark Batchâ”‚    â”‚ â€¢ Flink      â”‚\n",
    "â”‚ â€¢ Hadoop MR  â”‚    â”‚ â€¢ Spark SS   â”‚\n",
    "â”‚ â€¢ Hive       â”‚    â”‚ â€¢ Storm      â”‚\n",
    "â”‚              â”‚    â”‚              â”‚\n",
    "â”‚ Runs: Daily  â”‚    â”‚ Runs: Real-  â”‚\n",
    "â”‚       @2AM   â”‚    â”‚       time   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                   â”‚\n",
    "       â”‚ Master Dataset    â”‚ Delta Views\n",
    "       â”‚ (Immutable)       â”‚ (Mutable)\n",
    "       â”‚                   â”‚\n",
    "       â–¼                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      SERVING LAYER               â”‚\n",
    "â”‚                                  â”‚\n",
    "â”‚  Query(t) = Batch(0â†’t-1h) +     â”‚\n",
    "â”‚             Speed(t-1hâ†’t)        â”‚\n",
    "â”‚                                  â”‚\n",
    "â”‚  â€¢ Druid                         â”‚\n",
    "â”‚  â€¢ Cassandra                     â”‚\n",
    "â”‚  â€¢ ElasticSearch                 â”‚\n",
    "â”‚  â€¢ BigQuery                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   API    â”‚\n",
    "        â”‚Dashboard â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Componentes Detallados:**\n",
    "\n",
    "**1. Batch Layer (Verdad Inmutable):**\n",
    "\n",
    "```python\n",
    "# CaracterÃ­sticas:\n",
    "# - Procesamiento completo del histÃ³rico\n",
    "# - Recalculable desde el inicio\n",
    "# - Optimizado para throughput (no latencia)\n",
    "# - Tolerancia a fallos simple (restart)\n",
    "\n",
    "# Ejemplo: Agregaciones diarias con Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchLayer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer ALL histÃ³rico (full recompute)\n",
    "raw_events = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"s3://datalake/raw/events/\")\n",
    "\n",
    "# Agregaciones pesadas\n",
    "user_stats_batch = raw_events \\\n",
    "    .groupBy(\"user_id\", \"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer\n",
    "user_stats_batch.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"s3://datalake/serving/user_stats_batch\")\n",
    "\n",
    "# Schedule: Daily @2AM (Airflow DAG)\n",
    "# Duration: 2-4 horas para procesar 1 aÃ±o de datos\n",
    "# Cost: $$$ (large cluster, pero solo 1x/dÃ­a)\n",
    "```\n",
    "\n",
    "**2. Speed Layer (Low Latency Incremental):**\n",
    "\n",
    "```python\n",
    "# CaracterÃ­sticas:\n",
    "# - Solo datos recientes (Ãºltimas horas)\n",
    "# - Baja latencia (<1 min)\n",
    "# - Estado mutable (agregaciones incrementales)\n",
    "# - Complejidad de exactly-once\n",
    "\n",
    "# Ejemplo: Streaming con Spark Structured Streaming\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Leer solo NUEVOS eventos\n",
    "events_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Misma lÃ³gica que batch (pero incremental)\n",
    "user_stats_speed = events_stream \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"session_id\").alias(\"sessions\")\n",
    "    )\n",
    "\n",
    "# Escribir a serving layer (diferentes tablas)\n",
    "query = user_stats_speed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/speed\") \\\n",
    "    .start(\"s3://datalake/serving/user_stats_speed\")\n",
    "\n",
    "# Runs: Continuously\n",
    "# Latency: 30s - 2 min\n",
    "# Cost: $$ (small cluster, pero 24/7)\n",
    "```\n",
    "\n",
    "**3. Serving Layer (Query Merge):**\n",
    "\n",
    "```python\n",
    "# Merge batch + speed en query time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_user_stats(user_id: str, as_of: datetime = None):\n",
    "    \"\"\"\n",
    "    Query que merge batch + speed layers\n",
    "    \"\"\"\n",
    "    if as_of is None:\n",
    "        as_of = datetime.now()\n",
    "    \n",
    "    # Batch boundary (tÃ­picamente hace 1-2 horas)\n",
    "    batch_cutoff = as_of - timedelta(hours=2)\n",
    "    \n",
    "    # Query batch layer (histÃ³rico hasta cutoff)\n",
    "    batch_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_batch\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND date < '{batch_cutoff.date()}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Query speed layer (reciente desde cutoff)\n",
    "    speed_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(total_events) as events,\n",
    "            SUM(total_revenue) as revenue\n",
    "        FROM user_stats_speed\n",
    "        WHERE user_id = '{user_id}'\n",
    "          AND window.start >= '{batch_cutoff}'\n",
    "        GROUP BY user_id\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Merge results\n",
    "    total_events = batch_stats[0]['events'] + speed_stats[0]['events']\n",
    "    total_revenue = batch_stats[0]['revenue'] + speed_stats[0]['revenue']\n",
    "    \n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'total_events': total_events,\n",
    "        'total_revenue': total_revenue,\n",
    "        'as_of': as_of,\n",
    "        'batch_cutoff': batch_cutoff\n",
    "    }\n",
    "```\n",
    "\n",
    "**Ventajas de Lambda:**\n",
    "\n",
    "```python\n",
    "ventajas = {\n",
    "    \"1. Robustez\": \"\"\"\n",
    "        Batch layer es inmutable â†’ fÃ¡cil debugging\n",
    "        Speed layer falla â†’ aÃºn tienes batch como fallback\n",
    "        Disaster recovery: recompute desde raw data\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. PrecisiÃ³n Garantizada\": \"\"\"\n",
    "        Batch garantiza exactitud (full recompute)\n",
    "        Speed solo para latencia, batch corrige errores\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Separation of Concerns\": \"\"\"\n",
    "        Batch: optimiza throughput (large partitions, columnar)\n",
    "        Speed: optimiza latencia (small batches, in-memory)\n",
    "        Cada uno usa tecnologÃ­a ideal para su caso\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. AuditorÃ­a\": \"\"\"\n",
    "        Raw data inmutable â†’ compliance (GDPR, SOX)\n",
    "        Puedes recompute histÃ³rico para auditorÃ­as\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas (Â¿Por quÃ© Lambda estÃ¡ en declive?):**\n",
    "\n",
    "```python\n",
    "desventajas = {\n",
    "    \"1. CÃ³digo Duplicado\": \"\"\"\n",
    "        MISMA lÃ³gica implementada 2 veces:\n",
    "        - batch_aggregations.py (Spark Batch)\n",
    "        - speed_aggregations.py (Spark Streaming)\n",
    "        \n",
    "        Cambio de lÃ³gica â†’ actualizar AMBOS\n",
    "        Testing â†’ doble esfuerzo\n",
    "        Bugs â†’ pueden divergir silenciosamente\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Complejidad Operacional\": \"\"\"\n",
    "        Mantener 2 pipelines:\n",
    "        - Batch: Airflow DAG, cluster management, retry logic\n",
    "        - Speed: Streaming query monitoring, checkpoint management\n",
    "        \n",
    "        2x infraestructura, 2x alertas, 2x oncall\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Eventual Consistency\": \"\"\"\n",
    "        Periodo de gracia donde batch/speed no alineados\n",
    "        \n",
    "        Ejemplo:\n",
    "        10:00 AM: Evento llega\n",
    "        10:01 AM: Speed layer procesa â†’ visible en dashboard\n",
    "        02:00 AM: Batch recompute â†’ corrige pequeÃ±os errores\n",
    "        \n",
    "        Usuario puede ver nÃºmeros ligeramente diferentes\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Costos\": \"\"\"\n",
    "        Batch cluster: Large (100 nodes) @ 2-4h/dÃ­a\n",
    "        Speed cluster: Medium (20 nodes) @ 24/7\n",
    "        \n",
    "        Total: ~$50K/mes para org mediana\n",
    "        \n",
    "        vs Lakehouse unificado: ~$30K/mes\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales (CuÃ¡ndo Lambda Tiene Sentido):**\n",
    "\n",
    "```python\n",
    "# 1. LEGACY SYSTEMS con batch existente\n",
    "\"\"\"\n",
    "Empresa: Retail con 20 aÃ±os de Hadoop\n",
    "SituaciÃ³n: 500 Hive tables, cientos de Spark jobs\n",
    "Necesidad: Agregar real-time sin reescribir todo\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Mantener batch layer existente (no tocas legacy)\n",
    "- Agregar speed layer con Flink para casos crÃ­ticos\n",
    "- MigraciÃ³n gradual dominio por dominio\n",
    "\"\"\"\n",
    "\n",
    "# 2. ALTA PRECISIÃ“N requerida\n",
    "\"\"\"\n",
    "Empresa: Financial trading\n",
    "SituaciÃ³n: Regulaciones requieren recompute exacto\n",
    "Necesidad: AuditorÃ­as pueden pedir recalcular 5 aÃ±os atrÃ¡s\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Batch layer garantiza reproducibilidad exacta\n",
    "- Speed layer para trading real-time\n",
    "- Eventual consistency aceptable (batch corrige)\n",
    "\"\"\"\n",
    "\n",
    "# 3. MUY DIFERENTES SLAs batch vs stream\n",
    "\"\"\"\n",
    "Empresa: IoT con billones de sensores\n",
    "SituaciÃ³n: \n",
    "  - Alertas crÃ­ticas: <100ms (anomalÃ­as)\n",
    "  - AnÃ¡lisis histÃ³rico: dÃ­as OK (tendencias)\n",
    "\n",
    "SoluciÃ³n: Lambda\n",
    "- Speed layer: Flink para alertas ultra-rÃ¡pidas\n",
    "- Batch layer: Hadoop/Hive para anÃ¡lisis pesados\n",
    "- TecnologÃ­as muy diferentes, difÃ­cil unificar\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**MigraciÃ³n de Lambda â†’ Modern Stack:**\n",
    "\n",
    "```python\n",
    "# Estrategia: Unified Batch+Stream con Delta Lake\n",
    "\n",
    "# ANTES (Lambda):\n",
    "# 1. Batch: Spark job diario (4h, 100 nodes)\n",
    "# 2. Speed: Flink continuous (24/7, 20 nodes)\n",
    "# 3. Serving: Druid (merge queries)\n",
    "\n",
    "# DESPUÃ‰S (Delta/Lakehouse):\n",
    "# 1. Unified: Spark Structured Streaming (24/7, 30 nodes)\n",
    "# 2. Micro-batches cada 5 min\n",
    "# 3. Delta Lake: ACID transactions, no merge needed\n",
    "\n",
    "# Migration timeline:\n",
    "\"\"\"\n",
    "Month 1-2: Setup Delta Lake infrastructure\n",
    "Month 3-4: Migrate batch pipelines to Delta\n",
    "Month 5-6: Migrate speed pipelines to Structured Streaming\n",
    "Month 7: Dual-run (Lambda + Delta) for validation\n",
    "Month 8: Cutover to Delta, decommission Lambda\n",
    "Month 9: Cleanup, optimize\n",
    "\n",
    "Total: 9 months\n",
    "Cost: $200K engineering + $50K infra\n",
    "Savings: $20K/mes ongoing (ROI: 2.5 years)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: LinkedIn (Inventor de Lambda):**\n",
    "\n",
    "```python\n",
    "# LinkedIn inventÃ³ Lambda en 2011\n",
    "# Usaron Lambda 2011-2018 (7 aÃ±os)\n",
    "\n",
    "arquitectura_linkedin = {\n",
    "    \"Batch Layer\": {\n",
    "        \"TecnologÃ­a\": \"Hadoop MapReduce â†’ Spark\",\n",
    "        \"Datos\": \"Kafka topics replicados a HDFS\",\n",
    "        \"Frecuencia\": \"Daily @midnight\",\n",
    "        \"Output\": \"Hive tables (member profiles, connections, jobs)\"\n",
    "    },\n",
    "    \n",
    "    \"Speed Layer\": {\n",
    "        \"TecnologÃ­a\": \"Storm â†’ Samza â†’ Kafka Streams\",\n",
    "        \"Datos\": \"Kafka topics directamente\",\n",
    "        \"Latencia\": \"<1 second\",\n",
    "        \"Output\": \"Espresso (NoSQL) para low-latency reads\"\n",
    "    },\n",
    "    \n",
    "    \"Serving Layer\": {\n",
    "        \"TecnologÃ­a\": \"Espresso + Voldemort (key-value stores)\",\n",
    "        \"Pattern\": \"API merge batch + speed en read time\"\n",
    "    },\n",
    "    \n",
    "    \"2018 Migration\": \"\"\"\n",
    "        LinkedIn migrÃ³ a Unified Streaming (Samza + Kafka)\n",
    "        RazÃ³n: Complejidad de mantener doble lÃ³gica\n",
    "        Resultado: -30% cÃ³digo, -40% operaciones\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96454085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ—ï¸ ARQUITECTURA LAMBDA - SIMULACIÃ“N COMPLETA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset generado: 10,000 eventos en 7 dÃ­as\n",
      "Tipos: {np.str_('view'): 2521, np.str_('purchase'): 2512, np.str_('click'): 2502, np.str_('cart_add'): 2465}\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ BATCH LAYER - Procesamiento Completo (simulado overnight)\n",
      "================================================================================\n",
      "âœ… Batch completo procesado en 0.009s\n",
      "ğŸ“ˆ Agregaciones generadas: 5,577 registros (user-day level)\n",
      "ğŸ’° Revenue total: $758,138.30\n",
      "\n",
      "Ãšltima fecha batch: 2025-12-09\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ SPEED LAYER - Streaming en tiempo real\n",
      "================================================================================\n",
      "âš¡ Speed layer procesado en 0.003s\n",
      "ğŸ“Š Eventos recientes: 55 en Ãºltima hora\n",
      "ğŸ‘¥ Usuarios activos: 54\n",
      "ğŸ’° Revenue Ãºltima hora: $3,225.21\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ SERVING LAYER - Merge de Batch + Speed\n",
      "================================================================================\n",
      "\n",
      "ğŸ‘¤ U0385:\n",
      "   Batch (histÃ³rico): 6 eventos, $334.87\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 6 eventos, $334.87\n",
      "\n",
      "ğŸ‘¤ U0270:\n",
      "   Batch (histÃ³rico): 14 eventos, $1383.49\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 14 eventos, $1383.49\n",
      "\n",
      "ğŸ‘¤ U0456:\n",
      "   Batch (histÃ³rico): 10 eventos, $921.91\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 10 eventos, $921.91\n",
      "\n",
      "ğŸ‘¤ U0513:\n",
      "   Batch (histÃ³rico): 8 eventos, $114.39\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 8 eventos, $114.39\n",
      "\n",
      "ğŸ‘¤ U0681:\n",
      "   Batch (histÃ³rico): 11 eventos, $503.19\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 11 eventos, $503.19\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ANÃLISIS DE TRADE-OFFS\n",
      "================================================================================\n",
      "          Aspecto        Batch Layer       Speed Layer                      Impacto\n",
      "         Latencia         6-24 horas        <1 segundo âš ï¸ Queries complejas (merge)\n",
      "        PrecisiÃ³n       100% preciso        Aproximado       âœ… Eventual consistency\n",
      "      Complejidad Simple (MapReduce) Complejo (estado)              âš ï¸ Doble lÃ³gica\n",
      "Costo operacional       Bajo (batch)       Alto (24/7)         ğŸ’° 2x infraestructura\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Lambda completada\n",
      "   â€¢ Batch procesÃ³ 10,000 eventos histÃ³ricos\n",
      "   â€¢ Speed procesÃ³ 55 eventos en tiempo real\n",
      "   â€¢ Serving layer unificÃ³ resultados con Ã©xito\n",
      "================================================================================\n",
      "âœ… Batch completo procesado en 0.009s\n",
      "ğŸ“ˆ Agregaciones generadas: 5,577 registros (user-day level)\n",
      "ğŸ’° Revenue total: $758,138.30\n",
      "\n",
      "Ãšltima fecha batch: 2025-12-09\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ SPEED LAYER - Streaming en tiempo real\n",
      "================================================================================\n",
      "âš¡ Speed layer procesado en 0.003s\n",
      "ğŸ“Š Eventos recientes: 55 en Ãºltima hora\n",
      "ğŸ‘¥ Usuarios activos: 54\n",
      "ğŸ’° Revenue Ãºltima hora: $3,225.21\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ SERVING LAYER - Merge de Batch + Speed\n",
      "================================================================================\n",
      "\n",
      "ğŸ‘¤ U0385:\n",
      "   Batch (histÃ³rico): 6 eventos, $334.87\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 6 eventos, $334.87\n",
      "\n",
      "ğŸ‘¤ U0270:\n",
      "   Batch (histÃ³rico): 14 eventos, $1383.49\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 14 eventos, $1383.49\n",
      "\n",
      "ğŸ‘¤ U0456:\n",
      "   Batch (histÃ³rico): 10 eventos, $921.91\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 10 eventos, $921.91\n",
      "\n",
      "ğŸ‘¤ U0513:\n",
      "   Batch (histÃ³rico): 8 eventos, $114.39\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 8 eventos, $114.39\n",
      "\n",
      "ğŸ‘¤ U0681:\n",
      "   Batch (histÃ³rico): 11 eventos, $503.19\n",
      "   Speed (real-time): 0 eventos, $0.00\n",
      "   â¡ï¸ TOTAL: 11 eventos, $503.19\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ANÃLISIS DE TRADE-OFFS\n",
      "================================================================================\n",
      "          Aspecto        Batch Layer       Speed Layer                      Impacto\n",
      "         Latencia         6-24 horas        <1 segundo âš ï¸ Queries complejas (merge)\n",
      "        PrecisiÃ³n       100% preciso        Aproximado       âœ… Eventual consistency\n",
      "      Complejidad Simple (MapReduce) Complejo (estado)              âš ï¸ Doble lÃ³gica\n",
      "Costo operacional       Bajo (batch)       Alto (24/7)         ğŸ’° 2x infraestructura\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Lambda completada\n",
      "   â€¢ Batch procesÃ³ 10,000 eventos histÃ³ricos\n",
      "   â€¢ Speed procesÃ³ 55 eventos en tiempo real\n",
      "   â€¢ Serving layer unificÃ³ resultados con Ã©xito\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ SimulaciÃ³n PrÃ¡ctica: Arquitectura Lambda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ—ï¸ ARQUITECTURA LAMBDA - SIMULACIÃ“N COMPLETA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= GENERACIÃ“N DE DATOS =============\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Generar 10,000 eventos en los Ãºltimos 7 dÃ­as\n",
    "events = []\n",
    "for i in range(10000):\n",
    "    timestamp = now - timedelta(days=np.random.randint(0, 7), \n",
    "                                 hours=np.random.randint(0, 24),\n",
    "                                 minutes=np.random.randint(0, 60))\n",
    "    events.append({\n",
    "        'event_id': f'E{i:06d}',\n",
    "        'timestamp': timestamp,\n",
    "        'user_id': f'U{np.random.randint(1, 1000):04d}',\n",
    "        'event_type': np.random.choice(['view', 'click', 'purchase', 'cart_add']),\n",
    "        'amount': np.random.uniform(10, 500) if np.random.random() > 0.7 else 0\n",
    "    })\n",
    "\n",
    "df_events = pd.DataFrame(events)\n",
    "print(f\"\\nğŸ“Š Dataset generado: {len(df_events):,} eventos en 7 dÃ­as\")\n",
    "print(f\"Tipos: {df_events['event_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# ============= BATCH LAYER =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1ï¸âƒ£ BATCH LAYER - Procesamiento Completo (simulado overnight)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Simular procesamiento batch: agregaciÃ³n por dÃ­a y usuario\n",
    "df_batch = df_events.copy()\n",
    "df_batch['date'] = df_batch['timestamp'].dt.date\n",
    "\n",
    "batch_results = df_batch.groupby(['date', 'user_id']).agg({\n",
    "    'event_id': 'count',\n",
    "    'amount': 'sum'\n",
    "}).rename(columns={'event_id': 'total_events', 'amount': 'total_revenue'}).reset_index()\n",
    "\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Batch completo procesado en {batch_time:.3f}s\")\n",
    "print(f\"ğŸ“ˆ Agregaciones generadas: {len(batch_results):,} registros (user-day level)\")\n",
    "print(f\"ğŸ’° Revenue total: ${batch_results['total_revenue'].sum():,.2f}\")\n",
    "print(f\"\\nÃšltima fecha batch: {batch_results['date'].max()}\")\n",
    "\n",
    "# ============= SPEED LAYER =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2ï¸âƒ£ SPEED LAYER - Streaming en tiempo real\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular eventos recientes (Ãºltima hora)\n",
    "cutoff_time = now - timedelta(hours=1)\n",
    "df_speed = df_events[df_events['timestamp'] > cutoff_time].copy()\n",
    "\n",
    "start_time = time.time()\n",
    "speed_results = df_speed.groupby('user_id').agg({\n",
    "    'event_id': 'count',\n",
    "    'amount': 'sum'\n",
    "}).rename(columns={'event_id': 'events_last_hour', 'amount': 'revenue_last_hour'}).reset_index()\n",
    "\n",
    "speed_time = time.time() - start_time\n",
    "\n",
    "print(f\"âš¡ Speed layer procesado en {speed_time:.3f}s\")\n",
    "print(f\"ğŸ“Š Eventos recientes: {len(df_speed):,} en Ãºltima hora\")\n",
    "print(f\"ğŸ‘¥ Usuarios activos: {len(speed_results):,}\")\n",
    "print(f\"ğŸ’° Revenue Ãºltima hora: ${speed_results['revenue_last_hour'].sum():,.2f}\")\n",
    "\n",
    "# ============= SERVING LAYER =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3ï¸âƒ£ SERVING LAYER - Merge de Batch + Speed\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular consulta: obtener mÃ©tricas completas para usuarios\n",
    "sample_users = df_events['user_id'].sample(5).unique()\n",
    "\n",
    "for user in sample_users:\n",
    "    # Batch data (histÃ³rico hasta hace 1 hora)\n",
    "    user_batch = batch_results[batch_results['user_id'] == user]\n",
    "    batch_events = user_batch['total_events'].sum() if len(user_batch) > 0 else 0\n",
    "    batch_revenue = user_batch['total_revenue'].sum() if len(user_batch) > 0 else 0\n",
    "    \n",
    "    # Speed data (Ãºltima hora)\n",
    "    user_speed = speed_results[speed_results['user_id'] == user]\n",
    "    speed_events = user_speed['events_last_hour'].iloc[0] if len(user_speed) > 0 else 0\n",
    "    speed_revenue = user_speed['revenue_last_hour'].iloc[0] if len(user_speed) > 0 else 0\n",
    "    \n",
    "    # MERGE\n",
    "    total_events = batch_events + speed_events\n",
    "    total_revenue = batch_revenue + speed_revenue\n",
    "    \n",
    "    print(f\"\\nğŸ‘¤ {user}:\")\n",
    "    print(f\"   Batch (histÃ³rico): {batch_events} eventos, ${batch_revenue:.2f}\")\n",
    "    print(f\"   Speed (real-time): {speed_events} eventos, ${speed_revenue:.2f}\")\n",
    "    print(f\"   â¡ï¸ TOTAL: {total_events} eventos, ${total_revenue:.2f}\")\n",
    "\n",
    "# ============= TRADE-OFFS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š ANÃLISIS DE TRADE-OFFS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trade_offs = pd.DataFrame([\n",
    "    {'Aspecto': 'Latencia', 'Batch Layer': '6-24 horas', 'Speed Layer': '<1 segundo', 'Impacto': 'âš ï¸ Queries complejas (merge)'},\n",
    "    {'Aspecto': 'PrecisiÃ³n', 'Batch Layer': '100% preciso', 'Speed Layer': 'Aproximado', 'Impacto': 'âœ… Eventual consistency'},\n",
    "    {'Aspecto': 'Complejidad', 'Batch Layer': 'Simple (MapReduce)', 'Speed Layer': 'Complejo (estado)', 'Impacto': 'âš ï¸ Doble lÃ³gica'},\n",
    "    {'Aspecto': 'Costo operacional', 'Batch Layer': 'Bajo (batch)', 'Speed Layer': 'Alto (24/7)', 'Impacto': 'ğŸ’° 2x infraestructura'},\n",
    "])\n",
    "\n",
    "print(trade_offs.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SimulaciÃ³n Lambda completada\")\n",
    "print(f\"   â€¢ Batch procesÃ³ {len(df_events):,} eventos histÃ³ricos\")\n",
    "print(f\"   â€¢ Speed procesÃ³ {len(df_speed):,} eventos en tiempo real\")\n",
    "print(f\"   â€¢ Serving layer unificÃ³ resultados con Ã©xito\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd386004",
   "metadata": {},
   "source": [
    "### ğŸ”„ **Kappa Architecture: Stream-Only Simplification**\n",
    "\n",
    "**Origen (Jay Kreps, LinkedIn/Confluent, 2014):**\n",
    "\n",
    "```\n",
    "CrÃ­tica a Lambda:\n",
    "\"Â¿Por quÃ© mantener 2 sistemas si streaming puede hacer todo?\"\n",
    "\n",
    "Propuesta Kappa:\n",
    "- Eliminar batch layer completamente\n",
    "- Todo es streaming (batch = replay del log)\n",
    "- Inmutable log como single source of truth\n",
    "```\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         DATA SOURCES                     â”‚\n",
    "â”‚  â€¢ Applications                          â”‚\n",
    "â”‚  â€¢ Databases (CDC)                       â”‚\n",
    "â”‚  â€¢ IoT Devices                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    KAFKA (Immutable Log)                â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  â€¢ Infinite retention (or very long)    â”‚\n",
    "â”‚  â€¢ Partitioned & Replicated             â”‚\n",
    "â”‚  â€¢ Serves as \"Master Dataset\"           â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Topics:                                 â”‚\n",
    "â”‚  â”œâ”€ events (retention: 1 year)          â”‚\n",
    "â”‚  â”œâ”€ transactions (retention: 5 years)   â”‚\n",
    "â”‚  â””â”€ user_actions (retention: 90 days)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   STREAM PROCESSING                     â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Version 1: Flink Job                   â”‚\n",
    "â”‚  â”œâ”€ Aggregations                        â”‚\n",
    "â”‚  â”œâ”€ Joins                               â”‚\n",
    "â”‚  â””â”€ Output â†’ Cassandra                  â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  Version 2: New logic needed            â”‚\n",
    "â”‚  â”œâ”€ Deploy new Flink job                â”‚\n",
    "â”‚  â”œâ”€ Replay from offset 0                â”‚\n",
    "â”‚  â””â”€ Output â†’ Cassandra v2               â”‚\n",
    "â”‚                                          â”‚\n",
    "â”‚  ğŸ”„ Reprocessing = Replay log           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       SERVING LAYER                     â”‚\n",
    "â”‚  â€¢ Cassandra                            â”‚\n",
    "â”‚  â€¢ ElasticSearch                        â”‚\n",
    "â”‚  â€¢ Redis                                â”‚\n",
    "â”‚  â€¢ PostgreSQL                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Principios Clave:**\n",
    "\n",
    "```python\n",
    "principios_kappa = {\n",
    "    \"1. Everything is a Stream\": \"\"\"\n",
    "        No distinciÃ³n entre batch y stream\n",
    "        \n",
    "        Batch tradicional:\n",
    "        SELECT SUM(revenue) FROM sales WHERE date = '2025-10-30'\n",
    "        \n",
    "        Kappa:\n",
    "        Consume Kafka topic 'sales', aggregate, done\n",
    "        (No diferencia conceptual con real-time)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Immutable Log as Source of Truth\": \"\"\"\n",
    "        Kafka = Database of record\n",
    "        \n",
    "        Ventajas:\n",
    "        - Time travel: replay desde cualquier offset\n",
    "        - Debugging: reproduce bug con datos exactos\n",
    "        - Migration: deploy nueva versiÃ³n, replay, compare\n",
    "        \n",
    "        Ejemplo:\n",
    "        offset 0 â†’ offset 1M (1 aÃ±o de datos)\n",
    "        Reprocessing: ~4 horas con 50 partitions\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution in Log\": \"\"\"\n",
    "        Log contiene todos los schemas histÃ³ricos\n",
    "        \n",
    "        V1: {\"user_id\": 123, \"action\": \"click\"}\n",
    "        V2: {\"user_id\": 123, \"action\": \"click\", \"device\": \"mobile\"}\n",
    "        V3: {\"user_id\": 123, \"event_type\": \"click\", \"metadata\": {...}}\n",
    "        \n",
    "        Consumer handle all versions gracefully\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Reprocessing for Code Changes\": \"\"\"\n",
    "        Cambio de lÃ³gica â†’ no reescribir batch\n",
    "        \n",
    "        Workflow:\n",
    "        1. Deploy new stream processor (version 2)\n",
    "        2. Replay desde offset 0 (parallel con v1)\n",
    "        3. Validate output v2 matches expected\n",
    "        4. Cutover traffic to v2\n",
    "        5. Decommission v1\n",
    "        \n",
    "        Duration: horas/dÃ­as (no semanas de reescribir batch)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**ImplementaciÃ³n Real con Kafka + Flink:**\n",
    "\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.table.descriptors import Kafka, Schema, Json\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# ConfiguraciÃ³n Kafka source con retenciÃ³n larga\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        product_id STRING,\n",
    "        revenue DOUBLE,\n",
    "        event_timestamp TIMESTAMP(3),\n",
    "        WATERMARK FOR event_timestamp AS event_timestamp - INTERVAL '10' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'properties.group.id' = 'user-stats-processor-v2',\n",
    "        'scan.startup.mode' = 'earliest-offset',  -- Replay desde inicio\n",
    "        'format' = 'json',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Stream processing (unificado batch+stream)\n",
    "user_stats = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        TUMBLE_START(event_timestamp, INTERVAL '1' DAY) as day,\n",
    "        COUNT(*) as total_events,\n",
    "        SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,\n",
    "        COUNT(DISTINCT product_id) as unique_products\n",
    "    FROM events\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_timestamp, INTERVAL '1' DAY)\n",
    "\"\"\")\n",
    "\n",
    "# Output a Cassandra\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_stats_cassandra (\n",
    "        user_id STRING,\n",
    "        day TIMESTAMP(3),\n",
    "        total_events BIGINT,\n",
    "        total_revenue DOUBLE,\n",
    "        unique_products BIGINT,\n",
    "        PRIMARY KEY (user_id, day) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'cassandra',\n",
    "        'host' = 'cassandra',\n",
    "        'table-name' = 'user_stats'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "user_stats.execute_insert('user_stats_cassandra').wait()\n",
    "\n",
    "# Reprocessing:\n",
    "# 1. Stop old job\n",
    "# 2. Deploy this code (nueva lÃ³gica)\n",
    "# 3. Kafka consumer group lee desde offset 0\n",
    "# 4. Reprocesa 1 aÃ±o de datos en ~4 horas\n",
    "# 5. Nueva tabla user_stats_cassandra tiene datos corregidos\n",
    "```\n",
    "\n",
    "**ComparaciÃ³n Kafka Retention:**\n",
    "\n",
    "```python\n",
    "# ConfiguraciÃ³n tÃ­pica para Kappa\n",
    "kafka_retention = {\n",
    "    \"Short Retention (Anti-Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 604800000  # 7 dÃ­as\",\n",
    "        \"Disk\": \"1 TB / partition\",\n",
    "        \"Cost\": \"$500/mes\",\n",
    "        \"Reprocessing\": \"âŒ Solo Ãºltimos 7 dÃ­as disponibles\",\n",
    "        \"Use Case\": \"Lambda architecture (Kafka solo buffer)\"\n",
    "    },\n",
    "    \n",
    "    \"Long Retention (Kappa)\": {\n",
    "        \"Config\": \"retention.ms = 31536000000  # 1 aÃ±o\",\n",
    "        \"Disk\": \"52 TB / partition (con compaction)\",\n",
    "        \"Cost\": \"$2,600/mes (S3 tiered storage)\",\n",
    "        \"Reprocessing\": \"âœ… 1 aÃ±o completo disponible\",\n",
    "        \"Use Case\": \"Kappa architecture (Kafka es source of truth)\"\n",
    "    },\n",
    "    \n",
    "    \"Infinite Retention (Extreme Kappa)\": {\n",
    "        \"Config\": \"retention.ms = -1  # Forever\",\n",
    "        \"Disk\": \"Ilimitado (requiere tiered storage)\",\n",
    "        \"Cost\": \"$5K/mes (S3 archival)\",\n",
    "        \"Reprocessing\": \"âœ… HistÃ³rico completo\",\n",
    "        \"Use Case\": \"Event sourcing, auditabilidad estricta\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Kafka Tiered Storage (KIP-405, Kafka 3.0+)\n",
    "# Archiva logs antiguos a S3, mantiene recientes en SSD\n",
    "\"\"\"\n",
    "kafka-storage.properties:\n",
    "  remote.log.storage.enable=true\n",
    "  remote.log.storage.manager.class=org.apache.kafka.server.log.remote.storage.RemoteLogStorageManager\n",
    "  \n",
    "Estructura:\n",
    "  Local SSD: Ãšltimos 7 dÃ­as (hot data)\n",
    "  S3 Bucket: >7 dÃ­as (cold data, comprimido)\n",
    "  \n",
    "Cost reduction: 70% vs all-SSD\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_kappa = {\n",
    "    \"1. Simplicidad Operacional\": \"\"\"\n",
    "        Un solo pipeline â†’ un solo sistema para monitorear\n",
    "        Un solo lenguaje/framework (Flink o Spark)\n",
    "        Un solo cluster para operar\n",
    "        \n",
    "        On-call rotation:\n",
    "        Lambda: 2 sistemas Ã— 2 equipos = 4 rotaciones/semana\n",
    "        Kappa: 1 sistema Ã— 1 equipo = 2 rotaciones/semana\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. No CÃ³digo Duplicado\": \"\"\"\n",
    "        Misma lÃ³gica para batch y stream\n",
    "        \n",
    "        Lambda: 2000 lÃ­neas batch.py + 2000 lÃ­neas stream.py = 4000\n",
    "        Kappa: 2000 lÃ­neas unified.py = 2000 (50% menos cÃ³digo)\n",
    "        \n",
    "        Bug fix: 1 cambio vs 2 cambios\n",
    "        Testing: 1 suite vs 2 suites\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Reprocessing RÃ¡pido\": \"\"\"\n",
    "        Cambio de lÃ³gica â†’ replay log en horas\n",
    "        \n",
    "        Lambda:\n",
    "        - Reescribir batch job (dÃ­as de dev)\n",
    "        - Ejecutar en cluster grande (4h)\n",
    "        - Validar, debug, iterar\n",
    "        Total: 1-2 semanas\n",
    "        \n",
    "        Kappa:\n",
    "        - Modificar stream job (horas de dev)\n",
    "        - Replay desde offset 0 (4h)\n",
    "        - Compare old vs new output\n",
    "        Total: 1 dÃ­a\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Exactly-Once MÃ¡s Simple\": \"\"\"\n",
    "        Kafka transactions + idempotent producers\n",
    "        \n",
    "        Flink + Kafka:\n",
    "        - Checkpointing en Kafka offsets\n",
    "        - Transactional writes a sinks\n",
    "        - No necesidad de merge batch+speed\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch tiene at-least-once (reintentos)\n",
    "        - Speed tiene exactly-once (complejo)\n",
    "        - Merge puede duplicar sin deduplication\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Desventajas de Kappa:**\n",
    "\n",
    "```python\n",
    "desventajas_kappa = {\n",
    "    \"1. Costo de Almacenamiento\": \"\"\"\n",
    "        Kafka con 1 aÃ±o de retenciÃ³n = caro\n",
    "        \n",
    "        Ejemplo: 10K events/s Ã— 10 KB/event Ã— 1 aÃ±o\n",
    "        = 3.15 PB/aÃ±o crudo\n",
    "        Con compaction + tiered storage: 500 TB\n",
    "        Cost: $5K/mes S3 + $10K/mes Kafka brokers\n",
    "        \n",
    "        vs Hadoop HDFS: $2K/mes (pero sin streaming)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Reprocessing Largo\": \"\"\"\n",
    "        Replay 1 aÃ±o de datos = horas/dÃ­as\n",
    "        \n",
    "        1 TB data @ 100 MB/s = 2.8 horas ideal\n",
    "        Real (with processing): 6-12 horas\n",
    "        \n",
    "        Durante reprocessing:\n",
    "        - Nuevo cÃ³digo corre en paralelo (doble costo)\n",
    "        - Output dual (old version + new version)\n",
    "        - ValidaciÃ³n manual necesaria\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch recompute overnight (no impact users)\n",
    "        - Speed layer sigue sirviendo trÃ¡fico\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. State Management Complejo\": \"\"\"\n",
    "        Streaming state para 1 aÃ±o de datos = grande\n",
    "        \n",
    "        Ejemplo: User profiles con aggregations\n",
    "        10M users Ã— 1 KB state = 10 GB state\n",
    "        \n",
    "        Flink RocksDB:\n",
    "        - State en disco (slower)\n",
    "        - Checkpointing largo (minutes)\n",
    "        - Recovery largo si crash (restore state)\n",
    "        \n",
    "        Lambda batch:\n",
    "        - No state (stateless join con full tables)\n",
    "        - Simpler, pero mÃ¡s lento\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. No OptimizaciÃ³n por Caso\": \"\"\"\n",
    "        Streaming debe servir TODOS los casos\n",
    "        \n",
    "        Lambda:\n",
    "        - Batch: Columnar Parquet, predicate pushdown, Z-order\n",
    "        - Speed: Row-based, in-memory, indexes\n",
    "        \n",
    "        Kappa:\n",
    "        - Un formato compromiso (no Ã³ptimo para ninguno)\n",
    "        - Queries complejos (joins 5 tables) lentos en stream\n",
    "        - Queries simples (count) overkill con stream\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Ideales para Kappa:**\n",
    "\n",
    "```python\n",
    "# 1. REAL-TIME FIRST con poco histÃ³rico\n",
    "\"\"\"\n",
    "Empresa: Fintech fraud detection\n",
    "Datos: 90 dÃ­as de transacciones\n",
    "Procesamiento: <100ms latency requerido\n",
    "Reprocessing: Semanal (ajustar modelos ML)\n",
    "\n",
    "Kappa perfecto:\n",
    "- 90 dÃ­as en Kafka (100 TB, manejable)\n",
    "- Flink streaming para detecciÃ³n real-time\n",
    "- Replay semanal para ajustar modelos (4h)\n",
    "\"\"\"\n",
    "\n",
    "# 2. EVENT SOURCING puro\n",
    "\"\"\"\n",
    "Empresa: Booking platform\n",
    "PatrÃ³n: Event sourcing (todo es evento inmutable)\n",
    "Datos: bookings, cancellations, modifications\n",
    "Queries: Reconstruir estado actual desde eventos\n",
    "\n",
    "Kappa natural:\n",
    "- Kafka como event store (immutable log)\n",
    "- Flink materializa vistas (current bookings)\n",
    "- Replay para nuevas vistas (add \"bookings by hotel\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. HIGH CHANGE FREQUENCY en lÃ³gica\n",
    "\"\"\"\n",
    "Empresa: Ad-tech con modelos A/B testing constante\n",
    "SituaciÃ³n: Cambio de lÃ³gica 2-3x/semana\n",
    "Necesidad: Deploy rÃ¡pido, validar, iterar\n",
    "\n",
    "Kappa ventaja:\n",
    "- Modify stream job (1h dev)\n",
    "- Deploy v2 parallel v1 (30 min)\n",
    "- Replay Ãºltimo dÃ­a (30 min)\n",
    "- Compare outputs, cutover (1h)\n",
    "Total: 3 horas por cambio vs 3 dÃ­as Lambda\n",
    "\"\"\"\n",
    "\n",
    "# 4. SMALL-MEDIUM data scale\n",
    "\"\"\"\n",
    "Empresa: SaaS startup\n",
    "Datos: <100 TB total\n",
    "Users: <1M\n",
    "Traffic: <1K events/s\n",
    "\n",
    "Kappa ventaja:\n",
    "- Un cluster Kafka+Flink (simple)\n",
    "- RetenciÃ³n 1 aÃ±o fÃ¡cil (10 TB en Kafka)\n",
    "- Equipo pequeÃ±o puede mantener\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Anti-Patterns (CuÃ¡ndo NO usar Kappa):**\n",
    "\n",
    "```python\n",
    "# âŒ 1. HUGE data scale con queries complejos\n",
    "\"\"\"\n",
    "Empresa: Retail con 10+ aÃ±os de histÃ³rico\n",
    "Datos: 100 PB en HDFS\n",
    "Queries: SQL complejo (20+ joins, window functions)\n",
    "\n",
    "Problema Kappa:\n",
    "- 100 PB en Kafka = $500K/mes (vs $50K HDFS)\n",
    "- Streaming joins lentos vs batch columnar\n",
    "- Replay 10 aÃ±os = semanas (inviable)\n",
    "\n",
    "Better: Lambda o Delta Lakehouse\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 2. BATCH-FIRST workloads\n",
    "\"\"\"\n",
    "Empresa: Data warehouse con reportes nocturnos\n",
    "Queries: ETL batch 90% workload, streaming 10%\n",
    "Latency: Horas OK para reportes\n",
    "\n",
    "Problema Kappa:\n",
    "- Overhead streaming para batch workloads\n",
    "- Kafka infraestructura innecesaria\n",
    "- Spark batch mÃ¡s eficiente que Spark Streaming para batch\n",
    "\n",
    "Better: Tradicional batch (Airflow + Spark)\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 3. ESTRICTA compliance con reprocessing prohibido\n",
    "\"\"\"\n",
    "Empresa: Banco con regulaciones estrictas\n",
    "Requerimiento: AuditorÃ­as requieren datos exactos \"as-of\"\n",
    "ProhibiciÃ³n: No se puede \"rehacer\" cÃ¡lculos pasados\n",
    "\n",
    "Problema Kappa:\n",
    "- Reprocessing es core feature (pero regulador dice no)\n",
    "- Inmutabilidad requiere batch layer aparte\n",
    "\n",
    "Better: Lambda con batch layer auditado/certificado\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Ejemplo Real: Uber (Kappa en ProducciÃ³n):**\n",
    "\n",
    "```python\n",
    "uber_kappa = {\n",
    "    \"Caso\": \"Surge pricing (precios dinÃ¡micos)\",\n",
    "    \n",
    "    \"Arquitectura\": {\n",
    "        \"Eventos\": \"ride_requests, driver_locations, ride_completions\",\n",
    "        \"Kafka\": \"500K events/s, retenciÃ³n 7 dÃ­as (suficiente)\",\n",
    "        \"Flink\": \"Windowed aggregations (supply/demand por Ã¡rea)\",\n",
    "        \"Output\": \"Redis (precios actuales), Cassandra (histÃ³rico)\",\n",
    "        \"Latency\": \"<200ms end-to-end\"\n",
    "    },\n",
    "    \n",
    "    \"Kappa Benefits\": \"\"\"\n",
    "        1. Cambio frecuente de algoritmo pricing (weekly)\n",
    "        2. Replay 7 dÃ­as para validar nuevo algoritmo (2h)\n",
    "        3. A/B testing: correr 2 versiones paralelo, compare\n",
    "        4. No batch layer needed (solo Ãºltimos dÃ­as relevantes)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Evolution\": \"\"\"\n",
    "        2015: Lambda (Spark batch + Storm streaming)\n",
    "        2016: Migrated to Kappa (Samza streaming only)\n",
    "        2018: Moved to Flink (better stateful processing)\n",
    "        2020: Hybrid (Kappa para real-time, Hudi para analytics)\n",
    "        \n",
    "        RazÃ³n hybrid: Analytics largo plazo necesita batch optimizations\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea605ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "â™¾ï¸ ARQUITECTURA KAPPA - STREAMING UNIFICADO\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Kafka Topic: 'events' (retention: 30 dÃ­as)\n",
      "âœ… Log almacenado: 15,000 eventos\n",
      "ğŸ“… Rango temporal: 2025-11-09 â†’ 2025-12-09\n",
      "ğŸ’¾ Storage aprox: 7.32 MB (50% compresiÃ³n)\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ STREAMING JOB V1 - Procesamiento en tiempo real\n",
      "================================================================================\n",
      "âš¡ Streaming job procesando desde offset 14,900\n",
      "ğŸ“Š Eventos procesados: 100\n",
      "â±ï¸ Latencia promedio: 0.13ms por evento\n",
      "ğŸ’° Revenue acumulado: $6,046.20\n",
      "ğŸ‘¥ Usuarios activos: 97\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ REPROCESSING - Deploy nueva versiÃ³n (replay completo)\n",
      "================================================================================\n",
      "ğŸ”„ Escenario: Nueva lÃ³gica requiere recalcular TODOS los resultados\n",
      "   â€¢ Cambio: Agregar detecciÃ³n de anomalÃ­as en compras\n",
      "   â€¢ SoluciÃ³n Kappa: Replay del log completo desde offset 0\n",
      "\n",
      "âœ… Replay completado en 0.01s\n",
      "ğŸ“ˆ Eventos reprocesados: 15,000 (100% del log)\n",
      "ğŸš¨ AnomalÃ­as detectadas: 3 (0.02%)\n",
      "ğŸ’° Total revenue: $1,142,590.63\n",
      "ğŸ‘¥ Usuarios con anomalÃ­as: 3\n",
      "\n",
      "ğŸ“‹ Top 5 usuarios con mÃ¡s anomalÃ­as:\n",
      "   U0184: 1 anomalÃ­as de 15 eventos ($942.03)\n",
      "   U0373: 1 anomalÃ­as de 22 eventos ($957.25)\n",
      "   U0590: 1 anomalÃ­as de 20 eventos ($900.70)\n",
      "   U0001: 0 anomalÃ­as de 14 eventos ($935.05)\n",
      "   U0002: 0 anomalÃ­as de 22 eventos ($1393.97)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š KAPPA vs LAMBDA\n",
      "================================================================================\n",
      "        Aspecto                       Lambda                           Kappa   Ganador\n",
      "         CÃ³digo 2 codebases (batch + stream)             1 codebase (stream)   âœ… Kappa\n",
      "   Reprocessing   Batch separado (MapReduce) Replay del log (mismo pipeline)   âœ… Kappa\n",
      "       Latencia     Speed: <1s, Batch: horas                     Siempre <1s   âœ… Kappa\n",
      "  Costo storage    Batch: S3, Speed: memoria             Kafka log (costoso) âš ï¸ Lambda\n",
      "Estado complejo             Batch: stateless      Stream: stateful (RocksDB) âš ï¸ Lambda\n",
      "\n",
      "================================================================================\n",
      "âš–ï¸ TRADE-OFFS DE KAPPA\n",
      "================================================================================\n",
      "\n",
      "Ventajas:\n",
      "  âœ… Simplicidad operacional (1 sistema)\n",
      "  âœ… Consistencia de lÃ³gica (sin duplicaciÃ³n)\n",
      "  âœ… Latencia baja para todo\n",
      "  âœ… Reprocessing con mismo cÃ³digo\n",
      "\n",
      "Desventajas:\n",
      "  âš ï¸ Kafka storage costoso (30+ dÃ­as de eventos)\n",
      "  âš ï¸ Estado complejo en streaming (checkpoints grandes)\n",
      "  âš ï¸ Replay lento para histÃ³ricos masivos (aÃ±os)\n",
      "  âš ï¸ No optimizado para queries batch complejos\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Kappa completada\n",
      "   â€¢ Log de Kafka: 15,000 eventos (30 dÃ­as)\n",
      "   â€¢ Streaming V1 procesÃ³ 100 eventos en tiempo real\n",
      "   â€¢ Reprocessing V2 replayÃ³ 15,000 eventos en 0.01s\n",
      "   â€¢ AnomalÃ­as detectadas: 3\n",
      "================================================================================\n",
      "âœ… Log almacenado: 15,000 eventos\n",
      "ğŸ“… Rango temporal: 2025-11-09 â†’ 2025-12-09\n",
      "ğŸ’¾ Storage aprox: 7.32 MB (50% compresiÃ³n)\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ STREAMING JOB V1 - Procesamiento en tiempo real\n",
      "================================================================================\n",
      "âš¡ Streaming job procesando desde offset 14,900\n",
      "ğŸ“Š Eventos procesados: 100\n",
      "â±ï¸ Latencia promedio: 0.13ms por evento\n",
      "ğŸ’° Revenue acumulado: $6,046.20\n",
      "ğŸ‘¥ Usuarios activos: 97\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ REPROCESSING - Deploy nueva versiÃ³n (replay completo)\n",
      "================================================================================\n",
      "ğŸ”„ Escenario: Nueva lÃ³gica requiere recalcular TODOS los resultados\n",
      "   â€¢ Cambio: Agregar detecciÃ³n de anomalÃ­as en compras\n",
      "   â€¢ SoluciÃ³n Kappa: Replay del log completo desde offset 0\n",
      "\n",
      "âœ… Replay completado en 0.01s\n",
      "ğŸ“ˆ Eventos reprocesados: 15,000 (100% del log)\n",
      "ğŸš¨ AnomalÃ­as detectadas: 3 (0.02%)\n",
      "ğŸ’° Total revenue: $1,142,590.63\n",
      "ğŸ‘¥ Usuarios con anomalÃ­as: 3\n",
      "\n",
      "ğŸ“‹ Top 5 usuarios con mÃ¡s anomalÃ­as:\n",
      "   U0184: 1 anomalÃ­as de 15 eventos ($942.03)\n",
      "   U0373: 1 anomalÃ­as de 22 eventos ($957.25)\n",
      "   U0590: 1 anomalÃ­as de 20 eventos ($900.70)\n",
      "   U0001: 0 anomalÃ­as de 14 eventos ($935.05)\n",
      "   U0002: 0 anomalÃ­as de 22 eventos ($1393.97)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š KAPPA vs LAMBDA\n",
      "================================================================================\n",
      "        Aspecto                       Lambda                           Kappa   Ganador\n",
      "         CÃ³digo 2 codebases (batch + stream)             1 codebase (stream)   âœ… Kappa\n",
      "   Reprocessing   Batch separado (MapReduce) Replay del log (mismo pipeline)   âœ… Kappa\n",
      "       Latencia     Speed: <1s, Batch: horas                     Siempre <1s   âœ… Kappa\n",
      "  Costo storage    Batch: S3, Speed: memoria             Kafka log (costoso) âš ï¸ Lambda\n",
      "Estado complejo             Batch: stateless      Stream: stateful (RocksDB) âš ï¸ Lambda\n",
      "\n",
      "================================================================================\n",
      "âš–ï¸ TRADE-OFFS DE KAPPA\n",
      "================================================================================\n",
      "\n",
      "Ventajas:\n",
      "  âœ… Simplicidad operacional (1 sistema)\n",
      "  âœ… Consistencia de lÃ³gica (sin duplicaciÃ³n)\n",
      "  âœ… Latencia baja para todo\n",
      "  âœ… Reprocessing con mismo cÃ³digo\n",
      "\n",
      "Desventajas:\n",
      "  âš ï¸ Kafka storage costoso (30+ dÃ­as de eventos)\n",
      "  âš ï¸ Estado complejo en streaming (checkpoints grandes)\n",
      "  âš ï¸ Replay lento para histÃ³ricos masivos (aÃ±os)\n",
      "  âš ï¸ No optimizado para queries batch complejos\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Kappa completada\n",
      "   â€¢ Log de Kafka: 15,000 eventos (30 dÃ­as)\n",
      "   â€¢ Streaming V1 procesÃ³ 100 eventos en tiempo real\n",
      "   â€¢ Reprocessing V2 replayÃ³ 15,000 eventos en 0.01s\n",
      "   â€¢ AnomalÃ­as detectadas: 3\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ SimulaciÃ³n PrÃ¡ctica: Arquitectura Kappa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"â™¾ï¸ ARQUITECTURA KAPPA - STREAMING UNIFICADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= KAFKA LOG INMUTABLE =============\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Simular Kafka log con 30 dÃ­as de retenciÃ³n\n",
    "print(\"\\nğŸ“š Kafka Topic: 'events' (retention: 30 dÃ­as)\")\n",
    "events_log = []\n",
    "for i in range(15000):\n",
    "    timestamp = now - timedelta(days=np.random.randint(0, 30),\n",
    "                                 hours=np.random.randint(0, 24),\n",
    "                                 minutes=np.random.randint(0, 60))\n",
    "    events_log.append({\n",
    "        'offset': i,\n",
    "        'timestamp': timestamp,\n",
    "        'user_id': f'U{np.random.randint(1, 800):04d}',\n",
    "        'event_type': np.random.choice(['view', 'click', 'purchase', 'cart_add']),\n",
    "        'amount': np.random.uniform(10, 500) if np.random.random() > 0.7 else 0\n",
    "    })\n",
    "\n",
    "df_kafka = pd.DataFrame(events_log).sort_values('timestamp').reset_index(drop=True)\n",
    "df_kafka['offset'] = range(len(df_kafka))\n",
    "\n",
    "print(f\"âœ… Log almacenado: {len(df_kafka):,} eventos\")\n",
    "print(f\"ğŸ“… Rango temporal: {df_kafka['timestamp'].min().date()} â†’ {df_kafka['timestamp'].max().date()}\")\n",
    "print(f\"ğŸ’¾ Storage aprox: {len(df_kafka) * 0.5 / 1024:.2f} MB (50% compresiÃ³n)\")\n",
    "\n",
    "# ============= STREAMING JOB V1 (ProducciÃ³n actual) =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1ï¸âƒ£ STREAMING JOB V1 - Procesamiento en tiempo real\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Simular consumo desde Ãºltimo offset (streaming continuo)\n",
    "current_offset = len(df_kafka) - 100  # Ãšltimos 100 eventos\n",
    "df_streaming = df_kafka[df_kafka['offset'] >= current_offset].copy()\n",
    "\n",
    "# Agregaciones en ventanas de tiempo (micro-batches)\n",
    "df_streaming['window'] = df_streaming['timestamp'].dt.floor('5min')\n",
    "stream_results = df_streaming.groupby(['window', 'user_id']).agg({\n",
    "    'offset': 'count',\n",
    "    'amount': 'sum'\n",
    "}).rename(columns={'offset': 'events', 'amount': 'revenue'}).reset_index()\n",
    "\n",
    "stream_time = time.time() - start_time\n",
    "\n",
    "print(f\"âš¡ Streaming job procesando desde offset {current_offset:,}\")\n",
    "print(f\"ğŸ“Š Eventos procesados: {len(df_streaming):,}\")\n",
    "print(f\"â±ï¸ Latencia promedio: {stream_time * 1000 / len(df_streaming):.2f}ms por evento\")\n",
    "print(f\"ğŸ’° Revenue acumulado: ${stream_results['revenue'].sum():,.2f}\")\n",
    "print(f\"ğŸ‘¥ Usuarios activos: {df_streaming['user_id'].nunique()}\")\n",
    "\n",
    "# ============= REPROCESSING (Nueva lÃ³gica) =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2ï¸âƒ£ REPROCESSING - Deploy nueva versiÃ³n (replay completo)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"ğŸ”„ Escenario: Nueva lÃ³gica requiere recalcular TODOS los resultados\")\n",
    "print(\"   â€¢ Cambio: Agregar detecciÃ³n de anomalÃ­as en compras\")\n",
    "print(\"   â€¢ SoluciÃ³n Kappa: Replay del log completo desde offset 0\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Replay completo del log\n",
    "df_replay = df_kafka.copy()\n",
    "\n",
    "# Nueva lÃ³gica: detectar compras anÃ³malas (>3x el promedio del usuario)\n",
    "user_avg = df_replay[df_replay['amount'] > 0].groupby('user_id')['amount'].mean()\n",
    "df_replay['user_avg_purchase'] = df_replay['user_id'].map(user_avg)\n",
    "df_replay['is_anomaly'] = (df_replay['amount'] > 0) & (df_replay['amount'] > df_replay['user_avg_purchase'] * 3)\n",
    "\n",
    "anomalies = df_replay[df_replay['is_anomaly'] == True]\n",
    "\n",
    "# ReagregaciÃ³n con nueva lÃ³gica\n",
    "replay_results = df_replay.groupby('user_id').agg({\n",
    "    'offset': 'count',\n",
    "    'amount': 'sum',\n",
    "    'is_anomaly': 'sum'\n",
    "}).rename(columns={'offset': 'total_events', 'amount': 'total_revenue', 'is_anomaly': 'anomaly_count'}).reset_index()\n",
    "\n",
    "replay_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Replay completado en {replay_time:.2f}s\")\n",
    "print(f\"ğŸ“ˆ Eventos reprocesados: {len(df_replay):,} (100% del log)\")\n",
    "print(f\"ğŸš¨ AnomalÃ­as detectadas: {len(anomalies):,} ({len(anomalies)/len(df_replay)*100:.2f}%)\")\n",
    "print(f\"ğŸ’° Total revenue: ${replay_results['total_revenue'].sum():,.2f}\")\n",
    "print(f\"ğŸ‘¥ Usuarios con anomalÃ­as: {replay_results[replay_results['anomaly_count'] > 0].shape[0]}\")\n",
    "\n",
    "# Ejemplo de usuarios con anomalÃ­as\n",
    "print(\"\\nğŸ“‹ Top 5 usuarios con mÃ¡s anomalÃ­as:\")\n",
    "top_anomalies = replay_results.nlargest(5, 'anomaly_count')[['user_id', 'total_events', 'anomaly_count', 'total_revenue']]\n",
    "for _, row in top_anomalies.iterrows():\n",
    "    print(f\"   {row['user_id']}: {int(row['anomaly_count'])} anomalÃ­as de {int(row['total_events'])} eventos (${row['total_revenue']:.2f})\")\n",
    "\n",
    "# ============= COMPARACIÃ“N CON LAMBDA =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š KAPPA vs LAMBDA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Aspecto': 'CÃ³digo',\n",
    "        'Lambda': '2 codebases (batch + stream)',\n",
    "        'Kappa': '1 codebase (stream)',\n",
    "        'Ganador': 'âœ… Kappa'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Reprocessing',\n",
    "        'Lambda': 'Batch separado (MapReduce)',\n",
    "        'Kappa': 'Replay del log (mismo pipeline)',\n",
    "        'Ganador': 'âœ… Kappa'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Latencia',\n",
    "        'Lambda': 'Speed: <1s, Batch: horas',\n",
    "        'Kappa': 'Siempre <1s',\n",
    "        'Ganador': 'âœ… Kappa'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Costo storage',\n",
    "        'Lambda': 'Batch: S3, Speed: memoria',\n",
    "        'Kappa': 'Kafka log (costoso)',\n",
    "        'Ganador': 'âš ï¸ Lambda'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Estado complejo',\n",
    "        'Lambda': 'Batch: stateless',\n",
    "        'Kappa': 'Stream: stateful (RocksDB)',\n",
    "        'Ganador': 'âš ï¸ Lambda'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# ============= TRADE-OFFS ESPECÃFICOS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âš–ï¸ TRADE-OFFS DE KAPPA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trade_offs = {\n",
    "    'Ventajas': [\n",
    "        'âœ… Simplicidad operacional (1 sistema)',\n",
    "        'âœ… Consistencia de lÃ³gica (sin duplicaciÃ³n)',\n",
    "        'âœ… Latencia baja para todo',\n",
    "        'âœ… Reprocessing con mismo cÃ³digo'\n",
    "    ],\n",
    "    'Desventajas': [\n",
    "        'âš ï¸ Kafka storage costoso (30+ dÃ­as de eventos)',\n",
    "        'âš ï¸ Estado complejo en streaming (checkpoints grandes)',\n",
    "        'âš ï¸ Replay lento para histÃ³ricos masivos (aÃ±os)',\n",
    "        'âš ï¸ No optimizado para queries batch complejos'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in trade_offs.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SimulaciÃ³n Kappa completada\")\n",
    "print(f\"   â€¢ Log de Kafka: {len(df_kafka):,} eventos (30 dÃ­as)\")\n",
    "print(f\"   â€¢ Streaming V1 procesÃ³ {len(df_streaming):,} eventos en tiempo real\")\n",
    "print(f\"   â€¢ Reprocessing V2 replayÃ³ {len(df_replay):,} eventos en {replay_time:.2f}s\")\n",
    "print(f\"   â€¢ AnomalÃ­as detectadas: {len(anomalies):,}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d7c4",
   "metadata": {},
   "source": [
    "### ğŸ  **Delta Architecture (Lakehouse): Unified Batch+Stream**\n",
    "\n",
    "**EvoluciÃ³n Natural (2020+):**\n",
    "\n",
    "```\n",
    "Lambda (2011): Batch + Speed layers separados\n",
    "   â†“\n",
    "Kappa (2014): Solo streaming (pero caro, complejo)\n",
    "   â†“\n",
    "Delta (2020): Unified sobre transactional storage\n",
    "```\n",
    "\n",
    "**Arquitectura Lakehouse:**\n",
    "\n",
    "```python\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATA SOURCES                              â”‚\n",
    "â”‚  â€¢ Applications â†’ Kafka                                      â”‚\n",
    "â”‚  â€¢ Databases â†’ CDC (Debezium)                                â”‚\n",
    "â”‚  â€¢ Batch files â†’ S3/ADLS                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚  INGESTION LAYER â”‚\n",
    "          â”‚                  â”‚\n",
    "          â”‚  Spark Streaming â”‚ â† Unified engine!\n",
    "          â”‚  (micro-batches) â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              DELTA LAKE / ICEBERG STORAGE                     â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Bronze (Raw)           Silver (Cleaned)      Gold (Curated) â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Append-onlyâ”‚        â”‚ Deduplicatedâ”‚      â”‚Aggregationsâ”‚  â”‚\n",
    "â”‚  â”‚ Parquet +  â”‚   â†’    â”‚ Validated   â”‚  â†’   â”‚Star Schema â”‚  â”‚\n",
    "â”‚  â”‚ Delta Log  â”‚        â”‚ Enriched    â”‚      â”‚Business    â”‚  â”‚\n",
    "â”‚  â”‚            â”‚        â”‚             â”‚      â”‚Logic       â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Features:                                                    â”‚\n",
    "â”‚  âœ… ACID Transactions                                        â”‚\n",
    "â”‚  âœ… Time Travel (rollback, audit)                           â”‚\n",
    "â”‚  âœ… Schema Evolution                                         â”‚\n",
    "â”‚  âœ… Upserts/Deletes                                          â”‚\n",
    "â”‚  âœ… Unified Batch+Stream reads                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚                    â”‚\n",
    "          â–¼                    â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ BATCH QUERIES â”‚    â”‚  STREAMING   â”‚\n",
    "  â”‚               â”‚    â”‚   QUERIES    â”‚\n",
    "  â”‚ Spark SQL     â”‚    â”‚ Spark SS     â”‚\n",
    "  â”‚ Presto/Trino  â”‚    â”‚ Flink        â”‚\n",
    "  â”‚ Athena        â”‚    â”‚ Real-time    â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                    â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   CONSUMPTION   â”‚\n",
    "            â”‚                 â”‚\n",
    "            â”‚  â€¢ BI Tools     â”‚\n",
    "            â”‚  â€¢ ML Training  â”‚\n",
    "            â”‚  â€¢ APIs         â”‚\n",
    "            â”‚  â€¢ Dashboards   â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**CaracterÃ­sticas Clave:**\n",
    "\n",
    "```python\n",
    "delta_features = {\n",
    "    \"1. ACID Transactions en Object Storage\": \"\"\"\n",
    "        Problema tradicional:\n",
    "        S3/ADLS no tienen transacciones\n",
    "        â†’ Race conditions, partial writes\n",
    "        \n",
    "        Delta Lake solution:\n",
    "        _delta_log/00000000000000000123.json\n",
    "        {\n",
    "          \"commitInfo\": {\"timestamp\": \"2025-10-30T10:30:00\"},\n",
    "          \"add\": [\n",
    "            {\"path\": \"part-00000.parquet\", \"size\": 1024000},\n",
    "            {\"path\": \"part-00001.parquet\", \"size\": 1024000}\n",
    "          ],\n",
    "          \"remove\": []\n",
    "        }\n",
    "        \n",
    "        Atomic commit: Write log entry = commit completo\n",
    "        Readers ven versiÃ³n consistente (snapshot isolation)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Unified Batch + Stream\": \"\"\"\n",
    "        SAME TABLE para batch y streaming\n",
    "        \n",
    "        Escritura streaming:\n",
    "        df.writeStream\n",
    "          .format(\"delta\")\n",
    "          .outputMode(\"append\")\n",
    "          .start(\"/delta/events\")\n",
    "        \n",
    "        Lectura batch:\n",
    "        spark.read\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")\n",
    "          .where(\"date = '2025-10-30'\")\n",
    "        \n",
    "        Lectura streaming:\n",
    "        spark.readStream\n",
    "          .format(\"delta\")\n",
    "          .load(\"/delta/events\")  # Lee cambios incrementales\n",
    "        \n",
    "        NO necesitas batch layer separada!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Time Travel\": \"\"\"\n",
    "        Acceso a versiones histÃ³ricas\n",
    "        \n",
    "        # Leer versiÃ³n especÃ­fica\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"versionAsOf\", 42)\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        # Leer timestamp especÃ­fico\n",
    "        df = spark.read\n",
    "          .format(\"delta\")\n",
    "          .option(\"timestampAsOf\", \"2025-10-29 00:00:00\")\n",
    "          .load(\"/delta/events\")\n",
    "        \n",
    "        Casos de uso:\n",
    "        - Rollback: Deployment malo â†’ revert\n",
    "        - Audit: \"Â¿QuÃ© datos vio el modelo ayer?\"\n",
    "        - Debugging: \"Reproduce bug con datos exactos\"\n",
    "        - Compliance: \"Muestra datos as-of Q3 2025\"\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Schema Evolution\": \"\"\"\n",
    "        Agregar columnas sin downtime\n",
    "        \n",
    "        V1: {user_id, action, timestamp}\n",
    "        V2: {user_id, action, timestamp, device}  â† Add column\n",
    "        \n",
    "        Delta:\n",
    "        df_v2.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .save(\"/delta/events\")\n",
    "        \n",
    "        Lectores antiguos: device = null\n",
    "        Lectores nuevos: device leÃ­do correctamente\n",
    "        \n",
    "        NO necesitas backfill!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"5. Upserts (MERGE)\": \"\"\"\n",
    "        CDC y SCD Type 2 nativos\n",
    "        \n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, \"/delta/users\")\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            updates.alias(\"source\"),\n",
    "            \"target.user_id = source.user_id\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"email\": \"source.email\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).whenNotMatchedInsert(values={\n",
    "            \"user_id\": \"source.user_id\",\n",
    "            \"email\": \"source.email\",\n",
    "            \"created_at\": \"source.created_at\",\n",
    "            \"updated_at\": \"source.updated_at\"\n",
    "        }).execute()\n",
    "        \n",
    "        Streaming UPSERT:\n",
    "        updates.writeStream\n",
    "          .foreachBatch(lambda df, _: upsert_function(df))\n",
    "          .start()\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Medallion Architecture (Bronze/Silver/Gold):**\n",
    "\n",
    "```python\n",
    "# BRONZE: Raw data, append-only\n",
    "bronze_events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").cast(\"string\").alias(\"raw_json\"),\n",
    "        col(\"timestamp\").alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "bronze_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(\"/delta/bronze/events\")\n",
    "\n",
    "# SILVER: Cleaned, validated, deduplicated\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "silver_events = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/bronze/events\") \\\n",
    "    .select(\n",
    "        from_json(col(\"raw_json\"), event_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\  # ValidaciÃ³n\n",
    "    .dropDuplicates([\"user_id\", \"event_timestamp\"])  # Dedup\n",
    "\n",
    "silver_events.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver\") \\\n",
    "    .start(\"/delta/silver/events\")\n",
    "\n",
    "# GOLD: Business aggregations\n",
    "gold_user_stats = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/silver/events\") \\\n",
    "    .groupBy(\"user_id\", window(\"event_timestamp\", \"1 day\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"daily_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"event_types\")\n",
    "    )\n",
    "\n",
    "gold_user_stats.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/gold\") \\\n",
    "    .start(\"/delta/gold/user_daily_stats\")\n",
    "\n",
    "# BI Tool lee desde Gold\n",
    "df_dashboard = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/gold/user_daily_stats\") \\\n",
    "    .where(\"window.start >= current_date() - interval 7 days\")\n",
    "```\n",
    "\n",
    "**Ventajas sobre Lambda/Kappa:**\n",
    "\n",
    "```python\n",
    "ventajas_delta = {\n",
    "    \"vs Lambda\": {\n",
    "        \"CÃ³digo Unificado\": \"\"\"\n",
    "            Lambda: 2 pipelines (batch.py + stream.py)\n",
    "            Delta: 1 pipeline (unified.py)\n",
    "            \n",
    "            Reduction: 50% cÃ³digo, 50% tests, 50% bugs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Sin Merge Layer\": \"\"\"\n",
    "            Lambda: Query = merge(batch, speed) en serving\n",
    "            Delta: Query directo a Delta (single source)\n",
    "            \n",
    "            Latency: -100ms (no merge overhead)\n",
    "            Consistency: Guaranteed (ACID transactions)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Operaciones Simples\": \"\"\"\n",
    "            Lambda: Mantener 2 clusters + serving layer\n",
    "            Delta: 1 cluster Spark + Delta storage\n",
    "            \n",
    "            On-call: -50% alerts, -40% incidents\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Kappa\": {\n",
    "        \"Costo Storage\": \"\"\"\n",
    "            Kappa: Kafka 1 aÃ±o = $15K/mes\n",
    "            Delta: S3 object storage = $2K/mes\n",
    "            \n",
    "            Savings: 85% storage costs\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Batch Optimized\": \"\"\"\n",
    "            Kappa: Streaming para todo (no Ã³ptimo)\n",
    "            Delta: Columnar Parquet + statistics\n",
    "            \n",
    "            Query performance: 10x faster para analytics\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Reprocessing Flexible\": \"\"\"\n",
    "            Kappa: Replay Kafka (limited retention)\n",
    "            Delta: Time travel infinito\n",
    "            \n",
    "            Auditabilidad: 5+ aÃ±os histÃ³rico disponible\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    \"vs Traditional Warehouse\": {\n",
    "        \"Cost\": \"\"\"\n",
    "            Warehouse: $25/TB/month (Snowflake)\n",
    "            Delta: $0.023/GB/month (S3) = $23/TB\n",
    "            \n",
    "            Savings: ~50% at petabyte scale\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Flexibility\": \"\"\"\n",
    "            Warehouse: Vendor lock-in, SQL only\n",
    "            Delta: Open format, Spark/Presto/Flink/Python\n",
    "            \n",
    "            ML Integration: Native (same storage)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Scalability\": \"\"\"\n",
    "            Warehouse: Scale up (expensive)\n",
    "            Delta: Scale out (S3 infinite)\n",
    "            \n",
    "            Growth: No limits, pay-as-you-go\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "```python\n",
    "# 1. Databricks - Inventor de Delta Lake\n",
    "\"\"\"\n",
    "Cliente: Comcast (Telecomunicaciones)\n",
    "Datos: 2 PB diarios (network logs, streaming video, IoT)\n",
    "Arquitectura anterior: Lambda (Hadoop batch + Kafka streaming)\n",
    "\n",
    "Migration a Delta:\n",
    "- 2018: Pilot con 10 TB (1 mes)\n",
    "- 2019: Migration 500 TB (6 meses)\n",
    "- 2020: Full adoption 2 PB/dÃ­a\n",
    "\n",
    "Resultados:\n",
    "- Cost: -30% ($2M/aÃ±o savings)\n",
    "- Latency: 4h â†’ 15 min para dashboards\n",
    "- Code: -50% (unified batch+stream)\n",
    "- Incidents: -60% (ACID transactions)\n",
    "\n",
    "Stack:\n",
    "- Ingestion: Spark Streaming\n",
    "- Storage: Delta Lake en S3\n",
    "- Compute: Databricks clusters\n",
    "- Consumption: Tableau + ML models\n",
    "\"\"\"\n",
    "\n",
    "# 2. Riot Games (League of Legends)\n",
    "\"\"\"\n",
    "Caso: Game analytics + Player behavior\n",
    "Datos: 100M+ games/day, petabytes de events\n",
    "Latency: <5 min para anti-cheat, <1 hour para balance\n",
    "\n",
    "Delta Architecture:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Game Servers â”‚ â†’ Kafka (1M events/s)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Spark Stream â”‚ â†’ Delta Bronze (raw events)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Enrichment   â”‚ â†’ Delta Silver (validated)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "    â”Œâ”€â”€â”´â”€â”€â”\n",
    "    â”‚     â”‚\n",
    "    â–¼     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”\n",
    "â”‚ ML â”‚  â”‚ BI â”‚ â†’ Delta Gold (aggregations)\n",
    "â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜\n",
    "\n",
    "Benefits:\n",
    "- Unified: Game designers query same data ML uses\n",
    "- Time travel: \"Replay match from 2 days ago\"\n",
    "- Schema evolution: Add features without downtime\n",
    "\"\"\"\n",
    "\n",
    "# 3. Adobe Experience Platform\n",
    "\"\"\"\n",
    "Challenge: 100s of tenants, isolated data, compliance\n",
    "Solution: Delta Lake multi-tenant\n",
    "\n",
    "Architecture:\n",
    "/delta/tenant_001/events/\n",
    "/delta/tenant_002/events/\n",
    "...\n",
    "/delta/tenant_500/events/\n",
    "\n",
    "Features:\n",
    "- Row-level security (GDPR compliance)\n",
    "- Tenant isolation (Delta sharing)\n",
    "- Unified operations (single platform)\n",
    "- Time travel (data recovery per tenant)\n",
    "\n",
    "Scale:\n",
    "- 500+ tenants\n",
    "- 10 PB total data\n",
    "- 100K writes/s\n",
    "- <100ms p99 latency\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**MigraciÃ³n Lambda â†’ Delta:**\n",
    "\n",
    "```python\n",
    "migration_strategy = {\n",
    "    \"Phase 1: Setup (Month 1-2)\": \"\"\"\n",
    "        1. Provision Delta Lake infrastructure\n",
    "           - S3 bucket con versioning\n",
    "           - Unity Catalog setup\n",
    "           - Spark clusters (Databricks/EMR)\n",
    "        \n",
    "        2. Crear medallion structure\n",
    "           /delta/bronze/\n",
    "           /delta/silver/\n",
    "           /delta/gold/\n",
    "        \n",
    "        3. Setup CI/CD pipelines\n",
    "           - GitHub Actions para deploy\n",
    "           - Testing framework (pytest)\n",
    "           - Monitoring (Datadog/Prometheus)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 2: Bronze Layer (Month 3-4)\": \"\"\"\n",
    "        1. Migrate raw ingestion\n",
    "           Kafka â†’ Delta Bronze (append-only)\n",
    "           \n",
    "        2. Dual-write period\n",
    "           - Write to both Lambda batch + Delta\n",
    "           - Compare row counts, checksums\n",
    "           \n",
    "        3. Backfill histÃ³rico\n",
    "           HDFS/Hive â†’ Delta Bronze\n",
    "           (1-time copy, then delete HDFS)\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 3: Silver Layer (Month 5-6)\": \"\"\"\n",
    "        1. Migrate transformation logic\n",
    "           Batch Spark jobs â†’ Delta pipelines\n",
    "           \n",
    "        2. Unified batch+stream\n",
    "           - Remove duplicate code\n",
    "           - Single pipeline para ambos\n",
    "           \n",
    "        3. Data quality checks\n",
    "           Great Expectations integration\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 4: Gold Layer (Month 7-8)\": \"\"\"\n",
    "        1. Migrate aggregations\n",
    "           Speed layer â†’ Delta Gold micro-batches\n",
    "           \n",
    "        2. Decommission serving layer\n",
    "           Druid/Cassandra â†’ query Delta directly\n",
    "           \n",
    "        3. BI tools reconfigure\n",
    "           Point Tableau/Power BI to Delta\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 5: Validation (Month 9)\": \"\"\"\n",
    "        1. Parallel run\n",
    "           - Lambda still running (backup)\n",
    "           - Delta serving production traffic\n",
    "           \n",
    "        2. Compare metrics\n",
    "           - Latency, accuracy, cost\n",
    "           - User feedback (BI teams)\n",
    "        \n",
    "        3. Fix edge cases\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Phase 6: Cutover (Month 10)\": \"\"\"\n",
    "        1. Decommission Lambda\n",
    "           - Stop batch jobs\n",
    "           - Stop speed layer\n",
    "           - Delete Druid/Cassandra\n",
    "        \n",
    "        2. Cleanup\n",
    "           - Delete HDFS clusters\n",
    "           - Reclaim savings\n",
    "        \n",
    "        3. Training\n",
    "           - Team training on Delta\n",
    "           - Documentation update\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total\": \"\"\"\n",
    "        Duration: 10 months\n",
    "        Cost: $300K engineering + $100K infra\n",
    "        Ongoing savings: $50K/mes\n",
    "        ROI: 8 months\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "best_practices = {\n",
    "    \"1. Partition Strategy\": \"\"\"\n",
    "        âœ… Date partitioning (daily/hourly)\n",
    "        partitionBy(\"date\")\n",
    "        \n",
    "        âš ï¸ Z-ordering para high-cardinality\n",
    "        OPTIMIZE events ZORDER BY (user_id, product_id)\n",
    "        \n",
    "        âŒ Over-partitioning\n",
    "        partitionBy(\"date\", \"hour\", \"user_id\")  # Millones de particiones!\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Compaction\": \"\"\"\n",
    "        Auto-compact para streaming:\n",
    "        .option(\"autoCompact\", \"true\")\n",
    "        \n",
    "        Scheduled OPTIMIZE:\n",
    "        OPTIMIZE events WHERE date >= current_date() - 7\n",
    "        \n",
    "        VACUUM para cleanup:\n",
    "        VACUUM events RETAIN 168 HOURS  # 7 dÃ­as\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Schema Evolution\": \"\"\"\n",
    "        Always mergeSchema para backward compatibility:\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        Schema validation:\n",
    "        df.write.format(\"delta\")\n",
    "          .option(\"enforceSchema\", \"true\")\n",
    "          .save()\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Monitoring\": \"\"\"\n",
    "        Metrics to track:\n",
    "        - Write throughput (records/s)\n",
    "        - Read latency (p50, p95, p99)\n",
    "        - File count (small files problem)\n",
    "        - Version count (retention policy)\n",
    "        - Transaction log size\n",
    "        \n",
    "        Alerts:\n",
    "        - File count >100K per partition\n",
    "        - Transaction log >10 MB\n",
    "        - Write latency p99 >5s\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd882d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ›ï¸ ARQUITECTURA DELTA (LAKEHOUSE) - UNIFIED BATCH + STREAMING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Lakehouse inicializado en: outputs\\delta_lakehouse_demo\\events_delta\n",
      "   â€¢ Data layer: Parquet files\n",
      "   â€¢ Metadata layer: Transaction log (ACID)\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ BATCH INGESTION - Cargar histÃ³rico\n",
      "================================================================================\n",
      "âœ… Batch write completado en 0.08s\n",
      "ğŸ“Š Registros escritos: 20,000\n",
      "ğŸ’¾ TamaÃ±o: 0.27 MB\n",
      "ğŸ“… Rango temporal: 2025-11-09 â†’ 2025-12-09\n",
      "ğŸ”– Version: 0 (initial load)\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ STREAMING INGESTION - Micro-batches cada 5 segundos\n",
      "================================================================================\n",
      "\n",
      "âš¡ Micro-batch #1:\n",
      "   âœ… Escrito en 5.53ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 1\n",
      "\n",
      "âš¡ Micro-batch #2:\n",
      "   âœ… Escrito en 3.01ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 2\n",
      "\n",
      "âš¡ Micro-batch #3:\n",
      "   âœ… Escrito en 4.80ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 3\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ UNIFIED QUERY - Lectura con Time Travel\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¸ Snapshot Query: Leer version 0 (solo batch histÃ³rico)\n",
      "   Registros: 20,000\n",
      "   Revenue: $1,516,429.23\n",
      "\n",
      "ğŸ“¸ Snapshot Query: Leer version 3 (batch + todos los streams)\n",
      "   Registros: 20,300\n",
      "   Revenue: $1,539,993.67\n",
      "   Incremento: +300 registros\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ ACID GUARANTEES - Transacciones\n",
      "================================================================================\n",
      "\n",
      "ğŸ”’ Transaction Log (linearizable history):\n",
      "\n",
      "   Version 0: WRITE\n",
      "   â†’ 20,000 rows, 271.59 KB\n",
      "\n",
      "   Version 1: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.06 KB\n",
      "\n",
      "   Version 2: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.20 KB\n",
      "\n",
      "   Version 3: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.08 KB\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š DELTA vs LAMBDA vs KAPPA\n",
      "================================================================================\n",
      "CaracterÃ­stica       Lambda                Kappa                Delta        Mejor\n",
      "        CÃ³digo  2 pipelines    1 pipeline stream 1 pipeline unificado  Delta/Kappa\n",
      "       Storage   S3 + Kafka      Kafka (costoso)     S3/ADLS (barato)        Delta\n",
      "          ACID         âŒ No                 âŒ No                 âœ… SÃ­        Delta\n",
      "   Time Travel         âŒ No Solo Kafka retention           âœ… Infinito        Delta\n",
      "      Latencia   Speed: <1s                  <1s  5-30s (micro-batch)        Kappa\n",
      " Batch queries âœ… Optimizado             âš ï¸ Lento         âœ… Optimizado Lambda/Delta\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ CUÃNDO USAR DELTA\n",
      "================================================================================\n",
      "\n",
      "âœ… Ideal:\n",
      "  â€¢ Lakehouse unificado (data warehouse + lake)\n",
      "  â€¢ GDPR/Compliance (auditabilidad, time travel)\n",
      "  â€¢ Batch + Streaming con misma lÃ³gica\n",
      "  â€¢ Latencia <30s aceptable\n",
      "  â€¢ Data science + analytics\n",
      "\n",
      "âš ï¸ Evitar:\n",
      "  â€¢ Latencia <1s crÃ­tica (usar Kappa)\n",
      "  â€¢ Streaming puro sin batch (usar Kappa)\n",
      "  â€¢ Legacy con Lambda existente (migraciÃ³n costosa)\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Delta completada\n",
      "   â€¢ Batch histÃ³rico: 20,000 eventos\n",
      "   â€¢ Streaming: 3 micro-batches, 300 eventos\n",
      "   â€¢ Total: 20,300 eventos\n",
      "   â€¢ Transaction log: 4 versiones con ACID\n",
      "   â€¢ Storage: 0.28 MB\n",
      "================================================================================\n",
      "âœ… Batch write completado en 0.08s\n",
      "ğŸ“Š Registros escritos: 20,000\n",
      "ğŸ’¾ TamaÃ±o: 0.27 MB\n",
      "ğŸ“… Rango temporal: 2025-11-09 â†’ 2025-12-09\n",
      "ğŸ”– Version: 0 (initial load)\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ STREAMING INGESTION - Micro-batches cada 5 segundos\n",
      "================================================================================\n",
      "\n",
      "âš¡ Micro-batch #1:\n",
      "   âœ… Escrito en 5.53ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 1\n",
      "\n",
      "âš¡ Micro-batch #2:\n",
      "   âœ… Escrito en 3.01ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 2\n",
      "\n",
      "âš¡ Micro-batch #3:\n",
      "   âœ… Escrito en 4.80ms\n",
      "   ğŸ“Š Registros: 100\n",
      "   ğŸ”– Version: 3\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ UNIFIED QUERY - Lectura con Time Travel\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¸ Snapshot Query: Leer version 0 (solo batch histÃ³rico)\n",
      "   Registros: 20,000\n",
      "   Revenue: $1,516,429.23\n",
      "\n",
      "ğŸ“¸ Snapshot Query: Leer version 3 (batch + todos los streams)\n",
      "   Registros: 20,300\n",
      "   Revenue: $1,539,993.67\n",
      "   Incremento: +300 registros\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ ACID GUARANTEES - Transacciones\n",
      "================================================================================\n",
      "\n",
      "ğŸ”’ Transaction Log (linearizable history):\n",
      "\n",
      "   Version 0: WRITE\n",
      "   â†’ 20,000 rows, 271.59 KB\n",
      "\n",
      "   Version 1: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.06 KB\n",
      "\n",
      "   Version 2: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.20 KB\n",
      "\n",
      "   Version 3: STREAMING_UPDATE\n",
      "   â†’ 100 rows, 5.08 KB\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š DELTA vs LAMBDA vs KAPPA\n",
      "================================================================================\n",
      "CaracterÃ­stica       Lambda                Kappa                Delta        Mejor\n",
      "        CÃ³digo  2 pipelines    1 pipeline stream 1 pipeline unificado  Delta/Kappa\n",
      "       Storage   S3 + Kafka      Kafka (costoso)     S3/ADLS (barato)        Delta\n",
      "          ACID         âŒ No                 âŒ No                 âœ… SÃ­        Delta\n",
      "   Time Travel         âŒ No Solo Kafka retention           âœ… Infinito        Delta\n",
      "      Latencia   Speed: <1s                  <1s  5-30s (micro-batch)        Kappa\n",
      " Batch queries âœ… Optimizado             âš ï¸ Lento         âœ… Optimizado Lambda/Delta\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ CUÃNDO USAR DELTA\n",
      "================================================================================\n",
      "\n",
      "âœ… Ideal:\n",
      "  â€¢ Lakehouse unificado (data warehouse + lake)\n",
      "  â€¢ GDPR/Compliance (auditabilidad, time travel)\n",
      "  â€¢ Batch + Streaming con misma lÃ³gica\n",
      "  â€¢ Latencia <30s aceptable\n",
      "  â€¢ Data science + analytics\n",
      "\n",
      "âš ï¸ Evitar:\n",
      "  â€¢ Latencia <1s crÃ­tica (usar Kappa)\n",
      "  â€¢ Streaming puro sin batch (usar Kappa)\n",
      "  â€¢ Legacy con Lambda existente (migraciÃ³n costosa)\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Delta completada\n",
      "   â€¢ Batch histÃ³rico: 20,000 eventos\n",
      "   â€¢ Streaming: 3 micro-batches, 300 eventos\n",
      "   â€¢ Total: 20,300 eventos\n",
      "   â€¢ Transaction log: 4 versiones con ACID\n",
      "   â€¢ Storage: 0.28 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ SimulaciÃ³n PrÃ¡ctica: Arquitectura Delta (Lakehouse)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ›ï¸ ARQUITECTURA DELTA (LAKEHOUSE) - UNIFIED BATCH + STREAMING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= SETUP LAKEHOUSE =============\n",
    "base_path = Path(\"outputs/delta_lakehouse_demo\")\n",
    "base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Simular Delta Lake con Parquet + transaction log\n",
    "delta_table_path = base_path / \"events_delta\"\n",
    "delta_table_path.mkdir(exist_ok=True)\n",
    "transaction_log_path = delta_table_path / \"_delta_log\"\n",
    "transaction_log_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ Lakehouse inicializado en: {delta_table_path}\")\n",
    "print(f\"   â€¢ Data layer: Parquet files\")\n",
    "print(f\"   â€¢ Metadata layer: Transaction log (ACID)\")\n",
    "\n",
    "# ============= BATCH INGESTION (HistÃ³rico) =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1ï¸âƒ£ BATCH INGESTION - Cargar histÃ³rico\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "# Generar datos histÃ³ricos (30 dÃ­as)\n",
    "historical_events = []\n",
    "for i in range(20000):\n",
    "    timestamp = now - timedelta(days=np.random.randint(0, 30),\n",
    "                                 hours=np.random.randint(0, 24))\n",
    "    historical_events.append({\n",
    "        'event_id': f'E{i:06d}',\n",
    "        'timestamp': timestamp,\n",
    "        'user_id': f'U{np.random.randint(1, 1000):04d}',\n",
    "        'event_type': np.random.choice(['view', 'click', 'purchase', 'cart_add']),\n",
    "        'amount': np.random.uniform(10, 500) if np.random.random() > 0.7 else 0\n",
    "    })\n",
    "\n",
    "df_batch = pd.DataFrame(historical_events)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Escribir como Parquet particionado (simulando Delta)\n",
    "df_batch['date'] = df_batch['timestamp'].dt.date\n",
    "batch_file = delta_table_path / \"batch_v0.parquet\"\n",
    "df_batch.to_parquet(batch_file, compression='snappy', index=False)\n",
    "\n",
    "# Transaction log entry\n",
    "transaction = {\n",
    "    'version': 0,\n",
    "    'timestamp': now.isoformat(),\n",
    "    'operation': 'WRITE',\n",
    "    'operationMetrics': {\n",
    "        'numFiles': 1,\n",
    "        'numOutputRows': len(df_batch),\n",
    "        'numOutputBytes': batch_file.stat().st_size\n",
    "    }\n",
    "}\n",
    "with open(transaction_log_path / \"00000000000000000000.json\", 'w') as f:\n",
    "    json.dump(transaction, f, indent=2)\n",
    "\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Batch write completado en {batch_time:.2f}s\")\n",
    "print(f\"ğŸ“Š Registros escritos: {len(df_batch):,}\")\n",
    "print(f\"ğŸ’¾ TamaÃ±o: {batch_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"ğŸ“… Rango temporal: {df_batch['date'].min()} â†’ {df_batch['date'].max()}\")\n",
    "print(f\"ğŸ”– Version: 0 (initial load)\")\n",
    "\n",
    "# ============= STREAMING INGESTION (Micro-batches) =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2ï¸âƒ£ STREAMING INGESTION - Micro-batches cada 5 segundos\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simular 3 micro-batches de streaming\n",
    "for batch_num in range(1, 4):\n",
    "    print(f\"\\nâš¡ Micro-batch #{batch_num}:\")\n",
    "    \n",
    "    # Generar nuevos eventos\n",
    "    new_events = []\n",
    "    for i in range(100):\n",
    "        new_events.append({\n",
    "            'event_id': f'S{batch_num}{i:04d}',\n",
    "            'timestamp': now + timedelta(seconds=batch_num * 5),\n",
    "            'user_id': f'U{np.random.randint(1, 1000):04d}',\n",
    "            'event_type': np.random.choice(['view', 'click', 'purchase', 'cart_add']),\n",
    "            'amount': np.random.uniform(10, 500) if np.random.random() > 0.7 else 0\n",
    "        })\n",
    "    \n",
    "    df_stream = pd.DataFrame(new_events)\n",
    "    df_stream['date'] = df_stream['timestamp'].dt.date\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Escribir micro-batch (ACID transaction)\n",
    "    stream_file = delta_table_path / f\"stream_v{batch_num}.parquet\"\n",
    "    df_stream.to_parquet(stream_file, compression='snappy', index=False)\n",
    "    \n",
    "    # Transaction log\n",
    "    transaction = {\n",
    "        'version': batch_num,\n",
    "        'timestamp': (now + timedelta(seconds=batch_num * 5)).isoformat(),\n",
    "        'operation': 'STREAMING_UPDATE',\n",
    "        'operationMetrics': {\n",
    "            'numFiles': 1,\n",
    "            'numOutputRows': len(df_stream),\n",
    "            'numOutputBytes': stream_file.stat().st_size\n",
    "        }\n",
    "    }\n",
    "    with open(transaction_log_path / f\"{batch_num:020d}.json\", 'w') as f:\n",
    "        json.dump(transaction, f, indent=2)\n",
    "    \n",
    "    stream_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… Escrito en {stream_time*1000:.2f}ms\")\n",
    "    print(f\"   ğŸ“Š Registros: {len(df_stream)}\")\n",
    "    print(f\"   ğŸ”– Version: {batch_num}\")\n",
    "\n",
    "# ============= UNIFIED QUERY (Time Travel) =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3ï¸âƒ£ UNIFIED QUERY - Lectura con Time Travel\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Leer versiÃ³n especÃ­fica (snapshot isolation)\n",
    "print(\"\\nğŸ“¸ Snapshot Query: Leer version 0 (solo batch histÃ³rico)\")\n",
    "df_v0 = pd.read_parquet(delta_table_path / \"batch_v0.parquet\")\n",
    "print(f\"   Registros: {len(df_v0):,}\")\n",
    "print(f\"   Revenue: ${df_v0['amount'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\nğŸ“¸ Snapshot Query: Leer version 3 (batch + todos los streams)\")\n",
    "all_files = list(delta_table_path.glob(\"*.parquet\"))\n",
    "df_current = pd.concat([pd.read_parquet(f) for f in all_files], ignore_index=True)\n",
    "print(f\"   Registros: {len(df_current):,}\")\n",
    "print(f\"   Revenue: ${df_current['amount'].sum():,.2f}\")\n",
    "print(f\"   Incremento: +{len(df_current) - len(df_v0):,} registros\")\n",
    "\n",
    "# ============= ACID GUARANTEES =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4ï¸âƒ£ ACID GUARANTEES - Transacciones\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ”’ Transaction Log (linearizable history):\")\n",
    "for log_file in sorted(transaction_log_path.glob(\"*.json\")):\n",
    "    with open(log_file) as f:\n",
    "        tx = json.load(f)\n",
    "    print(f\"\\n   Version {tx['version']}: {tx['operation']}\")\n",
    "    print(f\"   â†’ {tx['operationMetrics']['numOutputRows']:,} rows, \"\n",
    "          f\"{tx['operationMetrics']['numOutputBytes'] / 1024:.2f} KB\")\n",
    "\n",
    "# ============= COMPARACIÃ“N CON LAMBDA/KAPPA =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š DELTA vs LAMBDA vs KAPPA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'CaracterÃ­stica': 'CÃ³digo',\n",
    "        'Lambda': '2 pipelines',\n",
    "        'Kappa': '1 pipeline stream',\n",
    "        'Delta': '1 pipeline unificado',\n",
    "        'Mejor': 'Delta/Kappa'\n",
    "    },\n",
    "    {\n",
    "        'CaracterÃ­stica': 'Storage',\n",
    "        'Lambda': 'S3 + Kafka',\n",
    "        'Kappa': 'Kafka (costoso)',\n",
    "        'Delta': 'S3/ADLS (barato)',\n",
    "        'Mejor': 'Delta'\n",
    "    },\n",
    "    {\n",
    "        'CaracterÃ­stica': 'ACID',\n",
    "        'Lambda': 'âŒ No',\n",
    "        'Kappa': 'âŒ No',\n",
    "        'Delta': 'âœ… SÃ­',\n",
    "        'Mejor': 'Delta'\n",
    "    },\n",
    "    {\n",
    "        'CaracterÃ­stica': 'Time Travel',\n",
    "        'Lambda': 'âŒ No',\n",
    "        'Kappa': 'Solo Kafka retention',\n",
    "        'Delta': 'âœ… Infinito',\n",
    "        'Mejor': 'Delta'\n",
    "    },\n",
    "    {\n",
    "        'CaracterÃ­stica': 'Latencia',\n",
    "        'Lambda': 'Speed: <1s',\n",
    "        'Kappa': '<1s',\n",
    "        'Delta': '5-30s (micro-batch)',\n",
    "        'Mejor': 'Kappa'\n",
    "    },\n",
    "    {\n",
    "        'CaracterÃ­stica': 'Batch queries',\n",
    "        'Lambda': 'âœ… Optimizado',\n",
    "        'Kappa': 'âš ï¸ Lento',\n",
    "        'Delta': 'âœ… Optimizado',\n",
    "        'Mejor': 'Lambda/Delta'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# ============= CASOS DE USO IDEALES =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ CUÃNDO USAR DELTA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "use_cases = {\n",
    "    'âœ… Ideal': [\n",
    "        'Lakehouse unificado (data warehouse + lake)',\n",
    "        'GDPR/Compliance (auditabilidad, time travel)',\n",
    "        'Batch + Streaming con misma lÃ³gica',\n",
    "        'Latencia <30s aceptable',\n",
    "        'Data science + analytics'\n",
    "    ],\n",
    "    'âš ï¸ Evitar': [\n",
    "        'Latencia <1s crÃ­tica (usar Kappa)',\n",
    "        'Streaming puro sin batch (usar Kappa)',\n",
    "        'Legacy con Lambda existente (migraciÃ³n costosa)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in use_cases.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  â€¢ {item}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SimulaciÃ³n Delta completada\")\n",
    "print(f\"   â€¢ Batch histÃ³rico: {len(df_batch):,} eventos\")\n",
    "print(f\"   â€¢ Streaming: 3 micro-batches, 300 eventos\")\n",
    "print(f\"   â€¢ Total: {len(df_current):,} eventos\")\n",
    "print(f\"   â€¢ Transaction log: 4 versiones con ACID\")\n",
    "print(f\"   â€¢ Storage: {sum(f.stat().st_size for f in all_files) / 1024 / 1024:.2f} MB\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fb62d",
   "metadata": {},
   "source": [
    "### ğŸŒ **Data Mesh: Paradigma Organizacional Descentralizado**\n",
    "\n",
    "**Problema: Data Platform Centralizado (Monolito)**\n",
    "\n",
    "```\n",
    "OrganizaciÃ³n tradicional:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         CENTRAL DATA TEAM (20 personas)        â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  Responsable de:                                â”‚\n",
    "â”‚  â€¢ Todos los pipelines                         â”‚\n",
    "â”‚  â€¢ Todos los data models                       â”‚\n",
    "â”‚  â€¢ Todos los dashboards                        â”‚\n",
    "â”‚  â€¢ Todos los ML models                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  DATA WAREHOUSE   â”‚\n",
    "   â”‚   (Single DB)     â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                 â”‚\n",
    "    â–¼                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Ventas â”‚      â”‚LogÃ­stica â”‚ ... (10+ dominios)\n",
    "â”‚ Team   â”‚      â”‚ Team     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“                 â†“\n",
    "Request            Request\n",
    "pipeline          pipeline\n",
    "   â†“                 â†“\n",
    "  â° 3 meses        â° 3 meses\n",
    "  ğŸ› Bugs           ğŸ› Bugs\n",
    "  ğŸ“‰ Low quality    ğŸ“‰ Low quality\n",
    "\n",
    "Problemas:\n",
    "âŒ Bottleneck: Central team sobrecargado (backlog 6+ meses)\n",
    "âŒ Context loss: Data team no entiende dominio de negocio\n",
    "âŒ Scalability: Imposible crecer linearmente\n",
    "âŒ Ownership: Nadie responsable de calidad de datos\n",
    "```\n",
    "\n",
    "**SoluciÃ³n: Data Mesh (Zhamak Dehghani, 2019)**\n",
    "\n",
    "```\n",
    "Data Mesh: DescentralizaciÃ³n por dominio\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        FEDERATED GOVERNANCE                  â”‚\n",
    "â”‚  (PolÃ­ticas centrales, enforcement local)    â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚              â”‚              â”‚\n",
    "â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "â”‚ VENTAS   â”‚  â”‚LOGÃSTICA â”‚  â”‚ FINANZAS â”‚\n",
    "â”‚ DOMAIN   â”‚  â”‚ DOMAIN   â”‚  â”‚ DOMAIN   â”‚\n",
    "â”‚          â”‚  â”‚          â”‚  â”‚          â”‚\n",
    "â”‚ Data     â”‚  â”‚ Data     â”‚  â”‚ Data     â”‚\n",
    "â”‚ Product  â”‚  â”‚ Product  â”‚  â”‚ Product  â”‚\n",
    "â”‚ Owner    â”‚  â”‚ Owner    â”‚  â”‚ Owner    â”‚\n",
    "â”‚          â”‚  â”‚          â”‚  â”‚          â”‚\n",
    "â”‚ â€¢ ETL    â”‚  â”‚ â€¢ ETL    â”‚  â”‚ â€¢ ETL    â”‚\n",
    "â”‚ â€¢ Qualityâ”‚  â”‚ â€¢ Qualityâ”‚  â”‚ â€¢ Qualityâ”‚\n",
    "â”‚ â€¢ API    â”‚  â”‚ â€¢ API    â”‚  â”‚ â€¢ API    â”‚\n",
    "â”‚ â€¢ Docs   â”‚  â”‚ â€¢ Docs   â”‚  â”‚ â€¢ Docs   â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚              â”‚              â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ SELF-SERVE PLATFORM â”‚\n",
    "         â”‚ (Infra comÃºn)       â”‚\n",
    "         â”‚ â€¢ Spark/Airflow     â”‚\n",
    "         â”‚ â€¢ Monitoring        â”‚\n",
    "         â”‚ â€¢ Data Catalog      â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**4 Principios Fundamentales:**\n",
    "\n",
    "```python\n",
    "principios_data_mesh = {\n",
    "    \"1. Domain-Oriented Decentralization\": \"\"\"\n",
    "        Datos pertenecen al dominio de negocio\n",
    "        \n",
    "        Ejemplo: E-commerce\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ DOMAIN: Ventas                          â”‚\n",
    "        â”‚ Owner: VP of Sales                      â”‚\n",
    "        â”‚ Data Products:                          â”‚\n",
    "        â”‚  â€¢ orders (transaccional)               â”‚\n",
    "        â”‚  â€¢ orders_aggregated (analÃ­tico)        â”‚\n",
    "        â”‚  â€¢ customer_segments (ML)               â”‚\n",
    "        â”‚                                         â”‚\n",
    "        â”‚ Team composition:                       â”‚\n",
    "        â”‚  â€¢ Product Manager (prioridades)        â”‚\n",
    "        â”‚  â€¢ Data Engineer (pipelines)            â”‚\n",
    "        â”‚  â€¢ Analytics Engineer (dbt models)      â”‚\n",
    "        â”‚  â€¢ Data Analyst (consumers)             â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Ventajas:\n",
    "        âœ… Domain expertise: Equipo entiende negocio\n",
    "        âœ… Velocity: No esperar central team\n",
    "        âœ… Accountability: Owner claro de calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Data as a Product\": \"\"\"\n",
    "        Cada dataset es un producto con:\n",
    "        \n",
    "        ğŸ“‹ SLA (Service Level Agreement):\n",
    "        - Latency: <15 min para orders_aggregated\n",
    "        - Availability: 99.9% uptime\n",
    "        - Freshness: Updated cada 5 min\n",
    "        - Quality: >95% completeness\n",
    "        \n",
    "        ğŸ“– Documentation:\n",
    "        - Schema: Columnas, tipos, constraints\n",
    "        - Lineage: De dÃ³nde viene cada campo\n",
    "        - Examples: Query samples\n",
    "        - Contact: Slack channel, owner email\n",
    "        \n",
    "        ğŸ”’ Access Control:\n",
    "        - Public: Todos pueden leer\n",
    "        - Private: Solo equipo ventas\n",
    "        - PII: Masked para no-autorizados\n",
    "        \n",
    "        ğŸ“Š Observability:\n",
    "        - Metrics: Row count, size, update time\n",
    "        - Alerts: SLA violations\n",
    "        - Changelog: Version history\n",
    "        \n",
    "        Ejemplo manifest.yaml:\n",
    "        ```\n",
    "        product_name: orders_aggregated\n",
    "        owner: ventas-data@company.com\n",
    "        sla:\n",
    "          latency_minutes: 15\n",
    "          availability_percent: 99.9\n",
    "          quality_score: 0.95\n",
    "        schema_url: s3://catalog/ventas/orders_agg/schema.json\n",
    "        lineage:\n",
    "          sources:\n",
    "            - orders_raw\n",
    "            - customers\n",
    "            - products\n",
    "        access:\n",
    "          read: [sales_team, finance_team, executives]\n",
    "          write: [sales_data_engineers]\n",
    "        ```\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Self-Serve Data Platform\": \"\"\"\n",
    "        Infraestructura comÃºn para todos los dominios\n",
    "        \n",
    "        Platform Components:\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ COMPUTE                              â”‚\n",
    "        â”‚  â€¢ Spark clusters (on-demand)       â”‚\n",
    "        â”‚  â€¢ Airflow (orchestration)          â”‚\n",
    "        â”‚  â€¢ dbt (transformations)            â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ STORAGE                              â”‚\n",
    "        â”‚  â€¢ Delta Lake (lakehouse)           â”‚\n",
    "        â”‚  â€¢ S3 buckets (per-domain)          â”‚\n",
    "        â”‚  â€¢ Unity Catalog (metadata)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ OBSERVABILITY                        â”‚\n",
    "        â”‚  â€¢ Datadog (metrics, logs)          â”‚\n",
    "        â”‚  â€¢ Great Expectations (quality)     â”‚\n",
    "        â”‚  â€¢ OpenLineage (lineage tracking)   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ GOVERNANCE                           â”‚\n",
    "        â”‚  â€¢ Data Catalog (Amundsen/DataHub)  â”‚\n",
    "        â”‚  â€¢ Access Control (Ranger/Unity)    â”‚\n",
    "        â”‚  â€¢ Policy Engine (OPA)              â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Self-service workflow:\n",
    "        1. Team crea repo: github.com/company/ventas-data\n",
    "        2. Define product: manifest.yaml + dbt models\n",
    "        3. CI/CD deploy: Tests â†’ Stage â†’ Prod\n",
    "        4. Auto-register: Catalog actualizado\n",
    "        5. Monitoring: Dashboards auto-generated\n",
    "        \n",
    "        Platform team provee:\n",
    "        - Templates (cookiecutter)\n",
    "        - Best practices docs\n",
    "        - On-call support\n",
    "        - Cost monitoring\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Federated Computational Governance\": \"\"\"\n",
    "        PolÃ­ticas globales + AutonomÃ­a local\n",
    "        \n",
    "        GLOBAL POLICIES (Central governance):\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ 1. Security                         â”‚\n",
    "        â”‚    â€¢ PII must be encrypted at rest â”‚\n",
    "        â”‚    â€¢ Access logs retained 1 year   â”‚\n",
    "        â”‚    â€¢ MFA required for production   â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 2. Compliance (GDPR, CCPA)         â”‚\n",
    "        â”‚    â€¢ Right to be forgotten         â”‚\n",
    "        â”‚    â€¢ Data retention max 7 years    â”‚\n",
    "        â”‚    â€¢ Audit trail required          â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 3. Quality                          â”‚\n",
    "        â”‚    â€¢ All products have tests       â”‚\n",
    "        â”‚    â€¢ SLA violations reported       â”‚\n",
    "        â”‚    â€¢ Schema changes versioned      â”‚\n",
    "        â”‚                                     â”‚\n",
    "        â”‚ 4. Interoperability                â”‚\n",
    "        â”‚    â€¢ Standard formats (Parquet)    â”‚\n",
    "        â”‚    â€¢ Common IDs (user_id format)   â”‚\n",
    "        â”‚    â€¢ Shared ontology (terms)       â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        LOCAL AUTONOMY (Domain teams):\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ â€¢ Choose transformation tools       â”‚\n",
    "        â”‚   (dbt, Spark, Python)             â”‚\n",
    "        â”‚ â€¢ Define data models               â”‚\n",
    "        â”‚ â€¢ Set refresh frequency            â”‚\n",
    "        â”‚ â€¢ Manage access within domain      â”‚\n",
    "        â”‚ â€¢ Optimize costs                   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        Enforcement:\n",
    "        - Automated checks in CI/CD\n",
    "        - Policy-as-code (OPA policies)\n",
    "        - Failing checks block deploy\n",
    "        - Dashboards show compliance\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**ImplementaciÃ³n Real: Data Mesh Stack**\n",
    "\n",
    "```python\n",
    "# Estructura organizacional\n",
    "data_mesh_org = {\n",
    "    \"Domain Teams (AutÃ³nomos)\": {\n",
    "        \"Ventas Domain\": {\n",
    "            \"Members\": \"8 personas\",\n",
    "            \"Products\": [\n",
    "                \"orders (transactional)\",\n",
    "                \"orders_daily_agg (analytical)\",\n",
    "                \"customer_rfm_segments (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Spark, dbt, Delta Lake\",\n",
    "            \"Budget\": \"$50K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"LogÃ­stica Domain\": {\n",
    "            \"Members\": \"6 personas\",\n",
    "            \"Products\": [\n",
    "                \"shipments (operational)\",\n",
    "                \"delivery_performance (metrics)\",\n",
    "                \"route_optimization (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Flink, Kafka, Iceberg\",\n",
    "            \"Budget\": \"$40K/mes\"\n",
    "        },\n",
    "        \n",
    "        \"Marketing Domain\": {\n",
    "            \"Members\": \"5 personas\",\n",
    "            \"Products\": [\n",
    "                \"campaigns (metadata)\",\n",
    "                \"attribution (analytics)\",\n",
    "                \"propensity_scores (ML)\"\n",
    "            ],\n",
    "            \"Tech Stack\": \"Airflow, Python, Delta Lake\",\n",
    "            \"Budget\": \"$30K/mes\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Platform Team (Enabling)\": {\n",
    "        \"Members\": \"10 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Maintain Spark/Airflow infrastructure\",\n",
    "            \"Operate Data Catalog\",\n",
    "            \"Enforce governance policies\",\n",
    "            \"Cost optimization\",\n",
    "            \"Training & documentation\"\n",
    "        ],\n",
    "        \"Budget\": \"$100K/mes\",\n",
    "        \"Ratio\": \"1 platform engineer per 20 domain engineers\"\n",
    "    },\n",
    "    \n",
    "    \"Governance Team (Federated)\": {\n",
    "        \"Members\": \"3 personas\",\n",
    "        \"Responsibilities\": [\n",
    "            \"Define global policies\",\n",
    "            \"Compliance (GDPR, SOX)\",\n",
    "            \"Audit & reporting\",\n",
    "            \"Cross-domain standards\"\n",
    "        ],\n",
    "        \"Budget\": \"$20K/mes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data Product Example\n",
    "data_product_spec = '''\n",
    "# orders_daily_agg Data Product\n",
    "\n",
    "## Overview\n",
    "Daily aggregated orders for BI dashboards and reporting.\n",
    "\n",
    "## Owner\n",
    "- Team: Ventas Data Engineering\n",
    "- Contact: ventas-data@company.com\n",
    "- Slack: #ventas-data\n",
    "\n",
    "## SLA\n",
    "- Freshness: Updated daily @6AM UTC\n",
    "- Latency: <30 minutes processing time\n",
    "- Availability: 99.9% (max 8.76h downtime/year)\n",
    "- Quality: >99% completeness, >98% accuracy\n",
    "\n",
    "## Schema\n",
    "| Column | Type | Description | PII |\n",
    "|--------|------|-------------|-----|\n",
    "| date | DATE | Order date | No |\n",
    "| country | STRING | Country code (ISO) | No |\n",
    "| total_orders | BIGINT | Count of orders | No |\n",
    "| total_revenue | DECIMAL(10,2) | Sum of revenue (USD) | No |\n",
    "| avg_order_value | DECIMAL(10,2) | Average order value | No |\n",
    "\n",
    "## Lineage\n",
    "```\n",
    "orders_raw (Bronze)\n",
    "    â†“\n",
    "orders_validated (Silver)\n",
    "    â†“\n",
    "orders_daily_agg (Gold) â† THIS PRODUCT\n",
    "```\n",
    "\n",
    "## Access\n",
    "- Read: sales_team, finance_team, executives\n",
    "- Write: sales_data_engineers\n",
    "- Admin: sales_domain_owner\n",
    "\n",
    "## Usage Examples\n",
    "```sql\n",
    "-- Get last 7 days\n",
    "SELECT * FROM orders_daily_agg\n",
    "WHERE date >= CURRENT_DATE - INTERVAL 7 DAYS\n",
    "ORDER BY date DESC\n",
    "\n",
    "-- YoY comparison\n",
    "SELECT \n",
    "    date,\n",
    "    total_revenue,\n",
    "    LAG(total_revenue, 365) OVER (ORDER BY date) as prev_year_revenue\n",
    "FROM orders_daily_agg\n",
    "WHERE date >= '2024-01-01'\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "- Dashboard: https://monitoring.company.com/ventas/orders_agg\n",
    "- Alerts: PagerDuty team \"sales-data-oncall\"\n",
    "\n",
    "## Changelog\n",
    "- 2025-10-01: Added avg_order_value column\n",
    "- 2025-09-15: Changed country to ISO codes\n",
    "- 2025-08-01: Initial release\n",
    "'''\n",
    "```\n",
    "\n",
    "**Ventajas de Data Mesh:**\n",
    "\n",
    "```python\n",
    "ventajas_mesh = {\n",
    "    \"1. Scalability\": \"\"\"\n",
    "        Tradicional: Central team â†’ linear growth impossible\n",
    "        10 domains Ã— 1 central team = bottleneck\n",
    "        \n",
    "        Data Mesh: Domain teams â†’ linear scaling\n",
    "        10 domains Ã— 10 teams = 100 engineers productive\n",
    "        \n",
    "        Growth: Agregar dominio no afecta otros\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Domain Expertise\": \"\"\"\n",
    "        Central team:\n",
    "        \"Â¿QuÃ© significa 'adjusted_revenue'?\"\n",
    "        â†’ Ask business (delays)\n",
    "        \n",
    "        Domain team:\n",
    "        Engineers trabajan con business daily\n",
    "        â†’ Deep understanding, mejor calidad\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Velocity\": \"\"\"\n",
    "        Backlog central: 6 meses\n",
    "        Domain ownership: 2 semanas\n",
    "        \n",
    "        Feature request â†’ deploy:\n",
    "        Central: 3-6 meses\n",
    "        Mesh: 1-2 sprints\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Quality\": \"\"\"\n",
    "        Central team: Best effort (no ownership)\n",
    "        Domain team: Accountable (SLAs, metrics)\n",
    "        \n",
    "        Data quality:\n",
    "        Central: 85% average\n",
    "        Mesh: 95% average (incentivized)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**DesafÃ­os de Data Mesh:**\n",
    "\n",
    "```python\n",
    "desafios_mesh = {\n",
    "    \"1. Organizational Maturity\": \"\"\"\n",
    "        Requiere:\n",
    "        - DevOps culture (CI/CD, testing)\n",
    "        - Data literacy en equipos de negocio\n",
    "        - Executive buy-in (budget descentralizado)\n",
    "        \n",
    "        NO funciona si:\n",
    "        - OrganizaciÃ³n muy jerÃ¡rquica\n",
    "        - Equipos no autÃ³nomos\n",
    "        - Sin cultura de ownership\n",
    "        \n",
    "        Ejemplo fracaso:\n",
    "        Company X intentÃ³ mesh sin DevOps\n",
    "        â†’ Dominios no saben deploy pipelines\n",
    "        â†’ Rollback a central team en 6 meses\n",
    "    \"\"\",\n",
    "    \n",
    "    \"2. Platform Complexity\": \"\"\"\n",
    "        Self-serve platform = sophisticated\n",
    "        \n",
    "        Requiere:\n",
    "        - Multi-tenancy (aislamiento dominios)\n",
    "        - Cost tracking per domain\n",
    "        - Automated provisioning\n",
    "        - Standardized observability\n",
    "        \n",
    "        Platform team: 10-15 personas mÃ­nimo\n",
    "        Cost: $1M+/aÃ±o (salaries + infra)\n",
    "        \n",
    "        Break-even: ~100+ data engineers total\n",
    "    \"\"\",\n",
    "    \n",
    "    \"3. Duplication Risk\": \"\"\"\n",
    "        Sin governance fuerte:\n",
    "        - 3 equipos crean \"customer_dim\" diferente\n",
    "        - MÃ©tricas inconsistentes cross-domain\n",
    "        - Redundant data copies (cost)\n",
    "        \n",
    "        MitigaciÃ³n:\n",
    "        - Data Catalog mandatory\n",
    "        - Shared domain for common entities\n",
    "        - Regular cross-domain sync meetings\n",
    "    \"\"\",\n",
    "    \n",
    "    \"4. Skillset Gap\": \"\"\"\n",
    "        Domain engineers necesitan:\n",
    "        - Data engineering (ETL)\n",
    "        - Analytics engineering (dbt)\n",
    "        - DevOps (CI/CD)\n",
    "        - Domain knowledge (business)\n",
    "        \n",
    "        Hiring difficult: Full-stack data engineers\n",
    "        Training: 6-12 meses ramp-up\n",
    "        \n",
    "        Alternative: Embedded specialists\n",
    "        - 1 data engineer per 2 domains\n",
    "        - Rotates between teams\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Casos de Uso (CuÃ¡ndo Data Mesh Funciona):**\n",
    "\n",
    "```python\n",
    "# âœ… 1. LARGE ORG con mÃºltiples dominios\n",
    "\"\"\"\n",
    "Empresa: Zalando (E-commerce Europeo)\n",
    "TamaÃ±o: 10,000+ empleados, 20+ paÃ­ses\n",
    "Dominios: 50+ (Fashion, Logistics, Payments, ...)\n",
    "\n",
    "Data Mesh adoption (2020):\n",
    "- 50 domain teams autÃ³nomos\n",
    "- Platform team: 25 personas\n",
    "- 200+ data products published\n",
    "- Self-serve: 90% requests no need central\n",
    "\n",
    "Results:\n",
    "- Time-to-market: 6 meses â†’ 2 semanas\n",
    "- Data quality: 80% â†’ 95%\n",
    "- Team satisfaction: +40%\n",
    "\"\"\"\n",
    "\n",
    "# âœ… 2. DISTRIBUTED company\n",
    "\"\"\"\n",
    "Empresa: Confluent (remote-first)\n",
    "Dominios: Customer Success, Engineering, Sales\n",
    "Challenge: Central team = timezone hell\n",
    "\n",
    "Data Mesh benefits:\n",
    "- Teams own data in their timezone\n",
    "- Async collaboration (via catalog)\n",
    "- No central bottleneck\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 3. SMALL STARTUP (<100 personas)\n",
    "\"\"\"\n",
    "Mesh overhead > benefits\n",
    "Better: Small central team (5 personas)\n",
    "Mesh when: Reach 200+ engineers\n",
    "\"\"\"\n",
    "\n",
    "# âŒ 4. HIGHLY REGULATED (sin autonomÃ­a)\n",
    "\"\"\"\n",
    "Banking: Regulators require central control\n",
    "Mesh autonomy conflicts with compliance\n",
    "Better: Centralized with strong governance\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Migration Path: Central â†’ Data Mesh**\n",
    "\n",
    "```python\n",
    "migration_roadmap = {\n",
    "    \"Year 1: Foundation\": \"\"\"\n",
    "        Q1: Executive alignment\n",
    "        - Present mesh vision\n",
    "        - Secure budget\n",
    "        - Identify pilot domain\n",
    "        \n",
    "        Q2: Platform MVP\n",
    "        - Self-serve Spark/Airflow\n",
    "        - Data Catalog (Amundsen)\n",
    "        - Basic CI/CD templates\n",
    "        \n",
    "        Q3-Q4: Pilot domain\n",
    "        - Choose 1 domain (e.g., Sales)\n",
    "        - Migrate 3 products\n",
    "        - Learn lessons, iterate\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 2: Scale\": \"\"\"\n",
    "        Q1-Q2: Onboard 5 domains\n",
    "        - Training programs\n",
    "        - Platform improvements\n",
    "        - Governance policies drafted\n",
    "        \n",
    "        Q3-Q4: Federated governance\n",
    "        - Policy engine (OPA)\n",
    "        - Compliance automation\n",
    "        - Cross-domain standards\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Year 3: Maturity\": \"\"\"\n",
    "        Q1-Q2: Onboard remaining domains\n",
    "        - 100% domains on mesh\n",
    "        - Decommission central team legacy\n",
    "        \n",
    "        Q3-Q4: Optimization\n",
    "        - Cost optimization\n",
    "        - Advanced features (ML platform)\n",
    "        - Community of practice\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Total Investment\": \"\"\"\n",
    "        Platform team: 10 personas Ã— $150K Ã— 3 aÃ±os = $4.5M\n",
    "        Training: $500K\n",
    "        Tooling: $1M\n",
    "        Total: $6M\n",
    "        \n",
    "        Break-even: Year 4 (velocity gains)\n",
    "    \"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beaa6b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸŒ DATA MESH - ARQUITECTURA DESCENTRALIZADA POR DOMINIOS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¢ OrganizaciÃ³n: E-commerce Multilatino\n",
      "   â€¢ 3 dominios de negocio con ownership descentralizado\n",
      "   â€¢ Plataforma self-service compartida\n",
      "   â€¢ Gobernanza federada\n",
      "\n",
      "================================================================================\n",
      "ğŸ›’ DOMINIO 1: VENTAS (Sales Domain)\n",
      "================================================================================\n",
      "âœ… Data Product publicado: sales.transactions\n",
      "   Owner: sales-team@company.com\n",
      "   SLO Freshness: <15 min\n",
      "   Registros: 5,000\n",
      "   Revenue total: $1,269,904.63\n",
      "\n",
      "================================================================================\n",
      "ğŸ“¦ DOMINIO 2: LOGÃSTICA (Logistics Domain)\n",
      "================================================================================\n",
      "âœ… Data Product publicado: logistics.shipments\n",
      "   Owner: logistics-team@company.com\n",
      "   SLO Freshness: <5 min\n",
      "   Registros: 4,500\n",
      "   Entregas completadas: 1,197\n",
      "\n",
      "================================================================================\n",
      "ğŸ’° DOMINIO 3: FINANZAS (Finance Domain)\n",
      "================================================================================\n",
      "âœ… Data Product publicado: logistics.shipments\n",
      "   Owner: logistics-team@company.com\n",
      "   SLO Freshness: <5 min\n",
      "   Registros: 4,500\n",
      "   Entregas completadas: 1,197\n",
      "\n",
      "================================================================================\n",
      "ğŸ’° DOMINIO 3: FINANZAS (Finance Domain)\n",
      "================================================================================\n",
      "âœ… Data Product publicado: finance.payments\n",
      "   Owner: finance-team@company.com\n",
      "   SLO Freshness: <1 min (real-time)\n",
      "   Registros: 4,800\n",
      "   Pagos aprobados: $1,044,848.71\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š CONSUMER: ANALYTICS DOMAIN (Cross-Domain Query)\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Caso: AnÃ¡lisis de conversiÃ³n completo (Sales â†’ Logistics â†’ Finance)\n",
      "\n",
      "ğŸ“ˆ MÃ©tricas del Funnel:\n",
      "   Ventas iniciadas: 5,000\n",
      "   EnvÃ­os creados: 4,500 (90.0%)\n",
      "   Pagos aprobados: 4,085 (81.7%)\n",
      "   Entregas completadas: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "ğŸ” GOBERNANZA FEDERADA\n",
      "================================================================================\n",
      "   Domain Data Product          Owner               PII Retention SLO Freshness\n",
      "    Sales transactions     sales-team Yes (customer_id)   7 years       <15 min\n",
      "Logistics    shipments logistics-team                No   3 years        <5 min\n",
      "  Finance     payments   finance-team  Yes (payment_id)  10 years        <1 min\n",
      "\n",
      "================================================================================\n",
      "ğŸ§­ PRINCIPIOS DE DATA MESH APLICADOS\n",
      "================================================================================\n",
      "\n",
      "1. Domain-Oriented Decentralization:\n",
      "  âœ… 3 dominios con ownership claro\n",
      "  âœ… Cada equipo publica sus propios Data Products\n",
      "  âœ… No hay equipo central de datos\n",
      "\n",
      "2. Data as a Product:\n",
      "  âœ… Contratos versionados (sales v2.1.0, logistics v1.8.2, finance v3.0.1)\n",
      "  âœ… SLOs definidos (freshness, availability, quality)\n",
      "  âœ… DocumentaciÃ³n completa (schema, governance)\n",
      "\n",
      "3. Self-Serve Platform:\n",
      "  âœ… Storage comÃºn (Parquet en file system)\n",
      "  âœ… Formato estÃ¡ndar (Parquet + JSON contracts)\n",
      "  âœ… Discovery automÃ¡tico (contracts.json)\n",
      "\n",
      "4. Federated Governance:\n",
      "  âœ… PolÃ­ticas globales (PII marcado, retention definido)\n",
      "  âœ… AutonomÃ­a local (cada dominio elige su SLO)\n",
      "  âœ… Compliance distribuido (PCI-DSS en finance, GDPR en sales)\n",
      "\n",
      "================================================================================\n",
      "âš–ï¸ TRADE-OFFS DE DATA MESH\n",
      "================================================================================\n",
      "            Aspecto          Monolito                Data Mesh\n",
      "  Escalabilidad org âš ï¸ Cuello botella    âœ… Lineal con dominios\n",
      "     Time to market    Semanas (cola)         DÃ­as (autonomÃ­a)\n",
      "Complejidad tÃ©cnica Baja (1 pipeline)    âš ï¸ Alta (N pipelines)\n",
      "  Madurez requerida   Junior friendly âš ï¸ Senior+ (DevOps, SRE)\n",
      "   Costos iniciales              Bajo     âš ï¸ Alto (plataforma)\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Data Mesh completada\n",
      "   â€¢ 3 dominios publicaron Data Products\n",
      "   â€¢ 14,300 registros totales\n",
      "   â€¢ Analytics consumiÃ³ cross-domain exitosamente\n",
      "   â€¢ Gobernanza federada aplicada\n",
      "================================================================================\n",
      "âœ… Data Product publicado: finance.payments\n",
      "   Owner: finance-team@company.com\n",
      "   SLO Freshness: <1 min (real-time)\n",
      "   Registros: 4,800\n",
      "   Pagos aprobados: $1,044,848.71\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š CONSUMER: ANALYTICS DOMAIN (Cross-Domain Query)\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Caso: AnÃ¡lisis de conversiÃ³n completo (Sales â†’ Logistics â†’ Finance)\n",
      "\n",
      "ğŸ“ˆ MÃ©tricas del Funnel:\n",
      "   Ventas iniciadas: 5,000\n",
      "   EnvÃ­os creados: 4,500 (90.0%)\n",
      "   Pagos aprobados: 4,085 (81.7%)\n",
      "   Entregas completadas: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "ğŸ” GOBERNANZA FEDERADA\n",
      "================================================================================\n",
      "   Domain Data Product          Owner               PII Retention SLO Freshness\n",
      "    Sales transactions     sales-team Yes (customer_id)   7 years       <15 min\n",
      "Logistics    shipments logistics-team                No   3 years        <5 min\n",
      "  Finance     payments   finance-team  Yes (payment_id)  10 years        <1 min\n",
      "\n",
      "================================================================================\n",
      "ğŸ§­ PRINCIPIOS DE DATA MESH APLICADOS\n",
      "================================================================================\n",
      "\n",
      "1. Domain-Oriented Decentralization:\n",
      "  âœ… 3 dominios con ownership claro\n",
      "  âœ… Cada equipo publica sus propios Data Products\n",
      "  âœ… No hay equipo central de datos\n",
      "\n",
      "2. Data as a Product:\n",
      "  âœ… Contratos versionados (sales v2.1.0, logistics v1.8.2, finance v3.0.1)\n",
      "  âœ… SLOs definidos (freshness, availability, quality)\n",
      "  âœ… DocumentaciÃ³n completa (schema, governance)\n",
      "\n",
      "3. Self-Serve Platform:\n",
      "  âœ… Storage comÃºn (Parquet en file system)\n",
      "  âœ… Formato estÃ¡ndar (Parquet + JSON contracts)\n",
      "  âœ… Discovery automÃ¡tico (contracts.json)\n",
      "\n",
      "4. Federated Governance:\n",
      "  âœ… PolÃ­ticas globales (PII marcado, retention definido)\n",
      "  âœ… AutonomÃ­a local (cada dominio elige su SLO)\n",
      "  âœ… Compliance distribuido (PCI-DSS en finance, GDPR en sales)\n",
      "\n",
      "================================================================================\n",
      "âš–ï¸ TRADE-OFFS DE DATA MESH\n",
      "================================================================================\n",
      "            Aspecto          Monolito                Data Mesh\n",
      "  Escalabilidad org âš ï¸ Cuello botella    âœ… Lineal con dominios\n",
      "     Time to market    Semanas (cola)         DÃ­as (autonomÃ­a)\n",
      "Complejidad tÃ©cnica Baja (1 pipeline)    âš ï¸ Alta (N pipelines)\n",
      "  Madurez requerida   Junior friendly âš ï¸ Senior+ (DevOps, SRE)\n",
      "   Costos iniciales              Bajo     âš ï¸ Alto (plataforma)\n",
      "\n",
      "================================================================================\n",
      "âœ… SimulaciÃ³n Data Mesh completada\n",
      "   â€¢ 3 dominios publicaron Data Products\n",
      "   â€¢ 14,300 registros totales\n",
      "   â€¢ Analytics consumiÃ³ cross-domain exitosamente\n",
      "   â€¢ Gobernanza federada aplicada\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ SimulaciÃ³n PrÃ¡ctica: Data Mesh - Arquitectura Organizacional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸŒ DATA MESH - ARQUITECTURA DESCENTRALIZADA POR DOMINIOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= SETUP ORGANIZATION =============\n",
    "np.random.seed(42)\n",
    "now = datetime.now()\n",
    "\n",
    "mesh_base = Path(\"outputs/data_mesh_demo\")\n",
    "mesh_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nğŸ¢ OrganizaciÃ³n: E-commerce Multilatino\")\n",
    "print(\"   â€¢ 3 dominios de negocio con ownership descentralizado\")\n",
    "print(\"   â€¢ Plataforma self-service compartida\")\n",
    "print(\"   â€¢ Gobernanza federada\")\n",
    "\n",
    "# ============= DOMINIO 1: VENTAS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ›’ DOMINIO 1: VENTAS (Sales Domain)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data Product: Ventas Transaccionales\n",
    "sales_data = []\n",
    "for i in range(5000):\n",
    "    sales_data.append({\n",
    "        'transaction_id': f'T{i:06d}',\n",
    "        'timestamp': now - timedelta(days=np.random.randint(0, 30)),\n",
    "        'customer_id': f'C{np.random.randint(1, 500):04d}',\n",
    "        'product_id': f'P{np.random.randint(1, 100):03d}',\n",
    "        'amount': np.random.uniform(10, 500),\n",
    "        'channel': np.random.choice(['web', 'mobile', 'store'])\n",
    "    })\n",
    "\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Publicar Data Product\n",
    "sales_product_path = mesh_base / \"sales_domain\"\n",
    "sales_product_path.mkdir(exist_ok=True)\n",
    "df_sales.to_parquet(sales_product_path / \"transactions.parquet\", index=False)\n",
    "\n",
    "# Data Product Contract\n",
    "sales_contract = {\n",
    "    'name': 'sales.transactions',\n",
    "    'owner': 'sales-team@company.com',\n",
    "    'version': '2.1.0',\n",
    "    'slo': {\n",
    "        'freshness': '<15 min',\n",
    "        'availability': '99.9%',\n",
    "        'quality': 'completeness > 99%'\n",
    "    },\n",
    "    'schema': {\n",
    "        'transaction_id': 'string (unique)',\n",
    "        'timestamp': 'datetime',\n",
    "        'customer_id': 'string (FK to customers)',\n",
    "        'product_id': 'string (FK to products)',\n",
    "        'amount': 'decimal',\n",
    "        'channel': 'enum[web, mobile, store]'\n",
    "    },\n",
    "    'governance': {\n",
    "        'pii_fields': ['customer_id'],\n",
    "        'retention': '7 years',\n",
    "        'access': 'all authenticated users'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(sales_product_path / \"contract.json\", 'w') as f:\n",
    "    json.dump(sales_contract, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Data Product publicado: {sales_contract['name']}\")\n",
    "print(f\"   Owner: {sales_contract['owner']}\")\n",
    "print(f\"   SLO Freshness: {sales_contract['slo']['freshness']}\")\n",
    "print(f\"   Registros: {len(df_sales):,}\")\n",
    "print(f\"   Revenue total: ${df_sales['amount'].sum():,.2f}\")\n",
    "\n",
    "# ============= DOMINIO 2: LOGÃSTICA =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“¦ DOMINIO 2: LOGÃSTICA (Logistics Domain)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data Product: EnvÃ­os y Entregas\n",
    "logistics_data = []\n",
    "for i in range(4500):\n",
    "    logistics_data.append({\n",
    "        'shipment_id': f'S{i:06d}',\n",
    "        'transaction_id': df_sales['transaction_id'].sample(1).iloc[0],  # FK\n",
    "        'created_at': now - timedelta(days=np.random.randint(0, 30)),\n",
    "        'delivered_at': now - timedelta(days=np.random.randint(0, 25)) if np.random.random() > 0.2 else None,\n",
    "        'status': np.random.choice(['pending', 'in_transit', 'delivered', 'returned']),\n",
    "        'warehouse': np.random.choice(['MX-CDMX', 'CO-BOG', 'AR-BUE', 'CL-SCL'])\n",
    "    })\n",
    "\n",
    "df_logistics = pd.DataFrame(logistics_data)\n",
    "\n",
    "# Publicar Data Product\n",
    "logistics_product_path = mesh_base / \"logistics_domain\"\n",
    "logistics_product_path.mkdir(exist_ok=True)\n",
    "df_logistics.to_parquet(logistics_product_path / \"shipments.parquet\", index=False)\n",
    "\n",
    "# Data Product Contract\n",
    "logistics_contract = {\n",
    "    'name': 'logistics.shipments',\n",
    "    'owner': 'logistics-team@company.com',\n",
    "    'version': '1.8.2',\n",
    "    'slo': {\n",
    "        'freshness': '<5 min',\n",
    "        'availability': '99.95%',\n",
    "        'quality': 'no orphan shipments (FK integrity)'\n",
    "    },\n",
    "    'schema': {\n",
    "        'shipment_id': 'string (unique)',\n",
    "        'transaction_id': 'string (FK to sales.transactions)',\n",
    "        'created_at': 'datetime',\n",
    "        'delivered_at': 'datetime (nullable)',\n",
    "        'status': 'enum[pending, in_transit, delivered, returned]',\n",
    "        'warehouse': 'string'\n",
    "    },\n",
    "    'governance': {\n",
    "        'pii_fields': [],\n",
    "        'retention': '3 years',\n",
    "        'access': 'logistics + analytics teams'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(logistics_product_path / \"contract.json\", 'w') as f:\n",
    "    json.dump(logistics_contract, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Data Product publicado: {logistics_contract['name']}\")\n",
    "print(f\"   Owner: {logistics_contract['owner']}\")\n",
    "print(f\"   SLO Freshness: {logistics_contract['slo']['freshness']}\")\n",
    "print(f\"   Registros: {len(df_logistics):,}\")\n",
    "print(f\"   Entregas completadas: {df_logistics['status'].value_counts().get('delivered', 0):,}\")\n",
    "\n",
    "# ============= DOMINIO 3: FINANZAS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’° DOMINIO 3: FINANZAS (Finance Domain)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data Product: Pagos y FacturaciÃ³n\n",
    "finance_data = []\n",
    "for i in range(4800):\n",
    "    amount = np.random.uniform(10, 500)\n",
    "    finance_data.append({\n",
    "        'payment_id': f'PAY{i:06d}',\n",
    "        'transaction_id': df_sales['transaction_id'].sample(1).iloc[0],  # FK\n",
    "        'payment_date': now - timedelta(days=np.random.randint(0, 30)),\n",
    "        'amount': amount,\n",
    "        'fee': amount * 0.03,  # 3% payment processing fee\n",
    "        'method': np.random.choice(['credit_card', 'debit', 'transfer', 'cash']),\n",
    "        'status': np.random.choice(['approved', 'pending', 'declined'], p=[0.85, 0.10, 0.05])\n",
    "    })\n",
    "\n",
    "df_finance = pd.DataFrame(finance_data)\n",
    "\n",
    "# Publicar Data Product\n",
    "finance_product_path = mesh_base / \"finance_domain\"\n",
    "finance_product_path.mkdir(exist_ok=True)\n",
    "df_finance.to_parquet(finance_product_path / \"payments.parquet\", index=False)\n",
    "\n",
    "# Data Product Contract\n",
    "finance_contract = {\n",
    "    'name': 'finance.payments',\n",
    "    'owner': 'finance-team@company.com',\n",
    "    'version': '3.0.1',\n",
    "    'slo': {\n",
    "        'freshness': '<1 min (real-time)',\n",
    "        'availability': '99.99%',\n",
    "        'quality': 'amount = sales.amount (reconciliation)'\n",
    "    },\n",
    "    'schema': {\n",
    "        'payment_id': 'string (unique)',\n",
    "        'transaction_id': 'string (FK to sales.transactions)',\n",
    "        'payment_date': 'datetime',\n",
    "        'amount': 'decimal',\n",
    "        'fee': 'decimal',\n",
    "        'method': 'enum[credit_card, debit, transfer, cash]',\n",
    "        'status': 'enum[approved, pending, declined]'\n",
    "    },\n",
    "    'governance': {\n",
    "        'pii_fields': ['payment_id'],\n",
    "        'retention': '10 years (compliance)',\n",
    "        'access': 'finance team only (PCI-DSS)'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(finance_product_path / \"contract.json\", 'w') as f:\n",
    "    json.dump(finance_contract, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Data Product publicado: {finance_contract['name']}\")\n",
    "print(f\"   Owner: {finance_contract['owner']}\")\n",
    "print(f\"   SLO Freshness: {finance_contract['slo']['freshness']}\")\n",
    "print(f\"   Registros: {len(df_finance):,}\")\n",
    "print(f\"   Pagos aprobados: ${df_finance[df_finance['status']=='approved']['amount'].sum():,.2f}\")\n",
    "\n",
    "# ============= CONSUMER: ANALYTICS DOMAIN =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š CONSUMER: ANALYTICS DOMAIN (Cross-Domain Query)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ” Caso: AnÃ¡lisis de conversiÃ³n completo (Sales â†’ Logistics â†’ Finance)\")\n",
    "\n",
    "# Leer todos los Data Products publicados\n",
    "df_sales_consumed = pd.read_parquet(sales_product_path / \"transactions.parquet\")\n",
    "df_logistics_consumed = pd.read_parquet(logistics_product_path / \"shipments.parquet\")\n",
    "df_finance_consumed = pd.read_parquet(finance_product_path / \"payments.parquet\")\n",
    "\n",
    "# Join cross-domain\n",
    "df_full = df_sales_consumed.merge(\n",
    "    df_logistics_consumed, on='transaction_id', how='left'\n",
    ").merge(\n",
    "    df_finance_consumed, on='transaction_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ MÃ©tricas del Funnel:\")\n",
    "print(f\"   Ventas iniciadas: {len(df_sales_consumed):,}\")\n",
    "print(f\"   EnvÃ­os creados: {df_logistics_consumed['shipment_id'].notna().sum():,} ({len(df_logistics_consumed)/len(df_sales_consumed)*100:.1f}%)\")\n",
    "print(f\"   Pagos aprobados: {df_finance_consumed[df_finance_consumed['status']=='approved'].shape[0]:,} ({df_finance_consumed[df_finance_consumed['status']=='approved'].shape[0]/len(df_sales_consumed)*100:.1f}%)\")\n",
    "\n",
    "delivered = df_full[df_full['status_y'] == 'delivered']\n",
    "print(f\"   Entregas completadas: {len(delivered):,} ({len(delivered)/len(df_sales_consumed)*100:.1f}%)\")\n",
    "\n",
    "# ============= GOBERNANZA FEDERADA =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ” GOBERNANZA FEDERADA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "governance_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Domain': 'Sales',\n",
    "        'Data Product': 'transactions',\n",
    "        'Owner': 'sales-team',\n",
    "        'PII': 'Yes (customer_id)',\n",
    "        'Retention': '7 years',\n",
    "        'SLO Freshness': '<15 min'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'Logistics',\n",
    "        'Data Product': 'shipments',\n",
    "        'Owner': 'logistics-team',\n",
    "        'PII': 'No',\n",
    "        'Retention': '3 years',\n",
    "        'SLO Freshness': '<5 min'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'Finance',\n",
    "        'Data Product': 'payments',\n",
    "        'Owner': 'finance-team',\n",
    "        'PII': 'Yes (payment_id)',\n",
    "        'Retention': '10 years',\n",
    "        'SLO Freshness': '<1 min'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(governance_summary.to_string(index=False))\n",
    "\n",
    "# ============= PRINCIPIOS DATA MESH =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ§­ PRINCIPIOS DE DATA MESH APLICADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "principles = {\n",
    "    '1. Domain-Oriented Decentralization': [\n",
    "        f'âœ… 3 dominios con ownership claro',\n",
    "        f'âœ… Cada equipo publica sus propios Data Products',\n",
    "        f'âœ… No hay equipo central de datos'\n",
    "    ],\n",
    "    '2. Data as a Product': [\n",
    "        f'âœ… Contratos versionados (sales v2.1.0, logistics v1.8.2, finance v3.0.1)',\n",
    "        f'âœ… SLOs definidos (freshness, availability, quality)',\n",
    "        f'âœ… DocumentaciÃ³n completa (schema, governance)'\n",
    "    ],\n",
    "    '3. Self-Serve Platform': [\n",
    "        f'âœ… Storage comÃºn (Parquet en file system)',\n",
    "        f'âœ… Formato estÃ¡ndar (Parquet + JSON contracts)',\n",
    "        f'âœ… Discovery automÃ¡tico (contracts.json)'\n",
    "    ],\n",
    "    '4. Federated Governance': [\n",
    "        f'âœ… PolÃ­ticas globales (PII marcado, retention definido)',\n",
    "        f'âœ… AutonomÃ­a local (cada dominio elige su SLO)',\n",
    "        f'âœ… Compliance distribuido (PCI-DSS en finance, GDPR en sales)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for principle, items in principles.items():\n",
    "    print(f\"\\n{principle}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "# ============= TRADE-OFFS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âš–ï¸ TRADE-OFFS DE DATA MESH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trade_offs = pd.DataFrame([\n",
    "    {'Aspecto': 'Escalabilidad org', 'Monolito': 'âš ï¸ Cuello botella', 'Data Mesh': 'âœ… Lineal con dominios'},\n",
    "    {'Aspecto': 'Time to market', 'Monolito': 'Semanas (cola)', 'Data Mesh': 'DÃ­as (autonomÃ­a)'},\n",
    "    {'Aspecto': 'Complejidad tÃ©cnica', 'Monolito': 'Baja (1 pipeline)', 'Data Mesh': 'âš ï¸ Alta (N pipelines)'},\n",
    "    {'Aspecto': 'Madurez requerida', 'Monolito': 'Junior friendly', 'Data Mesh': 'âš ï¸ Senior+ (DevOps, SRE)'},\n",
    "    {'Aspecto': 'Costos iniciales', 'Monolito': 'Bajo', 'Data Mesh': 'âš ï¸ Alto (plataforma)'},\n",
    "])\n",
    "\n",
    "print(trade_offs.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SimulaciÃ³n Data Mesh completada\")\n",
    "print(f\"   â€¢ 3 dominios publicaron Data Products\")\n",
    "print(f\"   â€¢ {len(df_sales) + len(df_logistics) + len(df_finance):,} registros totales\")\n",
    "print(f\"   â€¢ Analytics consumiÃ³ cross-domain exitosamente\")\n",
    "print(f\"   â€¢ Gobernanza federada aplicada\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0bd31",
   "metadata": {},
   "source": [
    "- Batch layer: histÃ³rico completo, recalculable, alta latencia (Spark/Hadoop).\n",
    "- Speed layer: streaming incremental, baja latencia (Kafka/Flink/Spark Streaming).\n",
    "- Serving layer: merge de vistas batch + speed para consultas (Druid/Cassandra/BigQuery).\n",
    "- Trade-offs: doble lÃ³gica (batch + stream), complejidad operacional, eventual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aff4f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚   Data     â”‚\n",
      "â”‚  Sources   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
      "      â”‚\n",
      "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
      "   â”‚Queue â”‚ (Kafka)\n",
      "   â””â”€â”€â”¬â”€â”€â”€â”˜\n",
      "      â”‚\n",
      " â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
      " â”‚  Batch   â”‚  (Spark jobs nocturnos)\n",
      " â”‚  Layer   â”‚\n",
      " â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
      "      â”‚\n",
      "   Master\n",
      "   Dataset\n",
      "      â”‚\n",
      "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
      "   â”‚Servingâ”‚ (Druid/BigQuery)\n",
      "   â”‚ Layer â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
      "      â†‘\n",
      "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
      "   â”‚Speed â”‚ (Flink/Spark Streaming)\n",
      "   â”‚Layer â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_diagram = '''\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Data     â”‚\n",
    "â”‚  Sources   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Queue â”‚ (Kafka)\n",
    "   â””â”€â”€â”¬â”€â”€â”€â”˜\n",
    "      â”‚\n",
    " â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    " â”‚  Batch   â”‚  (Spark jobs nocturnos)\n",
    " â”‚  Layer   â”‚\n",
    " â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "   Master\n",
    "   Dataset\n",
    "      â”‚\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Servingâ”‚ (Druid/BigQuery)\n",
    "   â”‚ Layer â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â†‘\n",
    "   â”Œâ”€â”€â”´â”€â”€â”€â”\n",
    "   â”‚Speed â”‚ (Flink/Spark Streaming)\n",
    "   â”‚Layer â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "'''\n",
    "print(lambda_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a99dd",
   "metadata": {},
   "source": [
    "## 2. Arquitectura Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9117d8",
   "metadata": {},
   "source": [
    "- Un solo pipeline streaming para todo (batch = replay del log).\n",
    "- Simplifica operaciones y lÃ³gica unificada.\n",
    "- Requiere log infinito (Kafka con retenciÃ³n larga) y capacidad de reprocessing.\n",
    "- Trade-offs: coste de almacenamiento del log, complejidad del estado, menor optimizaciÃ³n batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a06c00",
   "metadata": {},
   "source": [
    "## 3. Arquitectura Delta (Lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11be385",
   "metadata": {},
   "source": [
    "- Unifica batch y streaming sobre un Ãºnico storage transaccional (Delta/Iceberg).\n",
    "- Streaming escribe micro-batches transaccionales; batch lee histÃ³rico con time travel.\n",
    "- Elimina duplicaciÃ³n de cÃ³digo y simplifica gobernanza (un catÃ¡logo, un linaje).\n",
    "- Trade-offs: requiere adopciÃ³n de formato y motor compatible (Spark/Trino)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab620719",
   "metadata": {},
   "source": [
    "## 4. Data Mesh: paradigma organizacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851967",
   "metadata": {},
   "source": [
    "- DescentralizaciÃ³n de la propiedad de datos por dominio de negocio.\n",
    "- Data products: APIs/tablas con SLOs, documentaciÃ³n, linaje y contratos.\n",
    "- Plataforma self-service: herramientas comunes (CI/CD, catÃ¡logo, observabilidad).\n",
    "- Gobernanza federada: polÃ­ticas globales + autonomÃ­a local.\n",
    "- Trade-offs: requiere madurez organizacional, infraestructura sÃ³lida, cambio cultural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0232c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Domain-oriented decentralization: equipos de dominio (ventas, logÃ­stica, finanzas) son dueÃ±os de sus datos.\n",
      "2. Data as a product: cada dataset tiene owner, SLO, versionado, documentaciÃ³n y calidad garantizada.\n",
      "3. Self-serve platform: infraestructura comÃºn (orquestaciÃ³n, observabilidad, catÃ¡logo) sin silos.\n",
      "4. Federated governance: polÃ­ticas de seguridad, privacidad y calidad definidas centralmente, aplicadas localmente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mesh_principles = '''\n",
    "1. Domain-oriented decentralization: equipos de dominio (ventas, logÃ­stica, finanzas) son dueÃ±os de sus datos.\n",
    "2. Data as a product: cada dataset tiene owner, SLO, versionado, documentaciÃ³n y calidad garantizada.\n",
    "3. Self-serve platform: infraestructura comÃºn (orquestaciÃ³n, observabilidad, catÃ¡logo) sin silos.\n",
    "4. Federated governance: polÃ­ticas de seguridad, privacidad y calidad definidas centralmente, aplicadas localmente.\n",
    "'''\n",
    "print(mesh_principles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5c55f",
   "metadata": {},
   "source": [
    "## 5. ComparaciÃ³n y cuÃ¡ndo elegir cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcb093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Arquitectura",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Latencia",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Complejidad",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Casos",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a1b2a512-8c5c-41e4-bf77-c2805b2e0b84",
       "rows": [
        [
         "0",
         "Lambda",
         "Batch: horas, Speed: segundos",
         "Alta (doble lÃ³gica)",
         "Legacy con requerimientos mixtos"
        ],
        [
         "1",
         "Kappa",
         "Baja (streaming)",
         "Media (un pipeline)",
         "Todo es streaming, reprocessing factible"
        ],
        [
         "2",
         "Delta",
         "Baja-Media (micro-batch)",
         "Media (un storage)",
         "Lakehouse, unificaciÃ³n batch/stream"
        ],
        [
         "3",
         "Data Mesh",
         "Variable (por dominio)",
         "Alta (organizacional)",
         "Grandes org con mÃºltiples dominios"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arquitectura</th>\n",
       "      <th>Latencia</th>\n",
       "      <th>Complejidad</th>\n",
       "      <th>Casos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lambda</td>\n",
       "      <td>Batch: horas, Speed: segundos</td>\n",
       "      <td>Alta (doble lÃ³gica)</td>\n",
       "      <td>Legacy con requerimientos mixtos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kappa</td>\n",
       "      <td>Baja (streaming)</td>\n",
       "      <td>Media (un pipeline)</td>\n",
       "      <td>Todo es streaming, reprocessing factible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delta</td>\n",
       "      <td>Baja-Media (micro-batch)</td>\n",
       "      <td>Media (un storage)</td>\n",
       "      <td>Lakehouse, unificaciÃ³n batch/stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Mesh</td>\n",
       "      <td>Variable (por dominio)</td>\n",
       "      <td>Alta (organizacional)</td>\n",
       "      <td>Grandes org con mÃºltiples dominios</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Arquitectura                       Latencia            Complejidad  \\\n",
       "0       Lambda  Batch: horas, Speed: segundos    Alta (doble lÃ³gica)   \n",
       "1        Kappa               Baja (streaming)    Media (un pipeline)   \n",
       "2        Delta       Baja-Media (micro-batch)     Media (un storage)   \n",
       "3    Data Mesh         Variable (por dominio)  Alta (organizacional)   \n",
       "\n",
       "                                      Casos  \n",
       "0          Legacy con requerimientos mixtos  \n",
       "1  Todo es streaming, reprocessing factible  \n",
       "2       Lakehouse, unificaciÃ³n batch/stream  \n",
       "3        Grandes org con mÃºltiples dominios  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "comp = pd.DataFrame([\n",
    "  {'Arquitectura':'Lambda', 'Latencia':'Batch: horas, Speed: segundos', 'Complejidad':'Alta (doble lÃ³gica)', 'Casos':'Legacy con requerimientos mixtos'},\n",
    "  {'Arquitectura':'Kappa', 'Latencia':'Baja (streaming)', 'Complejidad':'Media (un pipeline)', 'Casos':'Todo es streaming, reprocessing factible'},\n",
    "  {'Arquitectura':'Delta', 'Latencia':'Baja-Media (micro-batch)', 'Complejidad':'Media (un storage)', 'Casos':'Lakehouse, unificaciÃ³n batch/stream'},\n",
    "  {'Arquitectura':'Data Mesh', 'Latencia':'Variable (por dominio)', 'Complejidad':'Alta (organizacional)', 'Casos':'Grandes org con mÃºltiples dominios'}\n",
    "])\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff995925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ GUÃA DE DECISIÃ“N: Â¿QUÃ‰ ARQUITECTURA ELEGIR?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ MATRIZ DE DECISIÃ“N POR CASO DE USO\n",
      "\n",
      "                                      Escenario   Mejor OpciÃ³n                            RazÃ³n   Complejidad Costo\n",
      "         Legacy con batch + necesidad streaming         Lambda EvoluciÃ³n gradual sin reescribir       âš ï¸ Alta   ğŸ’°ğŸ’°ğŸ’°\n",
      "           Todo es real-time, histÃ³rico < 1 aÃ±o          Kappa          Simplicidad operacional       âœ… Media    ğŸ’°ğŸ’°\n",
      "                Data warehouse + lake unificado          Delta  ACID + time travel + bajo costo       âœ… Media     ğŸ’°\n",
      "Org grande (500+ empleados), mÃºltiples dominios      Data Mesh            Escala organizacional âš ï¸âš ï¸ Muy Alta  ğŸ’°ğŸ’°ğŸ’°ğŸ’°\n",
      "                    Startup MVP, equipo pequeÃ±o Delta (simple)   MÃ­nima complejidad operacional        âœ… Baja     ğŸ’°\n",
      "                    Fintech con latencia <100ms          Kappa        Streaming puro con estado       âš ï¸ Alta   ğŸ’°ğŸ’°ğŸ’°\n",
      "                Compliance GDPR + auditabilidad          Delta  Time travel + ACID transactions       âœ… Media     ğŸ’°\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š COMPARATIVA CUANTITATIVA (basada en simulaciones)\n",
      "================================================================================\n",
      "Arquitectura  Eventos Procesados Latencia Batch    Latencia Speed     Merge Overhead Complejidad CÃ³digo        Storage GB\n",
      "      Lambda              10,000            9ms               3ms                 SÃ­        2 pipelines       0.5 + Kafka\n",
      "       Kappa              15,000   N/A (replay)     0.13ms/evento                 No         1 pipeline  7.32 (Kafka 30d)\n",
      "       Delta              20,300           80ms 5ms (micro-batch)                 No         1 pipeline    0.28 (Parquet)\n",
      "   Data Mesh 14,300 (3 dominios)       Variable          Variable Cross-domain joins        N pipelines 0.5 (distributed)\n",
      "\n",
      "================================================================================\n",
      "ğŸ” CRITERIOS CLAVE PARA LA DECISIÃ“N\n",
      "================================================================================\n",
      "\n",
      "1. Latencia Requerida:\n",
      "  â€¢ < 100ms: â†’ Kappa (streaming puro)\n",
      "  â€¢ < 1 segundo: â†’ Kappa o Lambda speed layer\n",
      "  â€¢ < 30 segundos: â†’ Delta (micro-batch)\n",
      "  â€¢ > 1 minuto: â†’ Lambda batch o Delta batch\n",
      "\n",
      "2. TamaÃ±o HistÃ³rico:\n",
      "  â€¢ < 6 meses: â†’ Kappa (Kafka retention factible)\n",
      "  â€¢ 6 meses - 2 aÃ±os: â†’ Kappa o Delta (depende de costo)\n",
      "  â€¢ > 2 aÃ±os: â†’ Delta o Lambda (S3/ADLS barato)\n",
      "\n",
      "3. Madurez Organizacional:\n",
      "  â€¢ Startup (< 50): â†’ Delta (simplicidad)\n",
      "  â€¢ Mediana (50-500): â†’ Lambda o Delta (centralizado)\n",
      "  â€¢ Enterprise (> 500): â†’ Data Mesh (descentralizaciÃ³n)\n",
      "\n",
      "4. Requisitos de Compliance:\n",
      "  â€¢ Auditabilidad: â†’ Delta (time travel)\n",
      "  â€¢ GDPR right to be forgotten: â†’ Delta (ACID deletes)\n",
      "  â€¢ Inmutabilidad: â†’ Kappa (log inmutable)\n",
      "  â€¢ RetenciÃ³n 5-10 aÃ±os: â†’ Delta o Lambda\n",
      "\n",
      "================================================================================\n",
      "âŒ ANTI-PATTERNS (QuÃ© NO hacer)\n",
      "================================================================================\n",
      "  âŒ Usar Lambda si estÃ¡s empezando de cero (mejor Delta)\n",
      "  âŒ Usar Kappa con histÃ³ricos de 5+ aÃ±os (costo prohibitivo)\n",
      "  âŒ Implementar Data Mesh con equipo < 50 personas (overhead)\n",
      "  âŒ Usar Delta para latencia < 1s crÃ­tica (micro-batch tiene overhead)\n",
      "  âŒ Migrar de Lambda a Kappa sin plan de reprocessing\n",
      "  âŒ Adoptar Data Mesh sin plataforma self-service\n",
      "  âŒ Usar streaming para ETL batch simples (overkill)\n",
      "\n",
      "================================================================================\n",
      "âœ… RESUMEN EJECUTIVO\n",
      "================================================================================\n",
      "\n",
      "ğŸ—ï¸ LAMBDA: \n",
      "   âœ… Cuando: Legacy batch + necesitas agregar streaming\n",
      "   âš ï¸ Cuidado: 2x complejidad, doble cÃ³digo\n",
      "\n",
      "â™¾ï¸ KAPPA:\n",
      "   âœ… Cuando: Todo es streaming, histÃ³rico < 1 aÃ±o\n",
      "   âš ï¸ Cuidado: Kafka storage caro, estado complejo\n",
      "\n",
      "ğŸ›ï¸ DELTA:\n",
      "   âœ… Cuando: Lakehouse unificado, GDPR, analytics + streaming\n",
      "   âš ï¸ Cuidado: Latencia > 1s (micro-batch overhead)\n",
      "\n",
      "ğŸŒ DATA MESH:\n",
      "   âœ… Cuando: Enterprise > 500 personas, mÃºltiples dominios\n",
      "   âš ï¸ Cuidado: Requiere madurez DevOps/SRE senior\n",
      "\n",
      "ğŸ’¡ REGLA DE ORO:\n",
      "   â€¢ < 50 personas â†’ Delta (simplicidad)\n",
      "   â€¢ 50-500 personas â†’ Delta o Kappa (segÃºn latencia)\n",
      "   â€¢ > 500 personas â†’ Data Mesh sobre Delta/Kappa\n",
      "   â€¢ Legacy modernizando â†’ Lambda â†’ gradual a Delta\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ Notebook completado - Arquitecturas Modernas dominadas!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š RESUMEN EJECUTIVO: DecisiÃ³n de Arquitectura\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ GUÃA DE DECISIÃ“N: Â¿QUÃ‰ ARQUITECTURA ELEGIR?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============= MATRIZ DE DECISIÃ“N =============\n",
    "print(\"\\nğŸ“‹ MATRIZ DE DECISIÃ“N POR CASO DE USO\\n\")\n",
    "\n",
    "decision_matrix = pd.DataFrame([\n",
    "    {\n",
    "        'Escenario': 'Legacy con batch + necesidad streaming',\n",
    "        'Mejor OpciÃ³n': 'Lambda',\n",
    "        'RazÃ³n': 'EvoluciÃ³n gradual sin reescribir',\n",
    "        'Complejidad': 'âš ï¸ Alta',\n",
    "        'Costo': 'ğŸ’°ğŸ’°ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Todo es real-time, histÃ³rico < 1 aÃ±o',\n",
    "        'Mejor OpciÃ³n': 'Kappa',\n",
    "        'RazÃ³n': 'Simplicidad operacional',\n",
    "        'Complejidad': 'âœ… Media',\n",
    "        'Costo': 'ğŸ’°ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Data warehouse + lake unificado',\n",
    "        'Mejor OpciÃ³n': 'Delta',\n",
    "        'RazÃ³n': 'ACID + time travel + bajo costo',\n",
    "        'Complejidad': 'âœ… Media',\n",
    "        'Costo': 'ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Org grande (500+ empleados), mÃºltiples dominios',\n",
    "        'Mejor OpciÃ³n': 'Data Mesh',\n",
    "        'RazÃ³n': 'Escala organizacional',\n",
    "        'Complejidad': 'âš ï¸âš ï¸ Muy Alta',\n",
    "        'Costo': 'ğŸ’°ğŸ’°ğŸ’°ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Startup MVP, equipo pequeÃ±o',\n",
    "        'Mejor OpciÃ³n': 'Delta (simple)',\n",
    "        'RazÃ³n': 'MÃ­nima complejidad operacional',\n",
    "        'Complejidad': 'âœ… Baja',\n",
    "        'Costo': 'ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Fintech con latencia <100ms',\n",
    "        'Mejor OpciÃ³n': 'Kappa',\n",
    "        'RazÃ³n': 'Streaming puro con estado',\n",
    "        'Complejidad': 'âš ï¸ Alta',\n",
    "        'Costo': 'ğŸ’°ğŸ’°ğŸ’°'\n",
    "    },\n",
    "    {\n",
    "        'Escenario': 'Compliance GDPR + auditabilidad',\n",
    "        'Mejor OpciÃ³n': 'Delta',\n",
    "        'RazÃ³n': 'Time travel + ACID transactions',\n",
    "        'Complejidad': 'âœ… Media',\n",
    "        'Costo': 'ğŸ’°'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(decision_matrix.to_string(index=False))\n",
    "\n",
    "# ============= COMPARATIVA CUANTITATIVA =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š COMPARATIVA CUANTITATIVA (basada en simulaciones)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Arquitectura': 'Lambda',\n",
    "        'Eventos Procesados': '10,000',\n",
    "        'Latencia Batch': '9ms',\n",
    "        'Latencia Speed': '3ms',\n",
    "        'Merge Overhead': 'SÃ­',\n",
    "        'Complejidad CÃ³digo': '2 pipelines',\n",
    "        'Storage GB': '0.5 + Kafka'\n",
    "    },\n",
    "    {\n",
    "        'Arquitectura': 'Kappa',\n",
    "        'Eventos Procesados': '15,000',\n",
    "        'Latencia Batch': 'N/A (replay)',\n",
    "        'Latencia Speed': '0.13ms/evento',\n",
    "        'Merge Overhead': 'No',\n",
    "        'Complejidad CÃ³digo': '1 pipeline',\n",
    "        'Storage GB': '7.32 (Kafka 30d)'\n",
    "    },\n",
    "    {\n",
    "        'Arquitectura': 'Delta',\n",
    "        'Eventos Procesados': '20,300',\n",
    "        'Latencia Batch': '80ms',\n",
    "        'Latencia Speed': '5ms (micro-batch)',\n",
    "        'Merge Overhead': 'No',\n",
    "        'Complejidad CÃ³digo': '1 pipeline',\n",
    "        'Storage GB': '0.28 (Parquet)'\n",
    "    },\n",
    "    {\n",
    "        'Arquitectura': 'Data Mesh',\n",
    "        'Eventos Procesados': '14,300 (3 dominios)',\n",
    "        'Latencia Batch': 'Variable',\n",
    "        'Latencia Speed': 'Variable',\n",
    "        'Merge Overhead': 'Cross-domain joins',\n",
    "        'Complejidad CÃ³digo': 'N pipelines',\n",
    "        'Storage GB': '0.5 (distributed)'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# ============= CRITERIOS DE SELECCIÃ“N =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ” CRITERIOS CLAVE PARA LA DECISIÃ“N\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "criteria = {\n",
    "    '1. Latencia Requerida': {\n",
    "        '< 100ms': 'â†’ Kappa (streaming puro)',\n",
    "        '< 1 segundo': 'â†’ Kappa o Lambda speed layer',\n",
    "        '< 30 segundos': 'â†’ Delta (micro-batch)',\n",
    "        '> 1 minuto': 'â†’ Lambda batch o Delta batch'\n",
    "    },\n",
    "    '2. TamaÃ±o HistÃ³rico': {\n",
    "        '< 6 meses': 'â†’ Kappa (Kafka retention factible)',\n",
    "        '6 meses - 2 aÃ±os': 'â†’ Kappa o Delta (depende de costo)',\n",
    "        '> 2 aÃ±os': 'â†’ Delta o Lambda (S3/ADLS barato)'\n",
    "    },\n",
    "    '3. Madurez Organizacional': {\n",
    "        'Startup (< 50)': 'â†’ Delta (simplicidad)',\n",
    "        'Mediana (50-500)': 'â†’ Lambda o Delta (centralizado)',\n",
    "        'Enterprise (> 500)': 'â†’ Data Mesh (descentralizaciÃ³n)'\n",
    "    },\n",
    "    '4. Requisitos de Compliance': {\n",
    "        'Auditabilidad': 'â†’ Delta (time travel)',\n",
    "        'GDPR right to be forgotten': 'â†’ Delta (ACID deletes)',\n",
    "        'Inmutabilidad': 'â†’ Kappa (log inmutable)',\n",
    "        'RetenciÃ³n 5-10 aÃ±os': 'â†’ Delta o Lambda'\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, options in criteria.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for condition, recommendation in options.items():\n",
    "        print(f\"  â€¢ {condition}: {recommendation}\")\n",
    "\n",
    "# ============= ANTI-PATTERNS =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âŒ ANTI-PATTERNS (QuÃ© NO hacer)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anti_patterns = [\n",
    "    'âŒ Usar Lambda si estÃ¡s empezando de cero (mejor Delta)',\n",
    "    'âŒ Usar Kappa con histÃ³ricos de 5+ aÃ±os (costo prohibitivo)',\n",
    "    'âŒ Implementar Data Mesh con equipo < 50 personas (overhead)',\n",
    "    'âŒ Usar Delta para latencia < 1s crÃ­tica (micro-batch tiene overhead)',\n",
    "    'âŒ Migrar de Lambda a Kappa sin plan de reprocessing',\n",
    "    'âŒ Adoptar Data Mesh sin plataforma self-service',\n",
    "    'âŒ Usar streaming para ETL batch simples (overkill)'\n",
    "]\n",
    "\n",
    "for pattern in anti_patterns:\n",
    "    print(f\"  {pattern}\")\n",
    "\n",
    "# ============= RESUMEN FINAL =============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… RESUMEN EJECUTIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ—ï¸ LAMBDA: \n",
    "   âœ… Cuando: Legacy batch + necesitas agregar streaming\n",
    "   âš ï¸ Cuidado: 2x complejidad, doble cÃ³digo\n",
    "\n",
    "â™¾ï¸ KAPPA:\n",
    "   âœ… Cuando: Todo es streaming, histÃ³rico < 1 aÃ±o\n",
    "   âš ï¸ Cuidado: Kafka storage caro, estado complejo\n",
    "\n",
    "ğŸ›ï¸ DELTA:\n",
    "   âœ… Cuando: Lakehouse unificado, GDPR, analytics + streaming\n",
    "   âš ï¸ Cuidado: Latencia > 1s (micro-batch overhead)\n",
    "\n",
    "ğŸŒ DATA MESH:\n",
    "   âœ… Cuando: Enterprise > 500 personas, mÃºltiples dominios\n",
    "   âš ï¸ Cuidado: Requiere madurez DevOps/SRE senior\n",
    "\n",
    "ğŸ’¡ REGLA DE ORO:\n",
    "   â€¢ < 50 personas â†’ Delta (simplicidad)\n",
    "   â€¢ 50-500 personas â†’ Delta o Kappa (segÃºn latencia)\n",
    "   â€¢ > 500 personas â†’ Data Mesh sobre Delta/Kappa\n",
    "   â€¢ Legacy modernizando â†’ Lambda â†’ gradual a Delta\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“ Notebook completado - Arquitecturas Modernas dominadas!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cc56e",
   "metadata": {},
   "source": [
    "## 6. Ejercicio de diseÃ±o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b272baf",
   "metadata": {},
   "source": [
    "DiseÃ±a la arquitectura de datos para un e-commerce con:\n",
    "- Dominio Ventas: transacciones en tiempo real (Kafka).\n",
    "- Dominio LogÃ­stica: batch nocturno (inventario/envÃ­os).\n",
    "- Dominio AnalÃ­tica: dashboards con latencia < 5 min.\n",
    "- Requisitos: auditabilidad, GDPR, linaje, costos optimizados.\n",
    "\n",
    "Responde: Â¿Lambda, Kappa, Delta o hÃ­brida? Â¿Data Mesh aplicable? Justifica con trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "\n",
    "**Siguiente â†’:** [ğŸ¤– ML Pipelines y Feature Stores â†’](05_ml_pipelines_feature_stores.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Senior:**\n",
    "- [ğŸ›ï¸ Senior - 01. Data Governance y Calidad de Datos](01_data_governance_calidad.ipynb)\n",
    "- [ğŸ—ï¸ Data Lakehouse con Parquet, Delta Lake e Iceberg (conceptos y prÃ¡ctica ligera)](02_lakehouse_delta_iceberg.ipynb)\n",
    "- [Apache Spark Streaming: Procesamiento en Tiempo Real](03_spark_streaming.ipynb)\n",
    "- [ğŸ›ï¸ Arquitecturas Modernas de Datos: Lambda, Kappa, Delta y Data Mesh](04_arquitecturas_modernas.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [ğŸ¤– ML Pipelines y Feature Stores](05_ml_pipelines_feature_stores.ipynb)\n",
    "- [ğŸ’° Cost Optimization y FinOps en la Nube](06_cost_optimization_finops.ipynb)\n",
    "- [ğŸ” Seguridad, Compliance y AuditorÃ­a de Datos](07_seguridad_compliance.ipynb)\n",
    "- [ğŸ“Š Observabilidad y Linaje de Datos](08_observabilidad_linaje.ipynb)\n",
    "- [ğŸ† Proyecto Integrador Senior 1: Plataforma de Datos Completa](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸŒ Proyecto Integrador Senior 2: Data Mesh Multi-Dominio con Feature Store](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
