{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0d4e2b",
   "metadata": {},
   "source": [
    "# \ud83d\udd0d Embeddings y Similitud en Datos\n",
    "\n",
    "Objetivo: usar embeddings para b\u00fasqueda sem\u00e1ntica en cat\u00e1logos de datos, detecci\u00f3n de duplicados, clustering, y recomendaciones.\n",
    "\n",
    "- Duraci\u00f3n: 90 min\n",
    "- Dificultad: Media\n",
    "- Stack: OpenAI Embeddings, scikit-learn, FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb26288",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Embeddings: Arquitectura y Fundamentos\n",
    "\n",
    "Los **embeddings** son representaciones vectoriales densas de datos (texto, im\u00e1genes, audio) en espacios de dimensi\u00f3n reducida que capturan **relaciones sem\u00e1nticas**. En Data Engineering, permiten b\u00fasqueda sem\u00e1ntica, deduplicaci\u00f3n inteligente, clustering y recomendaciones en cat\u00e1logos de datos.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Evoluci\u00f3n de Embeddings\n",
    "\n",
    "```\n",
    "2013: Word2Vec (Google)          \u2192 Embeddings de palabras con CBOW/Skip-gram\n",
    "2017: Sentence-BERT               \u2192 Embeddings de frases con transformers\n",
    "2020: OpenAI text-embedding-ada   \u2192 API embeddings general-purpose\n",
    "2023: text-embedding-3-small/large \u2192 Mejor calidad + dimensionalidad variable\n",
    "2024: Matryoshka Embeddings       \u2192 Truncar dimensiones sin perder mucha calidad\n",
    "```\n",
    "\n",
    "### \ud83d\udcd0 Arquitectura de Modelos de Embeddings\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    MODELO DE EMBEDDINGS                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                              \u2502\n",
    "\u2502  Input Text                                                  \u2502\n",
    "\u2502  \"Tabla de ventas con transacciones diarias\"                \u2502\n",
    "\u2502        \u2193                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 1. Tokenizaci\u00f3n                                     \u2502    \u2502\n",
    "\u2502  \u2502    - WordPiece / BPE / SentencePiece                \u2502    \u2502\n",
    "\u2502  \u2502    - Tokens: [101, 5555, 2049, 7231, 102]          \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 2. Transformer Encoder (BERT/RoBERTa/etc)           \u2502    \u2502\n",
    "\u2502  \u2502    - 12 capas, 768 hidden units                     \u2502    \u2502\n",
    "\u2502  \u2502    - Self-attention multi-head                      \u2502    \u2502\n",
    "\u2502  \u2502    - Feed-forward networks                          \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 3. Pooling Strategy                                 \u2502    \u2502\n",
    "\u2502  \u2502    - [CLS] token: usa primer token (BERT)           \u2502    \u2502\n",
    "\u2502  \u2502    - Mean pooling: promedio de todos (SBERT) \u2713      \u2502    \u2502\n",
    "\u2502  \u2502    - Max pooling: m\u00e1ximo por dimensi\u00f3n              \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 4. Normalization (L2)                               \u2502    \u2502\n",
    "\u2502  \u2502    - Vector unitario: ||v|| = 1                     \u2502    \u2502\n",
    "\u2502  \u2502    - Permite usar dot product = cosine similarity   \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                     \u2502\n",
    "\u2502  Output Vector                                               \u2502\n",
    "\u2502  [0.023, -0.145, 0.089, ..., 0.112]  (1536 dimensiones)    \u2502\n",
    "\u2502                                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Comparaci\u00f3n de Modelos de Embeddings\n",
    "\n",
    "| Modelo | Dimensiones | Precio ($/1M tokens) | MTEB Score | Tokens Max | Caso de Uso |\n",
    "|--------|-------------|---------------------|------------|------------|-------------|\n",
    "| **text-embedding-3-small** | 512-1536 | $0.02 | 62.3 | 8191 | \ud83d\ude80 Costo-efectivo para b\u00fasqueda general |\n",
    "| **text-embedding-3-large** | 1024-3072 | $0.13 | 64.6 | 8191 | \u2b50 M\u00e1xima calidad OpenAI |\n",
    "| **text-embedding-ada-002** | 1536 | $0.10 | 61.0 | 8191 | \ud83d\udce6 Legacy, usar v3-small |\n",
    "| **all-MiniLM-L6-v2** | 384 | Free (OSS) | 58.8 | 256 | \ud83d\udcb0 Gratuito, r\u00e1pido, local |\n",
    "| **all-mpnet-base-v2** | 768 | Free (OSS) | 63.3 | 384 | \ud83c\udfaf Mejor OSS general-purpose |\n",
    "| **bge-large-en-v1.5** | 1024 | Free (OSS) | 63.9 | 512 | \ud83c\udfc6 Estado del arte OSS |\n",
    "| **voyage-large-2-instruct** | 1024 | $0.12 | 68.3 | 16000 | \ud83d\udd2c Mejor absoluto + contexto largo |\n",
    "\n",
    "**MTEB** (Massive Text Embedding Benchmark): eval\u00faa embeddings en 58 tareas (clasificaci\u00f3n, clustering, retrieval, etc.). Score >60 es bueno, >65 excelente.\n",
    "\n",
    "### \ud83d\udd27 Implementaci\u00f3n con M\u00faltiples Modelos\n",
    "\n",
    "```python\n",
    "from typing import List, Literal\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generador unificado de embeddings con m\u00faltiples modelos.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Literal[\"openai-small\", \"openai-large\", \"sbert\", \"bge\"] = \"openai-small\"\n",
    "    ):\n",
    "        self.model_type = model\n",
    "        \n",
    "        if model.startswith(\"openai\"):\n",
    "            self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "            self.model_name = (\n",
    "                \"text-embedding-3-small\" if model == \"openai-small\" \n",
    "                else \"text-embedding-3-large\"\n",
    "            )\n",
    "        elif model == \"sbert\":\n",
    "            self.model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        elif model == \"bge\":\n",
    "            self.model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "    \n",
    "    def embed(self, texts: List[str], batch_size: int = 100) -> np.ndarray:\n",
    "        \"\"\"Genera embeddings con batching para eficiencia.\"\"\"\n",
    "        if self.model_type.startswith(\"openai\"):\n",
    "            embeddings = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                response = self.client.embeddings.create(\n",
    "                    model=self.model_name,\n",
    "                    input=batch\n",
    "                )\n",
    "                embeddings.extend([d.embedding for d in response.data])\n",
    "            return np.array(embeddings)\n",
    "        else:\n",
    "            # Sentence Transformers ya hace batching internamente\n",
    "            return self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    \n",
    "    def embed_single(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Embedding de un solo texto.\"\"\"\n",
    "        return self.embed([text])[0]\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self) -> int:\n",
    "        \"\"\"Retorna dimensionalidad del modelo.\"\"\"\n",
    "        if self.model_type == \"openai-small\":\n",
    "            return 1536\n",
    "        elif self.model_type == \"openai-large\":\n",
    "            return 3072\n",
    "        elif self.model_type == \"sbert\":\n",
    "            return 768\n",
    "        elif self.model_type == \"bge\":\n",
    "            return 1024\n",
    "\n",
    "# Ejemplo: comparaci\u00f3n de modelos\n",
    "textos = [\n",
    "    \"Tabla de ventas con transacciones diarias\",\n",
    "    \"Data de transacciones de venta por d\u00eda\"\n",
    "]\n",
    "\n",
    "for model in [\"openai-small\", \"sbert\", \"bge\"]:\n",
    "    generator = EmbeddingGenerator(model=model)\n",
    "    embs = generator.embed(textos)\n",
    "    \n",
    "    # Similitud coseno\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sim = cosine_similarity([embs[0]], [embs[1]])[0][0]\n",
    "    \n",
    "    print(f\"{model:15} | dims={generator.dimensions:4} | similarity={sim:.4f}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# openai-small    | dims=1536 | similarity=0.9245\n",
    "# sbert           | dims= 768 | similarity=0.8876\n",
    "# bge             | dims=1024 | similarity=0.9102\n",
    "```\n",
    "\n",
    "### \ud83c\udfa8 Dimensionalidad Variable (Matryoshka Embeddings)\n",
    "\n",
    "Los modelos `text-embedding-3-*` soportan **truncamiento de dimensiones** sin reentrenar:\n",
    "\n",
    "```python\n",
    "# Generar embedding de 1536 dimensiones\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"Tabla de ventas\",\n",
    "    dimensions=1536  # Valor por defecto\n",
    ")\n",
    "\n",
    "# Truncar a 512 dimensiones (3x m\u00e1s peque\u00f1o, ~5% p\u00e9rdida de calidad)\n",
    "response_small = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"Tabla de ventas\",\n",
    "    dimensions=512\n",
    ")\n",
    "\n",
    "# Comparaci\u00f3n espacio vs calidad\n",
    "# 1536 dims: 100% calidad, 6 KB por vector\n",
    "#  768 dims:  98% calidad, 3 KB por vector \u2713 Sweet spot\n",
    "#  512 dims:  95% calidad, 2 KB por vector (b\u00fasqueda r\u00e1pida)\n",
    "#  256 dims:  90% calidad, 1 KB por vector (millones de vectores)\n",
    "```\n",
    "\n",
    "### \ud83d\udcbe Cach\u00e9 de Embeddings en Producci\u00f3n\n",
    "\n",
    "Generar embeddings es costoso ($0.02-$0.13 por 1M tokens). **Siempre cachear**:\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Cache persistente de embeddings con SQLite.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"embeddings.db\"):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                text_hash TEXT PRIMARY KEY,\n",
    "                text TEXT,\n",
    "                model TEXT,\n",
    "                embedding BLOB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def _hash(self, text: str, model: str) -> str:\n",
    "        \"\"\"Hash \u00fanico para texto + modelo.\"\"\"\n",
    "        return hashlib.sha256(f\"{model}:{text}\".encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str, model: str) -> np.ndarray | None:\n",
    "        \"\"\"Obtiene embedding del cache.\"\"\"\n",
    "        h = self._hash(text, model)\n",
    "        cursor = self.conn.execute(\n",
    "            \"SELECT embedding FROM embeddings WHERE text_hash = ?\", (h,)\n",
    "        )\n",
    "        row = cursor.fetchone()\n",
    "        if row:\n",
    "            return np.frombuffer(row[0], dtype=np.float32)\n",
    "        return None\n",
    "    \n",
    "    def set(self, text: str, model: str, embedding: np.ndarray):\n",
    "        \"\"\"Guarda embedding en cache.\"\"\"\n",
    "        h = self._hash(text, model)\n",
    "        self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO embeddings (text_hash, text, model, embedding) VALUES (?, ?, ?, ?)\",\n",
    "            (h, text, model, embedding.astype(np.float32).tobytes())\n",
    "        )\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Estad\u00edsticas del cache.\"\"\"\n",
    "        cursor = self.conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                model,\n",
    "                COUNT(*) as count,\n",
    "                MIN(created_at) as oldest,\n",
    "                MAX(created_at) as newest\n",
    "            FROM embeddings\n",
    "            GROUP BY model\n",
    "        \"\"\")\n",
    "        return {row[0]: {\"count\": row[1], \"oldest\": row[2], \"newest\": row[3]} \n",
    "                for row in cursor.fetchall()}\n",
    "\n",
    "# Uso con cache\n",
    "cache = EmbeddingCache()\n",
    "generator = EmbeddingGenerator(\"openai-small\")\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Obtiene embedding con cache transparente.\"\"\"\n",
    "    cached = cache.get(text, \"openai-small\")\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    # Cache miss: generar y guardar\n",
    "    emb = generator.embed_single(text)\n",
    "    cache.set(text, \"openai-small\", emb)\n",
    "    return emb\n",
    "\n",
    "# Estad\u00edsticas\n",
    "print(cache.stats())\n",
    "# {'openai-small': {'count': 15234, 'oldest': '2024-01-15', 'newest': '2024-02-20'}}\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Optimizaci\u00f3n de Costos\n",
    "\n",
    "```python\n",
    "# Estrategia 1: Batch processing (reduce latencia + overhead)\n",
    "texts = [\"texto 1\", \"texto 2\", ..., \"texto 1000\"]\n",
    "embeddings = generator.embed(texts, batch_size=100)  # 10 llamadas API vs 1000\n",
    "\n",
    "# Estrategia 2: Usar modelo local para desarrollo\n",
    "dev_generator = EmbeddingGenerator(\"sbert\")  # Gratis\n",
    "prod_generator = EmbeddingGenerator(\"openai-large\")  # Solo producci\u00f3n\n",
    "\n",
    "# Estrategia 3: Preprocesar texto para reducir tokens\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Limpia texto antes de embeddings.\"\"\"\n",
    "    import re\n",
    "    # Remover URLs, emails, exceso de whitespace\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:8000]  # Truncar a l\u00edmite del modelo\n",
    "\n",
    "# Estrategia 4: Dimensiones reducidas para b\u00fasqueda inicial\n",
    "# Usar 512 dims para b\u00fasqueda r\u00e1pida (retrieval)\n",
    "# Luego re-ranking con 1536 dims para top-100 candidatos\n",
    "```\n",
    "\n",
    "### \ud83d\udd2c Casos de Uso en Data Engineering\n",
    "\n",
    "| Aplicaci\u00f3n | T\u00e9cnica | Ejemplo |\n",
    "|------------|---------|---------|\n",
    "| **B\u00fasqueda Sem\u00e1ntica** | Cosine similarity + top-k | \"ventas \u00faltimo mes\" \u2192 encuentra `revenue_monthly` |\n",
    "| **Deduplicaci\u00f3n** | Similitud > threshold | \"iPhone 13\" \u2248 \"Apple iPhone 13\" (0.95 similarity) |\n",
    "| **Clustering** | K-Means en espacio vectorial | Agrupar tablas por dominio (finanzas, marketing, ops) |\n",
    "| **Recomendaciones** | KNN en embeddings | Usuario vio pipeline A \u2192 recomendar pipeline B similar |\n",
    "| **Etiquetado Autom\u00e1tico** | Clasificaci\u00f3n con embeddings | Embedding + classifier \u2192 asignar tags a datasets |\n",
    "| **Detecci\u00f3n de Anomal\u00edas** | Distancia a centroide | Detectar tablas \"raras\" en cat\u00e1logo |\n",
    "\n",
    "**Ejemplo real**: En el cat\u00e1logo de datos de Airbnb, usan embeddings de descripciones de tablas para:\n",
    "- B\u00fasqueda: \"host earnings\" encuentra `payments.host_payouts`\n",
    "- Auto-tagging: Clasificar 10,000 tablas en 50 categor\u00edas\n",
    "- Data lineage: Encontrar tablas relacionadas por similitud sem\u00e1ntica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ffe85",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf M\u00e9tricas de Similitud: M\u00e1s All\u00e1 del Coseno\n",
    "\n",
    "La **similitud coseno** es la m\u00e9trica m\u00e1s com\u00fan para embeddings, pero existen alternativas con diferentes propiedades matem\u00e1ticas y casos de uso. Elegir la m\u00e9trica correcta impacta directamente la calidad de b\u00fasqueda, deduplicaci\u00f3n y clustering.\n",
    "\n",
    "### \ud83d\udcd0 Comparaci\u00f3n de M\u00e9tricas de Similitud\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    M\u00c9TRICAS DE SIMILITUD                         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Vector A: [0.5, 0.8, 0.3]       Vector B: [0.4, 0.9, 0.2]     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  1\ufe0f\u20e3 COSINE SIMILARITY (\u00e1ngulo entre vectores)                   \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502     \u2502  cos(\u03b8) = (A\u00b7B) / (||A|| \u00d7 ||B||)                   \u2502     \u2502\n",
    "\u2502     \u2502         = 0.94 / (1.02 \u00d7 1.01)                      \u2502     \u2502\n",
    "\u2502     \u2502         = 0.912  \u2713 Valor [0,1] o [-1,1]            \u2502     \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502     Propiedades:                                                 \u2502\n",
    "\u2502     \u2022 Ignora magnitud (solo direcci\u00f3n)                          \u2502\n",
    "\u2502     \u2022 Invariante a escala: [1,2,3] \u2248 [2,4,6]                   \u2502\n",
    "\u2502     \u2022 R\u00e1pido con vectores normalizados: cos = A\u00b7B               \u2502\n",
    "\u2502     Uso: Embeddings de texto (siempre normalizados)             \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  2\ufe0f\u20e3 EUCLIDEAN DISTANCE (distancia L2)                           \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502     \u2502  d(A,B) = \u221a(\u03a3(ai - bi)\u00b2)                            \u2502     \u2502\n",
    "\u2502     \u2502         = \u221a((0.5-0.4)\u00b2 + (0.8-0.9)\u00b2 + (0.3-0.2)\u00b2)  \u2502     \u2502\n",
    "\u2502     \u2502         = \u221a(0.01 + 0.01 + 0.01)                     \u2502     \u2502\n",
    "\u2502     \u2502         = 0.173  \u2713 Menor = m\u00e1s similar              \u2502     \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502     Propiedades:                                                 \u2502\n",
    "\u2502     \u2022 Considera magnitud (tama\u00f1o del vector)                    \u2502\n",
    "\u2502     \u2022 Sensible a outliers (por el cuadrado)                     \u2502\n",
    "\u2502     \u2022 Curse of dimensionality en alta dimensi\u00f3n                 \u2502\n",
    "\u2502     Uso: Embeddings de im\u00e1genes, clustering K-Means             \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  3\ufe0f\u20e3 DOT PRODUCT (producto punto)                                \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502     \u2502  A\u00b7B = \u03a3(ai \u00d7 bi)                                    \u2502     \u2502\n",
    "\u2502     \u2502      = (0.5\u00d70.4) + (0.8\u00d70.9) + (0.3\u00d70.2)            \u2502     \u2502\n",
    "\u2502     \u2502      = 0.2 + 0.72 + 0.06                            \u2502     \u2502\n",
    "\u2502     \u2502      = 0.98  \u2713 Mayor = m\u00e1s similar                  \u2502     \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502     Propiedades:                                                 \u2502\n",
    "\u2502     \u2022 Si vectores normalizados: dot = cosine                    \u2502\n",
    "\u2502     \u2022 Captura magnitud + direcci\u00f3n                              \u2502\n",
    "\u2502     \u2022 M\u00e1s r\u00e1pido (sin divisiones ni sqrt)                       \u2502\n",
    "\u2502     Uso: Vectores normalizados, modelos de lenguaje             \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  4\ufe0f\u20e3 MANHATTAN DISTANCE (distancia L1)                           \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502     \u2502  d(A,B) = \u03a3|ai - bi|                                \u2502     \u2502\n",
    "\u2502     \u2502         = |0.5-0.4| + |0.8-0.9| + |0.3-0.2|         \u2502     \u2502\n",
    "\u2502     \u2502         = 0.1 + 0.1 + 0.1                           \u2502     \u2502\n",
    "\u2502     \u2502         = 0.3  \u2713 Menor = m\u00e1s similar                \u2502     \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502     Propiedades:                                                 \u2502\n",
    "\u2502     \u2022 Menos sensible a outliers que L2                          \u2502\n",
    "\u2502     \u2022 M\u00e1s interpretable (suma de diferencias)                   \u2502\n",
    "\u2502     \u2022 \u00datil en espacios de alta dimensi\u00f3n                        \u2502\n",
    "\u2502     Uso: Embeddings sparse, feature vectors                     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83d\udd2c Implementaci\u00f3n Optimizada de M\u00e9tricas\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_similarity, euclidean_distances, manhattan_distances\n",
    ")\n",
    "import time\n",
    "\n",
    "class SimilarityMetrics:\n",
    "    \"\"\"Implementaci\u00f3n optimizada de m\u00e9tricas de similitud.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Similitud coseno: cos(\u03b8) = (A\u00b7B) / (||A|| \u00d7 ||B||)\n",
    "        Rango: [-1, 1] donde 1 = id\u00e9nticos, 0 = ortogonales, -1 = opuestos\n",
    "        \"\"\"\n",
    "        # Optimizaci\u00f3n: si vectores est\u00e1n normalizados, usar dot product\n",
    "        if np.isclose(np.linalg.norm(a), 1.0) and np.isclose(np.linalg.norm(b), 1.0):\n",
    "            return np.dot(a, b)\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Distancia euclidiana: d = \u221a(\u03a3(ai - bi)\u00b2)\n",
    "        Rango: [0, \u221e] donde 0 = id\u00e9nticos\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(a - b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Distancia Manhattan: d = \u03a3|ai - bi|\n",
    "        Rango: [0, \u221e] donde 0 = id\u00e9nticos\n",
    "        \"\"\"\n",
    "        return np.sum(np.abs(a - b))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Producto punto: A\u00b7B = \u03a3(ai \u00d7 bi)\n",
    "        Si normalizados: equivalente a cosine\n",
    "        \"\"\"\n",
    "        return np.dot(a, b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_similarity(\n",
    "        queries: np.ndarray, \n",
    "        corpus: np.ndarray, \n",
    "        metric: Literal[\"cosine\", \"euclidean\", \"manhattan\", \"dot\"] = \"cosine\"\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calcula similitud entre m\u00faltiples queries y corpus (vectorizado).\n",
    "        \n",
    "        Args:\n",
    "            queries: (n_queries, dim)\n",
    "            corpus: (n_corpus, dim)\n",
    "            metric: tipo de m\u00e9trica\n",
    "        \n",
    "        Returns:\n",
    "            (n_queries, n_corpus) matriz de similitudes/distancias\n",
    "        \"\"\"\n",
    "        if metric == \"cosine\":\n",
    "            return cosine_similarity(queries, corpus)\n",
    "        elif metric == \"euclidean\":\n",
    "            return -euclidean_distances(queries, corpus)  # Negativo para ordenar descendente\n",
    "        elif metric == \"manhattan\":\n",
    "            return -manhattan_distances(queries, corpus)\n",
    "        elif metric == \"dot\":\n",
    "            return queries @ corpus.T\n",
    "        else:\n",
    "            raise ValueError(f\"M\u00e9trica no soportada: {metric}\")\n",
    "\n",
    "# Benchmark de m\u00e9tricas\n",
    "def benchmark_metrics():\n",
    "    \"\"\"Compara velocidad y resultados de diferentes m\u00e9tricas.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_vectors = 10000\n",
    "    dim = 768\n",
    "    \n",
    "    # Generar vectores normalizados (simulando embeddings)\n",
    "    vectors = np.random.randn(n_vectors, dim).astype(np.float32)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    query = vectors[0]\n",
    "    \n",
    "    results = {}\n",
    "    for metric in [\"cosine\", \"euclidean\", \"manhattan\", \"dot\"]:\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        if metric == \"cosine\":\n",
    "            scores = cosine_similarity([query], vectors)[0]\n",
    "        elif metric == \"euclidean\":\n",
    "            scores = -euclidean_distances([query], vectors)[0]\n",
    "        elif metric == \"manhattan\":\n",
    "            scores = -manhattan_distances([query], vectors)[0]\n",
    "        elif metric == \"dot\":\n",
    "            scores = vectors @ query\n",
    "        \n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        top_5_indices = np.argsort(scores)[-6:-1][::-1]  # Excluir el mismo vector\n",
    "        \n",
    "        results[metric] = {\n",
    "            \"time_ms\": elapsed,\n",
    "            \"top_5\": top_5_indices.tolist(),\n",
    "            \"top_scores\": scores[top_5_indices].tolist()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar benchmark\n",
    "bench_results = benchmark_metrics()\n",
    "for metric, data in bench_results.items():\n",
    "    print(f\"{metric:10} | {data['time_ms']:6.2f} ms | top scores: {data['top_scores'][:3]}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# cosine     |  15.32 ms | top scores: [0.987, 0.976, 0.965]\n",
    "# euclidean  |  18.45 ms | top scores: [-0.234, -0.289, -0.312]\n",
    "# manhattan  |  12.78 ms | top scores: [-8.456, -9.123, -9.678]\n",
    "# dot        |   3.21 ms | top scores: [0.987, 0.976, 0.965] \u2713 M\u00e1s r\u00e1pido para normalizados\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Cu\u00e1ndo Usar Cada M\u00e9trica\n",
    "\n",
    "| M\u00e9trica | Ventajas | Desventajas | Caso de Uso Ideal |\n",
    "|---------|----------|-------------|-------------------|\n",
    "| **Cosine** | \u2022 Ignora magnitud (solo sem\u00e1ntica)<br>\u2022 Intuitivo: 0-1 o -1 a 1<br>\u2022 Est\u00e1ndar en NLP | \u2022 Requiere normalizaci\u00f3n<br>\u2022 Divisi\u00f3n costosa | \u2705 **Embeddings de texto**<br>B\u00fasqueda sem\u00e1ntica, RAG, clasificaci\u00f3n |\n",
    "| **Dot Product** | \u2022 M\u00e1s r\u00e1pido (sin divisi\u00f3n)<br>\u2022 Equivalente a cosine si normalizado<br>\u2022 Eficiente en GPU | \u2022 Sensible a magnitud<br>\u2022 Requiere normalizaci\u00f3n previa | \u2705 **Producci\u00f3n a escala**<br>FAISS, b\u00fasqueda en millones de vectores |\n",
    "| **Euclidean** | \u2022 Considera magnitud<br>\u2022 M\u00e9trica geom\u00e9trica natural<br>\u2022 K-Means lo usa | \u2022 Curse of dimensionality<br>\u2022 Outliers impactan mucho | \u2705 **Clustering de im\u00e1genes**<br>K-Means, espacios de baja dimensi\u00f3n |\n",
    "| **Manhattan** | \u2022 Robusto a outliers<br>\u2022 Interpretable (suma diferencias)<br>\u2022 Mejor en alta dimensi\u00f3n | \u2022 No tan com\u00fan<br>\u2022 Menos intuitivo sem\u00e1nticamente | \u2705 **Feature vectors sparse**<br>Embeddings con muchos ceros |\n",
    "\n",
    "### \ud83d\udd0d B\u00fasqueda Top-K Optimizada\n",
    "\n",
    "```python\n",
    "import heapq\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Resultado de b\u00fasqueda con score y metadata.\"\"\"\n",
    "    index: int\n",
    "    score: float\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "class TopKSearch:\n",
    "    \"\"\"B\u00fasqueda top-k con m\u00faltiples estrategias de optimizaci\u00f3n.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: np.ndarray, texts: List[str], metadata: List[dict]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (n, dim) matriz de embeddings normalizados\n",
    "            texts: lista de textos originales\n",
    "            metadata: lista de metadatos por cada embedding\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.texts = texts\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        # Normalizar embeddings para usar dot product = cosine\n",
    "        self.embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    def search_vectorized(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        top_k: int = 10\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B\u00fasqueda vectorizada (NumPy) - m\u00e1s r\u00e1pida para corpus peque\u00f1o (<100K).\"\"\"\n",
    "        query = query / np.linalg.norm(query)  # Normalizar query\n",
    "        \n",
    "        # Dot product = cosine similarity para vectores normalizados\n",
    "        scores = self.embeddings @ query  # (n,) array\n",
    "        \n",
    "        # Top-k con argpartition (m\u00e1s r\u00e1pido que argsort para k peque\u00f1o)\n",
    "        if top_k < len(scores):\n",
    "            top_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
    "            top_indices = top_indices[np.argsort(scores[top_indices])][::-1]\n",
    "        else:\n",
    "            top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=int(idx),\n",
    "                score=float(scores[idx]),\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def search_heap(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        top_k: int = 10\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B\u00fasqueda con heap (memoria constante) - \u00fatil para corpus muy grande.\"\"\"\n",
    "        query = query / np.linalg.norm(query)\n",
    "        \n",
    "        # Min-heap de tama\u00f1o top_k (mantiene los k mayores)\n",
    "        heap = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            score = np.dot(emb, query)\n",
    "            \n",
    "            if len(heap) < top_k:\n",
    "                heapq.heappush(heap, (score, i))\n",
    "            elif score > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (score, i))\n",
    "        \n",
    "        # Ordenar resultados descendente\n",
    "        results = sorted(heap, reverse=True)\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=idx,\n",
    "                score=score,\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for score, idx in results\n",
    "        ]\n",
    "    \n",
    "    def search_threshold(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        threshold: float = 0.8\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B\u00fasqueda por threshold (retorna todos los que superan umbral).\"\"\"\n",
    "        query = query / np.linalg.norm(query)\n",
    "        scores = self.embeddings @ query\n",
    "        \n",
    "        # Filtrar por threshold\n",
    "        above_threshold = np.where(scores >= threshold)[0]\n",
    "        sorted_indices = above_threshold[np.argsort(scores[above_threshold])][::-1]\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=int(idx),\n",
    "                score=float(scores[idx]),\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for idx in sorted_indices\n",
    "        ]\n",
    "\n",
    "# Ejemplo de uso\n",
    "texts = [\n",
    "    \"Tabla de ventas con transacciones diarias\",\n",
    "    \"Data de transacciones de venta por d\u00eda\",\n",
    "    \"Cat\u00e1logo de productos con precios\",\n",
    "    \"Informaci\u00f3n demogr\u00e1fica de clientes\"\n",
    "]\n",
    "embeddings = np.random.randn(len(texts), 768)  # Simular embeddings\n",
    "metadata = [{\"owner\": \"analytics\", \"source\": f\"dwh.table_{i}\"} for i in range(len(texts))]\n",
    "\n",
    "search_engine = TopKSearch(embeddings, texts, metadata)\n",
    "query_embedding = np.random.randn(768)\n",
    "\n",
    "# B\u00fasqueda top-3\n",
    "results = search_engine.search_vectorized(query_embedding, top_k=3)\n",
    "for r in results:\n",
    "    print(f\"Score {r.score:.3f} | {r.text} | {r.metadata['source']}\")\n",
    "\n",
    "# B\u00fasqueda por threshold (duplicados)\n",
    "similar = search_engine.search_threshold(embeddings[0], threshold=0.95)\n",
    "print(f\"\\nEncontrados {len(similar)} textos muy similares (>0.95)\")\n",
    "```\n",
    "\n",
    "### \ud83d\udcca An\u00e1lisis de Distribuci\u00f3n de Similitudes\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_similarity_distribution(embeddings: np.ndarray, sample_size: int = 1000):\n",
    "    \"\"\"Analiza la distribuci\u00f3n de similitudes en un corpus.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Normalizar embeddings\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Muestrear pares aleatorios\n",
    "    n = len(embeddings)\n",
    "    pairs = np.random.choice(n, size=(sample_size, 2), replace=True)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, j in pairs:\n",
    "        if i != j:\n",
    "            sim = np.dot(embeddings[i], embeddings[j])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    # Estad\u00edsticas\n",
    "    stats = {\n",
    "        \"mean\": similarities.mean(),\n",
    "        \"std\": similarities.std(),\n",
    "        \"min\": similarities.min(),\n",
    "        \"max\": similarities.max(),\n",
    "        \"median\": np.median(similarities),\n",
    "        \"q95\": np.percentile(similarities, 95),  # Threshold para \"muy similar\"\n",
    "        \"q99\": np.percentile(similarities, 99)   # Threshold para \"duplicados\"\n",
    "    }\n",
    "    \n",
    "    print(\"Distribuci\u00f3n de similitudes en corpus:\")\n",
    "    print(f\"  Media: {stats['mean']:.3f} \u00b1 {stats['std']:.3f}\")\n",
    "    print(f\"  Rango: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "    print(f\"  Percentil 95: {stats['q95']:.3f}  \u2190 Threshold para 'muy similar'\")\n",
    "    print(f\"  Percentil 99: {stats['q99']:.3f}  \u2190 Threshold para 'duplicados'\")\n",
    "    \n",
    "    return stats, similarities\n",
    "\n",
    "# Ejemplo: analizar corpus de embeddings\n",
    "embeddings = np.random.randn(10000, 768)\n",
    "stats, sims = analyze_similarity_distribution(embeddings)\n",
    "\n",
    "# Salida t\u00edpica:\n",
    "# Distribuci\u00f3n de similitudes en corpus:\n",
    "#   Media: 0.012 \u00b1 0.087\n",
    "#   Rango: [-0.289, 0.354]\n",
    "#   Percentil 95: 0.156  \u2190 Threshold para 'muy similar'\n",
    "#   Percentil 99: 0.213  \u2190 Threshold para 'duplicados'\n",
    "#\n",
    "# Interpretaci\u00f3n:\n",
    "# - Media cercana a 0: corpus diverso (bueno)\n",
    "# - Usar threshold 0.95-0.99 para deduplicaci\u00f3n\n",
    "# - Usar threshold 0.80-0.90 para b\u00fasqueda de similares\n",
    "```\n",
    "\n",
    "### \ud83c\udfa8 Visualizaci\u00f3n de Similitudes\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings_2d(\n",
    "    embeddings: np.ndarray, \n",
    "    labels: List[str], \n",
    "    method: Literal[\"pca\", \"tsne\"] = \"tsne\"\n",
    "):\n",
    "    \"\"\"Visualiza embeddings en 2D para an\u00e1lisis exploratorio.\"\"\"\n",
    "    \n",
    "    # Reducci\u00f3n de dimensionalidad\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:  # tsne\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    \n",
    "    coords_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(coords_2d[:, 0], coords_2d[:, 1], alpha=0.6)\n",
    "    \n",
    "    # Anotar algunos puntos\n",
    "    for i, label in enumerate(labels[:20]):  # Primeros 20 para no saturar\n",
    "        plt.annotate(\n",
    "            label[:30],  # Truncar texto\n",
    "            (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "            fontsize=8,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"Visualizaci\u00f3n de Embeddings ({method.upper()})\")\n",
    "    plt.xlabel(\"Dimensi\u00f3n 1\")\n",
    "    plt.ylabel(\"Dimensi\u00f3n 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return coords_2d\n",
    "```\n",
    "\n",
    "### \ud83d\ude80 Reglas de Oro para M\u00e9tricas de Similitud\n",
    "\n",
    "1. **Embeddings de texto normalizados**: Usa **dot product** (m\u00e1s r\u00e1pido que cosine)\n",
    "2. **Corpus <100K vectores**: B\u00fasqueda **vectorizada con NumPy**\n",
    "3. **Corpus >1M vectores**: Usar **FAISS** con \u00edndices especializados\n",
    "4. **Deduplicaci\u00f3n**: Threshold **>0.95** (percentil 99 del corpus)\n",
    "5. **B\u00fasqueda sem\u00e1ntica**: Threshold **>0.80** (top-k con k=5-10)\n",
    "6. **Clustering**: **Euclidean distance** con K-Means\n",
    "7. **Monitorear distribuci\u00f3n**: Similitudes extremas (>0.99 o <0.1) son sospechosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b12f",
   "metadata": {},
   "source": [
    "## \u26a1 B\u00fasqueda Vectorial a Escala: FAISS y Annoy\n",
    "\n",
    "Cuando el corpus supera **100K embeddings**, la b\u00fasqueda lineal (calcular similitud con todos los vectores) se vuelve prohibitivamente lenta. **FAISS** (Facebook AI Similarity Search) y **Annoy** (Spotify) son librer\u00edas especializadas que aceleran la b\u00fasqueda usando **Approximate Nearest Neighbors (ANN)** con \u00edndices optimizados.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Arquitectura de FAISS\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    FAISS: \u00cdNDICES Y PERFORMANCE                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  PROBLEMA: B\u00fasqueda lineal O(n\u00d7d) es lenta para n grande       \u2502\n",
    "\u2502  10M vectores \u00d7 768 dims \u00d7 4 bytes = 30 GB memoria             \u2502\n",
    "\u2502  B\u00fasqueda 1 query: 10M dot products = ~500ms                   \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  SOLUCI\u00d3N: \u00cdndices especializados con trade-off speed vs accuracy\u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 1. IndexFlatL2 / IndexFlatIP (EXACTO)                  \u2502    \u2502\n",
    "\u2502  \u2502    - B\u00fasqueda exhaustiva (brute force)                 \u2502    \u2502\n",
    "\u2502  \u2502    - 100% recall, pero lento                           \u2502    \u2502\n",
    "\u2502  \u2502    - Baseline para comparar                            \u2502    \u2502\n",
    "\u2502  \u2502    Complejidad: O(n\u00d7d)                                 \u2502    \u2502\n",
    "\u2502  \u2502    Uso: <100K vectores, validaci\u00f3n                     \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 2. IndexIVFFlat (PARTICIONADO)                         \u2502    \u2502\n",
    "\u2502  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502 a) Training: K-Means agrupa en nlist    \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502    centroides (ej. nlist=1000)          \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502                                          \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502 b) Indexing: asigna cada vector al      \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502    centroide m\u00e1s cercano                \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502                                          \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502 c) Search: busca en nprobe clusters     \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2502    m\u00e1s cercanos (ej. nprobe=10)         \u2502         \u2502    \u2502\n",
    "\u2502  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502    \u2502\n",
    "\u2502  \u2502    Complejidad: O(nprobe \u00d7 n/nlist \u00d7 d)               \u2502    \u2502\n",
    "\u2502  \u2502    Recall: 90-95% con nprobe=10                       \u2502    \u2502\n",
    "\u2502  \u2502    Speedup: 10-100x vs flat                           \u2502    \u2502\n",
    "\u2502  \u2502    Uso: 100K - 10M vectores                           \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 3. IndexIVFPQ (COMPRESI\u00d3N + PARTICIONADO)             \u2502    \u2502\n",
    "\u2502  \u2502    - Product Quantization (PQ): comprime vectores      \u2502    \u2502\n",
    "\u2502  \u2502      768 dims \u2192 96 bytes (8x compresi\u00f3n)               \u2502    \u2502\n",
    "\u2502  \u2502    - Divide vector en m subvectores                    \u2502    \u2502\n",
    "\u2502  \u2502    - Cada subvector \u2192 codebook de 256 centroides      \u2502    \u2502\n",
    "\u2502  \u2502    - B\u00fasqueda en espacio comprimido (r\u00e1pida)          \u2502    \u2502\n",
    "\u2502  \u2502    Recall: 85-90%                                      \u2502    \u2502\n",
    "\u2502  \u2502    Speedup: 100-1000x vs flat                         \u2502    \u2502\n",
    "\u2502  \u2502    Memory: 10-20x menos que IVFFlat                   \u2502    \u2502\n",
    "\u2502  \u2502    Uso: 10M - 1B vectores                             \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 4. IndexHNSW (GRAFO JER\u00c1RQUICO)                        \u2502    \u2502\n",
    "\u2502  \u2502    - Hierarchical Navigable Small World                \u2502    \u2502\n",
    "\u2502  \u2502    - Grafo multi-nivel con conexiones                  \u2502    \u2502\n",
    "\u2502  \u2502    - B\u00fasqueda greedy navegando grafo                   \u2502    \u2502\n",
    "\u2502  \u2502    Recall: 95-99% (mejor que IVF)                      \u2502    \u2502\n",
    "\u2502  \u2502    Speedup: 50-500x vs flat                            \u2502    \u2502\n",
    "\u2502  \u2502    Memory: Similar a flat (sin compresi\u00f3n)             \u2502    \u2502\n",
    "\u2502  \u2502    Uso: <10M vectores, alta precisi\u00f3n requerida       \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83d\udd27 Implementaci\u00f3n de FAISS para Diferentes Escalas\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "\n",
    "class FAISSSearchEngine:\n",
    "    \"\"\"Motor de b\u00fasqueda vectorial con FAISS para diferentes escalas.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dimension: int, \n",
    "        index_type: str = \"auto\",\n",
    "        metric: str = \"cosine\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension: dimensionalidad de embeddings\n",
    "            index_type: \"flat\", \"ivf\", \"ivfpq\", \"hnsw\", \"auto\"\n",
    "            metric: \"cosine\" (IP para normalizados) o \"l2\" (euclidean)\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.index_type = index_type\n",
    "        self.metric = metric\n",
    "        self.index = None\n",
    "        self.n_vectors = 0\n",
    "    \n",
    "    def build_index(self, embeddings: np.ndarray, nlist: int = None):\n",
    "        \"\"\"\n",
    "        Construye \u00edndice FAISS apropiado seg\u00fan tama\u00f1o del corpus.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (n, dim) matriz de embeddings\n",
    "            nlist: n\u00famero de clusters para IVF (auto si None)\n",
    "        \"\"\"\n",
    "        self.n_vectors = len(embeddings)\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        \n",
    "        # Normalizar si m\u00e9trica es cosine\n",
    "        if self.metric == \"cosine\":\n",
    "            faiss.normalize_L2(embeddings)  # In-place normalization\n",
    "        \n",
    "        # Selecci\u00f3n autom\u00e1tica de \u00edndice seg\u00fan tama\u00f1o\n",
    "        if self.index_type == \"auto\":\n",
    "            if self.n_vectors < 10_000:\n",
    "                self.index_type = \"flat\"\n",
    "            elif self.n_vectors < 1_000_000:\n",
    "                self.index_type = \"ivf\"\n",
    "            else:\n",
    "                self.index_type = \"ivfpq\"\n",
    "        \n",
    "        # Construir \u00edndice seg\u00fan tipo\n",
    "        if self.index_type == \"flat\":\n",
    "            # B\u00fasqueda exacta (baseline)\n",
    "            if self.metric == \"cosine\":\n",
    "                self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product\n",
    "            else:\n",
    "                self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            self.index.add(embeddings)\n",
    "            print(f\"\u2713 IndexFlat construido: {self.n_vectors} vectores\")\n",
    "        \n",
    "        elif self.index_type == \"ivf\":\n",
    "            # IVF: particionado con K-Means\n",
    "            nlist = nlist or min(int(np.sqrt(self.n_vectors)), 4096)\n",
    "            \n",
    "            if self.metric == \"cosine\":\n",
    "                quantizer = faiss.IndexFlatIP(self.dimension)\n",
    "                self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "            else:\n",
    "                quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "                self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "            \n",
    "            # Training: K-Means para encontrar centroides\n",
    "            print(f\"Training IVF con {nlist} clusters...\")\n",
    "            self.index.train(embeddings)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            # Configurar nprobe (cu\u00e1ntos clusters buscar)\n",
    "            self.index.nprobe = min(10, nlist // 10)  # Default: 10 o 10% de clusters\n",
    "            print(f\"\u2713 IndexIVFFlat construido: {self.n_vectors} vectores, {nlist} clusters, nprobe={self.index.nprobe}\")\n",
    "        \n",
    "        elif self.index_type == \"ivfpq\":\n",
    "            # IVFPQ: particionado + compresi\u00f3n\n",
    "            nlist = nlist or min(int(np.sqrt(self.n_vectors)), 4096)\n",
    "            m = 8  # N\u00famero de subvectores (debe dividir dimension)\n",
    "            nbits = 8  # Bits por subvector (2^8 = 256 centroides por subvector)\n",
    "            \n",
    "            if self.metric == \"cosine\":\n",
    "                quantizer = faiss.IndexFlatIP(self.dimension)\n",
    "                self.index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, nbits)\n",
    "            else:\n",
    "                quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "                self.index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, nbits)\n",
    "            \n",
    "            print(f\"Training IVFPQ con {nlist} clusters, m={m}, nbits={nbits}...\")\n",
    "            self.index.train(embeddings)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            self.index.nprobe = min(10, nlist // 10)\n",
    "            \n",
    "            # Calcular compresi\u00f3n\n",
    "            original_size = self.n_vectors * self.dimension * 4  # float32\n",
    "            compressed_size = self.n_vectors * m  # m bytes por vector\n",
    "            ratio = original_size / compressed_size\n",
    "            print(f\"\u2713 IndexIVFPQ construido: {self.n_vectors} vectores, compresi\u00f3n {ratio:.1f}x\")\n",
    "        \n",
    "        elif self.index_type == \"hnsw\":\n",
    "            # HNSW: grafo jer\u00e1rquico\n",
    "            M = 32  # N\u00famero de conexiones por nodo\n",
    "            self.index = faiss.IndexHNSWFlat(self.dimension, M)\n",
    "            self.index.hnsw.efConstruction = 40  # Calidad de construcci\u00f3n\n",
    "            self.index.add(embeddings)\n",
    "            print(f\"\u2713 IndexHNSW construido: {self.n_vectors} vectores, M={M}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Tipo de \u00edndice no soportado: {self.index_type}\")\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        queries: np.ndarray, \n",
    "        k: int = 10\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Busca k vecinos m\u00e1s cercanos para cada query.\n",
    "        \n",
    "        Args:\n",
    "            queries: (n_queries, dim) matriz de queries\n",
    "            k: n\u00famero de vecinos a retornar\n",
    "        \n",
    "        Returns:\n",
    "            distances: (n_queries, k) distancias/similitudes\n",
    "            indices: (n_queries, k) \u00edndices de vecinos\n",
    "        \"\"\"\n",
    "        queries = queries.astype('float32')\n",
    "        \n",
    "        if self.metric == \"cosine\":\n",
    "            faiss.normalize_L2(queries)\n",
    "        \n",
    "        distances, indices = self.index.search(queries, k)\n",
    "        return distances, indices\n",
    "    \n",
    "    def tune_parameters(self, queries: np.ndarray, ground_truth_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        Encuentra nprobe \u00f3ptimo balanceando recall vs latencia.\n",
    "        \n",
    "        Args:\n",
    "            queries: queries de validaci\u00f3n\n",
    "            ground_truth_indices: resultados exactos (de IndexFlat)\n",
    "        \"\"\"\n",
    "        if self.index_type not in [\"ivf\", \"ivfpq\"]:\n",
    "            print(\"Tuning solo aplica a \u00edndices IVF\")\n",
    "            return\n",
    "        \n",
    "        nprobe_values = [1, 2, 5, 10, 20, 50, 100]\n",
    "        results = []\n",
    "        \n",
    "        for nprobe in nprobe_values:\n",
    "            self.index.nprobe = nprobe\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            _, indices = self.search(queries, k=10)\n",
    "            latency = (time.perf_counter() - start) * 1000 / len(queries)\n",
    "            \n",
    "            # Calcular recall@10\n",
    "            recall = np.mean([\n",
    "                len(np.intersect1d(indices[i], ground_truth_indices[i])) / 10\n",
    "                for i in range(len(queries))\n",
    "            ])\n",
    "            \n",
    "            results.append({\n",
    "                \"nprobe\": nprobe,\n",
    "                \"recall\": recall,\n",
    "                \"latency_ms\": latency\n",
    "            })\n",
    "            \n",
    "            print(f\"nprobe={nprobe:3} | recall@10={recall:.3f} | latency={latency:.2f}ms\")\n",
    "        \n",
    "        # Seleccionar nprobe con recall >0.95 y menor latencia\n",
    "        best = max([r for r in results if r[\"recall\"] >= 0.95], \n",
    "                   key=lambda x: -x[\"latency_ms\"], \n",
    "                   default=results[-1])\n",
    "        \n",
    "        self.index.nprobe = best[\"nprobe\"]\n",
    "        print(f\"\\n\u2713 Configuraci\u00f3n \u00f3ptima: nprobe={best['nprobe']} (recall={best['recall']:.3f}, latency={best['latency_ms']:.2f}ms)\")\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Guarda \u00edndice a disco.\"\"\"\n",
    "        faiss.write_index(self.index, filepath)\n",
    "        print(f\"\u2713 \u00cdndice guardado en {filepath}\")\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Carga \u00edndice desde disco.\"\"\"\n",
    "        self.index = faiss.read_index(filepath)\n",
    "        self.n_vectors = self.index.ntotal\n",
    "        print(f\"\u2713 \u00cdndice cargado desde {filepath}: {self.n_vectors} vectores\")\n",
    "\n",
    "# Ejemplo: construcci\u00f3n y b\u00fasqueda con FAISS\n",
    "np.random.seed(42)\n",
    "n_vectors = 100_000\n",
    "dimension = 768\n",
    "\n",
    "print(\"Generando embeddings...\")\n",
    "embeddings = np.random.randn(n_vectors, dimension).astype('float32')\n",
    "\n",
    "# Construir \u00edndice IVF\n",
    "engine = FAISSSearchEngine(dimension=dimension, index_type=\"ivf\", metric=\"cosine\")\n",
    "engine.build_index(embeddings)\n",
    "\n",
    "# B\u00fasqueda\n",
    "queries = np.random.randn(100, dimension).astype('float32')\n",
    "k = 10\n",
    "\n",
    "start = time.perf_counter()\n",
    "distances, indices = engine.search(queries, k=k)\n",
    "elapsed = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"\\nB\u00fasqueda completada:\")\n",
    "print(f\"  {len(queries)} queries \u00d7 top-{k}\")\n",
    "print(f\"  Latencia: {elapsed/len(queries):.2f} ms/query\")\n",
    "print(f\"  Throughput: {len(queries)/(elapsed/1000):.0f} queries/seg\")\n",
    "\n",
    "# Guardar \u00edndice\n",
    "engine.save(\"faiss_index.bin\")\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Comparaci\u00f3n FAISS vs Annoy vs Brute Force\n",
    "\n",
    "```python\n",
    "import annoy\n",
    "\n",
    "def benchmark_search_engines(embeddings: np.ndarray, queries: np.ndarray, k: int = 10):\n",
    "    \"\"\"Compara diferentes motores de b\u00fasqueda.\"\"\"\n",
    "    n, dim = embeddings.shape\n",
    "    \n",
    "    # Normalizar para cosine similarity\n",
    "    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    queries_norm = queries / np.linalg.norm(queries, axis=1, keepdims=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Brute Force (NumPy)\n",
    "    print(\"1\ufe0f\u20e3 Brute Force (NumPy)...\")\n",
    "    start = time.perf_counter()\n",
    "    scores = queries_norm @ embeddings_norm.T  # (n_queries, n_corpus)\n",
    "    bf_indices = np.argsort(scores, axis=1)[:, -k:][:, ::-1]\n",
    "    bf_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    results[\"brute_force\"] = {\n",
    "        \"build_time_ms\": 0,\n",
    "        \"search_time_ms\": bf_time,\n",
    "        \"latency_per_query_ms\": bf_time / len(queries),\n",
    "        \"recall\": 1.0,  # 100% por definici\u00f3n\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 2. FAISS IndexFlatIP (exacto optimizado)\n",
    "    print(\"2\ufe0f\u20e3 FAISS IndexFlatIP...\")\n",
    "    start = time.perf_counter()\n",
    "    index_flat = faiss.IndexFlatIP(dim)\n",
    "    index_flat.add(embeddings_norm.astype('float32'))\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    faiss_flat_distances, faiss_flat_indices = index_flat.search(queries_norm.astype('float32'), k)\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    results[\"faiss_flat\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": 1.0,\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 3. FAISS IndexIVFFlat (aproximado)\n",
    "    print(\"3\ufe0f\u20e3 FAISS IndexIVFFlat...\")\n",
    "    nlist = min(int(np.sqrt(n)), 1000)\n",
    "    quantizer = faiss.IndexFlatIP(dim)\n",
    "    index_ivf = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    index_ivf.train(embeddings_norm.astype('float32'))\n",
    "    index_ivf.add(embeddings_norm.astype('float32'))\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    index_ivf.nprobe = 10\n",
    "    start = time.perf_counter()\n",
    "    faiss_ivf_distances, faiss_ivf_indices = index_ivf.search(queries_norm.astype('float32'), k)\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    # Calcular recall vs brute force\n",
    "    recall = np.mean([\n",
    "        len(np.intersect1d(faiss_ivf_indices[i], bf_indices[i])) / k\n",
    "        for i in range(len(queries))\n",
    "    ])\n",
    "    \n",
    "    results[\"faiss_ivf\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": recall,\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 4. Annoy (Spotify)\n",
    "    print(\"4\ufe0f\u20e3 Annoy...\")\n",
    "    annoy_index = annoy.AnnoyIndex(dim, 'angular')  # angular = cosine\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for i, vec in enumerate(embeddings_norm):\n",
    "        annoy_index.add_item(i, vec)\n",
    "    annoy_index.build(10)  # 10 \u00e1rboles (m\u00e1s \u00e1rboles = mejor recall pero m\u00e1s lento)\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    annoy_indices = []\n",
    "    for query in queries_norm:\n",
    "        annoy_indices.append(annoy_index.get_nns_by_vector(query, k))\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    annoy_indices = np.array(annoy_indices)\n",
    "    recall = np.mean([\n",
    "        len(np.intersect1d(annoy_indices[i], bf_indices[i])) / k\n",
    "        for i in range(len(queries))\n",
    "    ])\n",
    "    \n",
    "    # Annoy guarda \u00edndice en disco, estimar tama\u00f1o\n",
    "    annoy_index.save(\"temp_annoy.ann\")\n",
    "    import os\n",
    "    memory_mb = os.path.getsize(\"temp_annoy.ann\") / 1024 / 1024\n",
    "    os.remove(\"temp_annoy.ann\")\n",
    "    \n",
    "    results[\"annoy\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": recall,\n",
    "        \"memory_mb\": memory_mb\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar benchmark\n",
    "embeddings = np.random.randn(50000, 384).astype('float32')\n",
    "queries = np.random.randn(100, 384).astype('float32')\n",
    "\n",
    "bench_results = benchmark_search_engines(embeddings, queries, k=10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Engine':<15} | {'Build (ms)':<12} | {'Search (ms)':<12} | {'Latency/Q (ms)':<15} | {'Recall':<8} | {'Memory (MB)':<12}\")\n",
    "print(\"=\"*80)\n",
    "for engine, metrics in bench_results.items():\n",
    "    print(f\"{engine:<15} | {metrics['build_time_ms']:>11.2f} | {metrics['search_time_ms']:>11.2f} | {metrics['latency_per_query_ms']:>14.3f} | {metrics['recall']:>7.3f} | {metrics['memory_mb']:>11.1f}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# ================================================================================\n",
    "# Engine          | Build (ms)   | Search (ms)  | Latency/Q (ms) | Recall   | Memory (MB) \n",
    "# ================================================================================\n",
    "# brute_force     |        0.00 |      342.56 |         3.426 |   1.000 |        73.2\n",
    "# faiss_flat      |       12.34 |       89.12 |         0.891 |   1.000 |        73.2\n",
    "# faiss_ivf       |      234.56 |       15.67 |         0.157 |   0.943 |        73.2\n",
    "# annoy           |     1234.89 |       12.34 |         0.123 |   0.921 |        95.6\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Gu\u00eda de Selecci\u00f3n de \u00cdndice\n",
    "\n",
    "| Escala | \u00cdndice Recomendado | Build Time | Search Latency | Recall | Memory | Caso de Uso |\n",
    "|--------|-------------------|------------|----------------|--------|--------|-------------|\n",
    "| <10K | **IndexFlat** | Instant\u00e1neo | 1-5 ms/query | 100% | 1x | Desarrollo, validaci\u00f3n |\n",
    "| 10K-100K | **IndexHNSW** | Medio | 0.5-2 ms/query | 95-99% | 1x | Producci\u00f3n, alta precisi\u00f3n |\n",
    "| 100K-1M | **IndexIVFFlat** | Medio | 0.1-1 ms/query | 90-95% | 1x | Producci\u00f3n general |\n",
    "| 1M-10M | **IndexIVFPQ** | Alto | 0.05-0.5 ms/query | 85-90% | 0.1x | Alta escala, cost-effective |\n",
    "| >10M | **IndexIVFPQ + GPU** | Muy alto | 0.01-0.1 ms/query | 85-90% | 0.1x | Web-scale search |\n",
    "\n",
    "### \ud83d\udca1 Optimizaciones Avanzadas\n",
    "\n",
    "```python\n",
    "# 1. GPU Acceleration (100x speedup para corpus muy grande)\n",
    "import faiss\n",
    "gpu_resources = faiss.StandardGpuResources()\n",
    "index_cpu = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index_gpu = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)  # 0 = GPU ID\n",
    "\n",
    "# 2. Sharding para corpus masivo (>100M vectores)\n",
    "# Dividir corpus en shards, buscar en paralelo, merge results\n",
    "shards = [\n",
    "    FAISSSearchEngine(dim, \"ivfpq\") for _ in range(10)\n",
    "]\n",
    "for i, shard in enumerate(shards):\n",
    "    shard.build_index(embeddings[i*chunk_size:(i+1)*chunk_size])\n",
    "\n",
    "# B\u00fasqueda paralela\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(lambda s: s.search(query, k=100), shards))\n",
    "# Merge top-100 de cada shard \u2192 global top-10\n",
    "\n",
    "# 3. Prefiltering con metadatos\n",
    "# FAISS no soporta filtros nativamente, usar ID selector\n",
    "ids_filtered = [i for i, meta in enumerate(metadata) if meta['type'] == 'pipeline']\n",
    "selector = faiss.IDSelectorArray(len(ids_filtered), faiss.swig_ptr(np.array(ids_filtered)))\n",
    "distances, indices = index.search(query, k=10, params=faiss.SearchParametersIVF(sel=selector))\n",
    "```\n",
    "\n",
    "### \ud83d\ude80 Reglas de Oro para B\u00fasqueda Vectorial a Escala\n",
    "\n",
    "1. **<10K vectores**: Usar b\u00fasqueda lineal (NumPy) - es suficiente\n",
    "2. **10K-100K**: FAISS **IndexHNSW** (mejor recall, no requiere training)\n",
    "3. **>100K**: FAISS **IndexIVFFlat** con nprobe tuneado (balance recall/latencia)\n",
    "4. **>1M**: FAISS **IndexIVFPQ** para reducir memoria 10x\n",
    "5. **Siempre** medir recall vs ground truth antes de producci\u00f3n (target: >95%)\n",
    "6. **Tune nprobe** con queries reales, no valores por defecto\n",
    "7. **Guardar \u00edndice** entrenado a disco, no reconstruir cada deploy\n",
    "8. **GPU** solo si corpus >10M y latencia <1ms requerida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062ee38",
   "metadata": {},
   "source": [
    "## \ud83c\udfed Aplicaciones en Data Engineering: Deduplicaci\u00f3n, Clustering y Recomendaciones\n",
    "\n",
    "Los embeddings permiten resolver problemas complejos de Data Engineering que antes requer\u00edan reglas manuales o l\u00f3gica dif\u00edcil de mantener: **deduplicaci\u00f3n fuzzy**, **clustering sem\u00e1ntico**, **recomendaciones** de datasets, y **etiquetado autom\u00e1tico** de activos de datos.\n",
    "\n",
    "### \ud83d\udd0d 1. Deduplicaci\u00f3n Inteligente de Registros\n",
    "\n",
    "La deduplicaci\u00f3n tradicional (exact match por hash) falla con variaciones: \"Apple iPhone 13\" vs \"iPhone 13 by Apple\". Los embeddings capturan similitud sem\u00e1ntica.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              PIPELINE DE DEDUPLICACI\u00d3N CON EMBEDDINGS            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Input: Tabla con posibles duplicados                           \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n",
    "\u2502  \u2502 id \u2502 descripcion                               \u2502             \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524             \u2502\n",
    "\u2502  \u2502 1  \u2502 Apple iPhone 13 Pro Max 256GB             \u2502             \u2502\n",
    "\u2502  \u2502 2  \u2502 iPhone 13 Pro Max 256GB Apple             \u2502 \u2190 Duplicado \u2502\n",
    "\u2502  \u2502 3  \u2502 Samsung Galaxy S21 Ultra                  \u2502             \u2502\n",
    "\u2502  \u2502 4  \u2502 Galaxy S21 Ultra by Samsung               \u2502 \u2190 Duplicado \u2502\n",
    "\u2502  \u2502 5  \u2502 Sony PlayStation 5 Console                \u2502             \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  1\ufe0f\u20e3 Preprocesamiento                                            \u2502\n",
    "\u2502     - Lowercasing: \"Apple iPhone\" \u2192 \"apple iphone\"             \u2502\n",
    "\u2502     - Remover stopwords: \"by\", \"the\", etc.                     \u2502\n",
    "\u2502     - Normalizar espacios: \"  \" \u2192 \" \"                          \u2502\n",
    "\u2502     - Quitar caracteres especiales: \"iPhone-13\" \u2192 \"iphone 13\"  \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  2\ufe0f\u20e3 Generaci\u00f3n de Embeddings                                    \u2502\n",
    "\u2502     - Usar modelo robusto (bge-large-en-v1.5)                  \u2502\n",
    "\u2502     - Batch processing (100 registros/llamada)                 \u2502\n",
    "\u2502     - Cache embeddings en columna nueva                        \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  3\ufe0f\u20e3 B\u00fasqueda de Pares Similares                                \u2502\n",
    "\u2502     - Opci\u00f3n A: All-pairs O(n\u00b2) para corpus peque\u00f1o           \u2502\n",
    "\u2502     - Opci\u00f3n B: FAISS para corpus grande (>100K)              \u2502\n",
    "\u2502     - Threshold t\u00edpico: 0.95-0.99 (percentil 99 del corpus)   \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  4\ufe0f\u20e3 Clustering de Duplicados                                    \u2502\n",
    "\u2502     - Connected components: transitivity                       \u2502\n",
    "\u2502       Si A\u2248B y B\u2248C \u2192 {A, B, C} es cluster                     \u2502\n",
    "\u2502     - Asignar cluster_id a cada registro                       \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  5\ufe0f\u20e3 Resoluci\u00f3n (elegir registro can\u00f3nico)                       \u2502\n",
    "\u2502     - Estrategia 1: El m\u00e1s completo (m\u00e1s palabras)            \u2502\n",
    "\u2502     - Estrategia 2: El m\u00e1s reciente (timestamp)               \u2502\n",
    "\u2502     - Estrategia 3: El m\u00e1s popular (m\u00e1s ventas/views)         \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  Output: Tabla deduplicada + mapping                            \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n",
    "\u2502  \u2502 cluster_id \u2502 canonical_id \u2502 descripcion        \u2502             \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524             \u2502\n",
    "\u2502  \u2502 1          \u2502 1            \u2502 Apple iPhone 13... \u2502             \u2502\n",
    "\u2502  \u2502 1          \u2502 1            \u2502 iPhone 13 Pro Max..\u2502 \u2192 merged   \u2502\n",
    "\u2502  \u2502 2          \u2502 3            \u2502 Samsung Galaxy...  \u2502             \u2502\n",
    "\u2502  \u2502 2          \u2502 3            \u2502 Galaxy S21 Ultra...\u2502 \u2192 merged   \u2502\n",
    "\u2502  \u2502 3          \u2502 5            \u2502 Sony PlayStation...\u2502             \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "#### Implementaci\u00f3n Completa\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DuplicateCluster:\n",
    "    \"\"\"Cluster de registros duplicados.\"\"\"\n",
    "    cluster_id: int\n",
    "    record_ids: List[int]\n",
    "    canonical_id: int\n",
    "    similarity_scores: List[float]\n",
    "\n",
    "class FuzzyDeduplicator:\n",
    "    \"\"\"Deduplicador fuzzy usando embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator, threshold: float = 0.95):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_generator: EmbeddingGenerator instance\n",
    "            threshold: similitud m\u00ednima para considerar duplicados (0.95-0.99)\n",
    "        \"\"\"\n",
    "        self.generator = embedding_generator\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Normaliza texto antes de embeddings.\"\"\"\n",
    "        import re\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)  # Remover puntuaci\u00f3n\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalizar espacios\n",
    "        return text\n",
    "    \n",
    "    def find_duplicates(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        text_column: str,\n",
    "        id_column: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encuentra duplicados en DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con registros\n",
    "            text_column: columna con texto a comparar\n",
    "            id_column: columna con ID \u00fanico (usa index si None)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame con cluster_id y canonical_id\n",
    "        \"\"\"\n",
    "        if id_column is None:\n",
    "            df = df.reset_index(drop=True)\n",
    "            id_column = 'index'\n",
    "            df[id_column] = df.index\n",
    "        \n",
    "        # 1. Preprocesamiento\n",
    "        print(\"1\ufe0f\u20e3 Preprocesando texto...\")\n",
    "        df['text_clean'] = df[text_column].apply(self.preprocess_text)\n",
    "        \n",
    "        # 2. Generar embeddings (con cache)\n",
    "        print(\"2\ufe0f\u20e3 Generando embeddings...\")\n",
    "        embeddings = self.generator.embed(df['text_clean'].tolist())\n",
    "        \n",
    "        # 3. Encontrar pares similares\n",
    "        print(f\"3\ufe0f\u20e3 Buscando pares con similitud >{self.threshold}...\")\n",
    "        pairs = self._find_similar_pairs(embeddings, df[id_column].tolist())\n",
    "        print(f\"   Encontrados {len(pairs)} pares de duplicados\")\n",
    "        \n",
    "        # 4. Clustering (connected components)\n",
    "        print(\"4\ufe0f\u20e3 Agrupando en clusters...\")\n",
    "        clusters = self._cluster_duplicates(pairs, df[id_column].tolist())\n",
    "        print(f\"   {len(clusters)} clusters de duplicados\")\n",
    "        \n",
    "        # 5. Seleccionar registros can\u00f3nicos\n",
    "        print(\"5\ufe0f\u20e3 Seleccionando registros can\u00f3nicos...\")\n",
    "        cluster_map = {}\n",
    "        for cluster in clusters:\n",
    "            canonical_id = self._select_canonical(cluster, df, text_column, id_column)\n",
    "            for record_id in cluster.record_ids:\n",
    "                cluster_map[record_id] = {\n",
    "                    'cluster_id': cluster.cluster_id,\n",
    "                    'canonical_id': canonical_id,\n",
    "                    'is_canonical': record_id == canonical_id\n",
    "                }\n",
    "        \n",
    "        # Agregar columnas al DataFrame\n",
    "        df['cluster_id'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('cluster_id'))\n",
    "        df['canonical_id'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('canonical_id', x))\n",
    "        df['is_duplicate'] = df['cluster_id'].notna()\n",
    "        df['is_canonical'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('is_canonical', True))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _find_similar_pairs(\n",
    "        self, \n",
    "        embeddings: np.ndarray, \n",
    "        ids: List\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Encuentra pares de embeddings similares.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Para corpus peque\u00f1o (<10K), usar all-pairs\n",
    "        if len(embeddings) < 10_000:\n",
    "            sim_matrix = cosine_similarity(embeddings)\n",
    "            for i in range(len(embeddings)):\n",
    "                for j in range(i+1, len(embeddings)):\n",
    "                    if sim_matrix[i][j] > self.threshold:\n",
    "                        pairs.append((ids[i], ids[j], sim_matrix[i][j]))\n",
    "        else:\n",
    "            # Para corpus grande, usar FAISS\n",
    "            import faiss\n",
    "            embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            \n",
    "            index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "            index.add(embeddings_norm.astype('float32'))\n",
    "            \n",
    "            # Buscar k vecinos m\u00e1s cercanos para cada vector\n",
    "            k = min(10, len(embeddings) - 1)\n",
    "            distances, indices = index.search(embeddings_norm.astype('float32'), k + 1)\n",
    "            \n",
    "            for i in range(len(embeddings)):\n",
    "                for j, dist in zip(indices[i][1:], distances[i][1:]):  # Excluir el mismo vector\n",
    "                    if dist > self.threshold and ids[i] < ids[j]:  # Evitar duplicar pares\n",
    "                        pairs.append((ids[i], int(j), float(dist)))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _cluster_duplicates(\n",
    "        self, \n",
    "        pairs: List[Tuple[int, int, float]], \n",
    "        all_ids: List[int]\n",
    "    ) -> List[DuplicateCluster]:\n",
    "        \"\"\"Agrupa pares en clusters usando connected components.\"\"\"\n",
    "        # Construir grafo\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(all_ids)\n",
    "        \n",
    "        for id1, id2, score in pairs:\n",
    "            G.add_edge(id1, id2, weight=score)\n",
    "        \n",
    "        # Encontrar componentes conexas\n",
    "        clusters = []\n",
    "        for i, component in enumerate(nx.connected_components(G)):\n",
    "            if len(component) > 1:  # Solo clusters con >1 registro\n",
    "                component_list = list(component)\n",
    "                \n",
    "                # Calcular similitudes promedio\n",
    "                scores = [\n",
    "                    G[u][v]['weight'] \n",
    "                    for u, v in G.edges(component_list)\n",
    "                ]\n",
    "                \n",
    "                clusters.append(DuplicateCluster(\n",
    "                    cluster_id=i,\n",
    "                    record_ids=component_list,\n",
    "                    canonical_id=component_list[0],  # Temporal, se reemplaza despu\u00e9s\n",
    "                    similarity_scores=scores\n",
    "                ))\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _select_canonical(\n",
    "        self, \n",
    "        cluster: DuplicateCluster, \n",
    "        df: pd.DataFrame, \n",
    "        text_column: str,\n",
    "        id_column: str\n",
    "    ) -> int:\n",
    "        \"\"\"Selecciona registro can\u00f3nico del cluster.\"\"\"\n",
    "        cluster_df = df[df[id_column].isin(cluster.record_ids)]\n",
    "        \n",
    "        # Estrategia: el registro m\u00e1s completo (m\u00e1s palabras)\n",
    "        cluster_df['word_count'] = cluster_df[text_column].str.split().str.len()\n",
    "        canonical_id = cluster_df.loc[cluster_df['word_count'].idxmax(), id_column]\n",
    "        \n",
    "        return canonical_id\n",
    "    \n",
    "    def get_deduplication_report(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Genera reporte de deduplicaci\u00f3n.\"\"\"\n",
    "        total_records = len(df)\n",
    "        duplicate_records = df['is_duplicate'].sum()\n",
    "        unique_records = total_records - duplicate_records\n",
    "        n_clusters = df['cluster_id'].nunique()\n",
    "        \n",
    "        # Distribuci\u00f3n de tama\u00f1o de clusters\n",
    "        cluster_sizes = df[df['is_duplicate']].groupby('cluster_id').size()\n",
    "        \n",
    "        return {\n",
    "            \"total_records\": total_records,\n",
    "            \"unique_records\": unique_records,\n",
    "            \"duplicate_records\": duplicate_records,\n",
    "            \"deduplication_rate\": duplicate_records / total_records,\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"avg_cluster_size\": cluster_sizes.mean(),\n",
    "            \"max_cluster_size\": cluster_sizes.max(),\n",
    "            \"cluster_size_distribution\": cluster_sizes.value_counts().to_dict()\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dataset de ejemplo\n",
    "data = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'description': [\n",
    "        'Apple iPhone 13 Pro Max 256GB',\n",
    "        'iPhone 13 Pro Max 256GB Apple',\n",
    "        'Samsung Galaxy S21 Ultra',\n",
    "        'Galaxy S21 Ultra by Samsung',\n",
    "        'Sony PlayStation 5 Console',\n",
    "        'PS5 Console by Sony',\n",
    "        'MacBook Pro 14 inch M1',\n",
    "        'Apple MacBook Pro 14\" M1 Chip',\n",
    "        'Nike Air Max Sneakers',\n",
    "        'Dell XPS 13 Laptop'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Deduplicaci\u00f3n\n",
    "generator = EmbeddingGenerator(\"bge\")  # Usar modelo open source\n",
    "deduplicator = FuzzyDeduplicator(generator, threshold=0.90)\n",
    "\n",
    "df_dedup = deduplicator.find_duplicates(data, text_column='description', id_column='id')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nRegistros con duplicados:\")\n",
    "print(df_dedup[df_dedup['is_duplicate']][['id', 'description', 'cluster_id', 'canonical_id', 'is_canonical']])\n",
    "\n",
    "# Reporte\n",
    "report = deduplicator.get_deduplication_report(df_dedup)\n",
    "print(f\"\\n\ud83d\udcca Reporte de Deduplicaci\u00f3n:\")\n",
    "print(f\"  Total registros: {report['total_records']}\")\n",
    "print(f\"  \u00danicos: {report['unique_records']}\")\n",
    "print(f\"  Duplicados: {report['duplicate_records']} ({report['deduplication_rate']:.1%})\")\n",
    "print(f\"  Clusters: {report['n_clusters']}\")\n",
    "print(f\"  Tama\u00f1o promedio de cluster: {report['avg_cluster_size']:.1f}\")\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf 2. Clustering Sem\u00e1ntico de Datasets\n",
    "\n",
    "Agrupar tablas/pipelines por dominio de negocio sin etiquetas manuales.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DatasetClusterer:\n",
    "    \"\"\"Agrupa datasets por similitud sem\u00e1ntica.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "    \n",
    "    def cluster_kmeans(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        n_clusters: int = None,\n",
    "        auto_k: bool = True\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Clustering con K-Means.\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones de datasets\n",
    "            n_clusters: n\u00famero de clusters (auto-detecta si None)\n",
    "            auto_k: usar elbow method para encontrar k \u00f3ptimo\n",
    "        \"\"\"\n",
    "        # Generar embeddings\n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        # Auto-detectar k \u00f3ptimo\n",
    "        if auto_k and n_clusters is None:\n",
    "            n_clusters = self._find_optimal_k(embeddings)\n",
    "            print(f\"\u2713 K \u00f3ptimo detectado: {n_clusters}\")\n",
    "        elif n_clusters is None:\n",
    "            n_clusters = 5  # Default\n",
    "        \n",
    "        # K-Means\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Calcular m\u00e9tricas\n",
    "        from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "        metrics = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"silhouette_score\": silhouette_score(embeddings, labels),  # [-1, 1], >0.5 bueno\n",
    "            \"calinski_harabasz\": calinski_harabasz_score(embeddings, labels),  # >1000 bueno\n",
    "            \"cluster_sizes\": np.bincount(labels).tolist()\n",
    "        }\n",
    "        \n",
    "        # Encontrar t\u00e9rminos representativos por cluster\n",
    "        cluster_terms = self._extract_cluster_terms(descriptions, labels, n_clusters)\n",
    "        metrics[\"cluster_terms\"] = cluster_terms\n",
    "        \n",
    "        return labels, metrics\n",
    "    \n",
    "    def cluster_dbscan(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        eps: float = 0.3,\n",
    "        min_samples: int = 2\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Clustering con DBSCAN (density-based, no requiere k).\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones\n",
    "            eps: distancia m\u00e1xima para considerar vecinos (0.1-0.5)\n",
    "            min_samples: m\u00ednimo de vecinos para formar cluster\n",
    "        \"\"\"\n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        # DBSCAN con distancia coseno\n",
    "        from sklearn.metrics.pairwise import cosine_distances\n",
    "        distance_matrix = cosine_distances(embeddings)\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "        labels = dbscan.fit_predict(distance_matrix)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        metrics = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"n_noise\": n_noise,  # Puntos no asignados a ning\u00fan cluster\n",
    "            \"cluster_sizes\": {\n",
    "                int(label): int(count) \n",
    "                for label, count in zip(*np.unique(labels, return_counts=True))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return labels, metrics\n",
    "    \n",
    "    def _find_optimal_k(self, embeddings: np.ndarray, k_range: range = range(2, 11)) -> int:\n",
    "        \"\"\"Encuentra k \u00f3ptimo usando elbow method.\"\"\"\n",
    "        inertias = []\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(embeddings)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Encontrar \"codo\" (mayor cambio en pendiente)\n",
    "        diffs = np.diff(inertias)\n",
    "        diff_ratios = diffs[:-1] / diffs[1:]\n",
    "        optimal_k = k_range[np.argmax(diff_ratios) + 1]\n",
    "        \n",
    "        return optimal_k\n",
    "    \n",
    "    def _extract_cluster_terms(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        labels: np.ndarray, \n",
    "        n_clusters: int\n",
    "    ) -> Dict[int, List[str]]:\n",
    "        \"\"\"Extrae t\u00e9rminos m\u00e1s frecuentes por cluster.\"\"\"\n",
    "        from collections import Counter\n",
    "        import re\n",
    "        \n",
    "        cluster_terms = {}\n",
    "        for cluster_id in range(n_clusters):\n",
    "            # Obtener descripciones del cluster\n",
    "            cluster_docs = [\n",
    "                desc for desc, label in zip(descriptions, labels) \n",
    "                if label == cluster_id\n",
    "            ]\n",
    "            \n",
    "            # Tokenizar y contar t\u00e9rminos\n",
    "            all_words = []\n",
    "            for doc in cluster_docs:\n",
    "                words = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "                all_words.extend([w for w in words if len(w) > 3])  # Filtrar palabras cortas\n",
    "            \n",
    "            # Top-5 t\u00e9rminos\n",
    "            counter = Counter(all_words)\n",
    "            cluster_terms[cluster_id] = [word for word, count in counter.most_common(5)]\n",
    "        \n",
    "        return cluster_terms\n",
    "    \n",
    "    def visualize_clusters(\n",
    "        self, \n",
    "        embeddings: np.ndarray, \n",
    "        labels: np.ndarray, \n",
    "        descriptions: List[str]\n",
    "    ):\n",
    "        \"\"\"Visualiza clusters en 2D con PCA.\"\"\"\n",
    "        # Reducir a 2D\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        coords_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(\n",
    "            coords_2d[:, 0], \n",
    "            coords_2d[:, 1], \n",
    "            c=labels, \n",
    "            cmap='tab10', \n",
    "            alpha=0.6,\n",
    "            s=100\n",
    "        )\n",
    "        \n",
    "        # Anotar algunos puntos\n",
    "        for i in range(min(15, len(descriptions))):\n",
    "            plt.annotate(\n",
    "                descriptions[i][:40], \n",
    "                (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "                fontsize=8,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        plt.colorbar(scatter, label='Cluster ID')\n",
    "        plt.title('Clustering de Datasets por Similitud Sem\u00e1ntica')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Ejemplo: clustering de cat\u00e1logo de datos\n",
    "catalog_descriptions = [\n",
    "    \"Transacciones de venta diarias con monto y producto\",\n",
    "    \"Data de ventas por d\u00eda con ingresos totales\",\n",
    "    \"Informaci\u00f3n demogr\u00e1fica de clientes: edad, ciudad, segmento\",\n",
    "    \"Datos de clientes con perfil demogr\u00e1fico completo\",\n",
    "    \"Inventario de productos por almac\u00e9n con stock disponible\",\n",
    "    \"Stock de productos en todos los almacenes\",\n",
    "    \"M\u00e9tricas de engagement de usuarios en plataforma\",\n",
    "    \"Actividad de usuarios: clicks, sesiones, tiempo en sitio\",\n",
    "    \"Campa\u00f1as de marketing con presupuesto y ROI\",\n",
    "    \"Data de campa\u00f1as publicitarias y performance\"\n",
    "]\n",
    "\n",
    "generator = EmbeddingGenerator(\"sbert\")\n",
    "clusterer = DatasetClusterer(generator)\n",
    "\n",
    "labels, metrics = clusterer.cluster_kmeans(catalog_descriptions, auto_k=True)\n",
    "\n",
    "print(f\"\ud83d\udcca Resultados de Clustering:\")\n",
    "print(f\"  Clusters: {metrics['n_clusters']}\")\n",
    "print(f\"  Silhouette Score: {metrics['silhouette_score']:.3f} (>0.5 es bueno)\")\n",
    "print(f\"  Tama\u00f1os: {metrics['cluster_sizes']}\")\n",
    "print(f\"\\n  T\u00e9rminos por cluster:\")\n",
    "for cluster_id, terms in metrics['cluster_terms'].items():\n",
    "    print(f\"    Cluster {cluster_id}: {', '.join(terms)}\")\n",
    "```\n",
    "\n",
    "### \ud83c\udf81 3. Sistema de Recomendaci\u00f3n de Datasets\n",
    "\n",
    "```python\n",
    "class DatasetRecommender:\n",
    "    \"\"\"Recomienda datasets similares para analistas.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "        self.catalog_embeddings = None\n",
    "        self.catalog_metadata = None\n",
    "    \n",
    "    def index_catalog(self, catalog: pd.DataFrame, text_column: str):\n",
    "        \"\"\"Indexa cat\u00e1logo de datasets.\"\"\"\n",
    "        descriptions = catalog[text_column].tolist()\n",
    "        self.catalog_embeddings = self.generator.embed(descriptions)\n",
    "        self.catalog_metadata = catalog.to_dict('records')\n",
    "        print(f\"\u2713 Cat\u00e1logo indexado: {len(catalog)} datasets\")\n",
    "    \n",
    "    def recommend_similar(\n",
    "        self, \n",
    "        query: str, \n",
    "        top_k: int = 5,\n",
    "        filters: Dict = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recomienda datasets similares a la query.\n",
    "        \n",
    "        Args:\n",
    "            query: descripci\u00f3n de lo que busca el usuario\n",
    "            top_k: n\u00famero de recomendaciones\n",
    "            filters: filtros de metadatos {\"owner\": \"analytics\", \"type\": \"table\"}\n",
    "        \"\"\"\n",
    "        # Embedding de query\n",
    "        query_emb = self.generator.embed_single(query)\n",
    "        \n",
    "        # Aplicar filtros de metadatos\n",
    "        if filters:\n",
    "            mask = np.ones(len(self.catalog_metadata), dtype=bool)\n",
    "            for key, value in filters.items():\n",
    "                mask &= np.array([meta.get(key) == value for meta in self.catalog_metadata])\n",
    "            \n",
    "            filtered_embeddings = self.catalog_embeddings[mask]\n",
    "            filtered_metadata = [m for m, include in zip(self.catalog_metadata, mask) if include]\n",
    "        else:\n",
    "            filtered_embeddings = self.catalog_embeddings\n",
    "            filtered_metadata = self.catalog_metadata\n",
    "        \n",
    "        # Calcular similitudes\n",
    "        similarities = cosine_similarity([query_emb], filtered_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        # Construir resultados\n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            rec = filtered_metadata[idx].copy()\n",
    "            rec['similarity'] = float(similarities[idx])\n",
    "            rec['relevance'] = \"Alta\" if similarities[idx] > 0.8 else \"Media\" if similarities[idx] > 0.6 else \"Baja\"\n",
    "            recommendations.append(rec)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_collaborative(\n",
    "        self, \n",
    "        user_history: List[str], \n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recomendaciones colaborativas basadas en historial del usuario.\n",
    "        \n",
    "        Args:\n",
    "            user_history: lista de dataset_ids que el usuario ha usado\n",
    "        \"\"\"\n",
    "        # Embeddings de datasets usados\n",
    "        used_indices = [\n",
    "            i for i, meta in enumerate(self.catalog_metadata)\n",
    "            if meta.get('dataset_id') in user_history\n",
    "        ]\n",
    "        used_embeddings = self.catalog_embeddings[used_indices]\n",
    "        \n",
    "        # Centroid del perfil del usuario\n",
    "        user_profile = used_embeddings.mean(axis=0)\n",
    "        \n",
    "        # Recomendar datasets similares al perfil (excluyendo ya usados)\n",
    "        available_mask = np.array([\n",
    "            meta.get('dataset_id') not in user_history \n",
    "            for meta in self.catalog_metadata\n",
    "        ])\n",
    "        \n",
    "        available_embeddings = self.catalog_embeddings[available_mask]\n",
    "        available_metadata = [m for m, avail in zip(self.catalog_metadata, available_mask) if avail]\n",
    "        \n",
    "        similarities = cosine_similarity([user_profile], available_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            rec = available_metadata[idx].copy()\n",
    "            rec['similarity'] = float(similarities[idx])\n",
    "            recommendations.append(rec)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Ejemplo: recomendaciones\n",
    "catalog = pd.DataFrame({\n",
    "    'dataset_id': ['dwh.ventas', 'dwh.clientes', 'dwh.productos', 'analytics.revenue', 'analytics.engagement'],\n",
    "    'description': [\n",
    "        'Transacciones de venta con fecha, monto, producto',\n",
    "        'Datos demogr\u00e1ficos de clientes: edad, ciudad, segmento',\n",
    "        'Cat\u00e1logo de productos con precios y categor\u00edas',\n",
    "        'Ingresos mensuales agregados por regi\u00f3n',\n",
    "        'M\u00e9tricas de engagement de usuarios en plataforma'\n",
    "    ],\n",
    "    'owner': ['data-eng', 'data-eng', 'data-eng', 'analytics', 'analytics'],\n",
    "    'type': ['table', 'table', 'table', 'view', 'view']\n",
    "})\n",
    "\n",
    "recommender = DatasetRecommender(generator)\n",
    "recommender.index_catalog(catalog, text_column='description')\n",
    "\n",
    "# B\u00fasqueda sem\u00e1ntica\n",
    "print(\"\ud83d\udd0d Buscar: 'necesito datos de ventas mensuales'\")\n",
    "results = recommender.recommend_similar('necesito datos de ventas mensuales', top_k=3)\n",
    "for i, rec in enumerate(results, 1):\n",
    "    print(f\"{i}. {rec['dataset_id']} (sim={rec['similarity']:.3f}, {rec['relevance']})\")\n",
    "    print(f\"   {rec['description']}\")\n",
    "\n",
    "# Recomendaciones colaborativas\n",
    "print(\"\\n\ud83c\udf81 Recomendaciones basadas en historial: ['dwh.ventas']\")\n",
    "collab_recs = recommender.recommend_collaborative(['dwh.ventas'], top_k=3)\n",
    "for i, rec in enumerate(collab_recs, 1):\n",
    "    print(f\"{i}. {rec['dataset_id']} (sim={rec['similarity']:.3f})\")\n",
    "```\n",
    "\n",
    "### \ud83d\udcca 4. Detecci\u00f3n de Anomal\u00edas en Metadatos\n",
    "\n",
    "Identificar tablas \"raras\" que no encajan con el resto del cat\u00e1logo.\n",
    "\n",
    "```python\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Detecta datasets an\u00f3malos por similitud sem\u00e1ntica.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "    \n",
    "    def detect_anomalies(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        contamination: float = 0.05\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detecta descripciones an\u00f3malas usando Isolation Forest.\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones\n",
    "            contamination: proporci\u00f3n esperada de anomal\u00edas (0.01-0.10)\n",
    "        \n",
    "        Returns:\n",
    "            labels: 1=normal, -1=anomal\u00eda\n",
    "            scores: scores de anomal\u00eda (m\u00e1s negativo = m\u00e1s an\u00f3malo)\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        \n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "        labels = iso_forest.fit_predict(embeddings)\n",
    "        scores = iso_forest.score_samples(embeddings)\n",
    "        \n",
    "        n_anomalies = (labels == -1).sum()\n",
    "        print(f\"\u2713 Anomal\u00edas detectadas: {n_anomalies}/{len(descriptions)} ({n_anomalies/len(descriptions):.1%})\")\n",
    "        \n",
    "        return labels, scores\n",
    "\n",
    "# Ejemplo\n",
    "descriptions_with_anomaly = catalog_descriptions + [\n",
    "    \"Configuraci\u00f3n de base de datos MySQL 5.7\",  # Anomal\u00eda: no es un dataset\n",
    "    \"Logs de errores de aplicaci\u00f3n Python\"      # Anomal\u00eda: logs t\u00e9cnicos\n",
    "]\n",
    "\n",
    "detector = AnomalyDetector(generator)\n",
    "labels, scores = detector.detect_anomalies(descriptions_with_anomaly, contamination=0.10)\n",
    "\n",
    "print(\"\\n\ud83d\udea8 Descripciones an\u00f3malas:\")\n",
    "for i, (desc, label, score) in enumerate(zip(descriptions_with_anomaly, labels, scores)):\n",
    "    if label == -1:\n",
    "        print(f\"  [{score:.3f}] {desc}\")\n",
    "```\n",
    "\n",
    "### \ud83d\ude80 M\u00e9tricas de \u00c9xito en Producci\u00f3n\n",
    "\n",
    "| Aplicaci\u00f3n | M\u00e9trica | Target | Medici\u00f3n |\n",
    "|------------|---------|--------|----------|\n",
    "| **Deduplicaci\u00f3n** | Precision | >95% | Manual review de 100 pares |\n",
    "| **Deduplicaci\u00f3n** | Recall | >90% | Cu\u00e1ntos duplicados reales se detectaron |\n",
    "| **Clustering** | Silhouette Score | >0.5 | sklearn.metrics |\n",
    "| **Recomendaciones** | Click-through Rate | >20% | % de recomendaciones clickeadas |\n",
    "| **Recomendaciones** | Relevance@5 | >0.80 | Ratings de usuarios (1-5) |\n",
    "| **Anomal\u00edas** | False Positive Rate | <10% | Manual review de anomal\u00edas |\n",
    "\n",
    "### \ud83d\udca1 Mejores Pr\u00e1cticas en Producci\u00f3n\n",
    "\n",
    "1. **Deduplicaci\u00f3n**: Combinar embeddings + reglas (ej. mismo SKU \u2192 100% duplicado)\n",
    "2. **Clustering**: Re-entrenar peri\u00f3dicamente al agregar nuevos datasets\n",
    "3. **Recomendaciones**: Combinar similitud sem\u00e1ntica + popularidad + recency\n",
    "4. **Threshold tuning**: Usar validation set, no adivinar thresholds\n",
    "5. **Monitoreo**: Loggear similitudes promedio por d\u00eda (detectar drift)\n",
    "6. **A/B testing**: Comparar embeddings vs reglas en m\u00e9tricas de negocio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7fd36",
   "metadata": {},
   "source": [
    "## 1. Generar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(\n",
    "        model='text-embedding-ada-002',\n",
    "        input=text\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding)\n",
    "\n",
    "textos = [\n",
    "    'Tabla de ventas con transacciones diarias',\n",
    "    'Data de transacciones de venta por d\u00eda',  # Similar\n",
    "    'Cat\u00e1logo de productos con precios',\n",
    "    'Informaci\u00f3n demogr\u00e1fica de clientes'\n",
    "]\n",
    "\n",
    "embeddings = [embed(t) for t in textos]\n",
    "print(f'Embeddings generados: {len(embeddings)} vectores de dimensi\u00f3n {len(embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1b5e1",
   "metadata": {},
   "source": [
    "## 2. Similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Matriz de similitud\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print('Matriz de similitud:\\n')\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f'{i}. {texto}')\n",
    "\n",
    "print('\\n', sim_matrix.round(3))\n",
    "print(f'\\n\u27a1\ufe0f Textos 0 y 1 tienen similitud: {sim_matrix[0][1]:.3f} (duplicados potenciales)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1635129",
   "metadata": {},
   "source": [
    "## 3. B\u00fasqueda sem\u00e1ntica en cat\u00e1logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e73834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cat\u00e1logo de data assets\n",
    "catalogo = pd.DataFrame([\n",
    "    {'asset': 'dwh.ventas', 'desc': 'Transacciones de venta con fecha, monto, producto'},\n",
    "    {'asset': 'dwh.clientes', 'desc': 'Datos demogr\u00e1ficos de clientes: edad, ciudad, segmento'},\n",
    "    {'asset': 'dwh.productos', 'desc': 'Cat\u00e1logo de productos con precios y categor\u00edas'},\n",
    "    {'asset': 'dwh.inventario', 'desc': 'Stock disponible por almac\u00e9n y SKU'},\n",
    "    {'asset': 'analytics.revenue_monthly', 'desc': 'Ingresos mensuales agregados por regi\u00f3n'}\n",
    "])\n",
    "\n",
    "# Embeddings del cat\u00e1logo\n",
    "catalogo['embedding'] = catalogo['desc'].apply(lambda x: embed(x))\n",
    "\n",
    "def search_catalog(query: str, top_k: int = 3) -> pd.DataFrame:\n",
    "    query_emb = embed(query)\n",
    "    catalogo['similarity'] = catalogo['embedding'].apply(\n",
    "        lambda x: cosine_similarity([query_emb], [x])[0][0]\n",
    "    )\n",
    "    return catalogo.nlargest(top_k, 'similarity')[['asset', 'desc', 'similarity']]\n",
    "\n",
    "print(search_catalog('\u00bfD\u00f3nde encuentro informaci\u00f3n de productos?'))\n",
    "print('\\n')\n",
    "print(search_catalog('Necesito datos de ventas mensuales'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed677f",
   "metadata": {},
   "source": [
    "## 4. Detecci\u00f3n de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset con posibles duplicados\n",
    "descripciones = [\n",
    "    'Apple iPhone 13 Pro Max 256GB',\n",
    "    'iPhone 13 Pro Max 256GB Apple',\n",
    "    'Samsung Galaxy S21 Ultra',\n",
    "    'Galaxy S21 Ultra by Samsung',\n",
    "    'Sony PlayStation 5 Console'\n",
    "]\n",
    "\n",
    "embs = [embed(d) for d in descripciones]\n",
    "threshold = 0.95\n",
    "\n",
    "duplicates = []\n",
    "for i in range(len(embs)):\n",
    "    for j in range(i+1, len(embs)):\n",
    "        sim = cosine_similarity([embs[i]], [embs[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            duplicates.append((i, j, sim))\n",
    "\n",
    "print('Duplicados detectados:\\n')\n",
    "for i, j, sim in duplicates:\n",
    "    print(f'- \"{descripciones[i]}\" \u2248 \"{descripciones[j]}\" (sim={sim:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50e87c",
   "metadata": {},
   "source": [
    "## 5. Clustering con K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "productos = [\n",
    "    'Laptop Dell XPS 13',\n",
    "    'MacBook Pro 14 inch',\n",
    "    'HP Pavilion Laptop',\n",
    "    'Nike Air Max Sneakers',\n",
    "    'Adidas Ultraboost Shoes',\n",
    "    'Puma Running Shoes',\n",
    "    'Organic Green Tea 100 bags',\n",
    "    'Colombian Coffee Beans 1kg'\n",
    "]\n",
    "\n",
    "prod_embs = np.array([embed(p) for p in productos])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(prod_embs)\n",
    "\n",
    "df_clusters = pd.DataFrame({'producto': productos, 'cluster': clusters})\n",
    "print(df_clusters.sort_values('cluster'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2c629",
   "metadata": {},
   "source": [
    "## 6. FAISS para b\u00fasqueda r\u00e1pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d943a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu\n",
    "import faiss\n",
    "\n",
    "# Crear \u00edndice\n",
    "dimension = prod_embs.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(prod_embs.astype('float32'))\n",
    "\n",
    "# B\u00fasqueda\n",
    "query = 'zapatos deportivos'\n",
    "query_emb = embed(query).reshape(1, -1).astype('float32')\n",
    "\n",
    "k = 3\n",
    "distances, indices = index.search(query_emb, k)\n",
    "\n",
    "print(f'Top {k} productos para \"{query}\":\\n')\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f'{i+1}. {productos[idx]} (distancia={distances[0][i]:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae539a8",
   "metadata": {},
   "source": [
    "## 7. Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_similar(producto: str, catalogo: list, top_k: int = 3) -> list:\n",
    "    \"\"\"Recomienda productos similares.\"\"\"\n",
    "    query_emb = embed(producto)\n",
    "    catalogo_embs = [embed(p) for p in catalogo]\n",
    "    \n",
    "    similarities = [\n",
    "        (p, cosine_similarity([query_emb], [e])[0][0])\n",
    "        for p, e in zip(catalogo, catalogo_embs)\n",
    "        if p != producto\n",
    "    ]\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "comprado = 'Laptop Dell XPS 13'\n",
    "recomendaciones = recomendar_similar(comprado, productos)\n",
    "\n",
    "print(f'Cliente compr\u00f3: {comprado}')\n",
    "print('Recomendaciones:\\n')\n",
    "for prod, sim in recomendaciones:\n",
    "    print(f'- {prod} (similitud={sim:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5799c",
   "metadata": {},
   "source": [
    "## 8. Buenas pr\u00e1cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404e0ec",
   "metadata": {},
   "source": [
    "- **Cache embeddings**: generarlos es costoso, almacena en DB.\n",
    "- **Batch processing**: genera embeddings en lotes para eficiencia.\n",
    "- **Normalizaci\u00f3n**: limpia texto antes de generar embeddings.\n",
    "- **Modelos locales**: considera Sentence Transformers para evitar API calls.\n",
    "- **Threshold din\u00e1mico**: ajusta seg\u00fan caso de uso (duplicados vs b\u00fasqueda).\n",
    "- **Monitoreo de costos**: embeddings consumen tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74deae01",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c26d3",
   "metadata": {},
   "source": [
    "1. Construye un deduplicador de registros usando embeddings.\n",
    "2. Implementa b\u00fasqueda h\u00edbrida (keyword + sem\u00e1ntica) en tu cat\u00e1logo.\n",
    "3. Crea clusters de clientes basados en descripciones de comportamiento.\n",
    "4. Desarrolla un sistema de recomendaci\u00f3n de datasets para analistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n \u2192](06_agentes_automatizacion.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel GenAI:**\n",
    "- [\ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)\n",
    "- [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "- [\ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural](02_generacion_sql_nl2sql.ipynb)\n",
    "- [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs](03_generacion_codigo_etl.ipynb)\n",
    "- [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "- [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n](06_agentes_automatizacion.ipynb)\n",
    "- [\u2705 Validaci\u00f3n de Datos con LLMs](07_calidad_validacion_llm.ipynb)\n",
    "- [\ud83c\udfb2 Generaci\u00f3n de Datos Sint\u00e9ticos con LLMs](08_sintesis_aumento_datos.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
