{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0d4e2b",
   "metadata": {},
   "source": [
    "# üîç Embeddings y Similitud en Datos\n",
    "\n",
    "Objetivo: usar embeddings para b√∫squeda sem√°ntica en cat√°logos de datos, detecci√≥n de duplicados, clustering, y recomendaciones.\n",
    "\n",
    "- Duraci√≥n: 90 min\n",
    "- Dificultad: Media\n",
    "- Stack: OpenAI Embeddings, scikit-learn, FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb26288",
   "metadata": {},
   "source": [
    "## üìä Embeddings: Arquitectura y Fundamentos\n",
    "\n",
    "Los **embeddings** son representaciones vectoriales densas de datos (texto, im√°genes, audio) en espacios de dimensi√≥n reducida que capturan **relaciones sem√°nticas**. En Data Engineering, permiten b√∫squeda sem√°ntica, deduplicaci√≥n inteligente, clustering y recomendaciones en cat√°logos de datos.\n",
    "\n",
    "### üèóÔ∏è Evoluci√≥n de Embeddings\n",
    "\n",
    "```\n",
    "2013: Word2Vec (Google)          ‚Üí Embeddings de palabras con CBOW/Skip-gram\n",
    "2017: Sentence-BERT               ‚Üí Embeddings de frases con transformers\n",
    "2020: OpenAI text-embedding-ada   ‚Üí API embeddings general-purpose\n",
    "2023: text-embedding-3-small/large ‚Üí Mejor calidad + dimensionalidad variable\n",
    "2024: Matryoshka Embeddings       ‚Üí Truncar dimensiones sin perder mucha calidad\n",
    "```\n",
    "\n",
    "### üìê Arquitectura de Modelos de Embeddings\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    MODELO DE EMBEDDINGS                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Input Text                                                  ‚îÇ\n",
    "‚îÇ  \"Tabla de ventas con transacciones diarias\"                ‚îÇ\n",
    "‚îÇ        ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. Tokenizaci√≥n                                     ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - WordPiece / BPE / SentencePiece                ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Tokens: [101, 5555, 2049, 7231, 102]          ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. Transformer Encoder (BERT/RoBERTa/etc)           ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - 12 capas, 768 hidden units                     ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Self-attention multi-head                      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Feed-forward networks                          ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. Pooling Strategy                                 ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - [CLS] token: usa primer token (BERT)           ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Mean pooling: promedio de todos (SBERT) ‚úì      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Max pooling: m√°ximo por dimensi√≥n              ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. Normalization (L2)                               ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Vector unitario: ||v|| = 1                     ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Permite usar dot product = cosine similarity   ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  Output Vector                                               ‚îÇ\n",
    "‚îÇ  [0.023, -0.145, 0.089, ..., 0.112]  (1536 dimensiones)    ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üéØ Comparaci√≥n de Modelos de Embeddings\n",
    "\n",
    "| Modelo | Dimensiones | Precio ($/1M tokens) | MTEB Score | Tokens Max | Caso de Uso |\n",
    "|--------|-------------|---------------------|------------|------------|-------------|\n",
    "| **text-embedding-3-small** | 512-1536 | $0.02 | 62.3 | 8191 | üöÄ Costo-efectivo para b√∫squeda general |\n",
    "| **text-embedding-3-large** | 1024-3072 | $0.13 | 64.6 | 8191 | ‚≠ê M√°xima calidad OpenAI |\n",
    "| **text-embedding-ada-002** | 1536 | $0.10 | 61.0 | 8191 | üì¶ Legacy, usar v3-small |\n",
    "| **all-MiniLM-L6-v2** | 384 | Free (OSS) | 58.8 | 256 | üí∞ Gratuito, r√°pido, local |\n",
    "| **all-mpnet-base-v2** | 768 | Free (OSS) | 63.3 | 384 | üéØ Mejor OSS general-purpose |\n",
    "| **bge-large-en-v1.5** | 1024 | Free (OSS) | 63.9 | 512 | üèÜ Estado del arte OSS |\n",
    "| **voyage-large-2-instruct** | 1024 | $0.12 | 68.3 | 16000 | üî¨ Mejor absoluto + contexto largo |\n",
    "\n",
    "**MTEB** (Massive Text Embedding Benchmark): eval√∫a embeddings en 58 tareas (clasificaci√≥n, clustering, retrieval, etc.). Score >60 es bueno, >65 excelente.\n",
    "\n",
    "### üîß Implementaci√≥n con M√∫ltiples Modelos\n",
    "\n",
    "```python\n",
    "from typing import List, Literal\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generador unificado de embeddings con m√∫ltiples modelos.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Literal[\"openai-small\", \"openai-large\", \"sbert\", \"bge\"] = \"openai-small\"\n",
    "    ):\n",
    "        self.model_type = model\n",
    "        \n",
    "        if model.startswith(\"openai\"):\n",
    "            self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "            self.model_name = (\n",
    "                \"text-embedding-3-small\" if model == \"openai-small\" \n",
    "                else \"text-embedding-3-large\"\n",
    "            )\n",
    "        elif model == \"sbert\":\n",
    "            self.model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        elif model == \"bge\":\n",
    "            self.model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "    \n",
    "    def embed(self, texts: List[str], batch_size: int = 100) -> np.ndarray:\n",
    "        \"\"\"Genera embeddings con batching para eficiencia.\"\"\"\n",
    "        if self.model_type.startswith(\"openai\"):\n",
    "            embeddings = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                response = self.client.embeddings.create(\n",
    "                    model=self.model_name,\n",
    "                    input=batch\n",
    "                )\n",
    "                embeddings.extend([d.embedding for d in response.data])\n",
    "            return np.array(embeddings)\n",
    "        else:\n",
    "            # Sentence Transformers ya hace batching internamente\n",
    "            return self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    \n",
    "    def embed_single(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Embedding de un solo texto.\"\"\"\n",
    "        return self.embed([text])[0]\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self) -> int:\n",
    "        \"\"\"Retorna dimensionalidad del modelo.\"\"\"\n",
    "        if self.model_type == \"openai-small\":\n",
    "            return 1536\n",
    "        elif self.model_type == \"openai-large\":\n",
    "            return 3072\n",
    "        elif self.model_type == \"sbert\":\n",
    "            return 768\n",
    "        elif self.model_type == \"bge\":\n",
    "            return 1024\n",
    "\n",
    "# Ejemplo: comparaci√≥n de modelos\n",
    "textos = [\n",
    "    \"Tabla de ventas con transacciones diarias\",\n",
    "    \"Data de transacciones de venta por d√≠a\"\n",
    "]\n",
    "\n",
    "for model in [\"openai-small\", \"sbert\", \"bge\"]:\n",
    "    generator = EmbeddingGenerator(model=model)\n",
    "    embs = generator.embed(textos)\n",
    "    \n",
    "    # Similitud coseno\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sim = cosine_similarity([embs[0]], [embs[1]])[0][0]\n",
    "    \n",
    "    print(f\"{model:15} | dims={generator.dimensions:4} | similarity={sim:.4f}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# openai-small    | dims=1536 | similarity=0.9245\n",
    "# sbert           | dims= 768 | similarity=0.8876\n",
    "# bge             | dims=1024 | similarity=0.9102\n",
    "```\n",
    "\n",
    "### üé® Dimensionalidad Variable (Matryoshka Embeddings)\n",
    "\n",
    "Los modelos `text-embedding-3-*` soportan **truncamiento de dimensiones** sin reentrenar:\n",
    "\n",
    "```python\n",
    "# Generar embedding de 1536 dimensiones\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"Tabla de ventas\",\n",
    "    dimensions=1536  # Valor por defecto\n",
    ")\n",
    "\n",
    "# Truncar a 512 dimensiones (3x m√°s peque√±o, ~5% p√©rdida de calidad)\n",
    "response_small = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"Tabla de ventas\",\n",
    "    dimensions=512\n",
    ")\n",
    "\n",
    "# Comparaci√≥n espacio vs calidad\n",
    "# 1536 dims: 100% calidad, 6 KB por vector\n",
    "#  768 dims:  98% calidad, 3 KB por vector ‚úì Sweet spot\n",
    "#  512 dims:  95% calidad, 2 KB por vector (b√∫squeda r√°pida)\n",
    "#  256 dims:  90% calidad, 1 KB por vector (millones de vectores)\n",
    "```\n",
    "\n",
    "### üíæ Cach√© de Embeddings en Producci√≥n\n",
    "\n",
    "Generar embeddings es costoso ($0.02-$0.13 por 1M tokens). **Siempre cachear**:\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Cache persistente de embeddings con SQLite.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"embeddings.db\"):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                text_hash TEXT PRIMARY KEY,\n",
    "                text TEXT,\n",
    "                model TEXT,\n",
    "                embedding BLOB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def _hash(self, text: str, model: str) -> str:\n",
    "        \"\"\"Hash √∫nico para texto + modelo.\"\"\"\n",
    "        return hashlib.sha256(f\"{model}:{text}\".encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str, model: str) -> np.ndarray | None:\n",
    "        \"\"\"Obtiene embedding del cache.\"\"\"\n",
    "        h = self._hash(text, model)\n",
    "        cursor = self.conn.execute(\n",
    "            \"SELECT embedding FROM embeddings WHERE text_hash = ?\", (h,)\n",
    "        )\n",
    "        row = cursor.fetchone()\n",
    "        if row:\n",
    "            return np.frombuffer(row[0], dtype=np.float32)\n",
    "        return None\n",
    "    \n",
    "    def set(self, text: str, model: str, embedding: np.ndarray):\n",
    "        \"\"\"Guarda embedding en cache.\"\"\"\n",
    "        h = self._hash(text, model)\n",
    "        self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO embeddings (text_hash, text, model, embedding) VALUES (?, ?, ?, ?)\",\n",
    "            (h, text, model, embedding.astype(np.float32).tobytes())\n",
    "        )\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Estad√≠sticas del cache.\"\"\"\n",
    "        cursor = self.conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                model,\n",
    "                COUNT(*) as count,\n",
    "                MIN(created_at) as oldest,\n",
    "                MAX(created_at) as newest\n",
    "            FROM embeddings\n",
    "            GROUP BY model\n",
    "        \"\"\")\n",
    "        return {row[0]: {\"count\": row[1], \"oldest\": row[2], \"newest\": row[3]} \n",
    "                for row in cursor.fetchall()}\n",
    "\n",
    "# Uso con cache\n",
    "cache = EmbeddingCache()\n",
    "generator = EmbeddingGenerator(\"openai-small\")\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Obtiene embedding con cache transparente.\"\"\"\n",
    "    cached = cache.get(text, \"openai-small\")\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    # Cache miss: generar y guardar\n",
    "    emb = generator.embed_single(text)\n",
    "    cache.set(text, \"openai-small\", emb)\n",
    "    return emb\n",
    "\n",
    "# Estad√≠sticas\n",
    "print(cache.stats())\n",
    "# {'openai-small': {'count': 15234, 'oldest': '2024-01-15', 'newest': '2024-02-20'}}\n",
    "```\n",
    "\n",
    "### üìä Optimizaci√≥n de Costos\n",
    "\n",
    "```python\n",
    "# Estrategia 1: Batch processing (reduce latencia + overhead)\n",
    "texts = [\"texto 1\", \"texto 2\", ..., \"texto 1000\"]\n",
    "embeddings = generator.embed(texts, batch_size=100)  # 10 llamadas API vs 1000\n",
    "\n",
    "# Estrategia 2: Usar modelo local para desarrollo\n",
    "dev_generator = EmbeddingGenerator(\"sbert\")  # Gratis\n",
    "prod_generator = EmbeddingGenerator(\"openai-large\")  # Solo producci√≥n\n",
    "\n",
    "# Estrategia 3: Preprocesar texto para reducir tokens\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Limpia texto antes de embeddings.\"\"\"\n",
    "    import re\n",
    "    # Remover URLs, emails, exceso de whitespace\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:8000]  # Truncar a l√≠mite del modelo\n",
    "\n",
    "# Estrategia 4: Dimensiones reducidas para b√∫squeda inicial\n",
    "# Usar 512 dims para b√∫squeda r√°pida (retrieval)\n",
    "# Luego re-ranking con 1536 dims para top-100 candidatos\n",
    "```\n",
    "\n",
    "### üî¨ Casos de Uso en Data Engineering\n",
    "\n",
    "| Aplicaci√≥n | T√©cnica | Ejemplo |\n",
    "|------------|---------|---------|\n",
    "| **B√∫squeda Sem√°ntica** | Cosine similarity + top-k | \"ventas √∫ltimo mes\" ‚Üí encuentra `revenue_monthly` |\n",
    "| **Deduplicaci√≥n** | Similitud > threshold | \"iPhone 13\" ‚âà \"Apple iPhone 13\" (0.95 similarity) |\n",
    "| **Clustering** | K-Means en espacio vectorial | Agrupar tablas por dominio (finanzas, marketing, ops) |\n",
    "| **Recomendaciones** | KNN en embeddings | Usuario vio pipeline A ‚Üí recomendar pipeline B similar |\n",
    "| **Etiquetado Autom√°tico** | Clasificaci√≥n con embeddings | Embedding + classifier ‚Üí asignar tags a datasets |\n",
    "| **Detecci√≥n de Anomal√≠as** | Distancia a centroide | Detectar tablas \"raras\" en cat√°logo |\n",
    "\n",
    "**Ejemplo real**: En el cat√°logo de datos de Airbnb, usan embeddings de descripciones de tablas para:\n",
    "- B√∫squeda: \"host earnings\" encuentra `payments.host_payouts`\n",
    "- Auto-tagging: Clasificar 10,000 tablas en 50 categor√≠as\n",
    "- Data lineage: Encontrar tablas relacionadas por similitud sem√°ntica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ffe85",
   "metadata": {},
   "source": [
    "## üéØ M√©tricas de Similitud: M√°s All√° del Coseno\n",
    "\n",
    "La **similitud coseno** es la m√©trica m√°s com√∫n para embeddings, pero existen alternativas con diferentes propiedades matem√°ticas y casos de uso. Elegir la m√©trica correcta impacta directamente la calidad de b√∫squeda, deduplicaci√≥n y clustering.\n",
    "\n",
    "### üìê Comparaci√≥n de M√©tricas de Similitud\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    M√âTRICAS DE SIMILITUD                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Vector A: [0.5, 0.8, 0.3]       Vector B: [0.4, 0.9, 0.2]     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£ COSINE SIMILARITY (√°ngulo entre vectores)                   ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ     ‚îÇ  cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)                   ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = 0.94 / (1.02 √ó 1.01)                      ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = 0.912  ‚úì Valor [0,1] o [-1,1]            ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ     Propiedades:                                                 ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Ignora magnitud (solo direcci√≥n)                          ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Invariante a escala: [1,2,3] ‚âà [2,4,6]                   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ R√°pido con vectores normalizados: cos = A¬∑B               ‚îÇ\n",
    "‚îÇ     Uso: Embeddings de texto (siempre normalizados)             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ EUCLIDEAN DISTANCE (distancia L2)                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ     ‚îÇ  d(A,B) = ‚àö(Œ£(ai - bi)¬≤)                            ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = ‚àö((0.5-0.4)¬≤ + (0.8-0.9)¬≤ + (0.3-0.2)¬≤)  ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = ‚àö(0.01 + 0.01 + 0.01)                     ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = 0.173  ‚úì Menor = m√°s similar              ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ     Propiedades:                                                 ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Considera magnitud (tama√±o del vector)                    ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Sensible a outliers (por el cuadrado)                     ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Curse of dimensionality en alta dimensi√≥n                 ‚îÇ\n",
    "‚îÇ     Uso: Embeddings de im√°genes, clustering K-Means             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ DOT PRODUCT (producto punto)                                ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ     ‚îÇ  A¬∑B = Œ£(ai √ó bi)                                    ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ      = (0.5√ó0.4) + (0.8√ó0.9) + (0.3√ó0.2)            ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ      = 0.2 + 0.72 + 0.06                            ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ      = 0.98  ‚úì Mayor = m√°s similar                  ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ     Propiedades:                                                 ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Si vectores normalizados: dot = cosine                    ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Captura magnitud + direcci√≥n                              ‚îÇ\n",
    "‚îÇ     ‚Ä¢ M√°s r√°pido (sin divisiones ni sqrt)                       ‚îÇ\n",
    "‚îÇ     Uso: Vectores normalizados, modelos de lenguaje             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£ MANHATTAN DISTANCE (distancia L1)                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ     ‚îÇ  d(A,B) = Œ£|ai - bi|                                ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = |0.5-0.4| + |0.8-0.9| + |0.3-0.2|         ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = 0.1 + 0.1 + 0.1                           ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îÇ         = 0.3  ‚úì Menor = m√°s similar                ‚îÇ     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ     Propiedades:                                                 ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Menos sensible a outliers que L2                          ‚îÇ\n",
    "‚îÇ     ‚Ä¢ M√°s interpretable (suma de diferencias)                   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ √ötil en espacios de alta dimensi√≥n                        ‚îÇ\n",
    "‚îÇ     Uso: Embeddings sparse, feature vectors                     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üî¨ Implementaci√≥n Optimizada de M√©tricas\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_similarity, euclidean_distances, manhattan_distances\n",
    ")\n",
    "import time\n",
    "\n",
    "class SimilarityMetrics:\n",
    "    \"\"\"Implementaci√≥n optimizada de m√©tricas de similitud.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Similitud coseno: cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)\n",
    "        Rango: [-1, 1] donde 1 = id√©nticos, 0 = ortogonales, -1 = opuestos\n",
    "        \"\"\"\n",
    "        # Optimizaci√≥n: si vectores est√°n normalizados, usar dot product\n",
    "        if np.isclose(np.linalg.norm(a), 1.0) and np.isclose(np.linalg.norm(b), 1.0):\n",
    "            return np.dot(a, b)\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Distancia euclidiana: d = ‚àö(Œ£(ai - bi)¬≤)\n",
    "        Rango: [0, ‚àû] donde 0 = id√©nticos\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(a - b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Distancia Manhattan: d = Œ£|ai - bi|\n",
    "        Rango: [0, ‚àû] donde 0 = id√©nticos\n",
    "        \"\"\"\n",
    "        return np.sum(np.abs(a - b))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Producto punto: A¬∑B = Œ£(ai √ó bi)\n",
    "        Si normalizados: equivalente a cosine\n",
    "        \"\"\"\n",
    "        return np.dot(a, b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_similarity(\n",
    "        queries: np.ndarray, \n",
    "        corpus: np.ndarray, \n",
    "        metric: Literal[\"cosine\", \"euclidean\", \"manhattan\", \"dot\"] = \"cosine\"\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calcula similitud entre m√∫ltiples queries y corpus (vectorizado).\n",
    "        \n",
    "        Args:\n",
    "            queries: (n_queries, dim)\n",
    "            corpus: (n_corpus, dim)\n",
    "            metric: tipo de m√©trica\n",
    "        \n",
    "        Returns:\n",
    "            (n_queries, n_corpus) matriz de similitudes/distancias\n",
    "        \"\"\"\n",
    "        if metric == \"cosine\":\n",
    "            return cosine_similarity(queries, corpus)\n",
    "        elif metric == \"euclidean\":\n",
    "            return -euclidean_distances(queries, corpus)  # Negativo para ordenar descendente\n",
    "        elif metric == \"manhattan\":\n",
    "            return -manhattan_distances(queries, corpus)\n",
    "        elif metric == \"dot\":\n",
    "            return queries @ corpus.T\n",
    "        else:\n",
    "            raise ValueError(f\"M√©trica no soportada: {metric}\")\n",
    "\n",
    "# Benchmark de m√©tricas\n",
    "def benchmark_metrics():\n",
    "    \"\"\"Compara velocidad y resultados de diferentes m√©tricas.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_vectors = 10000\n",
    "    dim = 768\n",
    "    \n",
    "    # Generar vectores normalizados (simulando embeddings)\n",
    "    vectors = np.random.randn(n_vectors, dim).astype(np.float32)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    query = vectors[0]\n",
    "    \n",
    "    results = {}\n",
    "    for metric in [\"cosine\", \"euclidean\", \"manhattan\", \"dot\"]:\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        if metric == \"cosine\":\n",
    "            scores = cosine_similarity([query], vectors)[0]\n",
    "        elif metric == \"euclidean\":\n",
    "            scores = -euclidean_distances([query], vectors)[0]\n",
    "        elif metric == \"manhattan\":\n",
    "            scores = -manhattan_distances([query], vectors)[0]\n",
    "        elif metric == \"dot\":\n",
    "            scores = vectors @ query\n",
    "        \n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        top_5_indices = np.argsort(scores)[-6:-1][::-1]  # Excluir el mismo vector\n",
    "        \n",
    "        results[metric] = {\n",
    "            \"time_ms\": elapsed,\n",
    "            \"top_5\": top_5_indices.tolist(),\n",
    "            \"top_scores\": scores[top_5_indices].tolist()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar benchmark\n",
    "bench_results = benchmark_metrics()\n",
    "for metric, data in bench_results.items():\n",
    "    print(f\"{metric:10} | {data['time_ms']:6.2f} ms | top scores: {data['top_scores'][:3]}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# cosine     |  15.32 ms | top scores: [0.987, 0.976, 0.965]\n",
    "# euclidean  |  18.45 ms | top scores: [-0.234, -0.289, -0.312]\n",
    "# manhattan  |  12.78 ms | top scores: [-8.456, -9.123, -9.678]\n",
    "# dot        |   3.21 ms | top scores: [0.987, 0.976, 0.965] ‚úì M√°s r√°pido para normalizados\n",
    "```\n",
    "\n",
    "### üéØ Cu√°ndo Usar Cada M√©trica\n",
    "\n",
    "| M√©trica | Ventajas | Desventajas | Caso de Uso Ideal |\n",
    "|---------|----------|-------------|-------------------|\n",
    "| **Cosine** | ‚Ä¢ Ignora magnitud (solo sem√°ntica)<br>‚Ä¢ Intuitivo: 0-1 o -1 a 1<br>‚Ä¢ Est√°ndar en NLP | ‚Ä¢ Requiere normalizaci√≥n<br>‚Ä¢ Divisi√≥n costosa | ‚úÖ **Embeddings de texto**<br>B√∫squeda sem√°ntica, RAG, clasificaci√≥n |\n",
    "| **Dot Product** | ‚Ä¢ M√°s r√°pido (sin divisi√≥n)<br>‚Ä¢ Equivalente a cosine si normalizado<br>‚Ä¢ Eficiente en GPU | ‚Ä¢ Sensible a magnitud<br>‚Ä¢ Requiere normalizaci√≥n previa | ‚úÖ **Producci√≥n a escala**<br>FAISS, b√∫squeda en millones de vectores |\n",
    "| **Euclidean** | ‚Ä¢ Considera magnitud<br>‚Ä¢ M√©trica geom√©trica natural<br>‚Ä¢ K-Means lo usa | ‚Ä¢ Curse of dimensionality<br>‚Ä¢ Outliers impactan mucho | ‚úÖ **Clustering de im√°genes**<br>K-Means, espacios de baja dimensi√≥n |\n",
    "| **Manhattan** | ‚Ä¢ Robusto a outliers<br>‚Ä¢ Interpretable (suma diferencias)<br>‚Ä¢ Mejor en alta dimensi√≥n | ‚Ä¢ No tan com√∫n<br>‚Ä¢ Menos intuitivo sem√°nticamente | ‚úÖ **Feature vectors sparse**<br>Embeddings con muchos ceros |\n",
    "\n",
    "### üîç B√∫squeda Top-K Optimizada\n",
    "\n",
    "```python\n",
    "import heapq\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Resultado de b√∫squeda con score y metadata.\"\"\"\n",
    "    index: int\n",
    "    score: float\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "class TopKSearch:\n",
    "    \"\"\"B√∫squeda top-k con m√∫ltiples estrategias de optimizaci√≥n.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: np.ndarray, texts: List[str], metadata: List[dict]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (n, dim) matriz de embeddings normalizados\n",
    "            texts: lista de textos originales\n",
    "            metadata: lista de metadatos por cada embedding\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.texts = texts\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        # Normalizar embeddings para usar dot product = cosine\n",
    "        self.embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    def search_vectorized(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        top_k: int = 10\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B√∫squeda vectorizada (NumPy) - m√°s r√°pida para corpus peque√±o (<100K).\"\"\"\n",
    "        query = query / np.linalg.norm(query)  # Normalizar query\n",
    "        \n",
    "        # Dot product = cosine similarity para vectores normalizados\n",
    "        scores = self.embeddings @ query  # (n,) array\n",
    "        \n",
    "        # Top-k con argpartition (m√°s r√°pido que argsort para k peque√±o)\n",
    "        if top_k < len(scores):\n",
    "            top_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
    "            top_indices = top_indices[np.argsort(scores[top_indices])][::-1]\n",
    "        else:\n",
    "            top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=int(idx),\n",
    "                score=float(scores[idx]),\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def search_heap(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        top_k: int = 10\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B√∫squeda con heap (memoria constante) - √∫til para corpus muy grande.\"\"\"\n",
    "        query = query / np.linalg.norm(query)\n",
    "        \n",
    "        # Min-heap de tama√±o top_k (mantiene los k mayores)\n",
    "        heap = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            score = np.dot(emb, query)\n",
    "            \n",
    "            if len(heap) < top_k:\n",
    "                heapq.heappush(heap, (score, i))\n",
    "            elif score > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (score, i))\n",
    "        \n",
    "        # Ordenar resultados descendente\n",
    "        results = sorted(heap, reverse=True)\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=idx,\n",
    "                score=score,\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for score, idx in results\n",
    "        ]\n",
    "    \n",
    "    def search_threshold(\n",
    "        self, \n",
    "        query: np.ndarray, \n",
    "        threshold: float = 0.8\n",
    "    ) -> List[SearchResult]:\n",
    "        \"\"\"B√∫squeda por threshold (retorna todos los que superan umbral).\"\"\"\n",
    "        query = query / np.linalg.norm(query)\n",
    "        scores = self.embeddings @ query\n",
    "        \n",
    "        # Filtrar por threshold\n",
    "        above_threshold = np.where(scores >= threshold)[0]\n",
    "        sorted_indices = above_threshold[np.argsort(scores[above_threshold])][::-1]\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                index=int(idx),\n",
    "                score=float(scores[idx]),\n",
    "                text=self.texts[idx],\n",
    "                metadata=self.metadata[idx]\n",
    "            )\n",
    "            for idx in sorted_indices\n",
    "        ]\n",
    "\n",
    "# Ejemplo de uso\n",
    "texts = [\n",
    "    \"Tabla de ventas con transacciones diarias\",\n",
    "    \"Data de transacciones de venta por d√≠a\",\n",
    "    \"Cat√°logo de productos con precios\",\n",
    "    \"Informaci√≥n demogr√°fica de clientes\"\n",
    "]\n",
    "embeddings = np.random.randn(len(texts), 768)  # Simular embeddings\n",
    "metadata = [{\"owner\": \"analytics\", \"source\": f\"dwh.table_{i}\"} for i in range(len(texts))]\n",
    "\n",
    "search_engine = TopKSearch(embeddings, texts, metadata)\n",
    "query_embedding = np.random.randn(768)\n",
    "\n",
    "# B√∫squeda top-3\n",
    "results = search_engine.search_vectorized(query_embedding, top_k=3)\n",
    "for r in results:\n",
    "    print(f\"Score {r.score:.3f} | {r.text} | {r.metadata['source']}\")\n",
    "\n",
    "# B√∫squeda por threshold (duplicados)\n",
    "similar = search_engine.search_threshold(embeddings[0], threshold=0.95)\n",
    "print(f\"\\nEncontrados {len(similar)} textos muy similares (>0.95)\")\n",
    "```\n",
    "\n",
    "### üìä An√°lisis de Distribuci√≥n de Similitudes\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_similarity_distribution(embeddings: np.ndarray, sample_size: int = 1000):\n",
    "    \"\"\"Analiza la distribuci√≥n de similitudes en un corpus.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Normalizar embeddings\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Muestrear pares aleatorios\n",
    "    n = len(embeddings)\n",
    "    pairs = np.random.choice(n, size=(sample_size, 2), replace=True)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, j in pairs:\n",
    "        if i != j:\n",
    "            sim = np.dot(embeddings[i], embeddings[j])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    stats = {\n",
    "        \"mean\": similarities.mean(),\n",
    "        \"std\": similarities.std(),\n",
    "        \"min\": similarities.min(),\n",
    "        \"max\": similarities.max(),\n",
    "        \"median\": np.median(similarities),\n",
    "        \"q95\": np.percentile(similarities, 95),  # Threshold para \"muy similar\"\n",
    "        \"q99\": np.percentile(similarities, 99)   # Threshold para \"duplicados\"\n",
    "    }\n",
    "    \n",
    "    print(\"Distribuci√≥n de similitudes en corpus:\")\n",
    "    print(f\"  Media: {stats['mean']:.3f} ¬± {stats['std']:.3f}\")\n",
    "    print(f\"  Rango: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "    print(f\"  Percentil 95: {stats['q95']:.3f}  ‚Üê Threshold para 'muy similar'\")\n",
    "    print(f\"  Percentil 99: {stats['q99']:.3f}  ‚Üê Threshold para 'duplicados'\")\n",
    "    \n",
    "    return stats, similarities\n",
    "\n",
    "# Ejemplo: analizar corpus de embeddings\n",
    "embeddings = np.random.randn(10000, 768)\n",
    "stats, sims = analyze_similarity_distribution(embeddings)\n",
    "\n",
    "# Salida t√≠pica:\n",
    "# Distribuci√≥n de similitudes en corpus:\n",
    "#   Media: 0.012 ¬± 0.087\n",
    "#   Rango: [-0.289, 0.354]\n",
    "#   Percentil 95: 0.156  ‚Üê Threshold para 'muy similar'\n",
    "#   Percentil 99: 0.213  ‚Üê Threshold para 'duplicados'\n",
    "#\n",
    "# Interpretaci√≥n:\n",
    "# - Media cercana a 0: corpus diverso (bueno)\n",
    "# - Usar threshold 0.95-0.99 para deduplicaci√≥n\n",
    "# - Usar threshold 0.80-0.90 para b√∫squeda de similares\n",
    "```\n",
    "\n",
    "### üé® Visualizaci√≥n de Similitudes\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings_2d(\n",
    "    embeddings: np.ndarray, \n",
    "    labels: List[str], \n",
    "    method: Literal[\"pca\", \"tsne\"] = \"tsne\"\n",
    "):\n",
    "    \"\"\"Visualiza embeddings en 2D para an√°lisis exploratorio.\"\"\"\n",
    "    \n",
    "    # Reducci√≥n de dimensionalidad\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:  # tsne\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    \n",
    "    coords_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(coords_2d[:, 0], coords_2d[:, 1], alpha=0.6)\n",
    "    \n",
    "    # Anotar algunos puntos\n",
    "    for i, label in enumerate(labels[:20]):  # Primeros 20 para no saturar\n",
    "        plt.annotate(\n",
    "            label[:30],  # Truncar texto\n",
    "            (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "            fontsize=8,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"Visualizaci√≥n de Embeddings ({method.upper()})\")\n",
    "    plt.xlabel(\"Dimensi√≥n 1\")\n",
    "    plt.ylabel(\"Dimensi√≥n 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return coords_2d\n",
    "```\n",
    "\n",
    "### üöÄ Reglas de Oro para M√©tricas de Similitud\n",
    "\n",
    "1. **Embeddings de texto normalizados**: Usa **dot product** (m√°s r√°pido que cosine)\n",
    "2. **Corpus <100K vectores**: B√∫squeda **vectorizada con NumPy**\n",
    "3. **Corpus >1M vectores**: Usar **FAISS** con √≠ndices especializados\n",
    "4. **Deduplicaci√≥n**: Threshold **>0.95** (percentil 99 del corpus)\n",
    "5. **B√∫squeda sem√°ntica**: Threshold **>0.80** (top-k con k=5-10)\n",
    "6. **Clustering**: **Euclidean distance** con K-Means\n",
    "7. **Monitorear distribuci√≥n**: Similitudes extremas (>0.99 o <0.1) son sospechosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b12f",
   "metadata": {},
   "source": [
    "## ‚ö° B√∫squeda Vectorial a Escala: FAISS y Annoy\n",
    "\n",
    "Cuando el corpus supera **100K embeddings**, la b√∫squeda lineal (calcular similitud con todos los vectores) se vuelve prohibitivamente lenta. **FAISS** (Facebook AI Similarity Search) y **Annoy** (Spotify) son librer√≠as especializadas que aceleran la b√∫squeda usando **Approximate Nearest Neighbors (ANN)** con √≠ndices optimizados.\n",
    "\n",
    "### üèóÔ∏è Arquitectura de FAISS\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    FAISS: √çNDICES Y PERFORMANCE                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  PROBLEMA: B√∫squeda lineal O(n√ód) es lenta para n grande       ‚îÇ\n",
    "‚îÇ  10M vectores √ó 768 dims √ó 4 bytes = 30 GB memoria             ‚îÇ\n",
    "‚îÇ  B√∫squeda 1 query: 10M dot products = ~500ms                   ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  SOLUCI√ìN: √çndices especializados con trade-off speed vs accuracy‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. IndexFlatL2 / IndexFlatIP (EXACTO)                  ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - B√∫squeda exhaustiva (brute force)                 ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - 100% recall, pero lento                           ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Baseline para comparar                            ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Complejidad: O(n√ód)                                 ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Uso: <100K vectores, validaci√≥n                     ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. IndexIVFFlat (PARTICIONADO)                         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ a) Training: K-Means agrupa en nlist    ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ    centroides (ej. nlist=1000)          ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ                                          ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ b) Indexing: asigna cada vector al      ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ    centroide m√°s cercano                ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ                                          ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ c) Search: busca en nprobe clusters     ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îÇ    m√°s cercanos (ej. nprobe=10)         ‚îÇ         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Complejidad: O(nprobe √ó n/nlist √ó d)               ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Recall: 90-95% con nprobe=10                       ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Speedup: 10-100x vs flat                           ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Uso: 100K - 10M vectores                           ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. IndexIVFPQ (COMPRESI√ìN + PARTICIONADO)             ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Product Quantization (PQ): comprime vectores      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ      768 dims ‚Üí 96 bytes (8x compresi√≥n)               ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Divide vector en m subvectores                    ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Cada subvector ‚Üí codebook de 256 centroides      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - B√∫squeda en espacio comprimido (r√°pida)          ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Recall: 85-90%                                      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Speedup: 100-1000x vs flat                         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Memory: 10-20x menos que IVFFlat                   ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Uso: 10M - 1B vectores                             ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. IndexHNSW (GRAFO JER√ÅRQUICO)                        ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Hierarchical Navigable Small World                ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Grafo multi-nivel con conexiones                  ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    - B√∫squeda greedy navegando grafo                   ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Recall: 95-99% (mejor que IVF)                      ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Speedup: 50-500x vs flat                            ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Memory: Similar a flat (sin compresi√≥n)             ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ    Uso: <10M vectores, alta precisi√≥n requerida       ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üîß Implementaci√≥n de FAISS para Diferentes Escalas\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "\n",
    "class FAISSSearchEngine:\n",
    "    \"\"\"Motor de b√∫squeda vectorial con FAISS para diferentes escalas.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dimension: int, \n",
    "        index_type: str = \"auto\",\n",
    "        metric: str = \"cosine\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension: dimensionalidad de embeddings\n",
    "            index_type: \"flat\", \"ivf\", \"ivfpq\", \"hnsw\", \"auto\"\n",
    "            metric: \"cosine\" (IP para normalizados) o \"l2\" (euclidean)\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.index_type = index_type\n",
    "        self.metric = metric\n",
    "        self.index = None\n",
    "        self.n_vectors = 0\n",
    "    \n",
    "    def build_index(self, embeddings: np.ndarray, nlist: int = None):\n",
    "        \"\"\"\n",
    "        Construye √≠ndice FAISS apropiado seg√∫n tama√±o del corpus.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (n, dim) matriz de embeddings\n",
    "            nlist: n√∫mero de clusters para IVF (auto si None)\n",
    "        \"\"\"\n",
    "        self.n_vectors = len(embeddings)\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        \n",
    "        # Normalizar si m√©trica es cosine\n",
    "        if self.metric == \"cosine\":\n",
    "            faiss.normalize_L2(embeddings)  # In-place normalization\n",
    "        \n",
    "        # Selecci√≥n autom√°tica de √≠ndice seg√∫n tama√±o\n",
    "        if self.index_type == \"auto\":\n",
    "            if self.n_vectors < 10_000:\n",
    "                self.index_type = \"flat\"\n",
    "            elif self.n_vectors < 1_000_000:\n",
    "                self.index_type = \"ivf\"\n",
    "            else:\n",
    "                self.index_type = \"ivfpq\"\n",
    "        \n",
    "        # Construir √≠ndice seg√∫n tipo\n",
    "        if self.index_type == \"flat\":\n",
    "            # B√∫squeda exacta (baseline)\n",
    "            if self.metric == \"cosine\":\n",
    "                self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product\n",
    "            else:\n",
    "                self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            self.index.add(embeddings)\n",
    "            print(f\"‚úì IndexFlat construido: {self.n_vectors} vectores\")\n",
    "        \n",
    "        elif self.index_type == \"ivf\":\n",
    "            # IVF: particionado con K-Means\n",
    "            nlist = nlist or min(int(np.sqrt(self.n_vectors)), 4096)\n",
    "            \n",
    "            if self.metric == \"cosine\":\n",
    "                quantizer = faiss.IndexFlatIP(self.dimension)\n",
    "                self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "            else:\n",
    "                quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "                self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "            \n",
    "            # Training: K-Means para encontrar centroides\n",
    "            print(f\"Training IVF con {nlist} clusters...\")\n",
    "            self.index.train(embeddings)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            # Configurar nprobe (cu√°ntos clusters buscar)\n",
    "            self.index.nprobe = min(10, nlist // 10)  # Default: 10 o 10% de clusters\n",
    "            print(f\"‚úì IndexIVFFlat construido: {self.n_vectors} vectores, {nlist} clusters, nprobe={self.index.nprobe}\")\n",
    "        \n",
    "        elif self.index_type == \"ivfpq\":\n",
    "            # IVFPQ: particionado + compresi√≥n\n",
    "            nlist = nlist or min(int(np.sqrt(self.n_vectors)), 4096)\n",
    "            m = 8  # N√∫mero de subvectores (debe dividir dimension)\n",
    "            nbits = 8  # Bits por subvector (2^8 = 256 centroides por subvector)\n",
    "            \n",
    "            if self.metric == \"cosine\":\n",
    "                quantizer = faiss.IndexFlatIP(self.dimension)\n",
    "                self.index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, nbits)\n",
    "            else:\n",
    "                quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "                self.index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, nbits)\n",
    "            \n",
    "            print(f\"Training IVFPQ con {nlist} clusters, m={m}, nbits={nbits}...\")\n",
    "            self.index.train(embeddings)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            self.index.nprobe = min(10, nlist // 10)\n",
    "            \n",
    "            # Calcular compresi√≥n\n",
    "            original_size = self.n_vectors * self.dimension * 4  # float32\n",
    "            compressed_size = self.n_vectors * m  # m bytes por vector\n",
    "            ratio = original_size / compressed_size\n",
    "            print(f\"‚úì IndexIVFPQ construido: {self.n_vectors} vectores, compresi√≥n {ratio:.1f}x\")\n",
    "        \n",
    "        elif self.index_type == \"hnsw\":\n",
    "            # HNSW: grafo jer√°rquico\n",
    "            M = 32  # N√∫mero de conexiones por nodo\n",
    "            self.index = faiss.IndexHNSWFlat(self.dimension, M)\n",
    "            self.index.hnsw.efConstruction = 40  # Calidad de construcci√≥n\n",
    "            self.index.add(embeddings)\n",
    "            print(f\"‚úì IndexHNSW construido: {self.n_vectors} vectores, M={M}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Tipo de √≠ndice no soportado: {self.index_type}\")\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        queries: np.ndarray, \n",
    "        k: int = 10\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Busca k vecinos m√°s cercanos para cada query.\n",
    "        \n",
    "        Args:\n",
    "            queries: (n_queries, dim) matriz de queries\n",
    "            k: n√∫mero de vecinos a retornar\n",
    "        \n",
    "        Returns:\n",
    "            distances: (n_queries, k) distancias/similitudes\n",
    "            indices: (n_queries, k) √≠ndices de vecinos\n",
    "        \"\"\"\n",
    "        queries = queries.astype('float32')\n",
    "        \n",
    "        if self.metric == \"cosine\":\n",
    "            faiss.normalize_L2(queries)\n",
    "        \n",
    "        distances, indices = self.index.search(queries, k)\n",
    "        return distances, indices\n",
    "    \n",
    "    def tune_parameters(self, queries: np.ndarray, ground_truth_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        Encuentra nprobe √≥ptimo balanceando recall vs latencia.\n",
    "        \n",
    "        Args:\n",
    "            queries: queries de validaci√≥n\n",
    "            ground_truth_indices: resultados exactos (de IndexFlat)\n",
    "        \"\"\"\n",
    "        if self.index_type not in [\"ivf\", \"ivfpq\"]:\n",
    "            print(\"Tuning solo aplica a √≠ndices IVF\")\n",
    "            return\n",
    "        \n",
    "        nprobe_values = [1, 2, 5, 10, 20, 50, 100]\n",
    "        results = []\n",
    "        \n",
    "        for nprobe in nprobe_values:\n",
    "            self.index.nprobe = nprobe\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            _, indices = self.search(queries, k=10)\n",
    "            latency = (time.perf_counter() - start) * 1000 / len(queries)\n",
    "            \n",
    "            # Calcular recall@10\n",
    "            recall = np.mean([\n",
    "                len(np.intersect1d(indices[i], ground_truth_indices[i])) / 10\n",
    "                for i in range(len(queries))\n",
    "            ])\n",
    "            \n",
    "            results.append({\n",
    "                \"nprobe\": nprobe,\n",
    "                \"recall\": recall,\n",
    "                \"latency_ms\": latency\n",
    "            })\n",
    "            \n",
    "            print(f\"nprobe={nprobe:3} | recall@10={recall:.3f} | latency={latency:.2f}ms\")\n",
    "        \n",
    "        # Seleccionar nprobe con recall >0.95 y menor latencia\n",
    "        best = max([r for r in results if r[\"recall\"] >= 0.95], \n",
    "                   key=lambda x: -x[\"latency_ms\"], \n",
    "                   default=results[-1])\n",
    "        \n",
    "        self.index.nprobe = best[\"nprobe\"]\n",
    "        print(f\"\\n‚úì Configuraci√≥n √≥ptima: nprobe={best['nprobe']} (recall={best['recall']:.3f}, latency={best['latency_ms']:.2f}ms)\")\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Guarda √≠ndice a disco.\"\"\"\n",
    "        faiss.write_index(self.index, filepath)\n",
    "        print(f\"‚úì √çndice guardado en {filepath}\")\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Carga √≠ndice desde disco.\"\"\"\n",
    "        self.index = faiss.read_index(filepath)\n",
    "        self.n_vectors = self.index.ntotal\n",
    "        print(f\"‚úì √çndice cargado desde {filepath}: {self.n_vectors} vectores\")\n",
    "\n",
    "# Ejemplo: construcci√≥n y b√∫squeda con FAISS\n",
    "np.random.seed(42)\n",
    "n_vectors = 100_000\n",
    "dimension = 768\n",
    "\n",
    "print(\"Generando embeddings...\")\n",
    "embeddings = np.random.randn(n_vectors, dimension).astype('float32')\n",
    "\n",
    "# Construir √≠ndice IVF\n",
    "engine = FAISSSearchEngine(dimension=dimension, index_type=\"ivf\", metric=\"cosine\")\n",
    "engine.build_index(embeddings)\n",
    "\n",
    "# B√∫squeda\n",
    "queries = np.random.randn(100, dimension).astype('float32')\n",
    "k = 10\n",
    "\n",
    "start = time.perf_counter()\n",
    "distances, indices = engine.search(queries, k=k)\n",
    "elapsed = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"\\nB√∫squeda completada:\")\n",
    "print(f\"  {len(queries)} queries √ó top-{k}\")\n",
    "print(f\"  Latencia: {elapsed/len(queries):.2f} ms/query\")\n",
    "print(f\"  Throughput: {len(queries)/(elapsed/1000):.0f} queries/seg\")\n",
    "\n",
    "# Guardar √≠ndice\n",
    "engine.save(\"faiss_index.bin\")\n",
    "```\n",
    "\n",
    "### üìä Comparaci√≥n FAISS vs Annoy vs Brute Force\n",
    "\n",
    "```python\n",
    "import annoy\n",
    "\n",
    "def benchmark_search_engines(embeddings: np.ndarray, queries: np.ndarray, k: int = 10):\n",
    "    \"\"\"Compara diferentes motores de b√∫squeda.\"\"\"\n",
    "    n, dim = embeddings.shape\n",
    "    \n",
    "    # Normalizar para cosine similarity\n",
    "    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    queries_norm = queries / np.linalg.norm(queries, axis=1, keepdims=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Brute Force (NumPy)\n",
    "    print(\"1Ô∏è‚É£ Brute Force (NumPy)...\")\n",
    "    start = time.perf_counter()\n",
    "    scores = queries_norm @ embeddings_norm.T  # (n_queries, n_corpus)\n",
    "    bf_indices = np.argsort(scores, axis=1)[:, -k:][:, ::-1]\n",
    "    bf_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    results[\"brute_force\"] = {\n",
    "        \"build_time_ms\": 0,\n",
    "        \"search_time_ms\": bf_time,\n",
    "        \"latency_per_query_ms\": bf_time / len(queries),\n",
    "        \"recall\": 1.0,  # 100% por definici√≥n\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 2. FAISS IndexFlatIP (exacto optimizado)\n",
    "    print(\"2Ô∏è‚É£ FAISS IndexFlatIP...\")\n",
    "    start = time.perf_counter()\n",
    "    index_flat = faiss.IndexFlatIP(dim)\n",
    "    index_flat.add(embeddings_norm.astype('float32'))\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    faiss_flat_distances, faiss_flat_indices = index_flat.search(queries_norm.astype('float32'), k)\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    results[\"faiss_flat\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": 1.0,\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 3. FAISS IndexIVFFlat (aproximado)\n",
    "    print(\"3Ô∏è‚É£ FAISS IndexIVFFlat...\")\n",
    "    nlist = min(int(np.sqrt(n)), 1000)\n",
    "    quantizer = faiss.IndexFlatIP(dim)\n",
    "    index_ivf = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    index_ivf.train(embeddings_norm.astype('float32'))\n",
    "    index_ivf.add(embeddings_norm.astype('float32'))\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    index_ivf.nprobe = 10\n",
    "    start = time.perf_counter()\n",
    "    faiss_ivf_distances, faiss_ivf_indices = index_ivf.search(queries_norm.astype('float32'), k)\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    # Calcular recall vs brute force\n",
    "    recall = np.mean([\n",
    "        len(np.intersect1d(faiss_ivf_indices[i], bf_indices[i])) / k\n",
    "        for i in range(len(queries))\n",
    "    ])\n",
    "    \n",
    "    results[\"faiss_ivf\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": recall,\n",
    "        \"memory_mb\": embeddings.nbytes / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    # 4. Annoy (Spotify)\n",
    "    print(\"4Ô∏è‚É£ Annoy...\")\n",
    "    annoy_index = annoy.AnnoyIndex(dim, 'angular')  # angular = cosine\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for i, vec in enumerate(embeddings_norm):\n",
    "        annoy_index.add_item(i, vec)\n",
    "    annoy_index.build(10)  # 10 √°rboles (m√°s √°rboles = mejor recall pero m√°s lento)\n",
    "    build_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    annoy_indices = []\n",
    "    for query in queries_norm:\n",
    "        annoy_indices.append(annoy_index.get_nns_by_vector(query, k))\n",
    "    search_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    annoy_indices = np.array(annoy_indices)\n",
    "    recall = np.mean([\n",
    "        len(np.intersect1d(annoy_indices[i], bf_indices[i])) / k\n",
    "        for i in range(len(queries))\n",
    "    ])\n",
    "    \n",
    "    # Annoy guarda √≠ndice en disco, estimar tama√±o\n",
    "    annoy_index.save(\"temp_annoy.ann\")\n",
    "    import os\n",
    "    memory_mb = os.path.getsize(\"temp_annoy.ann\") / 1024 / 1024\n",
    "    os.remove(\"temp_annoy.ann\")\n",
    "    \n",
    "    results[\"annoy\"] = {\n",
    "        \"build_time_ms\": build_time,\n",
    "        \"search_time_ms\": search_time,\n",
    "        \"latency_per_query_ms\": search_time / len(queries),\n",
    "        \"recall\": recall,\n",
    "        \"memory_mb\": memory_mb\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar benchmark\n",
    "embeddings = np.random.randn(50000, 384).astype('float32')\n",
    "queries = np.random.randn(100, 384).astype('float32')\n",
    "\n",
    "bench_results = benchmark_search_engines(embeddings, queries, k=10)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Engine':<15} | {'Build (ms)':<12} | {'Search (ms)':<12} | {'Latency/Q (ms)':<15} | {'Recall':<8} | {'Memory (MB)':<12}\")\n",
    "print(\"=\"*80)\n",
    "for engine, metrics in bench_results.items():\n",
    "    print(f\"{engine:<15} | {metrics['build_time_ms']:>11.2f} | {metrics['search_time_ms']:>11.2f} | {metrics['latency_per_query_ms']:>14.3f} | {metrics['recall']:>7.3f} | {metrics['memory_mb']:>11.1f}\")\n",
    "\n",
    "# Salida esperada:\n",
    "# ================================================================================\n",
    "# Engine          | Build (ms)   | Search (ms)  | Latency/Q (ms) | Recall   | Memory (MB) \n",
    "# ================================================================================\n",
    "# brute_force     |        0.00 |      342.56 |         3.426 |   1.000 |        73.2\n",
    "# faiss_flat      |       12.34 |       89.12 |         0.891 |   1.000 |        73.2\n",
    "# faiss_ivf       |      234.56 |       15.67 |         0.157 |   0.943 |        73.2\n",
    "# annoy           |     1234.89 |       12.34 |         0.123 |   0.921 |        95.6\n",
    "```\n",
    "\n",
    "### üéØ Gu√≠a de Selecci√≥n de √çndice\n",
    "\n",
    "| Escala | √çndice Recomendado | Build Time | Search Latency | Recall | Memory | Caso de Uso |\n",
    "|--------|-------------------|------------|----------------|--------|--------|-------------|\n",
    "| <10K | **IndexFlat** | Instant√°neo | 1-5 ms/query | 100% | 1x | Desarrollo, validaci√≥n |\n",
    "| 10K-100K | **IndexHNSW** | Medio | 0.5-2 ms/query | 95-99% | 1x | Producci√≥n, alta precisi√≥n |\n",
    "| 100K-1M | **IndexIVFFlat** | Medio | 0.1-1 ms/query | 90-95% | 1x | Producci√≥n general |\n",
    "| 1M-10M | **IndexIVFPQ** | Alto | 0.05-0.5 ms/query | 85-90% | 0.1x | Alta escala, cost-effective |\n",
    "| >10M | **IndexIVFPQ + GPU** | Muy alto | 0.01-0.1 ms/query | 85-90% | 0.1x | Web-scale search |\n",
    "\n",
    "### üí° Optimizaciones Avanzadas\n",
    "\n",
    "```python\n",
    "# 1. GPU Acceleration (100x speedup para corpus muy grande)\n",
    "import faiss\n",
    "gpu_resources = faiss.StandardGpuResources()\n",
    "index_cpu = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index_gpu = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)  # 0 = GPU ID\n",
    "\n",
    "# 2. Sharding para corpus masivo (>100M vectores)\n",
    "# Dividir corpus en shards, buscar en paralelo, merge results\n",
    "shards = [\n",
    "    FAISSSearchEngine(dim, \"ivfpq\") for _ in range(10)\n",
    "]\n",
    "for i, shard in enumerate(shards):\n",
    "    shard.build_index(embeddings[i*chunk_size:(i+1)*chunk_size])\n",
    "\n",
    "# B√∫squeda paralela\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(lambda s: s.search(query, k=100), shards))\n",
    "# Merge top-100 de cada shard ‚Üí global top-10\n",
    "\n",
    "# 3. Prefiltering con metadatos\n",
    "# FAISS no soporta filtros nativamente, usar ID selector\n",
    "ids_filtered = [i for i, meta in enumerate(metadata) if meta['type'] == 'pipeline']\n",
    "selector = faiss.IDSelectorArray(len(ids_filtered), faiss.swig_ptr(np.array(ids_filtered)))\n",
    "distances, indices = index.search(query, k=10, params=faiss.SearchParametersIVF(sel=selector))\n",
    "```\n",
    "\n",
    "### üöÄ Reglas de Oro para B√∫squeda Vectorial a Escala\n",
    "\n",
    "1. **<10K vectores**: Usar b√∫squeda lineal (NumPy) - es suficiente\n",
    "2. **10K-100K**: FAISS **IndexHNSW** (mejor recall, no requiere training)\n",
    "3. **>100K**: FAISS **IndexIVFFlat** con nprobe tuneado (balance recall/latencia)\n",
    "4. **>1M**: FAISS **IndexIVFPQ** para reducir memoria 10x\n",
    "5. **Siempre** medir recall vs ground truth antes de producci√≥n (target: >95%)\n",
    "6. **Tune nprobe** con queries reales, no valores por defecto\n",
    "7. **Guardar √≠ndice** entrenado a disco, no reconstruir cada deploy\n",
    "8. **GPU** solo si corpus >10M y latencia <1ms requerida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062ee38",
   "metadata": {},
   "source": [
    "## üè≠ Aplicaciones en Data Engineering: Deduplicaci√≥n, Clustering y Recomendaciones\n",
    "\n",
    "Los embeddings permiten resolver problemas complejos de Data Engineering que antes requer√≠an reglas manuales o l√≥gica dif√≠cil de mantener: **deduplicaci√≥n fuzzy**, **clustering sem√°ntico**, **recomendaciones** de datasets, y **etiquetado autom√°tico** de activos de datos.\n",
    "\n",
    "### üîç 1. Deduplicaci√≥n Inteligente de Registros\n",
    "\n",
    "La deduplicaci√≥n tradicional (exact match por hash) falla con variaciones: \"Apple iPhone 13\" vs \"iPhone 13 by Apple\". Los embeddings capturan similitud sem√°ntica.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              PIPELINE DE DEDUPLICACI√ìN CON EMBEDDINGS            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Input: Tabla con posibles duplicados                           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n",
    "‚îÇ  ‚îÇ id ‚îÇ descripcion                               ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n",
    "‚îÇ  ‚îÇ 1  ‚îÇ Apple iPhone 13 Pro Max 256GB             ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îÇ 2  ‚îÇ iPhone 13 Pro Max 256GB Apple             ‚îÇ ‚Üê Duplicado ‚îÇ\n",
    "‚îÇ  ‚îÇ 3  ‚îÇ Samsung Galaxy S21 Ultra                  ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îÇ 4  ‚îÇ Galaxy S21 Ultra by Samsung               ‚îÇ ‚Üê Duplicado ‚îÇ\n",
    "‚îÇ  ‚îÇ 5  ‚îÇ Sony PlayStation 5 Console                ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£ Preprocesamiento                                            ‚îÇ\n",
    "‚îÇ     - Lowercasing: \"Apple iPhone\" ‚Üí \"apple iphone\"             ‚îÇ\n",
    "‚îÇ     - Remover stopwords: \"by\", \"the\", etc.                     ‚îÇ\n",
    "‚îÇ     - Normalizar espacios: \"  \" ‚Üí \" \"                          ‚îÇ\n",
    "‚îÇ     - Quitar caracteres especiales: \"iPhone-13\" ‚Üí \"iphone 13\"  ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ Generaci√≥n de Embeddings                                    ‚îÇ\n",
    "‚îÇ     - Usar modelo robusto (bge-large-en-v1.5)                  ‚îÇ\n",
    "‚îÇ     - Batch processing (100 registros/llamada)                 ‚îÇ\n",
    "‚îÇ     - Cache embeddings en columna nueva                        ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ B√∫squeda de Pares Similares                                ‚îÇ\n",
    "‚îÇ     - Opci√≥n A: All-pairs O(n¬≤) para corpus peque√±o           ‚îÇ\n",
    "‚îÇ     - Opci√≥n B: FAISS para corpus grande (>100K)              ‚îÇ\n",
    "‚îÇ     - Threshold t√≠pico: 0.95-0.99 (percentil 99 del corpus)   ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£ Clustering de Duplicados                                    ‚îÇ\n",
    "‚îÇ     - Connected components: transitivity                       ‚îÇ\n",
    "‚îÇ       Si A‚âàB y B‚âàC ‚Üí {A, B, C} es cluster                     ‚îÇ\n",
    "‚îÇ     - Asignar cluster_id a cada registro                       ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  5Ô∏è‚É£ Resoluci√≥n (elegir registro can√≥nico)                       ‚îÇ\n",
    "‚îÇ     - Estrategia 1: El m√°s completo (m√°s palabras)            ‚îÇ\n",
    "‚îÇ     - Estrategia 2: El m√°s reciente (timestamp)               ‚îÇ\n",
    "‚îÇ     - Estrategia 3: El m√°s popular (m√°s ventas/views)         ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  Output: Tabla deduplicada + mapping                            ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n",
    "‚îÇ  ‚îÇ cluster_id ‚îÇ canonical_id ‚îÇ descripcion        ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n",
    "‚îÇ  ‚îÇ 1          ‚îÇ 1            ‚îÇ Apple iPhone 13... ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îÇ 1          ‚îÇ 1            ‚îÇ iPhone 13 Pro Max..‚îÇ ‚Üí merged   ‚îÇ\n",
    "‚îÇ  ‚îÇ 2          ‚îÇ 3            ‚îÇ Samsung Galaxy...  ‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îÇ 2          ‚îÇ 3            ‚îÇ Galaxy S21 Ultra...‚îÇ ‚Üí merged   ‚îÇ\n",
    "‚îÇ  ‚îÇ 3          ‚îÇ 5            ‚îÇ Sony PlayStation...‚îÇ             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### Implementaci√≥n Completa\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DuplicateCluster:\n",
    "    \"\"\"Cluster de registros duplicados.\"\"\"\n",
    "    cluster_id: int\n",
    "    record_ids: List[int]\n",
    "    canonical_id: int\n",
    "    similarity_scores: List[float]\n",
    "\n",
    "class FuzzyDeduplicator:\n",
    "    \"\"\"Deduplicador fuzzy usando embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator, threshold: float = 0.95):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_generator: EmbeddingGenerator instance\n",
    "            threshold: similitud m√≠nima para considerar duplicados (0.95-0.99)\n",
    "        \"\"\"\n",
    "        self.generator = embedding_generator\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Normaliza texto antes de embeddings.\"\"\"\n",
    "        import re\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)  # Remover puntuaci√≥n\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalizar espacios\n",
    "        return text\n",
    "    \n",
    "    def find_duplicates(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        text_column: str,\n",
    "        id_column: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encuentra duplicados en DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con registros\n",
    "            text_column: columna con texto a comparar\n",
    "            id_column: columna con ID √∫nico (usa index si None)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame con cluster_id y canonical_id\n",
    "        \"\"\"\n",
    "        if id_column is None:\n",
    "            df = df.reset_index(drop=True)\n",
    "            id_column = 'index'\n",
    "            df[id_column] = df.index\n",
    "        \n",
    "        # 1. Preprocesamiento\n",
    "        print(\"1Ô∏è‚É£ Preprocesando texto...\")\n",
    "        df['text_clean'] = df[text_column].apply(self.preprocess_text)\n",
    "        \n",
    "        # 2. Generar embeddings (con cache)\n",
    "        print(\"2Ô∏è‚É£ Generando embeddings...\")\n",
    "        embeddings = self.generator.embed(df['text_clean'].tolist())\n",
    "        \n",
    "        # 3. Encontrar pares similares\n",
    "        print(f\"3Ô∏è‚É£ Buscando pares con similitud >{self.threshold}...\")\n",
    "        pairs = self._find_similar_pairs(embeddings, df[id_column].tolist())\n",
    "        print(f\"   Encontrados {len(pairs)} pares de duplicados\")\n",
    "        \n",
    "        # 4. Clustering (connected components)\n",
    "        print(\"4Ô∏è‚É£ Agrupando en clusters...\")\n",
    "        clusters = self._cluster_duplicates(pairs, df[id_column].tolist())\n",
    "        print(f\"   {len(clusters)} clusters de duplicados\")\n",
    "        \n",
    "        # 5. Seleccionar registros can√≥nicos\n",
    "        print(\"5Ô∏è‚É£ Seleccionando registros can√≥nicos...\")\n",
    "        cluster_map = {}\n",
    "        for cluster in clusters:\n",
    "            canonical_id = self._select_canonical(cluster, df, text_column, id_column)\n",
    "            for record_id in cluster.record_ids:\n",
    "                cluster_map[record_id] = {\n",
    "                    'cluster_id': cluster.cluster_id,\n",
    "                    'canonical_id': canonical_id,\n",
    "                    'is_canonical': record_id == canonical_id\n",
    "                }\n",
    "        \n",
    "        # Agregar columnas al DataFrame\n",
    "        df['cluster_id'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('cluster_id'))\n",
    "        df['canonical_id'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('canonical_id', x))\n",
    "        df['is_duplicate'] = df['cluster_id'].notna()\n",
    "        df['is_canonical'] = df[id_column].map(lambda x: cluster_map.get(x, {}).get('is_canonical', True))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _find_similar_pairs(\n",
    "        self, \n",
    "        embeddings: np.ndarray, \n",
    "        ids: List\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Encuentra pares de embeddings similares.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Para corpus peque√±o (<10K), usar all-pairs\n",
    "        if len(embeddings) < 10_000:\n",
    "            sim_matrix = cosine_similarity(embeddings)\n",
    "            for i in range(len(embeddings)):\n",
    "                for j in range(i+1, len(embeddings)):\n",
    "                    if sim_matrix[i][j] > self.threshold:\n",
    "                        pairs.append((ids[i], ids[j], sim_matrix[i][j]))\n",
    "        else:\n",
    "            # Para corpus grande, usar FAISS\n",
    "            import faiss\n",
    "            embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            \n",
    "            index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "            index.add(embeddings_norm.astype('float32'))\n",
    "            \n",
    "            # Buscar k vecinos m√°s cercanos para cada vector\n",
    "            k = min(10, len(embeddings) - 1)\n",
    "            distances, indices = index.search(embeddings_norm.astype('float32'), k + 1)\n",
    "            \n",
    "            for i in range(len(embeddings)):\n",
    "                for j, dist in zip(indices[i][1:], distances[i][1:]):  # Excluir el mismo vector\n",
    "                    if dist > self.threshold and ids[i] < ids[j]:  # Evitar duplicar pares\n",
    "                        pairs.append((ids[i], int(j), float(dist)))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _cluster_duplicates(\n",
    "        self, \n",
    "        pairs: List[Tuple[int, int, float]], \n",
    "        all_ids: List[int]\n",
    "    ) -> List[DuplicateCluster]:\n",
    "        \"\"\"Agrupa pares en clusters usando connected components.\"\"\"\n",
    "        # Construir grafo\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(all_ids)\n",
    "        \n",
    "        for id1, id2, score in pairs:\n",
    "            G.add_edge(id1, id2, weight=score)\n",
    "        \n",
    "        # Encontrar componentes conexas\n",
    "        clusters = []\n",
    "        for i, component in enumerate(nx.connected_components(G)):\n",
    "            if len(component) > 1:  # Solo clusters con >1 registro\n",
    "                component_list = list(component)\n",
    "                \n",
    "                # Calcular similitudes promedio\n",
    "                scores = [\n",
    "                    G[u][v]['weight'] \n",
    "                    for u, v in G.edges(component_list)\n",
    "                ]\n",
    "                \n",
    "                clusters.append(DuplicateCluster(\n",
    "                    cluster_id=i,\n",
    "                    record_ids=component_list,\n",
    "                    canonical_id=component_list[0],  # Temporal, se reemplaza despu√©s\n",
    "                    similarity_scores=scores\n",
    "                ))\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _select_canonical(\n",
    "        self, \n",
    "        cluster: DuplicateCluster, \n",
    "        df: pd.DataFrame, \n",
    "        text_column: str,\n",
    "        id_column: str\n",
    "    ) -> int:\n",
    "        \"\"\"Selecciona registro can√≥nico del cluster.\"\"\"\n",
    "        cluster_df = df[df[id_column].isin(cluster.record_ids)]\n",
    "        \n",
    "        # Estrategia: el registro m√°s completo (m√°s palabras)\n",
    "        cluster_df['word_count'] = cluster_df[text_column].str.split().str.len()\n",
    "        canonical_id = cluster_df.loc[cluster_df['word_count'].idxmax(), id_column]\n",
    "        \n",
    "        return canonical_id\n",
    "    \n",
    "    def get_deduplication_report(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Genera reporte de deduplicaci√≥n.\"\"\"\n",
    "        total_records = len(df)\n",
    "        duplicate_records = df['is_duplicate'].sum()\n",
    "        unique_records = total_records - duplicate_records\n",
    "        n_clusters = df['cluster_id'].nunique()\n",
    "        \n",
    "        # Distribuci√≥n de tama√±o de clusters\n",
    "        cluster_sizes = df[df['is_duplicate']].groupby('cluster_id').size()\n",
    "        \n",
    "        return {\n",
    "            \"total_records\": total_records,\n",
    "            \"unique_records\": unique_records,\n",
    "            \"duplicate_records\": duplicate_records,\n",
    "            \"deduplication_rate\": duplicate_records / total_records,\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"avg_cluster_size\": cluster_sizes.mean(),\n",
    "            \"max_cluster_size\": cluster_sizes.max(),\n",
    "            \"cluster_size_distribution\": cluster_sizes.value_counts().to_dict()\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dataset de ejemplo\n",
    "data = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'description': [\n",
    "        'Apple iPhone 13 Pro Max 256GB',\n",
    "        'iPhone 13 Pro Max 256GB Apple',\n",
    "        'Samsung Galaxy S21 Ultra',\n",
    "        'Galaxy S21 Ultra by Samsung',\n",
    "        'Sony PlayStation 5 Console',\n",
    "        'PS5 Console by Sony',\n",
    "        'MacBook Pro 14 inch M1',\n",
    "        'Apple MacBook Pro 14\" M1 Chip',\n",
    "        'Nike Air Max Sneakers',\n",
    "        'Dell XPS 13 Laptop'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Deduplicaci√≥n\n",
    "generator = EmbeddingGenerator(\"bge\")  # Usar modelo open source\n",
    "deduplicator = FuzzyDeduplicator(generator, threshold=0.90)\n",
    "\n",
    "df_dedup = deduplicator.find_duplicates(data, text_column='description', id_column='id')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nRegistros con duplicados:\")\n",
    "print(df_dedup[df_dedup['is_duplicate']][['id', 'description', 'cluster_id', 'canonical_id', 'is_canonical']])\n",
    "\n",
    "# Reporte\n",
    "report = deduplicator.get_deduplication_report(df_dedup)\n",
    "print(f\"\\nüìä Reporte de Deduplicaci√≥n:\")\n",
    "print(f\"  Total registros: {report['total_records']}\")\n",
    "print(f\"  √önicos: {report['unique_records']}\")\n",
    "print(f\"  Duplicados: {report['duplicate_records']} ({report['deduplication_rate']:.1%})\")\n",
    "print(f\"  Clusters: {report['n_clusters']}\")\n",
    "print(f\"  Tama√±o promedio de cluster: {report['avg_cluster_size']:.1f}\")\n",
    "```\n",
    "\n",
    "### üéØ 2. Clustering Sem√°ntico de Datasets\n",
    "\n",
    "Agrupar tablas/pipelines por dominio de negocio sin etiquetas manuales.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DatasetClusterer:\n",
    "    \"\"\"Agrupa datasets por similitud sem√°ntica.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "    \n",
    "    def cluster_kmeans(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        n_clusters: int = None,\n",
    "        auto_k: bool = True\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Clustering con K-Means.\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones de datasets\n",
    "            n_clusters: n√∫mero de clusters (auto-detecta si None)\n",
    "            auto_k: usar elbow method para encontrar k √≥ptimo\n",
    "        \"\"\"\n",
    "        # Generar embeddings\n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        # Auto-detectar k √≥ptimo\n",
    "        if auto_k and n_clusters is None:\n",
    "            n_clusters = self._find_optimal_k(embeddings)\n",
    "            print(f\"‚úì K √≥ptimo detectado: {n_clusters}\")\n",
    "        elif n_clusters is None:\n",
    "            n_clusters = 5  # Default\n",
    "        \n",
    "        # K-Means\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "        metrics = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"silhouette_score\": silhouette_score(embeddings, labels),  # [-1, 1], >0.5 bueno\n",
    "            \"calinski_harabasz\": calinski_harabasz_score(embeddings, labels),  # >1000 bueno\n",
    "            \"cluster_sizes\": np.bincount(labels).tolist()\n",
    "        }\n",
    "        \n",
    "        # Encontrar t√©rminos representativos por cluster\n",
    "        cluster_terms = self._extract_cluster_terms(descriptions, labels, n_clusters)\n",
    "        metrics[\"cluster_terms\"] = cluster_terms\n",
    "        \n",
    "        return labels, metrics\n",
    "    \n",
    "    def cluster_dbscan(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        eps: float = 0.3,\n",
    "        min_samples: int = 2\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Clustering con DBSCAN (density-based, no requiere k).\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones\n",
    "            eps: distancia m√°xima para considerar vecinos (0.1-0.5)\n",
    "            min_samples: m√≠nimo de vecinos para formar cluster\n",
    "        \"\"\"\n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        # DBSCAN con distancia coseno\n",
    "        from sklearn.metrics.pairwise import cosine_distances\n",
    "        distance_matrix = cosine_distances(embeddings)\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "        labels = dbscan.fit_predict(distance_matrix)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        metrics = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"n_noise\": n_noise,  # Puntos no asignados a ning√∫n cluster\n",
    "            \"cluster_sizes\": {\n",
    "                int(label): int(count) \n",
    "                for label, count in zip(*np.unique(labels, return_counts=True))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return labels, metrics\n",
    "    \n",
    "    def _find_optimal_k(self, embeddings: np.ndarray, k_range: range = range(2, 11)) -> int:\n",
    "        \"\"\"Encuentra k √≥ptimo usando elbow method.\"\"\"\n",
    "        inertias = []\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(embeddings)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Encontrar \"codo\" (mayor cambio en pendiente)\n",
    "        diffs = np.diff(inertias)\n",
    "        diff_ratios = diffs[:-1] / diffs[1:]\n",
    "        optimal_k = k_range[np.argmax(diff_ratios) + 1]\n",
    "        \n",
    "        return optimal_k\n",
    "    \n",
    "    def _extract_cluster_terms(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        labels: np.ndarray, \n",
    "        n_clusters: int\n",
    "    ) -> Dict[int, List[str]]:\n",
    "        \"\"\"Extrae t√©rminos m√°s frecuentes por cluster.\"\"\"\n",
    "        from collections import Counter\n",
    "        import re\n",
    "        \n",
    "        cluster_terms = {}\n",
    "        for cluster_id in range(n_clusters):\n",
    "            # Obtener descripciones del cluster\n",
    "            cluster_docs = [\n",
    "                desc for desc, label in zip(descriptions, labels) \n",
    "                if label == cluster_id\n",
    "            ]\n",
    "            \n",
    "            # Tokenizar y contar t√©rminos\n",
    "            all_words = []\n",
    "            for doc in cluster_docs:\n",
    "                words = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "                all_words.extend([w for w in words if len(w) > 3])  # Filtrar palabras cortas\n",
    "            \n",
    "            # Top-5 t√©rminos\n",
    "            counter = Counter(all_words)\n",
    "            cluster_terms[cluster_id] = [word for word, count in counter.most_common(5)]\n",
    "        \n",
    "        return cluster_terms\n",
    "    \n",
    "    def visualize_clusters(\n",
    "        self, \n",
    "        embeddings: np.ndarray, \n",
    "        labels: np.ndarray, \n",
    "        descriptions: List[str]\n",
    "    ):\n",
    "        \"\"\"Visualiza clusters en 2D con PCA.\"\"\"\n",
    "        # Reducir a 2D\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        coords_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(\n",
    "            coords_2d[:, 0], \n",
    "            coords_2d[:, 1], \n",
    "            c=labels, \n",
    "            cmap='tab10', \n",
    "            alpha=0.6,\n",
    "            s=100\n",
    "        )\n",
    "        \n",
    "        # Anotar algunos puntos\n",
    "        for i in range(min(15, len(descriptions))):\n",
    "            plt.annotate(\n",
    "                descriptions[i][:40], \n",
    "                (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "                fontsize=8,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        plt.colorbar(scatter, label='Cluster ID')\n",
    "        plt.title('Clustering de Datasets por Similitud Sem√°ntica')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Ejemplo: clustering de cat√°logo de datos\n",
    "catalog_descriptions = [\n",
    "    \"Transacciones de venta diarias con monto y producto\",\n",
    "    \"Data de ventas por d√≠a con ingresos totales\",\n",
    "    \"Informaci√≥n demogr√°fica de clientes: edad, ciudad, segmento\",\n",
    "    \"Datos de clientes con perfil demogr√°fico completo\",\n",
    "    \"Inventario de productos por almac√©n con stock disponible\",\n",
    "    \"Stock de productos en todos los almacenes\",\n",
    "    \"M√©tricas de engagement de usuarios en plataforma\",\n",
    "    \"Actividad de usuarios: clicks, sesiones, tiempo en sitio\",\n",
    "    \"Campa√±as de marketing con presupuesto y ROI\",\n",
    "    \"Data de campa√±as publicitarias y performance\"\n",
    "]\n",
    "\n",
    "generator = EmbeddingGenerator(\"sbert\")\n",
    "clusterer = DatasetClusterer(generator)\n",
    "\n",
    "labels, metrics = clusterer.cluster_kmeans(catalog_descriptions, auto_k=True)\n",
    "\n",
    "print(f\"üìä Resultados de Clustering:\")\n",
    "print(f\"  Clusters: {metrics['n_clusters']}\")\n",
    "print(f\"  Silhouette Score: {metrics['silhouette_score']:.3f} (>0.5 es bueno)\")\n",
    "print(f\"  Tama√±os: {metrics['cluster_sizes']}\")\n",
    "print(f\"\\n  T√©rminos por cluster:\")\n",
    "for cluster_id, terms in metrics['cluster_terms'].items():\n",
    "    print(f\"    Cluster {cluster_id}: {', '.join(terms)}\")\n",
    "```\n",
    "\n",
    "### üéÅ 3. Sistema de Recomendaci√≥n de Datasets\n",
    "\n",
    "```python\n",
    "class DatasetRecommender:\n",
    "    \"\"\"Recomienda datasets similares para analistas.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "        self.catalog_embeddings = None\n",
    "        self.catalog_metadata = None\n",
    "    \n",
    "    def index_catalog(self, catalog: pd.DataFrame, text_column: str):\n",
    "        \"\"\"Indexa cat√°logo de datasets.\"\"\"\n",
    "        descriptions = catalog[text_column].tolist()\n",
    "        self.catalog_embeddings = self.generator.embed(descriptions)\n",
    "        self.catalog_metadata = catalog.to_dict('records')\n",
    "        print(f\"‚úì Cat√°logo indexado: {len(catalog)} datasets\")\n",
    "    \n",
    "    def recommend_similar(\n",
    "        self, \n",
    "        query: str, \n",
    "        top_k: int = 5,\n",
    "        filters: Dict = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recomienda datasets similares a la query.\n",
    "        \n",
    "        Args:\n",
    "            query: descripci√≥n de lo que busca el usuario\n",
    "            top_k: n√∫mero de recomendaciones\n",
    "            filters: filtros de metadatos {\"owner\": \"analytics\", \"type\": \"table\"}\n",
    "        \"\"\"\n",
    "        # Embedding de query\n",
    "        query_emb = self.generator.embed_single(query)\n",
    "        \n",
    "        # Aplicar filtros de metadatos\n",
    "        if filters:\n",
    "            mask = np.ones(len(self.catalog_metadata), dtype=bool)\n",
    "            for key, value in filters.items():\n",
    "                mask &= np.array([meta.get(key) == value for meta in self.catalog_metadata])\n",
    "            \n",
    "            filtered_embeddings = self.catalog_embeddings[mask]\n",
    "            filtered_metadata = [m for m, include in zip(self.catalog_metadata, mask) if include]\n",
    "        else:\n",
    "            filtered_embeddings = self.catalog_embeddings\n",
    "            filtered_metadata = self.catalog_metadata\n",
    "        \n",
    "        # Calcular similitudes\n",
    "        similarities = cosine_similarity([query_emb], filtered_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        # Construir resultados\n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            rec = filtered_metadata[idx].copy()\n",
    "            rec['similarity'] = float(similarities[idx])\n",
    "            rec['relevance'] = \"Alta\" if similarities[idx] > 0.8 else \"Media\" if similarities[idx] > 0.6 else \"Baja\"\n",
    "            recommendations.append(rec)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_collaborative(\n",
    "        self, \n",
    "        user_history: List[str], \n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recomendaciones colaborativas basadas en historial del usuario.\n",
    "        \n",
    "        Args:\n",
    "            user_history: lista de dataset_ids que el usuario ha usado\n",
    "        \"\"\"\n",
    "        # Embeddings de datasets usados\n",
    "        used_indices = [\n",
    "            i for i, meta in enumerate(self.catalog_metadata)\n",
    "            if meta.get('dataset_id') in user_history\n",
    "        ]\n",
    "        used_embeddings = self.catalog_embeddings[used_indices]\n",
    "        \n",
    "        # Centroid del perfil del usuario\n",
    "        user_profile = used_embeddings.mean(axis=0)\n",
    "        \n",
    "        # Recomendar datasets similares al perfil (excluyendo ya usados)\n",
    "        available_mask = np.array([\n",
    "            meta.get('dataset_id') not in user_history \n",
    "            for meta in self.catalog_metadata\n",
    "        ])\n",
    "        \n",
    "        available_embeddings = self.catalog_embeddings[available_mask]\n",
    "        available_metadata = [m for m, avail in zip(self.catalog_metadata, available_mask) if avail]\n",
    "        \n",
    "        similarities = cosine_similarity([user_profile], available_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            rec = available_metadata[idx].copy()\n",
    "            rec['similarity'] = float(similarities[idx])\n",
    "            recommendations.append(rec)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Ejemplo: recomendaciones\n",
    "catalog = pd.DataFrame({\n",
    "    'dataset_id': ['dwh.ventas', 'dwh.clientes', 'dwh.productos', 'analytics.revenue', 'analytics.engagement'],\n",
    "    'description': [\n",
    "        'Transacciones de venta con fecha, monto, producto',\n",
    "        'Datos demogr√°ficos de clientes: edad, ciudad, segmento',\n",
    "        'Cat√°logo de productos con precios y categor√≠as',\n",
    "        'Ingresos mensuales agregados por regi√≥n',\n",
    "        'M√©tricas de engagement de usuarios en plataforma'\n",
    "    ],\n",
    "    'owner': ['data-eng', 'data-eng', 'data-eng', 'analytics', 'analytics'],\n",
    "    'type': ['table', 'table', 'table', 'view', 'view']\n",
    "})\n",
    "\n",
    "recommender = DatasetRecommender(generator)\n",
    "recommender.index_catalog(catalog, text_column='description')\n",
    "\n",
    "# B√∫squeda sem√°ntica\n",
    "print(\"üîç Buscar: 'necesito datos de ventas mensuales'\")\n",
    "results = recommender.recommend_similar('necesito datos de ventas mensuales', top_k=3)\n",
    "for i, rec in enumerate(results, 1):\n",
    "    print(f\"{i}. {rec['dataset_id']} (sim={rec['similarity']:.3f}, {rec['relevance']})\")\n",
    "    print(f\"   {rec['description']}\")\n",
    "\n",
    "# Recomendaciones colaborativas\n",
    "print(\"\\nüéÅ Recomendaciones basadas en historial: ['dwh.ventas']\")\n",
    "collab_recs = recommender.recommend_collaborative(['dwh.ventas'], top_k=3)\n",
    "for i, rec in enumerate(collab_recs, 1):\n",
    "    print(f\"{i}. {rec['dataset_id']} (sim={rec['similarity']:.3f})\")\n",
    "```\n",
    "\n",
    "### üìä 4. Detecci√≥n de Anomal√≠as en Metadatos\n",
    "\n",
    "Identificar tablas \"raras\" que no encajan con el resto del cat√°logo.\n",
    "\n",
    "```python\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Detecta datasets an√≥malos por similitud sem√°ntica.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator):\n",
    "        self.generator = embedding_generator\n",
    "    \n",
    "    def detect_anomalies(\n",
    "        self, \n",
    "        descriptions: List[str], \n",
    "        contamination: float = 0.05\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detecta descripciones an√≥malas usando Isolation Forest.\n",
    "        \n",
    "        Args:\n",
    "            descriptions: lista de descripciones\n",
    "            contamination: proporci√≥n esperada de anomal√≠as (0.01-0.10)\n",
    "        \n",
    "        Returns:\n",
    "            labels: 1=normal, -1=anomal√≠a\n",
    "            scores: scores de anomal√≠a (m√°s negativo = m√°s an√≥malo)\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        \n",
    "        embeddings = self.generator.embed(descriptions)\n",
    "        \n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "        labels = iso_forest.fit_predict(embeddings)\n",
    "        scores = iso_forest.score_samples(embeddings)\n",
    "        \n",
    "        n_anomalies = (labels == -1).sum()\n",
    "        print(f\"‚úì Anomal√≠as detectadas: {n_anomalies}/{len(descriptions)} ({n_anomalies/len(descriptions):.1%})\")\n",
    "        \n",
    "        return labels, scores\n",
    "\n",
    "# Ejemplo\n",
    "descriptions_with_anomaly = catalog_descriptions + [\n",
    "    \"Configuraci√≥n de base de datos MySQL 5.7\",  # Anomal√≠a: no es un dataset\n",
    "    \"Logs de errores de aplicaci√≥n Python\"      # Anomal√≠a: logs t√©cnicos\n",
    "]\n",
    "\n",
    "detector = AnomalyDetector(generator)\n",
    "labels, scores = detector.detect_anomalies(descriptions_with_anomaly, contamination=0.10)\n",
    "\n",
    "print(\"\\nüö® Descripciones an√≥malas:\")\n",
    "for i, (desc, label, score) in enumerate(zip(descriptions_with_anomaly, labels, scores)):\n",
    "    if label == -1:\n",
    "        print(f\"  [{score:.3f}] {desc}\")\n",
    "```\n",
    "\n",
    "### üöÄ M√©tricas de √âxito en Producci√≥n\n",
    "\n",
    "| Aplicaci√≥n | M√©trica | Target | Medici√≥n |\n",
    "|------------|---------|--------|----------|\n",
    "| **Deduplicaci√≥n** | Precision | >95% | Manual review de 100 pares |\n",
    "| **Deduplicaci√≥n** | Recall | >90% | Cu√°ntos duplicados reales se detectaron |\n",
    "| **Clustering** | Silhouette Score | >0.5 | sklearn.metrics |\n",
    "| **Recomendaciones** | Click-through Rate | >20% | % de recomendaciones clickeadas |\n",
    "| **Recomendaciones** | Relevance@5 | >0.80 | Ratings de usuarios (1-5) |\n",
    "| **Anomal√≠as** | False Positive Rate | <10% | Manual review de anomal√≠as |\n",
    "\n",
    "### üí° Mejores Pr√°cticas en Producci√≥n\n",
    "\n",
    "1. **Deduplicaci√≥n**: Combinar embeddings + reglas (ej. mismo SKU ‚Üí 100% duplicado)\n",
    "2. **Clustering**: Re-entrenar peri√≥dicamente al agregar nuevos datasets\n",
    "3. **Recomendaciones**: Combinar similitud sem√°ntica + popularidad + recency\n",
    "4. **Threshold tuning**: Usar validation set, no adivinar thresholds\n",
    "5. **Monitoreo**: Loggear similitudes promedio por d√≠a (detectar drift)\n",
    "6. **A/B testing**: Comparar embeddings vs reglas en m√©tricas de negocio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7fd36",
   "metadata": {},
   "source": [
    "## 1. Generar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(\n",
    "        model='text-embedding-ada-002',\n",
    "        input=text\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding)\n",
    "\n",
    "textos = [\n",
    "    'Tabla de ventas con transacciones diarias',\n",
    "    'Data de transacciones de venta por d√≠a',  # Similar\n",
    "    'Cat√°logo de productos con precios',\n",
    "    'Informaci√≥n demogr√°fica de clientes'\n",
    "]\n",
    "\n",
    "embeddings = [embed(t) for t in textos]\n",
    "print(f'Embeddings generados: {len(embeddings)} vectores de dimensi√≥n {len(embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1b5e1",
   "metadata": {},
   "source": [
    "## 2. Similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Matriz de similitud\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print('Matriz de similitud:\\n')\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f'{i}. {texto}')\n",
    "\n",
    "print('\\n', sim_matrix.round(3))\n",
    "print(f'\\n‚û°Ô∏è Textos 0 y 1 tienen similitud: {sim_matrix[0][1]:.3f} (duplicados potenciales)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1635129",
   "metadata": {},
   "source": [
    "## 3. B√∫squeda sem√°ntica en cat√°logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e73834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cat√°logo de data assets\n",
    "catalogo = pd.DataFrame([\n",
    "    {'asset': 'dwh.ventas', 'desc': 'Transacciones de venta con fecha, monto, producto'},\n",
    "    {'asset': 'dwh.clientes', 'desc': 'Datos demogr√°ficos de clientes: edad, ciudad, segmento'},\n",
    "    {'asset': 'dwh.productos', 'desc': 'Cat√°logo de productos con precios y categor√≠as'},\n",
    "    {'asset': 'dwh.inventario', 'desc': 'Stock disponible por almac√©n y SKU'},\n",
    "    {'asset': 'analytics.revenue_monthly', 'desc': 'Ingresos mensuales agregados por regi√≥n'}\n",
    "])\n",
    "\n",
    "# Embeddings del cat√°logo\n",
    "catalogo['embedding'] = catalogo['desc'].apply(lambda x: embed(x))\n",
    "\n",
    "def search_catalog(query: str, top_k: int = 3) -> pd.DataFrame:\n",
    "    query_emb = embed(query)\n",
    "    catalogo['similarity'] = catalogo['embedding'].apply(\n",
    "        lambda x: cosine_similarity([query_emb], [x])[0][0]\n",
    "    )\n",
    "    return catalogo.nlargest(top_k, 'similarity')[['asset', 'desc', 'similarity']]\n",
    "\n",
    "print(search_catalog('¬øD√≥nde encuentro informaci√≥n de productos?'))\n",
    "print('\\n')\n",
    "print(search_catalog('Necesito datos de ventas mensuales'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed677f",
   "metadata": {},
   "source": [
    "## 4. Detecci√≥n de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset con posibles duplicados\n",
    "descripciones = [\n",
    "    'Apple iPhone 13 Pro Max 256GB',\n",
    "    'iPhone 13 Pro Max 256GB Apple',\n",
    "    'Samsung Galaxy S21 Ultra',\n",
    "    'Galaxy S21 Ultra by Samsung',\n",
    "    'Sony PlayStation 5 Console'\n",
    "]\n",
    "\n",
    "embs = [embed(d) for d in descripciones]\n",
    "threshold = 0.95\n",
    "\n",
    "duplicates = []\n",
    "for i in range(len(embs)):\n",
    "    for j in range(i+1, len(embs)):\n",
    "        sim = cosine_similarity([embs[i]], [embs[j]])[0][0]\n",
    "        if sim > threshold:\n",
    "            duplicates.append((i, j, sim))\n",
    "\n",
    "print('Duplicados detectados:\\n')\n",
    "for i, j, sim in duplicates:\n",
    "    print(f'- \"{descripciones[i]}\" ‚âà \"{descripciones[j]}\" (sim={sim:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50e87c",
   "metadata": {},
   "source": [
    "## 5. Clustering con K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "productos = [\n",
    "    'Laptop Dell XPS 13',\n",
    "    'MacBook Pro 14 inch',\n",
    "    'HP Pavilion Laptop',\n",
    "    'Nike Air Max Sneakers',\n",
    "    'Adidas Ultraboost Shoes',\n",
    "    'Puma Running Shoes',\n",
    "    'Organic Green Tea 100 bags',\n",
    "    'Colombian Coffee Beans 1kg'\n",
    "]\n",
    "\n",
    "prod_embs = np.array([embed(p) for p in productos])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(prod_embs)\n",
    "\n",
    "df_clusters = pd.DataFrame({'producto': productos, 'cluster': clusters})\n",
    "print(df_clusters.sort_values('cluster'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2c629",
   "metadata": {},
   "source": [
    "## 6. FAISS para b√∫squeda r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d943a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu\n",
    "import faiss\n",
    "\n",
    "# Crear √≠ndice\n",
    "dimension = prod_embs.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(prod_embs.astype('float32'))\n",
    "\n",
    "# B√∫squeda\n",
    "query = 'zapatos deportivos'\n",
    "query_emb = embed(query).reshape(1, -1).astype('float32')\n",
    "\n",
    "k = 3\n",
    "distances, indices = index.search(query_emb, k)\n",
    "\n",
    "print(f'Top {k} productos para \"{query}\":\\n')\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f'{i+1}. {productos[idx]} (distancia={distances[0][i]:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae539a8",
   "metadata": {},
   "source": [
    "## 7. Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_similar(producto: str, catalogo: list, top_k: int = 3) -> list:\n",
    "    \"\"\"Recomienda productos similares.\"\"\"\n",
    "    query_emb = embed(producto)\n",
    "    catalogo_embs = [embed(p) for p in catalogo]\n",
    "    \n",
    "    similarities = [\n",
    "        (p, cosine_similarity([query_emb], [e])[0][0])\n",
    "        for p, e in zip(catalogo, catalogo_embs)\n",
    "        if p != producto\n",
    "    ]\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "comprado = 'Laptop Dell XPS 13'\n",
    "recomendaciones = recomendar_similar(comprado, productos)\n",
    "\n",
    "print(f'Cliente compr√≥: {comprado}')\n",
    "print('Recomendaciones:\\n')\n",
    "for prod, sim in recomendaciones:\n",
    "    print(f'- {prod} (similitud={sim:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5799c",
   "metadata": {},
   "source": [
    "## 8. Buenas pr√°cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404e0ec",
   "metadata": {},
   "source": [
    "- **Cache embeddings**: generarlos es costoso, almacena en DB.\n",
    "- **Batch processing**: genera embeddings en lotes para eficiencia.\n",
    "- **Normalizaci√≥n**: limpia texto antes de generar embeddings.\n",
    "- **Modelos locales**: considera Sentence Transformers para evitar API calls.\n",
    "- **Threshold din√°mico**: ajusta seg√∫n caso de uso (duplicados vs b√∫squeda).\n",
    "- **Monitoreo de costos**: embeddings consumen tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74deae01",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c26d3",
   "metadata": {},
   "source": [
    "1. Construye un deduplicador de registros usando embeddings.\n",
    "2. Implementa b√∫squeda h√≠brida (keyword + sem√°ntica) en tu cat√°logo.\n",
    "3. Crea clusters de clientes basados en descripciones de comportamiento.\n",
    "4. Desarrolla un sistema de recomendaci√≥n de datasets para analistas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
