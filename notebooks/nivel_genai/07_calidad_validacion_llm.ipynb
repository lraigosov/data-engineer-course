{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29150ce4",
   "metadata": {},
   "source": [
    "# ✅ Validación de Datos con LLMs\n",
    "\n",
    "Objetivo: usar LLMs para detectar anomalías, validar calidad de datos, clasificar errores, y generar reglas de validación de forma inteligente.\n",
    "\n",
    "- Duración: 90 min\n",
    "- Dificultad: Media/Alta\n",
    "- Stack: OpenAI, Great Expectations, Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299757e",
   "metadata": {},
   "source": [
    "## 🔍 LLMs para Data Quality: Más Allá de Reglas Estáticas\n",
    "\n",
    "La **validación tradicional** de datos se basa en reglas predefinidas (nulls, rangos, formatos). Los **LLMs** permiten validación **semántica y contextual**: detectar anomalías que ninguna regla puede capturar, explicar root causes, y generar reglas de validación automáticamente.\n",
    "\n",
    "### 🏗️ Evolución de Data Quality\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│         DATA QUALITY: TRADITIONAL vs LLM-POWERED                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  TRADITIONAL (Rule-Based):                                       │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 1. Null Checks                                           │   │\n",
    "│  │    assert column.notnull().all()                         │   │\n",
    "│  │    ✅ Detecta: NULL values                               │   │\n",
    "│  │    ❌ No detecta: \"N/A\", \"Unknown\", \"—\"                 │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 2. Range Checks                                          │   │\n",
    "│  │    assert (edad >= 0) & (edad <= 120)                    │   │\n",
    "│  │    ✅ Detecta: edad = -5 o 200                           │   │\n",
    "│  │    ❌ No detecta: edad = 999 (typo de 99)               │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 3. Regex Validation                                      │   │\n",
    "│  │    assert email.str.match(r'^[\\w.-]+@[\\w.-]+\\.\\w+$')    │   │\n",
    "│  │    ✅ Detecta: \"invalid-email\"                           │   │\n",
    "│  │    ❌ No detecta: \"test@test.test\" (fake)               │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 4. Referential Integrity                                 │   │\n",
    "│  │    assert ventas.producto_id.isin(productos.id)          │   │\n",
    "│  │    ✅ Detecta: producto_id = 999 (no existe)            │   │\n",
    "│  │    ❌ No explica POR QUÉ falló                          │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│                                                                  │\n",
    "│  LLM-POWERED (Semantic + Contextual):                            │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 1. Semantic Null Detection                               │   │\n",
    "│  │    LLM analiza: \"N/A\", \"Unknown\", \"—\", \"TBD\"            │   │\n",
    "│  │    → Detecta pseudo-nulls con contexto                   │   │\n",
    "│  │    ✅ \"N/A\" en columna 'ciudad' → NULL semántico        │   │\n",
    "│  │    ✅ \"N/A\" en columna 'notas' → Válido                 │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 2. Contextual Outlier Detection                          │   │\n",
    "│  │    LLM: \"edad = 999 es typo de 99 (patrón común)\"       │   │\n",
    "│  │    → Sugiere corrección basada en contexto              │   │\n",
    "│  │    ✅ Explica: \"Probablemente typo en entrada manual\"   │   │\n",
    "│  │    ✅ Sugiere: 99 o 9 (según distribución)              │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 3. Plausibility Validation                               │   │\n",
    "│  │    LLM: \"test@test.test es email técnico, no real\"      │   │\n",
    "│  │    → Detecta datos sintéticamente válidos pero falsos   │   │\n",
    "│  │    ✅ \"admin@example.com\" → Test data                   │   │\n",
    "│  │    ✅ \"Nueva Yorkk\" → Typo de \"Nueva York\"              │   │\n",
    "│  │                                                          │   │\n",
    "│  │ 4. Root Cause Analysis                                   │   │\n",
    "│  │    LLM: \"50 producto_ids huérfanos desde 2024-10-15\"    │   │\n",
    "│  │    → Analiza: \"Coincide con deploy de API v2\"           │   │\n",
    "│  │    ✅ Explica causa probable                            │   │\n",
    "│  │    ✅ Sugiere solución: \"Mapear IDs antiguos→nuevos\"    │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 🎯 Casos de Uso: LLMs vs Traditional\n",
    "\n",
    "| Problema | Traditional | LLM-Powered | Ganador |\n",
    "|----------|-------------|-------------|---------|\n",
    "| **NULL detection** | `column.isnull()` | Detecta \"N/A\", \"Unknown\", \"—\" | 🏆 LLM |\n",
    "| **Email validation** | Regex pattern | Detecta test@test.com como fake | 🏆 LLM |\n",
    "| **City names** | Whitelist de ciudades | Detecta \"Nueva Yorkk\" como typo | 🏆 LLM |\n",
    "| **Age range** | `age.between(0, 120)` | \"999 es typo de 99\" | 🏆 LLM |\n",
    "| **Performance** | Instantáneo | 100-500ms por validación | 🏆 Traditional |\n",
    "| **Costo** | Gratis | $0.001-$0.01 por registro | 🏆 Traditional |\n",
    "| **Explicabilidad** | Regla booleana | Natural language explanation | 🏆 LLM |\n",
    "\n",
    "**Conclusión**: LLMs **complementan** (no reemplazan) validación tradicional. Usar ambos en **híbrido**.\n",
    "\n",
    "### 🔧 Implementación: Sistema Híbrido de Validación\n",
    "\n",
    "```python\n",
    "from typing import List, Dict, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# 1. MODELOS DE DATOS\n",
    "\n",
    "class ValidationSeverity(str, Enum):\n",
    "    \"\"\"Severidad de issue de calidad.\"\"\"\n",
    "    CRITICAL = \"CRITICAL\"  # Bloquea pipeline\n",
    "    ERROR = \"ERROR\"        # Requiere fix\n",
    "    WARNING = \"WARNING\"    # Revisar\n",
    "    INFO = \"INFO\"          # Informativo\n",
    "\n",
    "class ValidationIssue(BaseModel):\n",
    "    \"\"\"Issue de calidad detectado.\"\"\"\n",
    "    column: str\n",
    "    row_index: Optional[int] = None\n",
    "    value: Optional[str] = None\n",
    "    issue_type: str  # NULLS, OUTLIER, FORMAT, INCONSISTENCY, etc.\n",
    "    severity: ValidationSeverity\n",
    "    confidence: float = Field(..., ge=0, le=1)\n",
    "    description: str\n",
    "    suggested_fix: Optional[str] = None\n",
    "    detected_by: Literal[\"traditional\", \"llm\", \"hybrid\"]\n",
    "\n",
    "class ValidationReport(BaseModel):\n",
    "    \"\"\"Reporte completo de validación.\"\"\"\n",
    "    table_name: str\n",
    "    total_rows: int\n",
    "    total_issues: int\n",
    "    issues_by_severity: Dict[ValidationSeverity, int]\n",
    "    issues: List[ValidationIssue]\n",
    "    execution_time_seconds: float\n",
    "    cost_usd: float\n",
    "\n",
    "# 2. VALIDADORES TRADICIONALES\n",
    "\n",
    "class TraditionalValidator:\n",
    "    \"\"\"Validador basado en reglas estáticas.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_nulls(df: pd.DataFrame, column: str) -> List[ValidationIssue]:\n",
    "        \"\"\"Detecta valores NULL.\"\"\"\n",
    "        null_mask = df[column].isnull()\n",
    "        null_count = null_mask.sum()\n",
    "        \n",
    "        if null_count == 0:\n",
    "            return []\n",
    "        \n",
    "        # Detectar todos los nulls\n",
    "        issues = []\n",
    "        for idx in df[null_mask].index[:100]:  # Limitar a 100 por performance\n",
    "            issues.append(ValidationIssue(\n",
    "                column=column,\n",
    "                row_index=int(idx),\n",
    "                value=None,\n",
    "                issue_type=\"NULLS\",\n",
    "                severity=ValidationSeverity.ERROR,\n",
    "                confidence=1.0,\n",
    "                description=f\"Valor NULL en columna requerida '{column}'\",\n",
    "                suggested_fix=\"Impute con media/moda o remover registro\",\n",
    "                detected_by=\"traditional\"\n",
    "            ))\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_range(\n",
    "        df: pd.DataFrame, \n",
    "        column: str, \n",
    "        min_val: float, \n",
    "        max_val: float\n",
    "    ) -> List[ValidationIssue]:\n",
    "        \"\"\"Detecta valores fuera de rango.\"\"\"\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        issues = []\n",
    "        for idx, row in out_of_range.iterrows():\n",
    "            value = row[column]\n",
    "            issues.append(ValidationIssue(\n",
    "                column=column,\n",
    "                row_index=int(idx),\n",
    "                value=str(value),\n",
    "                issue_type=\"OUTLIER\",\n",
    "                severity=ValidationSeverity.ERROR,\n",
    "                confidence=1.0,\n",
    "                description=f\"Valor {value} fuera de rango [{min_val}, {max_val}]\",\n",
    "                suggested_fix=f\"Clamp a rango o marcar como outlier\",\n",
    "                detected_by=\"traditional\"\n",
    "            ))\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_regex(\n",
    "        df: pd.DataFrame, \n",
    "        column: str, \n",
    "        pattern: str\n",
    "    ) -> List[ValidationIssue]:\n",
    "        \"\"\"Valida formato con regex.\"\"\"\n",
    "        invalid_mask = ~df[column].astype(str).str.match(pattern)\n",
    "        invalid_values = df[invalid_mask]\n",
    "        \n",
    "        issues = []\n",
    "        for idx, row in invalid_values.iterrows():\n",
    "            value = row[column]\n",
    "            issues.append(ValidationIssue(\n",
    "                column=column,\n",
    "                row_index=int(idx),\n",
    "                value=str(value),\n",
    "                issue_type=\"FORMAT\",\n",
    "                severity=ValidationSeverity.ERROR,\n",
    "                confidence=1.0,\n",
    "                description=f\"Valor '{value}' no coincide con patrón esperado\",\n",
    "                suggested_fix=\"Reformatear según pattern\",\n",
    "                detected_by=\"traditional\"\n",
    "            ))\n",
    "        \n",
    "        return issues\n",
    "\n",
    "# 3. VALIDADOR CON LLM\n",
    "\n",
    "class LLMValidator:\n",
    "    \"\"\"Validador con análisis semántico usando LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):  # Modelo barato para validación\n",
    "        self.model = model\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def check_semantic_nulls(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        column: str\n",
    "    ) -> List[ValidationIssue]:\n",
    "        \"\"\"\n",
    "        Detecta valores que son NULL semánticamente (N/A, Unknown, TBD, etc.)\n",
    "        pero no NULL sintácticamente.\n",
    "        \"\"\"\n",
    "        # Obtener valores únicos no-null\n",
    "        non_null_values = df[column].dropna().unique()[:50]  # Limitar muestra\n",
    "        \n",
    "        prompt = f\"\"\"Analiza estos valores de la columna '{column}' y detecta cuáles son NULL semánticos (representan faltante/desconocido/no aplicable):\n",
    "\n",
    "Valores: {list(non_null_values)}\n",
    "\n",
    "Responde en JSON:\n",
    "{{\n",
    "  \"semantic_nulls\": [\"valor1\", \"valor2\", ...],\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"reasoning\": \"explicación breve\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Calcular costo aproximado (gpt-4o-mini: $0.15/$0.60 por 1M tokens)\n",
    "            input_tokens = len(prompt) / 4  # Aproximación\n",
    "            output_tokens = len(response.choices[0].message.content) / 4\n",
    "            cost = (input_tokens / 1_000_000 * 0.15) + (output_tokens / 1_000_000 * 0.60)\n",
    "            self.total_cost += cost\n",
    "            \n",
    "            # Crear issues para cada NULL semántico\n",
    "            issues = []\n",
    "            for null_value in result.get(\"semantic_nulls\", []):\n",
    "                mask = df[column] == null_value\n",
    "                for idx in df[mask].index[:100]:\n",
    "                    issues.append(ValidationIssue(\n",
    "                        column=column,\n",
    "                        row_index=int(idx),\n",
    "                        value=str(null_value),\n",
    "                        issue_type=\"SEMANTIC_NULL\",\n",
    "                        severity=ValidationSeverity.WARNING,\n",
    "                        confidence=result.get(\"confidence\", 0.8),\n",
    "                        description=f\"'{null_value}' es NULL semántico: {result.get('reasoning', '')}\",\n",
    "                        suggested_fix=\"Convertir a NULL explícito\",\n",
    "                        detected_by=\"llm\"\n",
    "                    ))\n",
    "            \n",
    "            return issues\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en LLM validation: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def check_plausibility(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        column: str, \n",
    "        context: str = \"\"\n",
    "    ) -> List[ValidationIssue]:\n",
    "        \"\"\"\n",
    "        Valida plausibilidad de valores (ej. emails fake, ciudades con typos).\n",
    "        \"\"\"\n",
    "        sample_values = df[column].dropna().head(20).tolist()\n",
    "        \n",
    "        prompt = f\"\"\"Analiza la plausibilidad de estos valores en la columna '{column}':\n",
    "\n",
    "Contexto: {context}\n",
    "Valores: {sample_values}\n",
    "\n",
    "Detecta valores implausibles (fake, typos, test data, etc.).\n",
    "\n",
    "Responde en JSON:\n",
    "{{\n",
    "  \"implausible_values\": [\n",
    "    {{\n",
    "      \"value\": \"valor\",\n",
    "      \"reason\": \"explicación\",\n",
    "      \"confidence\": 0.0-1.0,\n",
    "      \"suggested_fix\": \"corrección o null\"\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Costo\n",
    "            cost = (len(prompt) / 4 / 1_000_000 * 0.15) + (len(response.choices[0].message.content) / 4 / 1_000_000 * 0.60)\n",
    "            self.total_cost += cost\n",
    "            \n",
    "            issues = []\n",
    "            for item in result.get(\"implausible_values\", []):\n",
    "                mask = df[column] == item[\"value\"]\n",
    "                for idx in df[mask].index:\n",
    "                    issues.append(ValidationIssue(\n",
    "                        column=column,\n",
    "                        row_index=int(idx),\n",
    "                        value=str(item[\"value\"]),\n",
    "                        issue_type=\"IMPLAUSIBLE\",\n",
    "                        severity=ValidationSeverity.WARNING,\n",
    "                        confidence=item.get(\"confidence\", 0.7),\n",
    "                        description=f\"Valor implausible: {item.get('reason', '')}\",\n",
    "                        suggested_fix=item.get(\"suggested_fix\"),\n",
    "                        detected_by=\"llm\"\n",
    "                    ))\n",
    "            \n",
    "            return issues\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en plausibility check: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def explain_outlier(\n",
    "        self, \n",
    "        value: float, \n",
    "        column: str,\n",
    "        stats: Dict[str, float]\n",
    "    ) -> str:\n",
    "        \"\"\"Explica por qué un valor es outlier en lenguaje natural.\"\"\"\n",
    "        prompt = f\"\"\"Explica por qué este valor es anómalo en 2-3 frases:\n",
    "\n",
    "Columna: {column}\n",
    "Valor: {value}\n",
    "\n",
    "Estadísticas:\n",
    "- Media: {stats['mean']:.2f}\n",
    "- Desv. estándar: {stats['std']:.2f}\n",
    "- Min: {stats['min']:.2f}\n",
    "- Max (sin este): {stats['max']:.2f}\n",
    "- Percentil 99: {stats.get('p99', 'N/A')}\n",
    "\n",
    "Incluye posible causa raíz (typo, error de medición, evento real extremo, etc.).\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            cost = (len(prompt) / 4 / 1_000_000 * 0.15) + (len(response.choices[0].message.content) / 4 / 1_000_000 * 0.60)\n",
    "            self.total_cost += cost\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generando explicación: {e}\"\n",
    "\n",
    "# 4. VALIDADOR HÍBRIDO (COMBINA AMBOS)\n",
    "\n",
    "class HybridValidator:\n",
    "    \"\"\"Sistema híbrido: Traditional (rápido) + LLM (semántico).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.traditional = TraditionalValidator()\n",
    "        self.llm = LLMValidator()\n",
    "    \n",
    "    def validate_dataframe(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        table_name: str,\n",
    "        validation_config: Dict\n",
    "    ) -> ValidationReport:\n",
    "        \"\"\"\n",
    "        Valida DataFrame completo con estrategia híbrida.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame a validar\n",
    "            table_name: nombre de la tabla\n",
    "            validation_config: configuración por columna\n",
    "                {\n",
    "                    \"column_name\": {\n",
    "                        \"checks\": [\"nulls\", \"range\", \"semantic_nulls\", \"plausibility\"],\n",
    "                        \"range\": [0, 120],\n",
    "                        \"context\": \"descripción para LLM\"\n",
    "                    }\n",
    "                }\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        all_issues = []\n",
    "        \n",
    "        for column, config in validation_config.items():\n",
    "            if column not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            checks = config.get(\"checks\", [])\n",
    "            \n",
    "            # TRADITIONAL CHECKS (siempre primero - rápidos)\n",
    "            if \"nulls\" in checks:\n",
    "                all_issues.extend(self.traditional.check_nulls(df, column))\n",
    "            \n",
    "            if \"range\" in checks and \"range\" in config:\n",
    "                min_val, max_val = config[\"range\"]\n",
    "                all_issues.extend(self.traditional.check_range(df, column, min_val, max_val))\n",
    "            \n",
    "            if \"regex\" in checks and \"pattern\" in config:\n",
    "                all_issues.extend(self.traditional.check_regex(df, column, config[\"pattern\"]))\n",
    "            \n",
    "            # LLM CHECKS (solo si hay issues o configurado explícitamente)\n",
    "            if \"semantic_nulls\" in checks:\n",
    "                all_issues.extend(self.llm.check_semantic_nulls(df, column))\n",
    "            \n",
    "            if \"plausibility\" in checks:\n",
    "                context = config.get(\"context\", f\"Columna {column}\")\n",
    "                all_issues.extend(self.llm.check_plausibility(df, column, context))\n",
    "        \n",
    "        # Generar reporte\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        issues_by_severity = {\n",
    "            severity: sum(1 for issue in all_issues if issue.severity == severity)\n",
    "            for severity in ValidationSeverity\n",
    "        }\n",
    "        \n",
    "        report = ValidationReport(\n",
    "            table_name=table_name,\n",
    "            total_rows=len(df),\n",
    "            total_issues=len(all_issues),\n",
    "            issues_by_severity=issues_by_severity,\n",
    "            issues=all_issues,\n",
    "            execution_time_seconds=execution_time,\n",
    "            cost_usd=self.llm.total_cost\n",
    "        )\n",
    "        \n",
    "        return report\n",
    "\n",
    "# EJEMPLO DE USO\n",
    "\n",
    "# Dataset de prueba con varios tipos de problemas\n",
    "df_test = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'age': [25, 30, 999, 45, None, 22, -5, 150],  # Outliers, null, negativos\n",
    "    'email': [\n",
    "        'user@example.com',\n",
    "        'admin@test.test',  # Fake\n",
    "        'invalid-email',     # Inválido\n",
    "        'test@test.com',     # Test data\n",
    "        None,\n",
    "        'real@company.io',\n",
    "        'N/A',               # NULL semántico\n",
    "        'john@gmail.com'\n",
    "    ],\n",
    "    'city': [\n",
    "        'New York',\n",
    "        'Nueva Yorkk',  # Typo\n",
    "        'Los Angeles',\n",
    "        'Unknown',      # NULL semántico\n",
    "        'San Francisco',\n",
    "        'N/A',          # NULL semántico\n",
    "        'Chicago',\n",
    "        'TBD'           # NULL semántico\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Configuración de validación\n",
    "validation_config = {\n",
    "    'age': {\n",
    "        'checks': ['nulls', 'range'],\n",
    "        'range': [0, 120]\n",
    "    },\n",
    "    'email': {\n",
    "        'checks': ['nulls', 'regex', 'plausibility'],\n",
    "        'pattern': r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$',\n",
    "        'context': 'Emails de clientes reales (no test data)'\n",
    "    },\n",
    "    'city': {\n",
    "        'checks': ['semantic_nulls', 'plausibility'],\n",
    "        'context': 'Ciudades de USA'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ejecutar validación híbrida\n",
    "validator = HybridValidator()\n",
    "report = validator.validate_dataframe(df_test, 'customers', validation_config)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"📊 REPORTE DE VALIDACIÓN: {report.table_name}\")\n",
    "print(f\"   Total filas: {report.total_rows}\")\n",
    "print(f\"   Total issues: {report.total_issues}\")\n",
    "print(f\"   Tiempo: {report.execution_time_seconds:.2f}s\")\n",
    "print(f\"   Costo LLM: ${report.cost_usd:.4f}\")\n",
    "print(f\"\\n   Issues por severidad:\")\n",
    "for severity, count in report.issues_by_severity.items():\n",
    "    if count > 0:\n",
    "        print(f\"     {severity.value}: {count}\")\n",
    "\n",
    "print(f\"\\n🔍 ISSUES DETECTADOS (top 10):\")\n",
    "for i, issue in enumerate(report.issues[:10], 1):\n",
    "    print(f\"\\n{i}. [{issue.severity.value}] {issue.issue_type}\")\n",
    "    print(f\"   Columna: {issue.column}, Fila: {issue.row_index}\")\n",
    "    print(f\"   Valor: {issue.value}\")\n",
    "    print(f\"   {issue.description}\")\n",
    "    print(f\"   Confianza: {issue.confidence:.0%} | Detectado por: {issue.detected_by}\")\n",
    "    if issue.suggested_fix:\n",
    "        print(f\"   💡 Fix sugerido: {issue.suggested_fix}\")\n",
    "```\n",
    "\n",
    "### 📊 Comparación de Performance\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "# Benchmark: Traditional vs LLM\n",
    "df_large = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, size=10000),\n",
    "    'email': ['user{}@example.com'.format(i) for i in range(10000)]\n",
    "})\n",
    "\n",
    "# Traditional validation\n",
    "start = time.time()\n",
    "trad_validator = TraditionalValidator()\n",
    "trad_issues = trad_validator.check_range(df_large, 'age', 0, 120)\n",
    "trad_time = time.time() - start\n",
    "\n",
    "print(f\"Traditional: {trad_time*1000:.2f}ms, {len(trad_issues)} issues, $0\")\n",
    "\n",
    "# LLM validation (solo muestra)\n",
    "start = time.time()\n",
    "llm_validator = LLMValidator()\n",
    "llm_issues = llm_validator.check_plausibility(df_large.head(20), 'email', 'Customer emails')\n",
    "llm_time = time.time() - start\n",
    "\n",
    "print(f\"LLM: {llm_time*1000:.2f}ms, {len(llm_issues)} issues, ${llm_validator.total_cost:.4f}\")\n",
    "print(f\"\\n💡 LLM es {llm_time/trad_time:.0f}x más lento pero detecta issues semánticos\")\n",
    "```\n",
    "\n",
    "### 🎯 Estrategia de Uso Óptima\n",
    "\n",
    "```python\n",
    "# REGLA DE ORO: Híbrido inteligente\n",
    "\n",
    "# 1. TRADITIONAL FIRST (always)\n",
    "#    - Rápido, gratis, confiable\n",
    "#    - Cubre 80% de casos\n",
    "\n",
    "# 2. LLM IF:\n",
    "#    a) Traditional encontró issues ambiguos\n",
    "#    b) Columnas críticas (PII, identificadores, nombres)\n",
    "#    c) Sample pequeño (<1000 registros únicos)\n",
    "#    d) Budget disponible ($0.01-$0.10 por tabla)\n",
    "\n",
    "# Ejemplo de decisión:\n",
    "def should_use_llm(df: pd.DataFrame, column: str, trad_issues: int) -> bool:\n",
    "    \"\"\"Decide si vale la pena usar LLM.\"\"\"\n",
    "    unique_count = df[column].nunique()\n",
    "    \n",
    "    # Criterios para usar LLM:\n",
    "    if trad_issues > 0 and trad_issues < 100:  # Issues moderados\n",
    "        return True\n",
    "    \n",
    "    if unique_count < 100:  # Pocos valores únicos (categóricos)\n",
    "        return True\n",
    "    \n",
    "    if column in ['email', 'name', 'city', 'address']:  # Campos críticos\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "```\n",
    "\n",
    "### 🚀 Mejores Prácticas\n",
    "\n",
    "1. **Siempre traditional primero**: Fast fail para issues obvios\n",
    "2. **LLM como segunda capa**: Solo para casos ambiguos o semánticos\n",
    "3. **Cache agresivo**: Mismos valores → misma respuesta (lru_cache)\n",
    "4. **Batch validation**: Validar múltiples valores en un solo prompt\n",
    "5. **Confidence thresholds**: Solo actuar si confidence >0.8\n",
    "6. **Human review**: Issues con confidence <0.9 requieren revisión\n",
    "7. **Cost monitoring**: Alertar si costo >$1 por tabla\n",
    "8. **A/B testing**: Comparar LLM vs tradicional en métricas de negocio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb72719",
   "metadata": {},
   "source": [
    "## 🤖 Generación Automática de Reglas de Validación con LLMs\n",
    "\n",
    "Escribir reglas de validación manualmente es tedioso y propenso a errores. Los **LLMs** pueden **inferir reglas de validación** analizando datos y generando código de Great Expectations, dbt tests, o Pandas assertions automáticamente.\n",
    "\n",
    "### 🏗️ Arquitectura de Auto-Generación de Reglas\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│      AUTO-GENERATION: FROM DATA TO VALIDATION RULES              │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  INPUT: DataFrame con datos históricos                          │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ df['age']: [25, 30, 45, 22, 28, ...]                     │   │\n",
    "│  │ df['email']: ['user@x.com', 'admin@y.org', ...]          │   │\n",
    "│  │ df['status']: ['active', 'active', 'inactive', ...]      │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│        ↓                                                         │\n",
    "│  1️⃣ PROFILE DATA (EDA automático)                              │\n",
    "│     ┌────────────────────────────────────────────────────┐      │\n",
    "│     │ pandas-profiling / ydata-profiling                 │      │\n",
    "│     │ • Tipos de datos (int, str, datetime)             │      │\n",
    "│     │ • Distribuciones (mean, std, quantiles)           │      │\n",
    "│     │ • Valores únicos y frecuencias                    │      │\n",
    "│     │ • Nulls, duplicates, outliers                     │      │\n",
    "│     │ • Correlaciones entre columnas                    │      │\n",
    "│     └────────────────────────────────────────────────────┘      │\n",
    "│        ↓                                                         │\n",
    "│  2️⃣ INFER RULES (LLM analiza profile)                          │\n",
    "│     ┌────────────────────────────────────────────────────┐      │\n",
    "│     │ Prompt: \"Basándote en estas estadísticas,         │      │\n",
    "│     │          genera reglas de validación...\"           │      │\n",
    "│     │                                                    │      │\n",
    "│     │ LLM → Reglas inferidas:                           │      │\n",
    "│     │  age:                                              │      │\n",
    "│     │    - No NULL (0% nulls observed)                  │      │\n",
    "│     │    - Range [18, 65] (min=18, max=65, no outliers)│      │\n",
    "│     │    - Integer type                                 │      │\n",
    "│     │                                                    │      │\n",
    "│     │  email:                                            │      │\n",
    "│     │    - Match regex ^[\\w.-]+@[\\w.-]+\\.\\w+$          │      │\n",
    "│     │    - No duplicates (100% unique)                  │      │\n",
    "│     │    - Max length 100 chars                         │      │\n",
    "│     │                                                    │      │\n",
    "│     │  status:                                           │      │\n",
    "│     │    - IN ['active', 'inactive', 'pending']         │      │\n",
    "│     │    - No NULL (0% nulls)                           │      │\n",
    "│     └────────────────────────────────────────────────────┘      │\n",
    "│        ↓                                                         │\n",
    "│  3️⃣ GENERATE CODE (Target framework)                           │\n",
    "│     ┌────────────────────────────────────────────────────┐      │\n",
    "│     │ Great Expectations:                                │      │\n",
    "│     │ ```python                                          │      │\n",
    "│     │ suite.expect_column_values_to_not_be_null('age')  │      │\n",
    "│     │ suite.expect_column_values_to_be_between(         │      │\n",
    "│     │     'age', min_value=18, max_value=65)            │      │\n",
    "│     │ suite.expect_column_values_to_match_regex(        │      │\n",
    "│     │     'email', regex='^[\\w.-]+@[\\w.-]+\\.\\w+$')     │      │\n",
    "│     │ ```                                                │      │\n",
    "│     │                                                    │      │\n",
    "│     │ dbt tests:                                         │      │\n",
    "│     │ ```yaml                                            │      │\n",
    "│     │ - name: age                                        │      │\n",
    "│     │   tests:                                           │      │\n",
    "│     │     - not_null                                     │      │\n",
    "│     │     - dbt_utils.accepted_range:                   │      │\n",
    "│     │         min_value: 18                              │      │\n",
    "│     │         max_value: 65                              │      │\n",
    "│     │ ```                                                │      │\n",
    "│     └────────────────────────────────────────────────────┘      │\n",
    "│        ↓                                                         │\n",
    "│  4️⃣ VALIDATE & REFINE                                          │\n",
    "│     - Ejecutar reglas contra datos nuevos                       │\n",
    "│     - Si >5% false positives → Ajustar thresholds               │\n",
    "│     - Human review de reglas generadas                          │\n",
    "│        ↓                                                         │\n",
    "│  OUTPUT: Validation suite lista para producción                 │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 🔧 Implementación Completa\n",
    "\n",
    "```python\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# 1. DATA PROFILER\n",
    "\n",
    "class ColumnProfile(BaseModel):\n",
    "    \"\"\"Profile estadístico de una columna.\"\"\"\n",
    "    name: str\n",
    "    dtype: str\n",
    "    count: int\n",
    "    null_count: int\n",
    "    null_percentage: float\n",
    "    unique_count: int\n",
    "    unique_percentage: float\n",
    "    \n",
    "    # Numérico\n",
    "    mean: Optional[float] = None\n",
    "    std: Optional[float] = None\n",
    "    min: Optional[float] = None\n",
    "    max: Optional[float] = None\n",
    "    q25: Optional[float] = None\n",
    "    q50: Optional[float] = None\n",
    "    q75: Optional[float] = None\n",
    "    \n",
    "    # String\n",
    "    avg_length: Optional[float] = None\n",
    "    max_length: Optional[int] = None\n",
    "    \n",
    "    # Categorical\n",
    "    top_values: Optional[Dict[str, int]] = None\n",
    "    \n",
    "    # Ejemplos\n",
    "    sample_values: List[str] = []\n",
    "\n",
    "class DataProfiler:\n",
    "    \"\"\"Genera profile estadístico de DataFrame.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_column(df: pd.DataFrame, column: str) -> ColumnProfile:\n",
    "        \"\"\"Genera profile de una columna.\"\"\"\n",
    "        col_data = df[column]\n",
    "        \n",
    "        profile = ColumnProfile(\n",
    "            name=column,\n",
    "            dtype=str(col_data.dtype),\n",
    "            count=len(col_data),\n",
    "            null_count=int(col_data.isnull().sum()),\n",
    "            null_percentage=float(col_data.isnull().mean()),\n",
    "            unique_count=int(col_data.nunique()),\n",
    "            unique_percentage=float(col_data.nunique() / len(col_data)),\n",
    "            sample_values=[str(v) for v in col_data.dropna().head(5).tolist()]\n",
    "        )\n",
    "        \n",
    "        # Stats numéricos\n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            profile.mean = float(col_data.mean())\n",
    "            profile.std = float(col_data.std())\n",
    "            profile.min = float(col_data.min())\n",
    "            profile.max = float(col_data.max())\n",
    "            profile.q25 = float(col_data.quantile(0.25))\n",
    "            profile.q50 = float(col_data.quantile(0.50))\n",
    "            profile.q75 = float(col_data.quantile(0.75))\n",
    "        \n",
    "        # Stats string\n",
    "        if pd.api.types.is_string_dtype(col_data) or col_data.dtype == 'object':\n",
    "            str_lengths = col_data.dropna().astype(str).str.len()\n",
    "            if len(str_lengths) > 0:\n",
    "                profile.avg_length = float(str_lengths.mean())\n",
    "                profile.max_length = int(str_lengths.max())\n",
    "        \n",
    "        # Top values (para categóricas)\n",
    "        if profile.unique_count <= 50:  # Considerar categórica si <50 únicos\n",
    "            value_counts = col_data.value_counts().head(10)\n",
    "            profile.top_values = {str(k): int(v) for k, v in value_counts.items()}\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_dataframe(df: pd.DataFrame) -> Dict[str, ColumnProfile]:\n",
    "        \"\"\"Genera profiles de todas las columnas.\"\"\"\n",
    "        return {\n",
    "            col: DataProfiler.profile_column(df, col) \n",
    "            for col in df.columns\n",
    "        }\n",
    "\n",
    "# 2. RULE GENERATOR\n",
    "\n",
    "class ValidationRule(BaseModel):\n",
    "    \"\"\"Regla de validación generada.\"\"\"\n",
    "    column: str\n",
    "    rule_type: str  # not_null, range, regex, in_set, unique, etc.\n",
    "    parameters: Dict\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    great_expectations_code: str\n",
    "    dbt_test_yaml: str\n",
    "    pandas_assertion: str\n",
    "\n",
    "class RuleGenerator:\n",
    "    \"\"\"Genera reglas de validación usando LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def generate_rules(\n",
    "        self, \n",
    "        profile: ColumnProfile,\n",
    "        business_context: Optional[str] = None\n",
    "    ) -> List[ValidationRule]:\n",
    "        \"\"\"\n",
    "        Genera reglas de validación para una columna.\n",
    "        \n",
    "        Args:\n",
    "            profile: profile estadístico de la columna\n",
    "            business_context: contexto de negocio opcional\n",
    "        \n",
    "        Returns:\n",
    "            Lista de reglas de validación\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Analiza este profile de columna y genera reglas de validación apropiadas:\n",
    "\n",
    "COLUMNA: {profile.name}\n",
    "TIPO: {profile.dtype}\n",
    "REGISTROS: {profile.count}\n",
    "NULLS: {profile.null_count} ({profile.null_percentage:.1%})\n",
    "ÚNICOS: {profile.unique_count} ({profile.unique_percentage:.1%})\n",
    "\n",
    "{\"ESTADÍSTICAS NUMÉRICAS:\" if profile.mean is not None else \"\"}\n",
    "{f\"- Mean: {profile.mean:.2f}\" if profile.mean is not None else \"\"}\n",
    "{f\"- Std: {profile.std:.2f}\" if profile.std is not None else \"\"}\n",
    "{f\"- Range: [{profile.min}, {profile.max}]\" if profile.min is not None else \"\"}\n",
    "{f\"- Q25/Q50/Q75: {profile.q25:.2f}/{profile.q50:.2f}/{profile.q75:.2f}\" if profile.q25 is not None else \"\"}\n",
    "\n",
    "{\"ESTADÍSTICAS STRING:\" if profile.avg_length is not None else \"\"}\n",
    "{f\"- Avg length: {profile.avg_length:.1f}\" if profile.avg_length is not None else \"\"}\n",
    "{f\"- Max length: {profile.max_length}\" if profile.max_length is not None else \"\"}\n",
    "\n",
    "{\"TOP VALUES:\" if profile.top_values else \"\"}\n",
    "{json.dumps(profile.top_values, indent=2) if profile.top_values else \"\"}\n",
    "\n",
    "EJEMPLOS: {profile.sample_values}\n",
    "\n",
    "{f\"CONTEXTO DE NEGOCIO: {business_context}\" if business_context else \"\"}\n",
    "\n",
    "Genera 2-5 reglas de validación apropiadas. Para cada regla, incluye:\n",
    "1. Tipo de regla (not_null, range, regex, in_set, unique, etc.)\n",
    "2. Parámetros de la regla\n",
    "3. Confianza (0.0-1.0) basada en qué tan claro es el patrón\n",
    "4. Razonamiento (por qué esta regla tiene sentido)\n",
    "5. Código de Great Expectations\n",
    "6. dbt test en YAML\n",
    "7. Assertion de Pandas\n",
    "\n",
    "Responde en JSON:\n",
    "{{\n",
    "  \"rules\": [\n",
    "    {{\n",
    "      \"rule_type\": \"...\",\n",
    "      \"parameters\": {{}},\n",
    "      \"confidence\": 0.0-1.0,\n",
    "      \"reasoning\": \"...\",\n",
    "      \"great_expectations_code\": \"suite.expect_...\",\n",
    "      \"dbt_test_yaml\": \"- name: ...\\\\n  tests: ...\",\n",
    "      \"pandas_assertion\": \"assert ...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.2,  # Algo de creatividad pero no mucho\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            rules = []\n",
    "            for rule_data in result.get(\"rules\", []):\n",
    "                rules.append(ValidationRule(\n",
    "                    column=profile.name,\n",
    "                    rule_type=rule_data[\"rule_type\"],\n",
    "                    parameters=rule_data[\"parameters\"],\n",
    "                    confidence=rule_data[\"confidence\"],\n",
    "                    reasoning=rule_data[\"reasoning\"],\n",
    "                    great_expectations_code=rule_data[\"great_expectations_code\"],\n",
    "                    dbt_test_yaml=rule_data[\"dbt_test_yaml\"],\n",
    "                    pandas_assertion=rule_data[\"pandas_assertion\"]\n",
    "                ))\n",
    "            \n",
    "            return rules\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando reglas: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_table_validation_suite(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        table_name: str,\n",
    "        business_context: Optional[Dict[str, str]] = None\n",
    "    ) -> Dict[str, List[ValidationRule]]:\n",
    "        \"\"\"\n",
    "        Genera suite completo de validación para tabla.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame a analizar\n",
    "            table_name: nombre de la tabla\n",
    "            business_context: diccionario {columna: contexto}\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario {columna: [reglas]}\n",
    "        \"\"\"\n",
    "        print(f\"🔍 Generando validation suite para tabla '{table_name}'...\")\n",
    "        \n",
    "        # Profile todas las columnas\n",
    "        profiler = DataProfiler()\n",
    "        profiles = profiler.profile_dataframe(df)\n",
    "        \n",
    "        # Generar reglas para cada columna\n",
    "        all_rules = {}\n",
    "        business_context = business_context or {}\n",
    "        \n",
    "        for column, profile in profiles.items():\n",
    "            print(f\"  Analizando columna '{column}'...\")\n",
    "            \n",
    "            context = business_context.get(column, \"\")\n",
    "            rules = self.generate_rules(profile, context)\n",
    "            \n",
    "            all_rules[column] = rules\n",
    "            print(f\"    ✓ {len(rules)} reglas generadas\")\n",
    "        \n",
    "        return all_rules\n",
    "\n",
    "# 3. GENERADOR DE CÓDIGO EJECUTABLE\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Genera código ejecutable de frameworks populares.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_great_expectations_suite(\n",
    "        rules_by_column: Dict[str, List[ValidationRule]],\n",
    "        suite_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"Genera código de Great Expectations.\"\"\"\n",
    "        code = f'''\"\"\"\n",
    "Auto-generated Great Expectations suite: {suite_name}\n",
    "Generated by LLM-powered rule generator\n",
    "\"\"\"\n",
    "\n",
    "import great_expectations as gx\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "# Create expectation suite\n",
    "context = gx.get_context()\n",
    "suite = context.add_expectation_suite(\n",
    "    expectation_suite_name=\"{suite_name}\",\n",
    "    overwrite_existing=True\n",
    ")\n",
    "\n",
    "# Expectations by column\n",
    "'''\n",
    "        \n",
    "        for column, rules in rules_by_column.items():\n",
    "            code += f\"\\n# {column}\\n\"\n",
    "            for rule in rules:\n",
    "                code += f\"# Confidence: {rule.confidence:.0%} - {rule.reasoning}\\n\"\n",
    "                code += f\"{rule.great_expectations_code}\\n\"\n",
    "        \n",
    "        code += '''\n",
    "# Save suite\n",
    "context.add_or_update_expectation_suite(expectation_suite=suite)\n",
    "\n",
    "print(f\"✓ Suite '{suite_name}' created with {len(suite.expectations)} expectations\")\n",
    "'''\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_dbt_schema_yml(\n",
    "        rules_by_column: Dict[str, List[ValidationRule]],\n",
    "        model_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"Genera archivo schema.yml de dbt.\"\"\"\n",
    "        yml = f'''version: 2\n",
    "\n",
    "models:\n",
    "  - name: {model_name}\n",
    "    description: \"Auto-generated dbt tests\"\n",
    "    columns:\n",
    "'''\n",
    "        \n",
    "        for column, rules in rules_by_column.items():\n",
    "            yml += f\"      - name: {column}\\n\"\n",
    "            yml += \"        tests:\\n\"\n",
    "            \n",
    "            for rule in rules:\n",
    "                # Parsear YAML del rule (simplificado)\n",
    "                yml += f\"          # Confidence: {rule.confidence:.0%} - {rule.reasoning}\\n\"\n",
    "                yml += f\"          {rule.dbt_test_yaml}\\n\"\n",
    "        \n",
    "        return yml\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pandas_validation_script(\n",
    "        rules_by_column: Dict[str, List[ValidationRule]],\n",
    "        script_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"Genera script de validación con Pandas.\"\"\"\n",
    "        code = f'''\"\"\"\n",
    "Auto-generated Pandas validation script: {script_name}\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def validate_dataframe(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Valida DataFrame contra reglas generadas.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "'''\n",
    "        \n",
    "        for column, rules in rules_by_column.items():\n",
    "            code += f\"    # Validar columna '{column}'\\n\"\n",
    "            for rule in rules:\n",
    "                code += f\"    # {rule.reasoning} (confidence: {rule.confidence:.0%})\\n\"\n",
    "                code += f\"    try:\\n\"\n",
    "                code += f\"        {rule.pandas_assertion}\\n\"\n",
    "                code += f\"    except AssertionError as e:\\n\"\n",
    "                code += f\"        errors.append('{column}: {{e}}')\\n\"\n",
    "                code += f\"\\n\"\n",
    "        \n",
    "        code += '''    \n",
    "    if errors:\n",
    "        print(f\"❌ Validation failed with {len(errors)} errors:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✅ All validations passed!\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargar datos\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    \n",
    "    # Validar\n",
    "    success = validate_dataframe(df)\n",
    "    \n",
    "    sys.exit(0 if success else 1)\n",
    "'''\n",
    "        \n",
    "        return code\n",
    "\n",
    "# EJEMPLO COMPLETO\n",
    "\n",
    "# Dataset de ejemplo\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': range(1, 101),\n",
    "    'age': np.random.randint(18, 70, size=100),\n",
    "    'email': [f'user{i}@example.com' for i in range(100)],\n",
    "    'status': np.random.choice(['active', 'inactive', 'pending'], size=100),\n",
    "    'account_balance': np.random.uniform(-100, 10000, size=100),\n",
    "    'registration_date': pd.date_range('2020-01-01', periods=100)\n",
    "})\n",
    "\n",
    "# Contexto de negocio\n",
    "business_context = {\n",
    "    'customer_id': 'Identificador único de cliente, auto-incremental',\n",
    "    'age': 'Edad del cliente, debe ser adulto (18+)',\n",
    "    'email': 'Email de contacto, debe ser único y válido',\n",
    "    'status': 'Estado de la cuenta del cliente',\n",
    "    'account_balance': 'Balance de cuenta, puede ser negativo (deuda)',\n",
    "    'registration_date': 'Fecha de registro del cliente en la plataforma'\n",
    "}\n",
    "\n",
    "# Generar reglas\n",
    "generator = RuleGenerator()\n",
    "rules_by_column = generator.generate_table_validation_suite(\n",
    "    df_customers,\n",
    "    'customers',\n",
    "    business_context\n",
    ")\n",
    "\n",
    "# Mostrar reglas generadas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGLAS DE VALIDACIÓN GENERADAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for column, rules in rules_by_column.items():\n",
    "    print(f\"\\n📊 {column}\")\n",
    "    for i, rule in enumerate(rules, 1):\n",
    "        print(f\"\\n  {i}. {rule.rule_type.upper()}\")\n",
    "        print(f\"     Confianza: {rule.confidence:.0%}\")\n",
    "        print(f\"     Parámetros: {rule.parameters}\")\n",
    "        print(f\"     Razonamiento: {rule.reasoning}\")\n",
    "        print(f\"     GE: {rule.great_expectations_code[:80]}...\")\n",
    "\n",
    "# Generar código\n",
    "code_gen = CodeGenerator()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CÓDIGO GENERADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Great Expectations\n",
    "ge_code = code_gen.generate_great_expectations_suite(rules_by_column, 'customers_validation')\n",
    "print(\"\\n1️⃣ GREAT EXPECTATIONS:\")\n",
    "print(ge_code[:500] + \"...\\n\")\n",
    "\n",
    "# dbt\n",
    "dbt_yml = code_gen.generate_dbt_schema_yml(rules_by_column, 'stg_customers')\n",
    "print(\"\\n2️⃣ DBT SCHEMA.YML:\")\n",
    "print(dbt_yml[:500] + \"...\\n\")\n",
    "\n",
    "# Pandas\n",
    "pandas_script = code_gen.generate_pandas_validation_script(rules_by_column, 'validate_customers')\n",
    "print(\"\\n3️⃣ PANDAS SCRIPT:\")\n",
    "print(pandas_script[:500] + \"...\")\n",
    "```\n",
    "\n",
    "### 🔄 Refinamiento Iterativo de Reglas\n",
    "\n",
    "```python\n",
    "class RuleRefiner:\n",
    "    \"\"\"Refina reglas basándose en false positives/negatives.\"\"\"\n",
    "    \n",
    "    def __init__(self, generator: RuleGenerator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def evaluate_rule_quality(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        rule: ValidationRule,\n",
    "        ground_truth_issues: Optional[pd.Series] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evalúa calidad de una regla ejecutándola contra datos.\n",
    "        \n",
    "        Args:\n",
    "            df: datos a validar\n",
    "            rule: regla a evaluar\n",
    "            ground_truth_issues: máscara booleana de registros con issues reales\n",
    "        \n",
    "        Returns:\n",
    "            Métricas de calidad (precision, recall, false positive rate)\n",
    "        \"\"\"\n",
    "        # Ejecutar regla (simplificado)\n",
    "        try:\n",
    "            exec(rule.pandas_assertion, {\"df\": df})\n",
    "            violations = pd.Series([False] * len(df))\n",
    "        except AssertionError:\n",
    "            # Regla falló, detectar qué registros violaron\n",
    "            # (implementación real requiere parsear assertion)\n",
    "            violations = pd.Series([True] * len(df))\n",
    "        \n",
    "        if ground_truth_issues is None:\n",
    "            # Sin ground truth, solo reportar violations\n",
    "            return {\n",
    "                \"violations\": violations.sum(),\n",
    "                \"violation_rate\": violations.mean(),\n",
    "                \"precision\": None,\n",
    "                \"recall\": None\n",
    "            }\n",
    "        \n",
    "        # Con ground truth, calcular métricas\n",
    "        true_positives = (violations & ground_truth_issues).sum()\n",
    "        false_positives = (violations & ~ground_truth_issues).sum()\n",
    "        false_negatives = (~violations & ground_truth_issues).sum()\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"violations\": int(violations.sum()),\n",
    "            \"violation_rate\": float(violations.mean()),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"false_positive_rate\": false_positives / len(df)\n",
    "        }\n",
    "    \n",
    "    def refine_rule(\n",
    "        self,\n",
    "        rule: ValidationRule,\n",
    "        quality_metrics: Dict,\n",
    "        df: pd.DataFrame\n",
    "    ) -> ValidationRule:\n",
    "        \"\"\"Refina regla si false positive rate >5%.\"\"\"\n",
    "        \n",
    "        if quality_metrics[\"false_positive_rate\"] > 0.05:\n",
    "            print(f\"⚠️  High false positive rate ({quality_metrics['false_positive_rate']:.1%}), refining rule...\")\n",
    "            \n",
    "            # LLM sugiere ajustes\n",
    "            prompt = f\"\"\"Esta regla de validación tiene demasiados false positives:\n",
    "\n",
    "Regla: {rule.rule_type}\n",
    "Parámetros: {rule.parameters}\n",
    "Razonamiento: {rule.reasoning}\n",
    "\n",
    "False positive rate: {quality_metrics['false_positive_rate']:.1%}\n",
    "\n",
    "Analiza el profile actualizado y sugiere ajustes a los parámetros para reducir false positives.\n",
    "Mantén el mismo rule_type pero ajusta thresholds, ranges, o patterns.\n",
    "\n",
    "Responde en JSON con regla refinada.\"\"\"\n",
    "            \n",
    "            # (implementación LLM call omitida por brevedad)\n",
    "            \n",
    "        return rule\n",
    "```\n",
    "\n",
    "### 📊 Comparación: Manual vs Auto-Generated Rules\n",
    "\n",
    "| Aspecto | Manual | Auto-Generated (LLM) |\n",
    "|---------|--------|---------------------|\n",
    "| **Tiempo** | 2-4 horas por tabla | 5-10 minutos por tabla |\n",
    "| **Cobertura** | 60-70% de columnas | 95-100% de columnas |\n",
    "| **Calidad** | Alta (experto) | Media-Alta (requiere review) |\n",
    "| **Mantenimiento** | Manual al cambiar datos | Re-generar automáticamente |\n",
    "| **Costo** | $150-$300 (tiempo ingeniero) | $0.10-$1.00 (API calls) |\n",
    "| **Consistency** | Variable por ingeniero | Consistente |\n",
    "\n",
    "**ROI**: Auto-generación reduce tiempo **95%** y costo **99%**, pero requiere review humano.\n",
    "\n",
    "### 💡 Mejores Prácticas\n",
    "\n",
    "1. **Siempre revisar reglas generadas**: LLM puede inferir mal, especialmente sin contexto\n",
    "2. **Proveer business context**: Mejora significativamente calidad de reglas\n",
    "3. **Ejecutar en datos históricos**: Validar que reglas no tengan alta tasa de false positives\n",
    "4. **Iterar**: Refinar reglas basándose en feedback de producción\n",
    "5. **Versionar**: Guardar reglas en Git, no re-generar cada vez\n",
    "6. **Combinar con experto**: LLM genera draft, humano refina\n",
    "7. **Monitoring**: Track false positive/negative rates en producción\n",
    "8. **A/B test**: Comparar LLM-generated vs manual rules en métricas de calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3e532",
   "metadata": {},
   "source": [
    "## 🔬 Root Cause Analysis: LLMs como Data Quality Investigators\n",
    "\n",
    "Cuando las validaciones fallan, el desafío **real** no es detectar el problema sino **explicar POR QUÉ ocurrió** y **CÓMO solucionarlo**. Los LLMs actúan como **investigadores expertos** que analizan patrones, correlaciones temporales, y contexto de negocio para identificar causas raíz.\n",
    "\n",
    "### 🕵️ Arquitectura de Root Cause Analysis\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│         ROOT CAUSE ANALYSIS: FROM SYMPTOM TO SOLUTION            │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  SYMPTOM: 500 registros con email = NULL desde 2024-10-15       │\n",
    "│        ↓                                                         │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 1. TEMPORAL ANALYSIS                                     │   │\n",
    "│  │    ¿Cuándo comenzó el problema?                          │   │\n",
    "│  │                                                          │   │\n",
    "│  │    SELECT DATE(created_at), COUNT(*) as nulls            │   │\n",
    "│  │    FROM customers WHERE email IS NULL                    │   │\n",
    "│  │    GROUP BY 1 ORDER BY 1 DESC                            │   │\n",
    "│  │                                                          │   │\n",
    "│  │    Resultado:                                            │   │\n",
    "│  │    2024-10-14: 0 nulls                                   │   │\n",
    "│  │    2024-10-15: 125 nulls  ← SPIKE                       │   │\n",
    "│  │    2024-10-16: 187 nulls                                 │   │\n",
    "│  │    2024-10-17: 188 nulls                                 │   │\n",
    "│  │                                                          │   │\n",
    "│  │    💡 Insight: Problema comenzó 2024-10-15               │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│        ↓                                                         │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 2. CORRELATION ANALYSIS                                  │   │\n",
    "│  │    ¿Qué más cambió ese día?                              │   │\n",
    "│  │                                                          │   │\n",
    "│  │    - Git logs: Deploy de signup API v2.1 a las 14:30    │   │\n",
    "│  │    - Airflow DAG 'ingest_signups' cambió lógica         │   │\n",
    "│  │    - Tráfico aumentó 30% (marketing campaign)           │   │\n",
    "│  │                                                          │   │\n",
    "│  │    💡 Insight: Deploy coincide con inicio de nulls       │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│        ↓                                                         │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 3. PATTERN ANALYSIS                                      │   │\n",
    "│  │    ¿Qué tienen en común los registros afectados?        │   │\n",
    "│  │                                                          │   │\n",
    "│  │    SELECT source, COUNT(*)                               │   │\n",
    "│  │    FROM customers WHERE email IS NULL                    │   │\n",
    "│  │    AND created_at >= '2024-10-15'                        │   │\n",
    "│  │    GROUP BY 1                                            │   │\n",
    "│  │                                                          │   │\n",
    "│  │    Resultado:                                            │   │\n",
    "│  │    mobile_app: 500 nulls  ← TODOS los nulls             │   │\n",
    "│  │    web: 0 nulls                                          │   │\n",
    "│  │                                                          │   │\n",
    "│  │    💡 Insight: Solo afecta signups de mobile app         │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│        ↓                                                         │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ 4. LLM SYNTHESIS                                         │   │\n",
    "│  │    Prompt: \"Analiza estos hallazgos y determina causa\"  │   │\n",
    "│  │                                                          │   │\n",
    "│  │    LLM → ROOT CAUSE:                                     │   │\n",
    "│  │    \"El deploy de signup API v2.1 el 2024-10-15 introdujo│   │\n",
    "│  │     un bug en el endpoint de mobile app donde el campo  │   │\n",
    "│  │     'email' dejó de ser required en el request body.     │   │\n",
    "│  │     El backend ahora acepta signups sin email pero la   │   │\n",
    "│  │     base de datos espera NOT NULL, resultando en NULLs. │   │\n",
    "│  │                                                          │   │\n",
    "│  │     EVIDENCIA:                                           │   │\n",
    "│  │     1. Spike exacto en fecha de deploy                   │   │\n",
    "│  │     2. Solo afecta mobile app (endpoint modificado)      │   │\n",
    "│  │     3. Web no afectado (endpoint diferente sin cambios)  │   │\n",
    "│  │                                                          │   │\n",
    "│  │     SOLUCIÓN RECOMENDADA:                                │   │\n",
    "│  │     1. Rollback a API v2.0 (inmediato)                   │   │\n",
    "│  │     2. Fix: Agregar validación 'email required'          │   │\n",
    "│  │     3. Backfill: Contactar 500 usuarios para emails     │   │\n",
    "│  │     4. Prevention: Agregar test e2e para campo required\"│   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│        ↓                                                         │\n",
    "│  SOLUTION: Rollback + Fix + Backfill + Prevention                │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 🔧 Implementación Completa\n",
    "\n",
    "```python\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# 1. MODELOS DE DATOS\n",
    "\n",
    "class TemporalAnomaly(BaseModel):\n",
    "    \"\"\"Anomalía detectada con análisis temporal.\"\"\"\n",
    "    issue_type: str\n",
    "    affected_records: int\n",
    "    first_occurrence: datetime\n",
    "    spike_date: Optional[datetime] = None\n",
    "    trend: str  # \"increasing\", \"decreasing\", \"stable\", \"spike\"\n",
    "    time_series: List[Dict[str, any]]  # [{date, count}]\n",
    "\n",
    "class CorrelatedEvent(BaseModel):\n",
    "    \"\"\"Evento que coincide temporalmente con anomalía.\"\"\"\n",
    "    event_type: str  # \"deployment\", \"schema_change\", \"traffic_spike\", etc.\n",
    "    timestamp: datetime\n",
    "    description: str\n",
    "    confidence: float  # Qué tan probable es que sea la causa\n",
    "\n",
    "class PatternInsight(BaseModel):\n",
    "    \"\"\"Insight sobre patrón en datos afectados.\"\"\"\n",
    "    dimension: str  # Columna analizada (source, region, user_type, etc.)\n",
    "    pattern: str  # Descripción del patrón\n",
    "    affected_segments: Dict[str, int]  # {valor: count}\n",
    "    is_significant: bool\n",
    "\n",
    "class RootCauseHypothesis(BaseModel):\n",
    "    \"\"\"Hipótesis de causa raíz.\"\"\"\n",
    "    rank: int\n",
    "    confidence: float\n",
    "    root_cause: str\n",
    "    evidence: List[str]\n",
    "    recommended_fix: str\n",
    "    prevention_steps: List[str]\n",
    "\n",
    "class RootCauseReport(BaseModel):\n",
    "    \"\"\"Reporte completo de investigación.\"\"\"\n",
    "    issue_description: str\n",
    "    temporal_analysis: TemporalAnomaly\n",
    "    correlated_events: List[CorrelatedEvent]\n",
    "    pattern_insights: List[PatternInsight]\n",
    "    hypotheses: List[RootCauseHypothesis]\n",
    "    recommended_action: str\n",
    "    estimated_impact: str\n",
    "\n",
    "# 2. ROOT CAUSE ANALYZER\n",
    "\n",
    "class RootCauseAnalyzer:\n",
    "    \"\"\"Investigador automático de causas raíz.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_temporal_pattern(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        issue_column: str,\n",
    "        issue_condition: str,\n",
    "        date_column: str = 'created_at'\n",
    "    ) -> TemporalAnomaly:\n",
    "        \"\"\"\n",
    "        Analiza patrón temporal del issue.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame completo\n",
    "            issue_column: columna con el issue\n",
    "            issue_condition: condición para detectar issue (ej. \"email IS NULL\")\n",
    "            date_column: columna de fecha\n",
    "        \"\"\"\n",
    "        # Filtrar registros con issue\n",
    "        issue_mask = eval(f\"df['{issue_column}'].{issue_condition}\")\n",
    "        affected_df = df[issue_mask].copy()\n",
    "        \n",
    "        # Time series de issues por día\n",
    "        affected_df['date'] = pd.to_datetime(affected_df[date_column]).dt.date\n",
    "        daily_counts = affected_df.groupby('date').size().reset_index(name='count')\n",
    "        \n",
    "        # Detectar spike\n",
    "        if len(daily_counts) > 1:\n",
    "            mean_count = daily_counts['count'].mean()\n",
    "            std_count = daily_counts['count'].std()\n",
    "            spike_threshold = mean_count + (2 * std_count)\n",
    "            \n",
    "            spikes = daily_counts[daily_counts['count'] > spike_threshold]\n",
    "            spike_date = spikes.iloc[0]['date'] if len(spikes) > 0 else None\n",
    "        else:\n",
    "            spike_date = None\n",
    "        \n",
    "        # Determinar trend\n",
    "        if len(daily_counts) >= 3:\n",
    "            recent_trend = daily_counts['count'].tail(3).diff().mean()\n",
    "            if recent_trend > daily_counts['count'].std():\n",
    "                trend = \"increasing\"\n",
    "            elif recent_trend < -daily_counts['count'].std():\n",
    "                trend = \"decreasing\"\n",
    "            else:\n",
    "                trend = \"stable\"\n",
    "        else:\n",
    "            trend = \"spike\" if spike_date else \"stable\"\n",
    "        \n",
    "        return TemporalAnomaly(\n",
    "            issue_type=issue_condition,\n",
    "            affected_records=len(affected_df),\n",
    "            first_occurrence=datetime.combine(daily_counts.iloc[0]['date'], datetime.min.time()),\n",
    "            spike_date=datetime.combine(spike_date, datetime.min.time()) if spike_date else None,\n",
    "            trend=trend,\n",
    "            time_series=[\n",
    "                {\"date\": str(row['date']), \"count\": int(row['count'])}\n",
    "                for _, row in daily_counts.iterrows()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def find_correlated_events(\n",
    "        self,\n",
    "        anomaly: TemporalAnomaly,\n",
    "        event_log: List[Dict]\n",
    "    ) -> List[CorrelatedEvent]:\n",
    "        \"\"\"\n",
    "        Encuentra eventos que coinciden temporalmente con anomalía.\n",
    "        \n",
    "        Args:\n",
    "            anomaly: análisis temporal\n",
    "            event_log: lista de eventos [{type, timestamp, description}]\n",
    "        \"\"\"\n",
    "        reference_date = anomaly.spike_date or anomaly.first_occurrence\n",
    "        window_hours = 24  # Ventana de +/- 24 horas\n",
    "        \n",
    "        correlated = []\n",
    "        \n",
    "        for event in event_log:\n",
    "            event_time = datetime.fromisoformat(event['timestamp'])\n",
    "            time_diff = abs((event_time - reference_date).total_seconds() / 3600)\n",
    "            \n",
    "            if time_diff <= window_hours:\n",
    "                # Calcular confidence basado en proximidad temporal\n",
    "                confidence = 1.0 - (time_diff / window_hours)\n",
    "                \n",
    "                correlated.append(CorrelatedEvent(\n",
    "                    event_type=event['type'],\n",
    "                    timestamp=event_time,\n",
    "                    description=event['description'],\n",
    "                    confidence=confidence\n",
    "                ))\n",
    "        \n",
    "        # Ordenar por confidence\n",
    "        return sorted(correlated, key=lambda x: x.confidence, reverse=True)\n",
    "    \n",
    "    def analyze_patterns(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        issue_column: str,\n",
    "        issue_condition: str,\n",
    "        dimensions: List[str]\n",
    "    ) -> List[PatternInsight]:\n",
    "        \"\"\"\n",
    "        Analiza patrones en múltiples dimensiones.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame completo\n",
    "            issue_column: columna con issue\n",
    "            issue_condition: condición de issue\n",
    "            dimensions: columnas a analizar (source, region, etc.)\n",
    "        \"\"\"\n",
    "        issue_mask = eval(f\"df['{issue_column}'].{issue_condition}\")\n",
    "        affected_df = df[issue_mask]\n",
    "        \n",
    "        insights = []\n",
    "        \n",
    "        for dim in dimensions:\n",
    "            if dim not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            # Distribución en datos afectados\n",
    "            affected_dist = affected_df[dim].value_counts().to_dict()\n",
    "            \n",
    "            # Distribución en datos totales\n",
    "            total_dist = df[dim].value_counts().to_dict()\n",
    "            \n",
    "            # Detectar si algún segmento está sobre-representado\n",
    "            total_records = len(df)\n",
    "            significant_segments = {}\n",
    "            \n",
    "            for value, count in affected_dist.items():\n",
    "                expected_ratio = total_dist.get(value, 0) / total_records\n",
    "                actual_ratio = count / len(affected_df)\n",
    "                \n",
    "                # Si ratio es >2x esperado, es significativo\n",
    "                if actual_ratio > expected_ratio * 2:\n",
    "                    significant_segments[str(value)] = count\n",
    "            \n",
    "            is_significant = len(significant_segments) > 0\n",
    "            \n",
    "            if is_significant:\n",
    "                pattern = f\"Segmento(s) sobre-representado(s): {', '.join(significant_segments.keys())}\"\n",
    "            else:\n",
    "                pattern = \"Distribución uniforme entre segmentos\"\n",
    "            \n",
    "            insights.append(PatternInsight(\n",
    "                dimension=dim,\n",
    "                pattern=pattern,\n",
    "                affected_segments={str(k): int(v) for k, v in affected_dist.items()},\n",
    "                is_significant=is_significant\n",
    "            ))\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_hypotheses(\n",
    "        self,\n",
    "        issue_description: str,\n",
    "        temporal: TemporalAnomaly,\n",
    "        events: List[CorrelatedEvent],\n",
    "        patterns: List[PatternInsight]\n",
    "    ) -> List[RootCauseHypothesis]:\n",
    "        \"\"\"\n",
    "        Usa LLM para generar hipótesis de causa raíz.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Actúa como un Data Quality Investigator experto. Analiza estos hallazgos y genera hipótesis de causa raíz:\n",
    "\n",
    "PROBLEMA:\n",
    "{issue_description}\n",
    "\n",
    "ANÁLISIS TEMPORAL:\n",
    "- Registros afectados: {temporal.affected_records}\n",
    "- Primera ocurrencia: {temporal.first_occurrence}\n",
    "- Spike detectado: {temporal.spike_date or \"No\"}\n",
    "- Tendencia: {temporal.trend}\n",
    "- Serie temporal: {json.dumps(temporal.time_series[:7], indent=2)}\n",
    "\n",
    "EVENTOS CORRELACIONADOS:\n",
    "{chr(10).join([f\"- [{e.event_type}] {e.timestamp}: {e.description} (confidence: {e.confidence:.0%})\" for e in events[:5]])}\n",
    "\n",
    "PATRONES IDENTIFICADOS:\n",
    "{chr(10).join([f\"- {p.dimension}: {p.pattern}\" + (f\" (SIGNIFICATIVO)\" if p.is_significant else \"\") for p in patterns])}\n",
    "\n",
    "Genera 2-3 hipótesis de causa raíz, ordenadas por probabilidad. Para cada hipótesis:\n",
    "1. Explicación de la causa raíz\n",
    "2. Evidencia que la soporta\n",
    "3. Fix recomendado\n",
    "4. Pasos de prevención\n",
    "\n",
    "Responde en JSON:\n",
    "{{\n",
    "  \"hypotheses\": [\n",
    "    {{\n",
    "      \"confidence\": 0.0-1.0,\n",
    "      \"root_cause\": \"explicación concisa\",\n",
    "      \"evidence\": [\"evidencia 1\", \"evidencia 2\", ...],\n",
    "      \"recommended_fix\": \"acción inmediata\",\n",
    "      \"prevention_steps\": [\"paso 1\", \"paso 2\", ...]\n",
    "    }}\n",
    "  ],\n",
    "  \"recommended_action\": \"qué hacer AHORA\",\n",
    "  \"estimated_impact\": \"cuántos usuarios/registros/$ afectados\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            hypotheses = []\n",
    "            for i, hyp_data in enumerate(result.get(\"hypotheses\", []), 1):\n",
    "                hypotheses.append(RootCauseHypothesis(\n",
    "                    rank=i,\n",
    "                    confidence=hyp_data[\"confidence\"],\n",
    "                    root_cause=hyp_data[\"root_cause\"],\n",
    "                    evidence=hyp_data[\"evidence\"],\n",
    "                    recommended_fix=hyp_data[\"recommended_fix\"],\n",
    "                    prevention_steps=hyp_data[\"prevention_steps\"]\n",
    "                ))\n",
    "            \n",
    "            return hypotheses, result[\"recommended_action\"], result[\"estimated_impact\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando hipótesis: {e}\")\n",
    "            return [], \"Manual investigation required\", \"Unknown\"\n",
    "    \n",
    "    def investigate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        issue_description: str,\n",
    "        issue_column: str,\n",
    "        issue_condition: str,\n",
    "        event_log: List[Dict],\n",
    "        dimensions_to_analyze: List[str],\n",
    "        date_column: str = 'created_at'\n",
    "    ) -> RootCauseReport:\n",
    "        \"\"\"\n",
    "        Investigación completa de root cause.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con datos\n",
    "            issue_description: descripción del problema\n",
    "            issue_column: columna con issue\n",
    "            issue_condition: condición que define el issue\n",
    "            event_log: log de eventos (deployments, schema changes, etc.)\n",
    "            dimensions_to_analyze: columnas para pattern analysis\n",
    "            date_column: columna de timestamp\n",
    "        \n",
    "        Returns:\n",
    "            Reporte completo de investigación\n",
    "        \"\"\"\n",
    "        print(\"🔍 Iniciando investigación de root cause...\")\n",
    "        \n",
    "        # 1. Análisis temporal\n",
    "        print(\"  1️⃣ Analizando patrón temporal...\")\n",
    "        temporal = self.analyze_temporal_pattern(df, issue_column, issue_condition, date_column)\n",
    "        print(f\"     ✓ {temporal.affected_records} registros afectados, tendencia: {temporal.trend}\")\n",
    "        \n",
    "        # 2. Eventos correlacionados\n",
    "        print(\"  2️⃣ Buscando eventos correlacionados...\")\n",
    "        events = self.find_correlated_events(temporal, event_log)\n",
    "        print(f\"     ✓ {len(events)} eventos encontrados\")\n",
    "        \n",
    "        # 3. Patrones\n",
    "        print(\"  3️⃣ Analizando patrones en dimensiones...\")\n",
    "        patterns = self.analyze_patterns(df, issue_column, issue_condition, dimensions_to_analyze)\n",
    "        significant_patterns = [p for p in patterns if p.is_significant]\n",
    "        print(f\"     ✓ {len(significant_patterns)}/{len(patterns)} patrones significativos\")\n",
    "        \n",
    "        # 4. Generar hipótesis con LLM\n",
    "        print(\"  4️⃣ Generando hipótesis de causa raíz...\")\n",
    "        hypotheses, recommended_action, estimated_impact = self.generate_hypotheses(\n",
    "            issue_description, temporal, events, patterns\n",
    "        )\n",
    "        print(f\"     ✓ {len(hypotheses)} hipótesis generadas\")\n",
    "        \n",
    "        return RootCauseReport(\n",
    "            issue_description=issue_description,\n",
    "            temporal_analysis=temporal,\n",
    "            correlated_events=events,\n",
    "            pattern_insights=patterns,\n",
    "            hypotheses=hypotheses,\n",
    "            recommended_action=recommended_action,\n",
    "            estimated_impact=estimated_impact\n",
    "        )\n",
    "\n",
    "# EJEMPLO DE USO\n",
    "\n",
    "# Dataset simulado\n",
    "np.random.seed(42)\n",
    "\n",
    "dates = pd.date_range('2024-10-01', '2024-10-20', freq='H')\n",
    "n_records = len(dates)\n",
    "\n",
    "# Simular spike de NULLs desde 2024-10-15 solo en mobile_app\n",
    "df_signups = pd.DataFrame({\n",
    "    'user_id': range(n_records),\n",
    "    'created_at': dates,\n",
    "    'email': ['user{}@example.com'.format(i) for i in range(n_records)],\n",
    "    'source': np.random.choice(['web', 'mobile_app'], size=n_records, p=[0.6, 0.4]),\n",
    "    'region': np.random.choice(['US', 'EU', 'ASIA'], size=n_records)\n",
    "})\n",
    "\n",
    "# Introducir NULLs después de 2024-10-15 solo en mobile_app\n",
    "spike_start = pd.Timestamp('2024-10-15')\n",
    "mobile_mask = df_signups['source'] == 'mobile_app'\n",
    "after_spike = df_signups['created_at'] >= spike_start\n",
    "\n",
    "df_signups.loc[mobile_mask & after_spike, 'email'] = None\n",
    "\n",
    "# Event log simulado\n",
    "event_log = [\n",
    "    {\n",
    "        'type': 'deployment',\n",
    "        'timestamp': '2024-10-15T14:30:00',\n",
    "        'description': 'Deployed signup API v2.1 with mobile app endpoint changes'\n",
    "    },\n",
    "    {\n",
    "        'type': 'schema_change',\n",
    "        'timestamp': '2024-10-12T10:00:00',\n",
    "        'description': 'Added index on users.email for performance'\n",
    "    },\n",
    "    {\n",
    "        'type': 'traffic_spike',\n",
    "        'timestamp': '2024-10-15T08:00:00',\n",
    "        'description': 'Marketing campaign launch, traffic +30%'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Investigar\n",
    "analyzer = RootCauseAnalyzer()\n",
    "\n",
    "report = analyzer.investigate(\n",
    "    df=df_signups,\n",
    "    issue_description=\"500+ signups con email NULL desde 2024-10-15\",\n",
    "    issue_column='email',\n",
    "    issue_condition='isnull()',\n",
    "    event_log=event_log,\n",
    "    dimensions_to_analyze=['source', 'region'],\n",
    "    date_column='created_at'\n",
    ")\n",
    "\n",
    "# Mostrar reporte\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📋 ROOT CAUSE ANALYSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔴 PROBLEMA: {report.issue_description}\")\n",
    "print(f\"\\n📊 ANÁLISIS TEMPORAL:\")\n",
    "print(f\"   Afectados: {report.temporal_analysis.affected_records}\")\n",
    "print(f\"   Primera ocurrencia: {report.temporal_analysis.first_occurrence}\")\n",
    "print(f\"   Spike detectado: {report.temporal_analysis.spike_date}\")\n",
    "print(f\"   Tendencia: {report.temporal_analysis.trend}\")\n",
    "\n",
    "print(f\"\\n🔗 EVENTOS CORRELACIONADOS (top 3):\")\n",
    "for event in report.correlated_events[:3]:\n",
    "    print(f\"   [{event.confidence:.0%}] {event.event_type}: {event.description}\")\n",
    "\n",
    "print(f\"\\n🔍 PATRONES SIGNIFICATIVOS:\")\n",
    "for pattern in report.pattern_insights:\n",
    "    if pattern.is_significant:\n",
    "        print(f\"   {pattern.dimension}: {pattern.pattern}\")\n",
    "        print(f\"      Segmentos afectados: {pattern.affected_segments}\")\n",
    "\n",
    "print(f\"\\n💡 HIPÓTESIS DE CAUSA RAÍZ:\")\n",
    "for hyp in report.hypotheses:\n",
    "    print(f\"\\n   {hyp.rank}. [{hyp.confidence:.0%}] {hyp.root_cause}\")\n",
    "    print(f\"      Evidencia:\")\n",
    "    for evidence in hyp.evidence:\n",
    "        print(f\"        - {evidence}\")\n",
    "    print(f\"      Fix recomendado: {hyp.recommended_fix}\")\n",
    "    print(f\"      Prevención:\")\n",
    "    for step in hyp.prevention_steps:\n",
    "        print(f\"        - {step}\")\n",
    "\n",
    "print(f\"\\n🚀 ACCIÓN RECOMENDADA:\")\n",
    "print(f\"   {report.recommended_action}\")\n",
    "\n",
    "print(f\"\\n📈 IMPACTO ESTIMADO:\")\n",
    "print(f\"   {report.estimated_impact}\")\n",
    "```\n",
    "\n",
    "### 📊 Métricas de Éxito en Root Cause Analysis\n",
    "\n",
    "```python\n",
    "class RCAMetrics:\n",
    "    \"\"\"Métricas para evaluar calidad de RCA.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_time_to_root_cause(\n",
    "        issue_detected_at: datetime,\n",
    "        root_cause_identified_at: datetime\n",
    "    ) -> float:\n",
    "        \"\"\"Tiempo desde detección hasta identificación de causa.\"\"\"\n",
    "        return (root_cause_identified_at - issue_detected_at).total_seconds() / 3600  # horas\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_hypothesis_accuracy(\n",
    "        predicted_cause: str,\n",
    "        actual_cause: str,\n",
    "        llm_model: str = \"gpt-4o\"\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evalúa similitud semántica entre causa predicha y real.\n",
    "        Retorna score 0.0-1.0.\n",
    "        \"\"\"\n",
    "        # Usar embeddings para similitud semántica\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        \n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=[predicted_cause, actual_cause]\n",
    "        )\n",
    "        \n",
    "        emb1 = np.array(response.data[0].embedding)\n",
    "        emb2 = np.array(response.data[1].embedding)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        \n",
    "        return float(similarity)\n",
    "\n",
    "# Ejemplo de medición\n",
    "metrics = RCAMetrics()\n",
    "\n",
    "# Manual investigation: 4 horas\n",
    "manual_time = 4.0\n",
    "\n",
    "# LLM-powered: 15 minutos\n",
    "llm_time = 0.25\n",
    "\n",
    "print(f\"⚡ Speedup: {manual_time / llm_time:.0f}x más rápido\")\n",
    "print(f\"💰 Ahorro: ${manual_time * 75:.0f} (@ $75/hora ingeniero) vs ${0.50:.2f} (API calls)\")\n",
    "```\n",
    "\n",
    "### 💡 Mejores Prácticas\n",
    "\n",
    "1. **Siempre investigar**: No asumir causa, seguir proceso sistemático\n",
    "2. **Múltiples hipótesis**: LLM debe generar 2-3 opciones, no solo una\n",
    "3. **Evidencia cuantitativa**: Basar hipótesis en datos, no suposiciones\n",
    "4. **Timeline crítico**: Marcar eventos importantes (deploys, schema changes)\n",
    "5. **Documentar**: Guardar investigaciones para aprendizaje futuro\n",
    "6. **Validar hipótesis**: Confirmar causa raíz antes de aplicar fix masivo\n",
    "7. **Post-mortem automático**: Generar documento de incident con LLM\n",
    "8. **Learn from patterns**: Entrenar modelo en investigaciones previas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0264d",
   "metadata": {},
   "source": [
    "## 🏭 LLM-Powered Data Quality en Producción\n",
    "\n",
    "Implementar LLMs para Data Quality en producción requiere arquitectura robusta, monitoring exhaustivo, y estrategias para controlar costos y latencia. Aquí exploramos patrones de producción reales.\n",
    "\n",
    "### 🏗️ Arquitectura de Data Quality Platform con LLMs\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│         DATA QUALITY PLATFORM: HYBRID ARCHITECTURE               │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ INGESTION LAYER                                          │   │\n",
    "│  │  • Kafka / Kinesis streams                               │   │\n",
    "│  │  • Airflow DAGs                                          │   │\n",
    "│  │  • dbt models                                            │   │\n",
    "│  └────────────────┬─────────────────────────────────────────┘   │\n",
    "│                   ↓                                              │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ VALIDATION LAYER (Multi-Stage)                           │   │\n",
    "│  │                                                          │   │\n",
    "│  │  Stage 1: TRADITIONAL (100% traffic, <10ms)             │   │\n",
    "│  │  ┌────────────────────────────────────────────────┐     │   │\n",
    "│  │  │ • NULL checks (Pandas)                         │     │   │\n",
    "│  │  │ • Range validation (SQL)                       │     │   │\n",
    "│  │  │ • Regex patterns (Python re)                   │     │   │\n",
    "│  │  │ • Referential integrity (JOIN queries)         │     │   │\n",
    "│  │  │                                                │     │   │\n",
    "│  │  │ ✅ Pass → Continue                             │     │   │\n",
    "│  │  │ ❌ Fail → Block + Alert                        │     │   │\n",
    "│  │  └────────────────────────────────────────────────┘     │   │\n",
    "│  │                   ↓                                      │   │\n",
    "│  │  Stage 2: LLM SEMANTIC (1% sample, ~500ms)              │   │\n",
    "│  │  ┌────────────────────────────────────────────────┐     │   │\n",
    "│  │  │ • Semantic NULL detection                      │     │   │\n",
    "│  │  │ • Plausibility checks                          │     │   │\n",
    "│  │  │ • PII detection                                │     │   │\n",
    "│  │  │ • Anomaly explanation                          │     │   │\n",
    "│  │  │                                                │     │   │\n",
    "│  │  │ Cache Layer (Redis):                           │     │   │\n",
    "│  │  │  key: hash(value + context)                    │     │   │\n",
    "│  │  │  value: {is_valid, confidence, reasoning}      │     │   │\n",
    "│  │  │  TTL: 7 days                                   │     │   │\n",
    "│  │  │                                                │     │   │\n",
    "│  │  │ ⚠️  Issues → Queue for investigation           │     │   │\n",
    "│  │  └────────────────────────────────────────────────┘     │   │\n",
    "│  │                   ↓                                      │   │\n",
    "│  │  Stage 3: ROOT CAUSE ANALYSIS (on failures)             │   │\n",
    "│  │  ┌────────────────────────────────────────────────┐     │   │\n",
    "│  │  │ • Triggered solo si >threshold failures        │     │   │\n",
    "│  │  │ • Analiza temporal patterns                    │     │   │\n",
    "│  │  │ • Correlaciona con eventos (deploys, etc.)     │     │   │\n",
    "│  │  │ • Genera hipótesis con LLM                     │     │   │\n",
    "│  │  │ • Crea ticket automático (Jira)               │     │   │\n",
    "│  │  └────────────────────────────────────────────────┘     │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│                   ↓                                              │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ MONITORING & ALERTING                                    │   │\n",
    "│  │  • Prometheus metrics                                    │   │\n",
    "│  │  • Grafana dashboards                                    │   │\n",
    "│  │  • PagerDuty/Slack alerts                                │   │\n",
    "│  │  • Cost tracking (LLM API usage)                         │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│                   ↓                                              │\n",
    "│  ┌──────────────────────────────────────────────────────────┐   │\n",
    "│  │ DATA STORAGE                                             │   │\n",
    "│  │  • Validated data → Data Warehouse (Snowflake)           │   │\n",
    "│  │  • Validation logs → S3 → Athena queries                 │   │\n",
    "│  │  • Failed records → Dead Letter Queue (DLQ)              │   │\n",
    "│  └──────────────────────────────────────────────────────────┘   │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 🔧 Implementación: Production-Grade System\n",
    "\n",
    "```python\n",
    "from typing import Dict, List, Optional\n",
    "import asyncio\n",
    "import redis\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prometheus metrics\n",
    "VALIDATIONS_TOTAL = Counter('dq_validations_total', 'Total validations', ['stage', 'result'])\n",
    "VALIDATION_DURATION = Histogram('dq_validation_duration_seconds', 'Validation duration', ['stage'])\n",
    "LLM_API_COST = Gauge('dq_llm_api_cost_usd', 'LLM API cost')\n",
    "CACHE_HIT_RATE = Gauge('dq_cache_hit_rate', 'Cache hit rate')\n",
    "\n",
    "# 1. CACHE LAYER\n",
    "\n",
    "class ValidationCache:\n",
    "    \"\"\"Redis-backed cache para validaciones LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n",
    "        self.redis_client = redis.from_url(redis_url, decode_responses=True)\n",
    "        self.ttl_seconds = 7 * 24 * 3600  # 7 días\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _get_cache_key(self, value: str, context: str, validation_type: str) -> str:\n",
    "        \"\"\"Genera cache key determinístico.\"\"\"\n",
    "        content = f\"{validation_type}:{context}:{value}\"\n",
    "        return f\"dq:validation:{hashlib.sha256(content.encode()).hexdigest()}\"\n",
    "    \n",
    "    def get(self, value: str, context: str, validation_type: str) -> Optional[Dict]:\n",
    "        \"\"\"Obtiene resultado cacheado.\"\"\"\n",
    "        key = self._get_cache_key(value, context, validation_type)\n",
    "        \n",
    "        cached = self.redis_client.get(key)\n",
    "        \n",
    "        if cached:\n",
    "            self.hits += 1\n",
    "            CACHE_HIT_RATE.set(self.hits / (self.hits + self.misses))\n",
    "            return json.loads(cached)\n",
    "        else:\n",
    "            self.misses += 1\n",
    "            CACHE_HIT_RATE.set(self.hits / (self.hits + self.misses))\n",
    "            return None\n",
    "    \n",
    "    def set(self, value: str, context: str, validation_type: str, result: Dict):\n",
    "        \"\"\"Cachea resultado.\"\"\"\n",
    "        key = self._get_cache_key(value, context, validation_type)\n",
    "        self.redis_client.setex(key, self.ttl_seconds, json.dumps(result))\n",
    "\n",
    "# 2. VALIDATION ORCHESTRATOR\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    \"\"\"Configuración de validación.\"\"\"\n",
    "    traditional_enabled: bool = True\n",
    "    llm_enabled: bool = True\n",
    "    llm_sample_rate: float = 0.01  # 1% de tráfico\n",
    "    llm_max_concurrent: int = 10\n",
    "    llm_timeout_seconds: int = 5\n",
    "    cost_budget_daily_usd: float = 10.0\n",
    "    failure_threshold_for_rca: int = 100  # Trigger RCA si >100 failures\n",
    "\n",
    "class ProductionValidator:\n",
    "    \"\"\"Validador de producción con stages.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ValidationConfig,\n",
    "        cache: ValidationCache,\n",
    "        llm_client: AsyncOpenAI\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.cache = cache\n",
    "        self.llm_client = llm_client\n",
    "        self.daily_cost = 0.0\n",
    "        self.last_cost_reset = datetime.now()\n",
    "    \n",
    "    def _should_use_llm(self) -> bool:\n",
    "        \"\"\"Decide si usar LLM basado en sampling y budget.\"\"\"\n",
    "        # Reset daily cost\n",
    "        if datetime.now() - self.last_cost_reset > timedelta(days=1):\n",
    "            self.daily_cost = 0.0\n",
    "            self.last_cost_reset = datetime.now()\n",
    "        \n",
    "        # Check budget\n",
    "        if self.daily_cost >= self.config.cost_budget_daily_usd:\n",
    "            logger.warning(f\"Daily LLM budget exhausted: ${self.daily_cost:.2f}\")\n",
    "            return False\n",
    "        \n",
    "        # Sample rate\n",
    "        import random\n",
    "        return random.random() < self.config.llm_sample_rate\n",
    "    \n",
    "    async def validate_record(\n",
    "        self,\n",
    "        record: Dict,\n",
    "        validation_rules: Dict[str, Dict]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Valida un registro completo.\n",
    "        \n",
    "        Args:\n",
    "            record: diccionario con datos del registro\n",
    "            validation_rules: reglas por columna\n",
    "                {\n",
    "                    \"column_name\": {\n",
    "                        \"traditional\": {...},\n",
    "                        \"llm\": {\"enabled\": bool, \"context\": str}\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                \"valid\": bool,\n",
    "                \"issues\": List[Dict],\n",
    "                \"stages_executed\": List[str],\n",
    "                \"duration_ms\": float,\n",
    "                \"cost_usd\": float\n",
    "            }\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        issues = []\n",
    "        stages_executed = []\n",
    "        total_cost = 0.0\n",
    "        \n",
    "        # STAGE 1: TRADITIONAL (ALWAYS)\n",
    "        if self.config.traditional_enabled:\n",
    "            with VALIDATION_DURATION.labels(stage='traditional').time():\n",
    "                trad_issues = await self._validate_traditional(record, validation_rules)\n",
    "                issues.extend(trad_issues)\n",
    "                stages_executed.append('traditional')\n",
    "                VALIDATIONS_TOTAL.labels(stage='traditional', result='pass' if len(trad_issues)==0 else 'fail').inc()\n",
    "        \n",
    "        # Si traditional falla críticamente, no continuar\n",
    "        critical_issues = [i for i in issues if i.get('severity') == 'CRITICAL']\n",
    "        if critical_issues:\n",
    "            duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n",
    "            return {\n",
    "                \"valid\": False,\n",
    "                \"issues\": issues,\n",
    "                \"stages_executed\": stages_executed,\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"cost_usd\": 0.0\n",
    "            }\n",
    "        \n",
    "        # STAGE 2: LLM SEMANTIC (SAMPLED)\n",
    "        if self.config.llm_enabled and self._should_use_llm():\n",
    "            with VALIDATION_DURATION.labels(stage='llm_semantic').time():\n",
    "                llm_issues, llm_cost = await self._validate_llm_semantic(record, validation_rules)\n",
    "                issues.extend(llm_issues)\n",
    "                stages_executed.append('llm_semantic')\n",
    "                total_cost += llm_cost\n",
    "                self.daily_cost += llm_cost\n",
    "                LLM_API_COST.set(self.daily_cost)\n",
    "                VALIDATIONS_TOTAL.labels(stage='llm_semantic', result='pass' if len(llm_issues)==0 else 'fail').inc()\n",
    "        \n",
    "        duration_ms = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        \n",
    "        return {\n",
    "            \"valid\": len(issues) == 0,\n",
    "            \"issues\": issues,\n",
    "            \"stages_executed\": stages_executed,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"cost_usd\": total_cost\n",
    "        }\n",
    "    \n",
    "    async def _validate_traditional(\n",
    "        self,\n",
    "        record: Dict,\n",
    "        rules: Dict\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Stage 1: validación tradicional (rápida).\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for column, rule_config in rules.items():\n",
    "            if 'traditional' not in rule_config:\n",
    "                continue\n",
    "            \n",
    "            value = record.get(column)\n",
    "            trad_rules = rule_config['traditional']\n",
    "            \n",
    "            # NULL check\n",
    "            if trad_rules.get('not_null') and value is None:\n",
    "                issues.append({\n",
    "                    \"column\": column,\n",
    "                    \"issue_type\": \"NULL\",\n",
    "                    \"severity\": \"CRITICAL\",\n",
    "                    \"message\": f\"Column '{column}' cannot be NULL\"\n",
    "                })\n",
    "            \n",
    "            # Range check\n",
    "            if 'range' in trad_rules and value is not None:\n",
    "                min_val, max_val = trad_rules['range']\n",
    "                if not (min_val <= value <= max_val):\n",
    "                    issues.append({\n",
    "                        \"column\": column,\n",
    "                        \"issue_type\": \"RANGE\",\n",
    "                        \"severity\": \"ERROR\",\n",
    "                        \"message\": f\"Value {value} outside range [{min_val}, {max_val}]\"\n",
    "                    })\n",
    "            \n",
    "            # Regex check\n",
    "            if 'regex' in trad_rules and value is not None:\n",
    "                import re\n",
    "                if not re.match(trad_rules['regex'], str(value)):\n",
    "                    issues.append({\n",
    "                        \"column\": column,\n",
    "                        \"issue_type\": \"FORMAT\",\n",
    "                        \"severity\": \"ERROR\",\n",
    "                        \"message\": f\"Value '{value}' doesn't match expected format\"\n",
    "                    })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    async def _validate_llm_semantic(\n",
    "        self,\n",
    "        record: Dict,\n",
    "        rules: Dict\n",
    "    ) -> tuple[List[Dict], float]:\n",
    "        \"\"\"Stage 2: validación semántica con LLM (muestreada).\"\"\"\n",
    "        issues = []\n",
    "        total_cost = 0.0\n",
    "        \n",
    "        # Validar columnas con LLM enabled\n",
    "        llm_columns = {\n",
    "            col: config for col, config in rules.items()\n",
    "            if config.get('llm', {}).get('enabled', False)\n",
    "        }\n",
    "        \n",
    "        # Crear tasks concurrentes (con límite)\n",
    "        semaphore = asyncio.Semaphore(self.config.llm_max_concurrent)\n",
    "        \n",
    "        async def validate_column(column: str, config: Dict):\n",
    "            async with semaphore:\n",
    "                value = record.get(column)\n",
    "                if value is None:\n",
    "                    return None, 0.0\n",
    "                \n",
    "                context = config['llm']['context']\n",
    "                \n",
    "                # Check cache\n",
    "                cached = self.cache.get(str(value), context, 'plausibility')\n",
    "                if cached:\n",
    "                    if not cached['is_valid']:\n",
    "                        return {\n",
    "                            \"column\": column,\n",
    "                            \"issue_type\": \"IMPLAUSIBLE\",\n",
    "                            \"severity\": \"WARNING\",\n",
    "                            \"message\": f\"{cached['reasoning']} (cached)\",\n",
    "                            \"confidence\": cached['confidence']\n",
    "                        }, 0.0\n",
    "                    return None, 0.0\n",
    "                \n",
    "                # LLM validation\n",
    "                prompt = f\"\"\"Valida plausibilidad:\n",
    "Column: {column}\n",
    "Context: {context}\n",
    "Value: {value}\n",
    "\n",
    "¿Es válido? Responde JSON: {{\"is_valid\": bool, \"confidence\": 0-1, \"reasoning\": \"...\"}}\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    response = await asyncio.wait_for(\n",
    "                        self.llm_client.chat.completions.create(\n",
    "                            model=\"gpt-4o-mini\",\n",
    "                            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                            temperature=0,\n",
    "                            response_format={\"type\": \"json_object\"}\n",
    "                        ),\n",
    "                        timeout=self.config.llm_timeout_seconds\n",
    "                    )\n",
    "                    \n",
    "                    result = json.loads(response.choices[0].message.content)\n",
    "                    \n",
    "                    # Estimar costo\n",
    "                    cost = (len(prompt) / 4 / 1_000_000 * 0.15) + (len(response.choices[0].message.content) / 4 / 1_000_000 * 0.60)\n",
    "                    \n",
    "                    # Cache result\n",
    "                    self.cache.set(str(value), context, 'plausibility', result)\n",
    "                    \n",
    "                    if not result['is_valid']:\n",
    "                        return {\n",
    "                            \"column\": column,\n",
    "                            \"issue_type\": \"IMPLAUSIBLE\",\n",
    "                            \"severity\": \"WARNING\",\n",
    "                            \"message\": result['reasoning'],\n",
    "                            \"confidence\": result['confidence']\n",
    "                        }, cost\n",
    "                    \n",
    "                    return None, cost\n",
    "                    \n",
    "                except asyncio.TimeoutError:\n",
    "                    logger.warning(f\"LLM timeout for column {column}\")\n",
    "                    return None, 0.0\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"LLM error for column {column}: {e}\")\n",
    "                    return None, 0.0\n",
    "        \n",
    "        # Ejecutar validaciones en paralelo\n",
    "        tasks = [validate_column(col, config) for col, config in llm_columns.items()]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for issue, cost in results:\n",
    "            if issue:\n",
    "                issues.append(issue)\n",
    "            total_cost += cost\n",
    "        \n",
    "        return issues, total_cost\n",
    "\n",
    "# 3. BATCH VALIDATOR (para procesamiento masivo)\n",
    "\n",
    "class BatchValidator:\n",
    "    \"\"\"Validador optimizado para batches grandes.\"\"\"\n",
    "    \n",
    "    def __init__(self, validator: ProductionValidator):\n",
    "        self.validator = validator\n",
    "    \n",
    "    async def validate_batch(\n",
    "        self,\n",
    "        records: List[Dict],\n",
    "        validation_rules: Dict,\n",
    "        parallelism: int = 100\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Valida batch de registros en paralelo.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                \"total_records\": int,\n",
    "                \"valid_records\": int,\n",
    "                \"invalid_records\": int,\n",
    "                \"issues_by_type\": Dict[str, int],\n",
    "                \"total_duration_seconds\": float,\n",
    "                \"total_cost_usd\": float\n",
    "            }\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Semaphore para controlar paralelismo\n",
    "        semaphore = asyncio.Semaphore(parallelism)\n",
    "        \n",
    "        async def validate_with_semaphore(record):\n",
    "            async with semaphore:\n",
    "                return await self.validator.validate_record(record, validation_rules)\n",
    "        \n",
    "        # Validar todos los registros\n",
    "        tasks = [validate_with_semaphore(record) for record in records]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Agregar resultados\n",
    "        valid_count = sum(1 for r in results if r['valid'])\n",
    "        invalid_count = len(results) - valid_count\n",
    "        \n",
    "        issues_by_type = {}\n",
    "        total_cost = 0.0\n",
    "        \n",
    "        for result in results:\n",
    "            total_cost += result['cost_usd']\n",
    "            for issue in result['issues']:\n",
    "                issue_type = issue['issue_type']\n",
    "                issues_by_type[issue_type] = issues_by_type.get(issue_type, 0) + 1\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"total_records\": len(records),\n",
    "            \"valid_records\": valid_count,\n",
    "            \"invalid_records\": invalid_count,\n",
    "            \"issues_by_type\": issues_by_type,\n",
    "            \"total_duration_seconds\": duration,\n",
    "            \"total_cost_usd\": total_cost,\n",
    "            \"throughput_records_per_second\": len(records) / duration\n",
    "        }\n",
    "\n",
    "# EJEMPLO DE USO EN PRODUCCIÓN\n",
    "\n",
    "async def main():\n",
    "    # Setup\n",
    "    cache = ValidationCache()\n",
    "    llm_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    \n",
    "    config = ValidationConfig(\n",
    "        traditional_enabled=True,\n",
    "        llm_enabled=True,\n",
    "        llm_sample_rate=0.01,  # 1% de tráfico\n",
    "        llm_max_concurrent=10,\n",
    "        cost_budget_daily_usd=10.0\n",
    "    )\n",
    "    \n",
    "    validator = ProductionValidator(config, cache, llm_client)\n",
    "    batch_validator = BatchValidator(validator)\n",
    "    \n",
    "    # Reglas de validación\n",
    "    validation_rules = {\n",
    "        'email': {\n",
    "            'traditional': {\n",
    "                'not_null': True,\n",
    "                'regex': r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "            },\n",
    "            'llm': {\n",
    "                'enabled': True,\n",
    "                'context': 'Customer emails (no test data)'\n",
    "            }\n",
    "        },\n",
    "        'age': {\n",
    "            'traditional': {\n",
    "                'not_null': True,\n",
    "                'range': [18, 120]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Dataset de prueba (1000 registros)\n",
    "    records = [\n",
    "        {'email': f'user{i}@example.com', 'age': 25 + (i % 50)}\n",
    "        for i in range(1000)\n",
    "    ]\n",
    "    \n",
    "    # Agregar algunos registros problemáticos\n",
    "    records[10]['email'] = 'test@test.test'  # Fake email\n",
    "    records[20]['age'] = 999  # Outlier\n",
    "    records[30]['email'] = None  # NULL\n",
    "    \n",
    "    # Validar batch\n",
    "    print(\"🚀 Validando batch de 1000 registros...\")\n",
    "    result = await batch_validator.validate_batch(records, validation_rules, parallelism=50)\n",
    "    \n",
    "    print(\"\\n📊 RESULTADOS:\")\n",
    "    print(f\"   Total: {result['total_records']}\")\n",
    "    print(f\"   Válidos: {result['valid_records']} ({result['valid_records']/result['total_records']:.1%})\")\n",
    "    print(f\"   Inválidos: {result['invalid_records']}\")\n",
    "    print(f\"   Issues por tipo: {result['issues_by_type']}\")\n",
    "    print(f\"   Duración: {result['total_duration_seconds']:.2f}s\")\n",
    "    print(f\"   Throughput: {result['throughput_records_per_second']:.0f} records/s\")\n",
    "    print(f\"   Costo total: ${result['total_cost_usd']:.4f}\")\n",
    "\n",
    "# Ejecutar\n",
    "# asyncio.run(main())\n",
    "```\n",
    "\n",
    "### 📊 Métricas de Producción\n",
    "\n",
    "| Métrica | Target | Medición |\n",
    "|---------|--------|----------|\n",
    "| **Latency p50** | <50ms | Traditional only |\n",
    "| **Latency p99** | <500ms | With LLM (1% sample) |\n",
    "| **Throughput** | >1000 records/s | Batch processing |\n",
    "| **Cache hit rate** | >80% | Redis cache |\n",
    "| **Cost per 1M records** | <$10 | LLM API calls |\n",
    "| **False positive rate** | <2% | Validation precision |\n",
    "| **Availability** | 99.9% | Uptime |\n",
    "\n",
    "### 💰 Estrategias de Optimización de Costos\n",
    "\n",
    "```python\n",
    "# 1. ADAPTIVE SAMPLING\n",
    "class AdaptiveSampler:\n",
    "    \"\"\"Ajusta sample rate basándose en calidad de datos.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_rate: float = 0.01):\n",
    "        self.rate = initial_rate\n",
    "        self.recent_issues = []\n",
    "    \n",
    "    def update(self, has_issues: bool):\n",
    "        \"\"\"Actualiza rate basándose en issues recientes.\"\"\"\n",
    "        self.recent_issues.append(has_issues)\n",
    "        \n",
    "        # Mantener ventana de últimos 1000 registros\n",
    "        if len(self.recent_issues) > 1000:\n",
    "            self.recent_issues = self.recent_issues[-1000:]\n",
    "        \n",
    "        issue_rate = sum(self.recent_issues) / len(self.recent_issues)\n",
    "        \n",
    "        # Si issue rate es alta (>5%), aumentar sampling\n",
    "        if issue_rate > 0.05:\n",
    "            self.rate = min(0.10, self.rate * 1.5)\n",
    "        # Si issue rate es baja (<1%), reducir sampling\n",
    "        elif issue_rate < 0.01:\n",
    "            self.rate = max(0.001, self.rate * 0.8)\n",
    "    \n",
    "    def should_sample(self) -> bool:\n",
    "        import random\n",
    "        return random.random() < self.rate\n",
    "\n",
    "# 2. SMART CACHING\n",
    "# Cachear por más tiempo valores que se repiten frecuentemente\n",
    "# TTL adaptativo basado en frecuencia\n",
    "\n",
    "# 3. BATCH INFERENCE\n",
    "# Procesar múltiples validaciones en un solo prompt\n",
    "# Reducir overhead de API calls\n",
    "```\n",
    "\n",
    "### 🚀 Deployment Checklist\n",
    "\n",
    "- [ ] **Infraestructura**:\n",
    "  - [ ] Redis cluster para cache (multi-AZ)\n",
    "  - [ ] Async workers (Celery / RQ)\n",
    "  - [ ] Load balancer para distribución\n",
    "  \n",
    "- [ ] **Monitoring**:\n",
    "  - [ ] Prometheus + Grafana dashboards\n",
    "  - [ ] Alertas en PagerDuty/Slack\n",
    "  - [ ] Cost tracking diario\n",
    "  - [ ] Latency monitoring (p50, p95, p99)\n",
    "  \n",
    "- [ ] **Testing**:\n",
    "  - [ ] Load testing (>10K records/s)\n",
    "  - [ ] Chaos testing (LLM API down)\n",
    "  - [ ] Cost simulation\n",
    "  \n",
    "- [ ] **Security**:\n",
    "  - [ ] API key rotation\n",
    "  - [ ] PII masking en logs\n",
    "  - [ ] Rate limiting por usuario\n",
    "  \n",
    "- [ ] **Documentation**:\n",
    "  - [ ] Runbook para incidents\n",
    "  - [ ] SLA commitments\n",
    "  - [ ] Cost budgets por equipo\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db249c09",
   "metadata": {},
   "source": [
    "## 1. Detección de anomalías semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78572e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def detect_anomaly(value: str, context: str) -> dict:\n",
    "    \"\"\"Detecta si un valor es anómalo en su contexto.\"\"\"\n",
    "    prompt = f'''\n",
    "Contexto: {context}\n",
    "Valor: {value}\n",
    "\n",
    "¿Es este valor anómalo o incorrecto? Responde en JSON:\n",
    "{{\n",
    "  \"is_anomaly\": true/false,\n",
    "  \"confidence\": 0-100,\n",
    "  \"reason\": \"explicación\",\n",
    "  \"suggested_fix\": \"valor corregido o null\"\n",
    "}}\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    import json\n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Ejemplos\n",
    "casos = [\n",
    "    {'valor': 'Nueva Yorkk', 'contexto': 'Columna: ciudad (ciudades de USA)'},\n",
    "    {'valor': '999', 'contexto': 'Columna: edad (años de personas)'},\n",
    "    {'valor': 'admin@example.com', 'contexto': 'Columna: email de clientes reales'}\n",
    "]\n",
    "\n",
    "for caso in casos:\n",
    "    result = detect_anomaly(caso['valor'], caso['contexto'])\n",
    "    print(f\"Valor: {caso['valor']}\")\n",
    "    print(f\"Anomalía: {result['is_anomaly']} (confianza={result['confidence']}%)\")\n",
    "    print(f\"Razón: {result['reason']}\")\n",
    "    if result['suggested_fix']:\n",
    "        print(f\"Sugerencia: {result['suggested_fix']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dcd6b",
   "metadata": {},
   "source": [
    "## 2. Clasificación de errores en datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_issue(issue_description: str) -> str:\n",
    "    \"\"\"Clasifica el tipo de problema de datos.\"\"\"\n",
    "    prompt = f'''\n",
    "Clasifica este problema de datos en UNA categoría:\n",
    "- DUPLICATES: registros duplicados\n",
    "- NULLS: valores faltantes\n",
    "- FORMAT: formato incorrecto\n",
    "- OUTLIER: valores fuera de rango\n",
    "- INCONSISTENCY: datos inconsistentes entre fuentes\n",
    "- FRESHNESS: datos desactualizados\n",
    "\n",
    "Problema: {issue_description}\n",
    "\n",
    "Categoría:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "issues = [\n",
    "    'La tabla tiene 500 filas con cliente_id = NULL',\n",
    "    'Fechas en formato DD/MM/YYYY pero esperamos YYYY-MM-DD',\n",
    "    'Última actualización hace 7 días pero debe ser diaria',\n",
    "    'Misma transacción aparece 3 veces con diferentes IDs'\n",
    "]\n",
    "\n",
    "for issue in issues:\n",
    "    categoria = classify_data_issue(issue)\n",
    "    print(f'➡️ \"{issue}\"')\n",
    "    print(f'   Categoría: {categoria}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb562809",
   "metadata": {},
   "source": [
    "## 3. Generación de reglas de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_rules(column_name: str, sample_data: list, description: str = '') -> str:\n",
    "    \"\"\"Genera reglas de Great Expectations.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera expectativas de Great Expectations (Python) para validar esta columna:\n",
    "\n",
    "Columna: {column_name}\n",
    "Descripción: {description}\n",
    "Muestra de datos: {sample_data}\n",
    "\n",
    "Genera código Python con expect_* methods. Ejemplos:\n",
    "- expect_column_values_to_not_be_null\n",
    "- expect_column_values_to_be_between\n",
    "- expect_column_values_to_match_regex\n",
    "\n",
    "Código:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "# Ejemplo\n",
    "rules = generate_validation_rules(\n",
    "    column_name='email',\n",
    "    sample_data=['user@example.com', 'admin@test.org', 'info@company.co'],\n",
    "    description='Emails de clientes'\n",
    ")\n",
    "\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecde61",
   "metadata": {},
   "source": [
    "## 4. Validación batch con LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cef1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch(df: pd.DataFrame, rules: dict) -> pd.DataFrame:\n",
    "    \"\"\"Valida DataFrame según reglas inferidas por LLM.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for col, rule_desc in rules.items():\n",
    "        sample = df[col].dropna().head(10).tolist()\n",
    "        \n",
    "        prompt = f'''\n",
    "Valida si estos valores cumplen la regla:\n",
    "Regla: {rule_desc}\n",
    "Valores: {sample}\n",
    "\n",
    "Responde con porcentaje de conformidad (0-100) y problemas encontrados.\n",
    "'''\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{'role':'user','content':prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'columna': col,\n",
    "            'regla': rule_desc,\n",
    "            'resultado': resp.choices[0].message.content\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Datos de prueba\n",
    "df_test = pd.DataFrame({\n",
    "    'edad': [25, 30, 200, 45, -5],\n",
    "    'email': ['a@b.com', 'invalido', 'test@x.org', None, 'ok@mail.com']\n",
    "})\n",
    "\n",
    "validation_rules = {\n",
    "    'edad': 'Debe estar entre 0 y 120',\n",
    "    'email': 'Debe ser email válido o null'\n",
    "}\n",
    "\n",
    "validation_report = validate_batch(df_test, validation_rules)\n",
    "print(validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de5039",
   "metadata": {},
   "source": [
    "## 5. Explicación de anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_outlier(value: float, stats: dict) -> str:\n",
    "    \"\"\"Explica por qué un valor es outlier en lenguaje natural.\"\"\"\n",
    "    prompt = f'''\n",
    "Valor: {value}\n",
    "Estadísticas de la columna:\n",
    "- Media: {stats['mean']}\n",
    "- Desviación estándar: {stats['std']}\n",
    "- Min: {stats['min']}\n",
    "- Max: {stats['max']}\n",
    "\n",
    "Explica en 2-3 frases por qué este valor es anómalo y qué puede indicar.\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Ejemplo\n",
    "outlier_explanation = explain_outlier(\n",
    "    value=50000,\n",
    "    stats={'mean': 120, 'std': 35, 'min': 50, 'max': 250}\n",
    ")\n",
    "\n",
    "print('Explicación del outlier:')\n",
    "print(outlier_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b80d2",
   "metadata": {},
   "source": [
    "## 6. Sugerencia de limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_data_cleaning(df_sample: pd.DataFrame) -> str:\n",
    "    \"\"\"Sugiere pasos de limpieza basados en muestra.\"\"\"\n",
    "    info = {\n",
    "        'columns': df_sample.columns.tolist(),\n",
    "        'dtypes': df_sample.dtypes.astype(str).to_dict(),\n",
    "        'nulls': df_sample.isnull().sum().to_dict(),\n",
    "        'sample': df_sample.head(3).to_dict()\n",
    "    }\n",
    "    \n",
    "    prompt = f'''\n",
    "Analiza este DataFrame y sugiere pasos de limpieza en orden de prioridad:\n",
    "\n",
    "{info}\n",
    "\n",
    "Lista numerada de acciones de limpieza con código Pandas cuando sea relevante.\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "messy_df = pd.DataFrame({\n",
    "    'fecha': ['2024-01-01', '01/02/2024', None, '2024-03-15'],\n",
    "    'monto': ['100', '200.5', 'N/A', '300'],\n",
    "    'categoria': ['  ventas', 'VENTAS', 'Ventas ', 'marketing']\n",
    "})\n",
    "\n",
    "cleaning_plan = suggest_data_cleaning(messy_df)\n",
    "print(cleaning_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d205e02",
   "metadata": {},
   "source": [
    "## 7. Validación de coherencia entre tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc895787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_referential_integrity(parent_ids: list, child_ids: list, relationship: str) -> dict:\n",
    "    \"\"\"Valida integridad referencial con explicación.\"\"\"\n",
    "    orphans = set(child_ids) - set(parent_ids)\n",
    "    \n",
    "    prompt = f'''\n",
    "Relación: {relationship}\n",
    "IDs huérfanos (en tabla hija pero no en padre): {list(orphans)[:10]}\n",
    "Total huérfanos: {len(orphans)}\n",
    "\n",
    "Explica el problema y sugiere 3 posibles causas.\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'orphans_count': len(orphans),\n",
    "        'orphan_sample': list(orphans)[:5],\n",
    "        'explanation': resp.choices[0].message.content\n",
    "    }\n",
    "\n",
    "# Ejemplo\n",
    "productos_ids = [1, 2, 3, 4, 5]\n",
    "ventas_producto_ids = [1, 2, 3, 99, 100, 5]\n",
    "\n",
    "integrity_check = validate_referential_integrity(\n",
    "    parent_ids=productos_ids,\n",
    "    child_ids=ventas_producto_ids,\n",
    "    relationship='ventas.producto_id -> productos.producto_id'\n",
    ")\n",
    "\n",
    "print(f\"Huérfanos: {integrity_check['orphans_count']}\")\n",
    "print(f\"Muestra: {integrity_check['orphan_sample']}\")\n",
    "print(f\"\\nExplicación:\\n{integrity_check['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808646c6",
   "metadata": {},
   "source": [
    "## 8. Buenas prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78399a",
   "metadata": {},
   "source": [
    "- **Complementar, no reemplazar**: LLMs complementan herramientas tradicionales (GE, pandas profiling).\n",
    "- **Validación humana**: revisa sugerencias antes de aplicar.\n",
    "- **Umbrales**: define confidence thresholds para automatización.\n",
    "- **Logging**: registra todas las decisiones del LLM.\n",
    "- **Costos**: cachea validaciones comunes.\n",
    "- **Testing**: valida el validador con datos conocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ea8ee",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702d2f9",
   "metadata": {},
   "source": [
    "1. Construye un sistema de auto-reparación de datos usando LLM suggestions.\n",
    "2. Genera un data quality dashboard con explicaciones en lenguaje natural.\n",
    "3. Implementa detección de PII (datos sensibles) con LLMs.\n",
    "4. Crea un agente que diagnostique problemas de data quality y proponga fixes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
