{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f651aef1",
   "metadata": {},
   "source": [
    "{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\n",
    "   \"# 🤖 Fundamentos de LLMs y Prompting\\n\",\n",
    "   \"\\n\",\n",
    "   \"Objetivo: aprender a usar LLMs (OpenAI GPT, Google Gemini) para tareas de ingeniería de datos mediante técnicas de prompting efectivas.\\n\",\n",
    "   \"\\n\",\n",
    "   \"- Duración: 90-120 min\\n\",\n",
    "   \"- Dificultad: Media\\n\",\n",
    "   \"- APIs: OpenAI GPT-4/3.5, Google Gemini Pro\\n\"\n",
    "  ]},\n",
    "  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"## 1. Setup y primera llamada\"]},\n",
    "  {\"cell_type\":\"code\",\"metadata\":{},\"execution_count\":null,\"outputs\":[],\"source\":[\n",
    "   \"# pip install openai google-generativeai python-dotenv\\n\",\n",
    "   \"import os\\n\",\n",
    "   \"from openai import OpenAI\\n\",\n",
    "   \"import google.generativeai as genai\\n\",\n",
    "   \"\\n\",\n",
    "   \"# Configurar APIs\\n\",\n",
    "   \"client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\\n\",\n",
    "   \"genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\\n\",\n",
    "   \"\\n\",\n",
    "   \"print('✅ APIs configuradas')\\n\"\n",
    "  ]},\n",
    "  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"## 2. Primera consulta - OpenAI\"]},\n",
    "  {\"cell_type\":\"code\",\"metadata\":{},\"execution_count\":null,\"outputs\":[],\"source\":[\n",
    "   \"def ask_openai(prompt: str, model='gpt-3.5-turbo') -> str:\\n\",\n",
    "   \"    \\\"\\\"\\\"Consulta a OpenAI.\\\"\\\"\\\"\\n\",\n",
    "   \"    response = client_openai.chat.completions.create(\\n\",\n",
    "   \"        model=model,\\n\",\n",
    "   \"        messages=[{'role': 'user', 'content': prompt}],\\n\",\n",
    "   \"        temperature=0.7\\n\",\n",
    "   \"    )\\n\",\n",
    "   \"    return response.choices[0].message.content\\n\",\n",
    "   \"\\n\",\n",
    "   \"pregunta = 'Explica en 2 frases qué es un Data Lake'\\n\",\n",
    "   \"respuesta = ask_openai(pregunta)\\n\",\n",
    "   \"print(f'OpenAI: {respuesta}')\\n\"\n",
    "  ]},\n",
    "  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"## 3. Primera consulta - Google Gemini\"]},\n",
    "  {\"cell_type\":\"code\",\"metadata\":{},\"execution_count\":null,\"outputs\":[],\"source\":[\n",
    "   \"def ask_gemini(prompt: str, model='gemini-1.5-flash') -> str:\\n\",\n",
    "   \"    \\\"\\\"\\\"Consulta a Google Gemini.\\\"\\\"\\\"\\n\",\n",
    "   \"    model_gemini = genai.GenerativeModel(model)\\n\",\n",
    "   \"    response = model_gemini.generate_content(prompt)\\n\",\n",
    "   \"    return response.text\\n\",\n",
    "   \"\\n\",\n",
    "   \"respuesta_gemini = ask_gemini(pregunta)\\n\",\n",
    "   \"print(f'Gemini: {respuesta_gemini}')\\n\"\n",
    "  ]},\n",
    "  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"## 4. Comparación de respuestas\"]},\n",
    "  {\"cell_type\":\"code\",\"metadata\":{},\"execution_count\":null,\"outputs\":[],\"source\":[\n",
    "   \"# Comparar ambos modelos\\n\",\n",
    "   \"test_prompts = [\\n\",\n",
    "   \"    '¿Qué es ETL?',\\n\",\n",
    "   \"    'Diferencia entre Data Lake y Data Warehouse',\\n\",\n",
    "   \"    'Ventajas de usar Apache Airflow'\\n\",\n",
    "   \"]\\n\",\n",
    "   \"\\n\",\n",
    "   \"for prompt in test_prompts:\\n\",\n",
    "   \"    print(f'\\\\n❓ {prompt}')\\n\",\n",
    "   \"    print(f'OpenAI: {ask_openai(prompt)[:100]}...')\\n\",\n",
    "   \"    print(f'Gemini:  {ask_gemini(prompt)[:100]}...')\\n\"\n",
    "  ]},\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b91288",
   "metadata": {},
   "source": [
    "## 0. Configuración y dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb71f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai python-dotenv tiktoken\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    print('⚠️ OPENAI_API_KEY no configurada. Define en .env o variable de entorno.')\n",
    "else:\n",
    "    print('✅ API Key cargada')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd97a48",
   "metadata": {},
   "source": [
    "## 1. Primera llamada: completar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'Eres un asistente experto en ingeniería de datos.'},\n",
    "        {'role': 'user', 'content': '¿Qué es un data lakehouse en 2 líneas?'}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\n💰 Tokens usados: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6507857",
   "metadata": {},
   "source": [
    "## 2. Técnicas de prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a002f",
   "metadata": {},
   "source": [
    "### 2.1 Zero-shot: sin ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(prompt: str, model='gpt-3.5-turbo', temp=0.3):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "result = ask_llm('Clasifica este error como: database, network, o application:\\nError: Connection timeout after 30s')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d845701",
   "metadata": {},
   "source": [
    "### 2.2 Few-shot: con ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee552105",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = '''\n",
    "Clasifica el tipo de error (database, network, application).\n",
    "\n",
    "Ejemplos:\n",
    "Error: Connection timeout after 30s → network\n",
    "Error: Table 'users' does not exist → database\n",
    "Error: NullPointerException in transform.py → application\n",
    "\n",
    "Ahora clasifica:\n",
    "Error: Permission denied on SELECT query\n",
    "'''\n",
    "print(ask_llm(few_shot_prompt, temp=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f9280",
   "metadata": {},
   "source": [
    "### 2.3 Chain-of-Thought (CoT): razonamiento paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = '''\n",
    "Tengo una tabla 'ventas' con 10 millones de filas. Una consulta tarda 45 segundos.\n",
    "La consulta filtra por fecha (últimos 7 días) y cliente_id, y hace GROUP BY producto_id.\n",
    "\n",
    "Analiza paso a paso por qué es lenta y sugiere 3 optimizaciones concretas.\n",
    "'''\n",
    "print(ask_llm(cot_prompt, temp=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7e2ff",
   "metadata": {},
   "source": [
    "## 3. Uso práctico: generación de documentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = '''\n",
    "def extract_sales_data(start_date, end_date):\n",
    "    conn = psycopg2.connect(DB_URI)\n",
    "    query = f\"SELECT * FROM sales WHERE date >= '{start_date}' AND date <= '{end_date}'\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "'''\n",
    "\n",
    "doc_prompt = f'''\n",
    "Genera documentación en formato docstring para esta función Python:\n",
    "\n",
    "{code_snippet}\n",
    "\n",
    "Incluye: descripción, parámetros, retorno, ejemplo de uso.\n",
    "'''\n",
    "print(ask_llm(doc_prompt, temp=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84296236",
   "metadata": {},
   "source": [
    "## 4. Buenas prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcdcdd",
   "metadata": {},
   "source": [
    "### 4.1 Control de temperatura\n",
    "- `temperature=0`: determinístico, ideal para clasificación/extracción.\n",
    "- `temperature=0.7-1.0`: creativo, bueno para generación de ideas.\n",
    "\n",
    "### 4.2 Límites de tokens\n",
    "- Estima tokens con tiktoken antes de enviar.\n",
    "- Usa `max_tokens` para controlar costos.\n",
    "\n",
    "### 4.3 System prompts\n",
    "- Define rol y contexto en mensaje `system`.\n",
    "- Establece formato de salida esperado.\n",
    "\n",
    "### 4.4 Manejo de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_llm_call(prompt: str, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return ask_llm(prompt)\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                return f'Error after {retries} retries: {e}'\n",
    "            import time\n",
    "            time.sleep(2 ** i)\n",
    "    return None\n",
    "\n",
    "result = safe_llm_call('Resume en 1 línea qué es Apache Spark.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5563f",
   "metadata": {},
   "source": [
    "## 5. Conteo de tokens y estimación de costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model='gpt-3.5-turbo') -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "sample = 'Genera un pipeline ETL en Python que lea CSV, valide con Pandera y escriba a Parquet.'\n",
    "tokens = count_tokens(sample)\n",
    "cost_input = tokens * 0.0015 / 1000  # GPT-3.5-turbo input: $0.0015/1K tokens\n",
    "print(f'Prompt: {tokens} tokens, costo estimado: ${cost_input:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc36964",
   "metadata": {},
   "source": [
    "## 6. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39cdda",
   "metadata": {},
   "source": [
    "1. Crea un prompt que genere un schema Pandera desde una descripción en texto.\n",
    "2. Usa few-shot prompting para clasificar errores de logs en 5 categorías.\n",
    "3. Implementa una función que resuma un archivo de configuración YAML en lenguaje natural.\n",
    "4. Genera un prompt que convierta una consulta SQL a una explicación en español."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
