{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1c116d",
   "metadata": {},
   "source": [
    "# üìö RAG: Documentaci√≥n T√©cnica con LLMs\n",
    "\n",
    "Objetivo: construir un sistema RAG (Retrieval Augmented Generation) para consultar documentaci√≥n de datos, diccionarios de esquemas, y knowledge bases.\n",
    "\n",
    "- Duraci√≥n: 120 min\n",
    "- Dificultad: Alta\n",
    "- Stack: OpenAI, ChromaDB, LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4e1fa",
   "metadata": {},
   "source": [
    "## 1. Setup ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb openai langchain\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Cliente persistente\n",
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "# Crear colecci√≥n\n",
    "collection = client.get_or_create_collection(\n",
    "    name='data_docs',\n",
    "    metadata={'description': 'Documentaci√≥n t√©cnica de datos'}\n",
    ")\n",
    "\n",
    "print(f'Colecci√≥n creada: {collection.name}')\n",
    "print(f'Documentos: {collection.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912f4e5",
   "metadata": {},
   "source": [
    "## 2. Ingesti√≥n de documentaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Genera embedding con OpenAI.\"\"\"\n",
    "    resp = client_openai.embeddings.create(\n",
    "        model='text-embedding-ada-002',\n",
    "        input=text\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "docs = [\n",
    "    {\n",
    "        'id': 'tabla_ventas',\n",
    "        'text': '''\n",
    "Tabla: ventas\n",
    "Esquema: dwh.ventas\n",
    "Descripci√≥n: Transacciones de ventas diarias desde 2020.\n",
    "Columnas:\n",
    "- venta_id (BIGINT, PK): identificador √∫nico\n",
    "- fecha (DATE): fecha de la transacci√≥n\n",
    "- producto_id (INT, FK): referencia a dim_productos\n",
    "- cantidad (INT): unidades vendidas\n",
    "- total (DECIMAL): monto en USD\n",
    "Frecuencia: actualizaci√≥n diaria a las 3 AM\n",
    "Owner: equipo-analytics\n",
    "        ''',\n",
    "        'metadata': {'type': 'schema', 'owner': 'analytics'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'pipeline_ventas',\n",
    "        'text': '''\n",
    "Pipeline: ventas_daily_etl\n",
    "Descripci√≥n: procesa ventas del d√≠a anterior\n",
    "Pasos:\n",
    "1. Extracci√≥n de S3 (bucket: raw-data/ventas/)\n",
    "2. Validaci√≥n con Great Expectations\n",
    "3. Deduplicaci√≥n por venta_id\n",
    "4. Enriquecimiento con datos de productos\n",
    "5. Carga a Redshift\n",
    "Dependencias: dim_productos debe estar actualizado\n",
    "Alertas: email a data-eng si falla\n",
    "        ''',\n",
    "        'metadata': {'type': 'pipeline', 'owner': 'data-eng'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'metrica_revenue',\n",
    "        'text': '''\n",
    "M√©trica: monthly_revenue\n",
    "Definici√≥n: SUM(total) de ventas agrupado por mes\n",
    "F√≥rmula: SELECT DATE_TRUNC('month', fecha) mes, SUM(total) revenue FROM ventas GROUP BY 1\n",
    "Business owner: CFO\n",
    "Dashboards: Tableau (Revenue Overview)\n",
    "        ''',\n",
    "        'metadata': {'type': 'metric', 'owner': 'finance'}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Agregar a ChromaDB\n",
    "for doc in docs:\n",
    "    embedding = get_embedding(doc['text'])\n",
    "    collection.add(\n",
    "        ids=[doc['id']],\n",
    "        documents=[doc['text']],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[doc['metadata']]\n",
    "    )\n",
    "\n",
    "print(f'‚úÖ {len(docs)} documentos indexados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceeedb0",
   "metadata": {},
   "source": [
    "## 3. B√∫squeda sem√°ntica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 3) -> list:\n",
    "    \"\"\"Busca documentos relevantes.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "pregunta = '¬øQu√© tabla contiene informaci√≥n de ventas?'\n",
    "resultados = semantic_search(pregunta)\n",
    "\n",
    "print(f'Pregunta: {pregunta}\\n')\n",
    "for i, doc in enumerate(resultados['documents'][0]):\n",
    "    print(f'Resultado {i+1}:')\n",
    "    print(doc[:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1942ee3",
   "metadata": {},
   "source": [
    "## 4. RAG: respuesta con contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03111440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question: str) -> str:\n",
    "    \"\"\"Responde usando RAG.\"\"\"\n",
    "    # 1. Buscar contexto relevante\n",
    "    results = semantic_search(question, top_k=2)\n",
    "    context = '\\n\\n'.join(results['documents'][0])\n",
    "    \n",
    "    # 2. Prompt con contexto\n",
    "    prompt = f'''\n",
    "Eres un experto en ingenier√≠a de datos. Responde la pregunta usando SOLO la informaci√≥n del contexto.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta (menciona la fuente si es relevante):\n",
    "'''\n",
    "    \n",
    "    # 3. Generar respuesta\n",
    "    resp = client_openai.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "preguntas = [\n",
    "    '¬øCu√°l es el esquema de la tabla de ventas?',\n",
    "    '¬øA qu√© hora se actualiza la data de ventas?',\n",
    "    '¬øQu√© pipeline procesa las ventas?',\n",
    "    '¬øC√≥mo se calcula el monthly revenue?'\n",
    "]\n",
    "\n",
    "for q in preguntas:\n",
    "    answer = rag_answer(q)\n",
    "    print(f'‚ùì {q}')\n",
    "    print(f'‚úÖ {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd0647",
   "metadata": {},
   "source": [
    "## 5. RAG con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed49f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Embeddings y vectorstore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = Chroma(\n",
    "    persist_directory='./chroma_db',\n",
    "    embedding_function=embeddings,\n",
    "    collection_name='data_docs'\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({'query': '¬øQui√©n es el owner de la tabla ventas?'})\n",
    "print('Respuesta:', result['result'])\n",
    "print('\\nFuentes:')\n",
    "for doc in result['source_documents']:\n",
    "    print('-', doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da038e49",
   "metadata": {},
   "source": [
    "## 6. Chunking avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b95679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para documentos largos\n",
    "long_doc = '''\n",
    "# Data Warehouse - Gu√≠a Completa\n",
    "\n",
    "## Arquitectura\n",
    "Redshift cluster con 5 nodos dc2.large.\n",
    "Schemas: raw, staging, dwh, analytics.\n",
    "\n",
    "## Tablas principales\n",
    "- ventas: transacciones diarias\n",
    "- clientes: informaci√≥n demogr√°fica\n",
    "- productos: cat√°logo completo\n",
    "\n",
    "## Pipelines\n",
    "Airflow con 15 DAGs ejecut√°ndose diariamente.\n",
    "'''\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ']\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(long_doc)\n",
    "print(f'Documento dividido en {len(chunks)} chunks:\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16ccaf",
   "metadata": {},
   "source": [
    "## 7. Filtrado por metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1253cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar solo pipelines\n",
    "results_pipeline = collection.query(\n",
    "    query_embeddings=[get_embedding('automatizaci√≥n de datos')],\n",
    "    n_results=5,\n",
    "    where={'type': 'pipeline'}\n",
    ")\n",
    "\n",
    "print('Pipelines encontrados:')\n",
    "for doc in results_pipeline['documents'][0]:\n",
    "    print('-', doc.split('\\n')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95d08f",
   "metadata": {},
   "source": [
    "## 8. Buenas pr√°cticas RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800db9b8",
   "metadata": {},
   "source": [
    "- **Chunking inteligente**: divide por secciones l√≥gicas (headers, p√°rrafos).\n",
    "- **Metadatos ricos**: agrega source, timestamp, owner, versi√≥n.\n",
    "- **H√≠brido**: combina b√∫squeda sem√°ntica + keyword search.\n",
    "- **Re-ranking**: usa modelos como Cohere Rerank para mejorar resultados.\n",
    "- **Cache**: guarda respuestas frecuentes.\n",
    "- **Actualizaci√≥n**: sincroniza vectorstore con cambios en docs.\n",
    "- **Monitoreo**: loggea queries, latencia, quality de respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6f6b2",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfb600",
   "metadata": {},
   "source": [
    "1. Indexa tu documentaci√≥n real de data warehouse en ChromaDB.\n",
    "2. Construye un chatbot Slack que responda preguntas sobre esquemas.\n",
    "3. Implementa hybrid search (sem√°ntico + BM25) con LangChain.\n",
    "4. Crea un dashboard Streamlit para explorar el vectorstore."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
