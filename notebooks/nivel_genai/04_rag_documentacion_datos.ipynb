{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1c116d",
   "metadata": {},
   "source": [
    "# 📚 RAG: Documentación Técnica con LLMs\n",
    "\n",
    "Objetivo: construir un sistema RAG (Retrieval Augmented Generation) para consultar documentación de datos, diccionarios de esquemas, y knowledge bases.\n",
    "\n",
    "- Duración: 120 min\n",
    "- Dificultad: Alta\n",
    "- Stack: OpenAI, ChromaDB, LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feae7b",
   "metadata": {},
   "source": [
    "### 🏗️ **RAG Architecture: Knowledge Retrieval for Data Engineering**\n",
    "\n",
    "**Evolution of Knowledge Management:**\n",
    "\n",
    "```\n",
    "Traditional Approach (Pre-RAG):\n",
    "  ├─ Static documentation (Confluence, Notion)\n",
    "  ├─ Manual search (Ctrl+F)\n",
    "  ├─ Outdated quickly\n",
    "  ├─ Context scattered across tools\n",
    "  └─ No natural language queries\n",
    "\n",
    "RAG Approach (2023+):\n",
    "  ├─ Unified vector knowledge base\n",
    "  ├─ Semantic search (similarity-based)\n",
    "  ├─ Real-time context retrieval\n",
    "  ├─ Natural language interface\n",
    "  └─ LLM-powered answers with citations\n",
    "```\n",
    "\n",
    "**RAG System Architecture:**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  LAYER 1: DATA INGESTION (Indexing)                         │\n",
    "│  ┌──────────────────────────────────────────────────────┐   │\n",
    "│  │  Sources                                             │   │\n",
    "│  │  ├─ Confluence pages (API)                           │   │\n",
    "│  │  ├─ GitHub README/wikis (git clone)                  │   │\n",
    "│  │  ├─ DBT documentation (manifest.json)                │   │\n",
    "│  │  ├─ Airflow DAG docstrings (AST parsing)            │   │\n",
    "│  │  ├─ SQL queries (comments, EXPLAIN)                 │   │\n",
    "│  │  ├─ Data quality reports (Great Expectations)       │   │\n",
    "│  │  └─ Runbooks/SOPs (Markdown/PDF)                    │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Document Processing                                 │   │\n",
    "│  │  ├─ Text extraction (PyPDF2, Unstructured)          │   │\n",
    "│  │  ├─ Chunking (RecursiveCharacterTextSplitter)       │   │\n",
    "│  │  │   • Chunk size: 500-1000 tokens                  │   │\n",
    "│  │  │   • Overlap: 10-20% (100-200 tokens)             │   │\n",
    "│  │  │   • Separators: \\n\\n > \\n > . > space           │   │\n",
    "│  │  ├─ Metadata extraction (title, source, date, owner)│   │\n",
    "│  │  └─ Deduplication (hash-based)                      │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Embedding Generation                                │   │\n",
    "│  │  ├─ Model: text-embedding-3-small (OpenAI)          │   │\n",
    "│  │  │   • Dimensions: 1536                             │   │\n",
    "│  │  │   • Cost: $0.02 per 1M tokens                    │   │\n",
    "│  │  │   • Latency: ~50ms per document                  │   │\n",
    "│  │  ├─ Alternatives:                                    │   │\n",
    "│  │  │   • sentence-transformers/all-MiniLM-L6-v2 (OSS)│   │\n",
    "│  │  │   • Cohere embed-multilingual-v3.0               │   │\n",
    "│  │  │   • voyage-large-2-instruct (best accuracy)      │   │\n",
    "│  │  └─ Batch processing (100 docs at a time)           │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Vector Store (ChromaDB/Pinecone/Weaviate)          │   │\n",
    "│  │  ├─ Index type: HNSW (Hierarchical NSW)             │   │\n",
    "│  │  ├─ Distance metric: Cosine similarity               │   │\n",
    "│  │  ├─ Metadata storage: JSON                          │   │\n",
    "│  │  └─ Collections: data_catalog, pipelines, runbooks  │   │\n",
    "│  └──────────────────────────────────────────────────────┘   │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  LAYER 2: QUERY PROCESSING (Retrieval)                      │\n",
    "│  ┌──────────────────────────────────────────────────────┐   │\n",
    "│  │  User Query (Natural Language)                       │   │\n",
    "│  │  \"What tables contain customer PII?\"                 │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Query Understanding                                 │   │\n",
    "│  │  ├─ Intent classification (search, definition, how-to)│  │\n",
    "│  │  ├─ Entity extraction (table names, metrics)        │   │\n",
    "│  │  ├─ Query expansion (synonyms: \"PII\" → \"personal data\")│ │\n",
    "│  │  └─ Filter inference (metadata: type=schema)        │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Hybrid Search                                       │   │\n",
    "│  │  ├─ Vector search (semantic similarity, top-k=20)   │   │\n",
    "│  │  ├─ Keyword search (BM25, ElasticSearch)            │   │\n",
    "│  │  ├─ Metadata filtering (owner, type, freshness)     │   │\n",
    "│  │  └─ Fusion: Reciprocal Rank Fusion (RRF)            │   │\n",
    "│  │     score = Σ(1 / (k + rank_i)) for each method    │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Re-ranking (Optional)                               │   │\n",
    "│  │  ├─ Cross-encoder (ms-marco-MiniLM)                 │   │\n",
    "│  │  ├─ Cohere Rerank API (best, $2 per 1K searches)    │   │\n",
    "│  │  ├─ LLM-based scoring (GPT-4 rates relevance 1-10)  │   │\n",
    "│  │  └─ Top-k final: 3-5 documents                      │   │\n",
    "│  └──────────────────────────────────────────────────────┘   │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  LAYER 3: GENERATION (Answering)                            │\n",
    "│  ┌──────────────────────────────────────────────────────┐   │\n",
    "│  │  Context Construction                                │   │\n",
    "│  │  ├─ Retrieved chunks (3-5 most relevant)            │   │\n",
    "│  │  ├─ Source attribution (with URLs)                  │   │\n",
    "│  │  ├─ Token budget management (max 4K for context)    │   │\n",
    "│  │  └─ Chunk reordering (most relevant first)          │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Prompt Engineering                                  │   │\n",
    "│  │  System: \"You are a data engineer assistant...\"     │   │\n",
    "│  │  Context: [Retrieved documentation]                 │   │\n",
    "│  │  Query: [User question]                             │   │\n",
    "│  │  Instructions:                                       │   │\n",
    "│  │    - Answer ONLY from provided context             │   │\n",
    "│  │    - Cite sources with [1], [2] notation           │   │\n",
    "│  │    - If no info found, say \"I don't know\"          │   │\n",
    "│  │    - Include table/column names when relevant      │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  LLM Generation                                      │   │\n",
    "│  │  ├─ Model: GPT-4o (best accuracy)                   │   │\n",
    "│  │  ├─ Temperature: 0.1 (factual, deterministic)       │   │\n",
    "│  │  ├─ Max tokens: 500 (concise answers)               │   │\n",
    "│  │  └─ Streaming: yes (better UX)                      │   │\n",
    "│  └──────────────────┬───────────────────────────────────┘   │\n",
    "│                     │                                        │\n",
    "│  ┌──────────────────┴───────────────────────────────────┐   │\n",
    "│  │  Post-Processing                                     │   │\n",
    "│  │  ├─ Citation formatting ([1] → link to source)      │   │\n",
    "│  │  ├─ Code syntax highlighting                        │   │\n",
    "│  │  ├─ Table formatting (Markdown tables)              │   │\n",
    "│  │  └─ Confidence scoring (based on retrieval scores)  │   │\n",
    "│  └──────────────────────────────────────────────────────┘   │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  LAYER 4: EVALUATION & MONITORING                           │\n",
    "│  ├─ Retrieval metrics: Precision@k, Recall@k, MRR           │\n",
    "│  ├─ Generation metrics: Faithfulness, Relevance             │\n",
    "│  ├─ User feedback: 👍/👎, explicit ratings                  │\n",
    "│  ├─ Latency tracking: p50/p95/p99 response times            │\n",
    "│  └─ Cost tracking: embedding + LLM API calls                │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Vector Database Comparison:**\n",
    "\n",
    "| Feature | ChromaDB | Pinecone | Weaviate | Qdrant |\n",
    "|---------|----------|----------|----------|--------|\n",
    "| **Deployment** | Local/embedded | Cloud SaaS | Self-hosted/Cloud | Self-hosted/Cloud |\n",
    "| **Max vectors** | Millions | Billions | Billions | Billions |\n",
    "| **Metadata filtering** | ✅ Basic | ✅ Advanced | ✅ Advanced | ✅ Advanced |\n",
    "| **Hybrid search** | ❌ | ✅ (sparse-dense) | ✅ (BM25+vector) | ✅ |\n",
    "| **Cost (1M vectors)** | Free (local) | $70/month | $25/month (DO) | $0 (self-hosted) |\n",
    "| **Latency (p95)** | 50-100ms | 20-50ms | 30-80ms | 20-60ms |\n",
    "| **Best for** | Prototyping, small datasets | Production, scale | Flexibility, GraphQL | High performance, Rust |\n",
    "\n",
    "**Embedding Model Comparison:**\n",
    "\n",
    "| Model | Dimensions | Cost | MTEB Score | Use Case |\n",
    "|-------|-----------|------|------------|----------|\n",
    "| **text-embedding-3-small** (OpenAI) | 1536 | $0.02/1M tokens | 62.3 | Production, balanced |\n",
    "| **text-embedding-3-large** (OpenAI) | 3072 | $0.13/1M tokens | 64.6 | Best accuracy |\n",
    "| **all-MiniLM-L6-v2** (OSS) | 384 | Free | 58.8 | Budget, fast |\n",
    "| **bge-large-en-v1.5** (OSS) | 1024 | Free | 63.9 | Best open source |\n",
    "| **voyage-large-2-instruct** | 1024 | $0.12/1M tokens | 68.3 | Highest accuracy |\n",
    "| **Cohere embed-multilingual-v3** | 1024 | $0.10/1M tokens | 64.5 | Multilingual |\n",
    "\n",
    "**Implementation: Complete RAG System**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import hashlib\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document with metadata for indexing.\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    source: str\n",
    "    metadata: Dict\n",
    "    \n",
    "    def to_hash(self) -> str:\n",
    "        \"\"\"Generate unique hash for deduplication.\"\"\"\n",
    "        return hashlib.md5(self.content.encode()).hexdigest()\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system for data documentation.\n",
    "    \n",
    "    Features:\n",
    "    - Document ingestion with chunking\n",
    "    - Hybrid search (vector + metadata)\n",
    "    - LLM generation with citations\n",
    "    - Evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"data_docs\",\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-4o\"\n",
    "    ):\n",
    "        # Initialize ChromaDB\n",
    "        self.chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Cosine similarity\n",
    "        )\n",
    "        \n",
    "        # Initialize OpenAI\n",
    "        self.openai_client = OpenAI()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Cache for embeddings (reduce API calls)\n",
    "        self._embedding_cache = {}\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding with caching.\"\"\"\n",
    "        cache_key = hashlib.md5(text.encode()).hexdigest()\n",
    "        \n",
    "        if cache_key in self._embedding_cache:\n",
    "            return self._embedding_cache[cache_key]\n",
    "        \n",
    "        response = self.openai_client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=text\n",
    "        )\n",
    "        \n",
    "        embedding = response.data[0].embedding\n",
    "        self._embedding_cache[cache_key] = embedding\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def chunk_document(\n",
    "        self,\n",
    "        text: str,\n",
    "        chunk_size: int = 800,\n",
    "        chunk_overlap: int = 200\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split document into overlapping chunks.\n",
    "        \n",
    "        Strategy:\n",
    "        - Try to split at paragraph boundaries (\\n\\n)\n",
    "        - Fall back to sentence boundaries (. )\n",
    "        - Last resort: character boundaries\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            \n",
    "            # If not at end of document\n",
    "            if end < len(text):\n",
    "                # Try to find paragraph boundary\n",
    "                newline_idx = text.rfind('\\n\\n', start, end)\n",
    "                if newline_idx != -1 and newline_idx > start + chunk_size // 2:\n",
    "                    end = newline_idx\n",
    "                else:\n",
    "                    # Try sentence boundary\n",
    "                    period_idx = text.rfind('. ', start, end)\n",
    "                    if period_idx != -1 and period_idx > start + chunk_size // 2:\n",
    "                        end = period_idx + 1\n",
    "            \n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            # Move start with overlap\n",
    "            start = end - chunk_overlap if end < len(text) else len(text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def index_document(\n",
    "        self,\n",
    "        doc: Document,\n",
    "        chunk: bool = True\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Index document into vector store.\n",
    "        \n",
    "        Returns: Number of chunks indexed\n",
    "        \"\"\"\n",
    "        # Check for duplicates\n",
    "        doc_hash = doc.to_hash()\n",
    "        existing = self.collection.get(where={\"hash\": doc_hash})\n",
    "        if existing['ids']:\n",
    "            print(f\"⚠️ Document {doc.id} already indexed (skipping)\")\n",
    "            return 0\n",
    "        \n",
    "        # Chunk if needed\n",
    "        if chunk:\n",
    "            chunks = self.chunk_document(doc.content)\n",
    "        else:\n",
    "            chunks = [doc.content]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = [self.get_embedding(chunk) for chunk in chunks]\n",
    "        \n",
    "        # Prepare metadata\n",
    "        chunk_ids = [f\"{doc.id}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [\n",
    "            {\n",
    "                **doc.metadata,\n",
    "                \"source\": doc.source,\n",
    "                \"hash\": doc_hash,\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks)\n",
    "            }\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "        \n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            ids=chunk_ids,\n",
    "            documents=chunks,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Indexed {doc.id}: {len(chunks)} chunks\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        filters: Optional[Dict] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for relevant documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results\n",
    "            filters: Metadata filters (e.g., {\"type\": \"schema\"})\n",
    "        \n",
    "        Returns:\n",
    "            List of {content, metadata, score} dicts\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "            where=filters\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        documents = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            documents.append({\n",
    "                'id': results['ids'][0][i],\n",
    "                'content': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'distance': results['distances'][0][i] if 'distances' in results else None\n",
    "            })\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_docs: List[Dict],\n",
    "        include_citations: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using LLM with retrieved context.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'answer': str,\n",
    "                'sources': List[str],\n",
    "                'confidence': float\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for i, doc in enumerate(context_docs, 1):\n",
    "            context_parts.append(f\"[{i}] {doc['content']}\")\n",
    "            sources.append({\n",
    "                'id': doc['id'],\n",
    "                'source': doc['metadata'].get('source', 'Unknown'),\n",
    "                'citation': f\"[{i}]\"\n",
    "            })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"You are an expert data engineer assistant. Answer the question using ONLY the provided context.\n",
    "\n",
    "**Context:**\n",
    "{context}\n",
    "\n",
    "**Question:** {query}\n",
    "\n",
    "**Instructions:**\n",
    "- Answer based ONLY on the provided context\n",
    "- Cite sources using [1], [2], etc. notation\n",
    "- If the context doesn't contain enough information, say \"I don't have enough information to answer this question\"\n",
    "- Be specific and technical when referencing tables, columns, pipelines\n",
    "- Keep answer concise (max 3 paragraphs)\n",
    "\n",
    "**Answer:**\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Calculate confidence (based on retrieval scores)\n",
    "        avg_distance = sum(doc.get('distance', 1.0) for doc in context_docs) / len(context_docs)\n",
    "        confidence = 1.0 - avg_distance  # Lower distance = higher confidence\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'confidence': confidence,\n",
    "            'context_used': len(context_docs)\n",
    "        }\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "        filters: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline: search + generate.\n",
    "        \"\"\"\n",
    "        # Search\n",
    "        docs = self.search(question, top_k=top_k, filters=filters)\n",
    "        \n",
    "        # Generate\n",
    "        result = self.generate_answer(question, docs)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Index documents\n",
    "docs = [\n",
    "    Document(\n",
    "        id=\"table_customers\",\n",
    "        content=\"\"\"\n",
    "Table: customers\n",
    "Schema: dwh.customers\n",
    "Description: Customer master data with demographics and contact info\n",
    "Columns:\n",
    "- customer_id (BIGINT, PK): Unique customer identifier\n",
    "- email (VARCHAR, UNIQUE): Email address (PII)\n",
    "- phone (VARCHAR): Phone number (PII)\n",
    "- created_at (TIMESTAMP): Account creation date\n",
    "- country (VARCHAR): Country code (ISO 3166-1 alpha-2)\n",
    "Owner: data-engineering@company.com\n",
    "PII: Yes (email, phone must be encrypted at rest)\n",
    "Update frequency: Real-time via CDC from PostgreSQL\n",
    "        \"\"\",\n",
    "        source=\"data_catalog.md\",\n",
    "        metadata={\"type\": \"schema\", \"owner\": \"data-engineering\", \"pii\": True}\n",
    "    )\n",
    "]\n",
    "\n",
    "for doc in docs:\n",
    "    rag.index_document(doc)\n",
    "\n",
    "# Query\n",
    "result = rag.query(\"What tables contain customer PII?\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"Sources: {result['sources']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76b828",
   "metadata": {},
   "source": [
    "### 🔍 **Advanced Retrieval: Hybrid Search & Re-ranking**\n",
    "\n",
    "**Retrieval Methods Comparison:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  1. Vector Search (Semantic Similarity)                     │\n",
    "│  ┌───────────────────────────────────────────────────────┐  │\n",
    "│  │ Query: \"What tables have PII?\"                        │  │\n",
    "│  │ Embedding: [0.23, -0.45, 0.67, ...]                  │  │\n",
    "│  │ Search: Cosine similarity in vector space            │  │\n",
    "│  │ Strengths: Understands meaning, synonyms             │  │\n",
    "│  │ Weaknesses: Misses exact keywords                    │  │\n",
    "│  └───────────────────────────────────────────────────────┘  │\n",
    "│                                                              │\n",
    "│  2. Keyword Search (BM25/TF-IDF)                            │\n",
    "│  ┌───────────────────────────────────────────────────────┐  │\n",
    "│  │ Query: \"PII tables\"                                   │  │\n",
    "│  │ Tokenization: [\"pii\", \"tables\"]                      │  │\n",
    "│  │ Scoring: BM25(doc, query)                            │  │\n",
    "│  │ Strengths: Exact matches, fast, explainable          │  │\n",
    "│  │ Weaknesses: No semantic understanding                │  │\n",
    "│  └───────────────────────────────────────────────────────┘  │\n",
    "│                                                              │\n",
    "│  3. Hybrid Search (Best of Both)                            │\n",
    "│  ┌───────────────────────────────────────────────────────┐  │\n",
    "│  │ Vector results: [doc1, doc3, doc5, doc7]             │  │\n",
    "│  │ Keyword results: [doc2, doc1, doc4, doc5]            │  │\n",
    "│  │ Fusion: Reciprocal Rank Fusion (RRF)                 │  │\n",
    "│  │   score = Σ 1/(k + rank_i) for each method          │  │\n",
    "│  │ Final ranking: [doc1, doc5, doc3, doc2, doc4, doc7]  │  │\n",
    "│  │ Improvement: +15-25% accuracy vs single method       │  │\n",
    "│  └───────────────────────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF) Algorithm:**\n",
    "\n",
    "```python\n",
    "def reciprocal_rank_fusion(\n",
    "    rankings: List[List[str]],  # Multiple ranking lists\n",
    "    k: int = 60  # Constant (typical: 60)\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Combine multiple ranking lists using RRF.\n",
    "    \n",
    "    Formula: score(doc) = Σ 1/(k + rank_i)\n",
    "    \n",
    "    Paper: \"Reciprocal Rank Fusion outperforms Condorcet and\n",
    "            individual Rank Learning Methods\" (Cormack et al. 2009)\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for ranking in rankings:\n",
    "        for rank, doc_id in enumerate(ranking, start=1):\n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = 0\n",
    "            scores[doc_id] += 1 / (k + rank)\n",
    "    \n",
    "    # Sort by score descending\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [doc_id for doc_id, score in sorted_docs]\n",
    "\n",
    "# Example\n",
    "vector_results = ['doc1', 'doc3', 'doc5', 'doc7', 'doc9']\n",
    "keyword_results = ['doc2', 'doc1', 'doc4', 'doc5', 'doc8']\n",
    "\n",
    "fused = reciprocal_rank_fusion([vector_results, keyword_results])\n",
    "print(f\"Fused ranking: {fused}\")\n",
    "# Output: ['doc1', 'doc5', 'doc3', 'doc2', 'doc4', ...]\n",
    "```\n",
    "\n",
    "**Hybrid Search Implementation:**\n",
    "\n",
    "```python\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class HybridRAG:\n",
    "    \"\"\"\n",
    "    RAG with hybrid search (vector + BM25).\n",
    "    \n",
    "    Combines:\n",
    "    - Semantic search (ChromaDB)\n",
    "    - Keyword search (BM25)\n",
    "    - Reciprocal Rank Fusion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self._build_bm25_index()\n",
    "    \n",
    "    def _build_bm25_index(self):\n",
    "        \"\"\"Create BM25 index from all documents in collection.\"\"\"\n",
    "        # Get all documents\n",
    "        all_docs = self.rag.collection.get()\n",
    "        \n",
    "        self.doc_ids = all_docs['ids']\n",
    "        self.documents = all_docs['documents']\n",
    "        \n",
    "        # Tokenize for BM25\n",
    "        tokenized_corpus = [doc.lower().split() for doc in self.documents]\n",
    "        \n",
    "        # Create BM25 object\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        print(f\"✅ BM25 index built: {len(self.documents)} documents\")\n",
    "    \n",
    "    def search_vector(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 20,\n",
    "        filters: Dict = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Vector search returns doc IDs.\"\"\"\n",
    "        results = self.rag.search(query, top_k=top_k, filters=filters)\n",
    "        return [doc['id'] for doc in results]\n",
    "    \n",
    "    def search_bm25(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 20\n",
    "    ) -> List[str]:\n",
    "        \"\"\"BM25 keyword search returns doc IDs.\"\"\"\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return [self.doc_ids[i] for i in top_indices]\n",
    "    \n",
    "    def hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        vector_weight: float = 0.7,\n",
    "        bm25_weight: float = 0.3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Hybrid search with weighted fusion.\n",
    "        \n",
    "        Args:\n",
    "            vector_weight: Weight for semantic search (0-1)\n",
    "            bm25_weight: Weight for keyword search (0-1)\n",
    "        \"\"\"\n",
    "        # Get rankings from both methods\n",
    "        vector_ids = self.search_vector(query, top_k=20)\n",
    "        bm25_ids = self.search_bm25(query, top_k=20)\n",
    "        \n",
    "        # Apply RRF\n",
    "        fused_ids = reciprocal_rank_fusion([vector_ids, bm25_ids])\n",
    "        \n",
    "        # Get top-k\n",
    "        final_ids = fused_ids[:top_k]\n",
    "        \n",
    "        # Retrieve full documents\n",
    "        results = []\n",
    "        for doc_id in final_ids:\n",
    "            # Get from collection\n",
    "            doc_data = self.rag.collection.get(ids=[doc_id])\n",
    "            \n",
    "            if doc_data['ids']:\n",
    "                results.append({\n",
    "                    'id': doc_id,\n",
    "                    'content': doc_data['documents'][0],\n",
    "                    'metadata': doc_data['metadatas'][0]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "hybrid_rag = HybridRAG(rag)\n",
    "results = hybrid_rag.hybrid_search(\"customer PII data\")\n",
    "print(f\"Hybrid search returned {len(results)} documents\")\n",
    "```\n",
    "\n",
    "**Re-ranking with Cross-Encoder:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class RerankedRAG:\n",
    "    \"\"\"\n",
    "    RAG with re-ranking stage.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Initial retrieval (vector or hybrid): top-100\n",
    "    2. Re-rank with cross-encoder: top-5\n",
    "    3. Generate answer with best-ranked docs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "        \n",
    "        # Load cross-encoder (more accurate but slower)\n",
    "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    \n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Dict],\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Re-rank documents using cross-encoder.\n",
    "        \n",
    "        Cross-encoder scores each (query, doc) pair directly.\n",
    "        More accurate than bi-encoder (separate query/doc embeddings).\n",
    "        \"\"\"\n",
    "        # Prepare pairs\n",
    "        pairs = [(query, doc['content']) for doc in documents]\n",
    "        \n",
    "        # Score all pairs\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort by score\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # Return top-k\n",
    "        reranked = [documents[i] for i in ranked_indices[:top_k]]\n",
    "        \n",
    "        # Add scores to metadata\n",
    "        for i, doc in enumerate(reranked):\n",
    "            doc['rerank_score'] = float(scores[ranked_indices[i]])\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def query_with_rerank(\n",
    "        self,\n",
    "        question: str,\n",
    "        initial_k: int = 20,\n",
    "        final_k: int = 3\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Full RAG pipeline with re-ranking.\n",
    "        \"\"\"\n",
    "        # Step 1: Initial retrieval (cast wide net)\n",
    "        initial_docs = self.rag.search(question, top_k=initial_k)\n",
    "        print(f\"Initial retrieval: {len(initial_docs)} documents\")\n",
    "        \n",
    "        # Step 2: Re-rank (focus on best)\n",
    "        reranked_docs = self.rerank(question, initial_docs, top_k=final_k)\n",
    "        print(f\"After re-ranking: {len(reranked_docs)} documents\")\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        result = self.rag.generate_answer(question, reranked_docs)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Usage\n",
    "reranked_rag = RerankedRAG(rag)\n",
    "result = reranked_rag.query_with_rerank(\"Which pipelines process customer data?\")\n",
    "```\n",
    "\n",
    "**Query Expansion for Better Recall:**\n",
    "\n",
    "```python\n",
    "def expand_query(query: str, llm_client: OpenAI) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate multiple query variations for better recall.\n",
    "    \n",
    "    Techniques:\n",
    "    - Synonym expansion\n",
    "    - Question reformulation\n",
    "    - Keyword extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Given this data engineering question, generate 3 alternative phrasings that mean the same thing.\n",
    "\n",
    "Original question: {query}\n",
    "\n",
    "Alternative phrasings:\n",
    "1.\"\"\"\n",
    "    \n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Parse alternatives\n",
    "    alternatives = [query]  # Include original\n",
    "    lines = response.choices[0].message.content.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Extract after \"1. \" or \"2. \"\n",
    "        if line.strip() and any(line.startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            alt = line.split('.', 1)[1].strip()\n",
    "            if alt:\n",
    "                alternatives.append(alt)\n",
    "    \n",
    "    return alternatives\n",
    "\n",
    "# Multi-query retrieval\n",
    "def multi_query_search(\n",
    "    query: str,\n",
    "    rag_system: RAGSystem,\n",
    "    top_k_per_query: int = 10\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search with multiple query variations, deduplicate results.\n",
    "    \"\"\"\n",
    "    # Expand query\n",
    "    queries = expand_query(query, rag_system.openai_client)\n",
    "    print(f\"Expanded to {len(queries)} queries: {queries}\")\n",
    "    \n",
    "    # Search with each query\n",
    "    all_docs = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for q in queries:\n",
    "        docs = rag_system.search(q, top_k=top_k_per_query)\n",
    "        \n",
    "        for doc in docs:\n",
    "            if doc['id'] not in seen_ids:\n",
    "                all_docs.append(doc)\n",
    "                seen_ids.add(doc['id'])\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# Usage\n",
    "expanded_docs = multi_query_search(\"What is the update frequency of sales data?\", rag)\n",
    "print(f\"Multi-query search found {len(expanded_docs)} unique documents\")\n",
    "```\n",
    "\n",
    "**Metadata Filtering for Precision:**\n",
    "\n",
    "```python\n",
    "class FilteredRAG:\n",
    "    \"\"\"\n",
    "    RAG with advanced metadata filtering.\n",
    "    \n",
    "    Use cases:\n",
    "    - Search only schemas (type='schema')\n",
    "    - Find docs by specific owner\n",
    "    - Filter by date range (updated_at)\n",
    "    - Combine multiple filters with AND/OR logic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "    \n",
    "    def search_by_type(\n",
    "        self,\n",
    "        query: str,\n",
    "        doc_type: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search only specific document types.\"\"\"\n",
    "        return self.rag.search(\n",
    "            query,\n",
    "            top_k=top_k,\n",
    "            filters={\"type\": doc_type}\n",
    "        )\n",
    "    \n",
    "    def search_by_owner(\n",
    "        self,\n",
    "        query: str,\n",
    "        owner: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search docs owned by specific team.\"\"\"\n",
    "        return self.rag.search(\n",
    "            query,\n",
    "            top_k=top_k,\n",
    "            filters={\"owner\": owner}\n",
    "        )\n",
    "    \n",
    "    def search_pii_only(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search only tables with PII data.\"\"\"\n",
    "        return self.rag.search(\n",
    "            query,\n",
    "            top_k=top_k,\n",
    "            filters={\"pii\": True}\n",
    "        )\n",
    "    \n",
    "    def search_with_complex_filters(\n",
    "        self,\n",
    "        query: str,\n",
    "        filters: Dict,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Advanced filtering with ChromaDB where clause.\n",
    "        \n",
    "        Examples:\n",
    "        - {\"$and\": [{\"type\": \"schema\"}, {\"pii\": True}]}\n",
    "        - {\"$or\": [{\"owner\": \"data-eng\"}, {\"owner\": \"analytics\"}]}\n",
    "        \"\"\"\n",
    "        return self.rag.search(query, top_k=top_k, filters=filters)\n",
    "\n",
    "# Usage examples\n",
    "filtered_rag = FilteredRAG(rag)\n",
    "\n",
    "# Only schemas\n",
    "schemas = filtered_rag.search_by_type(\"customer data\", doc_type=\"schema\")\n",
    "\n",
    "# Only data-eng team docs\n",
    "eng_docs = filtered_rag.search_by_owner(\"ETL pipeline\", owner=\"data-engineering\")\n",
    "\n",
    "# PII tables only\n",
    "pii_tables = filtered_rag.search_pii_only(\"contact information\")\n",
    "\n",
    "# Complex: schemas with PII from analytics team\n",
    "complex_results = filtered_rag.search_with_complex_filters(\n",
    "    \"user data\",\n",
    "    filters={\n",
    "        \"$and\": [\n",
    "            {\"type\": \"schema\"},\n",
    "            {\"pii\": True},\n",
    "            {\"owner\": \"analytics\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Performance Optimization:**\n",
    "\n",
    "```python\n",
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "class OptimizedRAG:\n",
    "    \"\"\"\n",
    "    Performance-optimized RAG system.\n",
    "    \n",
    "    Optimizations:\n",
    "    - Embedding caching\n",
    "    - Query result caching\n",
    "    - Batch embedding generation\n",
    "    - Connection pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "        self._query_cache = {}\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def cached_search(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Cache search results (return JSON string for hashability).\n",
    "        \n",
    "        Cache hit rate: 40-60% for common queries\n",
    "        Latency reduction: 95% (2000ms → 100ms)\n",
    "        \"\"\"\n",
    "        docs = self.rag.search(query, top_k=top_k)\n",
    "        return json.dumps(docs)\n",
    "    \n",
    "    def batch_embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings in batches (more efficient).\n",
    "        \n",
    "        OpenAI allows up to 2048 texts per request.\n",
    "        Batching reduces API calls by 100x.\n",
    "        \"\"\"\n",
    "        response = self.rag.openai_client.embeddings.create(\n",
    "            model=self.rag.embedding_model,\n",
    "            input=texts\n",
    "        )\n",
    "        \n",
    "        return [item.embedding for item in response.data]\n",
    "    \n",
    "    def benchmark_search(self, query: str, runs: int = 10):\n",
    "        \"\"\"Benchmark search latency.\"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        for _ in range(runs):\n",
    "            start = time.time()\n",
    "            self.rag.search(query)\n",
    "            latency = time.time() - start\n",
    "            latencies.append(latency)\n",
    "        \n",
    "        return {\n",
    "            'mean': np.mean(latencies),\n",
    "            'p50': np.percentile(latencies, 50),\n",
    "            'p95': np.percentile(latencies, 95),\n",
    "            'p99': np.percentile(latencies, 99)\n",
    "        }\n",
    "\n",
    "# Benchmark\n",
    "optimized = OptimizedRAG(rag)\n",
    "metrics = optimized.benchmark_search(\"customer tables\")\n",
    "print(f\"Search latency - Mean: {metrics['mean']*1000:.1f}ms, P95: {metrics['p95']*1000:.1f}ms\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eb7e9",
   "metadata": {},
   "source": [
    "### 📊 **Evaluation: Measuring RAG Quality**\n",
    "\n",
    "**RAG Evaluation Metrics Framework:**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  RETRIEVAL METRICS (Measure search quality)                 │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│  1. Precision@k                                              │\n",
    "│     • Definition: Relevant docs in top-k / k                 │\n",
    "│     • Formula: P@k = |Relevant ∩ Retrieved| / k              │\n",
    "│     • Good: >0.80 (80%+ of retrieved docs are relevant)      │\n",
    "│                                                              │\n",
    "│  2. Recall@k                                                 │\n",
    "│     • Definition: Relevant docs retrieved / total relevant   │\n",
    "│     • Formula: R@k = |Relevant ∩ Retrieved| / |Relevant|     │\n",
    "│     • Good: >0.70 (70%+ of relevant docs found)              │\n",
    "│                                                              │\n",
    "│  3. Mean Reciprocal Rank (MRR)                               │\n",
    "│     • Definition: Average of 1/rank of first relevant doc    │\n",
    "│     • Formula: MRR = (1/n) Σ 1/rank_i                        │\n",
    "│     • Good: >0.80 (relevant doc in top 1-2 positions)        │\n",
    "│                                                              │\n",
    "│  4. NDCG@k (Normalized Discounted Cumulative Gain)           │\n",
    "│     • Definition: Quality of ranking with relevance grades   │\n",
    "│     • Formula: DCG = Σ (2^rel_i - 1) / log2(i + 1)           │\n",
    "│     • Good: >0.85 (excellent ranking quality)                │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  GENERATION METRICS (Measure answer quality)                 │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│  1. Faithfulness (Hallucination Detection)                   │\n",
    "│     • Definition: Answer is grounded in retrieved context    │\n",
    "│     • Method: LLM judges if answer follows from context      │\n",
    "│     • Target: >0.95 (minimal hallucination)                  │\n",
    "│                                                              │\n",
    "│  2. Answer Relevance                                         │\n",
    "│     • Definition: Answer directly addresses the question     │\n",
    "│     • Method: Cosine similarity(question, answer)            │\n",
    "│     • Target: >0.80 (highly relevant)                        │\n",
    "│                                                              │\n",
    "│  3. Context Precision                                        │\n",
    "│     • Definition: Retrieved context is useful for answer     │\n",
    "│     • Method: LLM rates if each chunk was used               │\n",
    "│     • Target: >0.70 (most context is useful)                 │\n",
    "│                                                              │\n",
    "│  4. Context Recall                                           │\n",
    "│     • Definition: Answer uses all relevant retrieved info    │\n",
    "│     • Method: LLM checks if facts in answer come from context│\n",
    "│     • Target: >0.85 (answer covers available info)           │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│  END-TO-END METRICS (Overall system quality)                 │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│  1. Correctness (Human or LLM-judged)                        │\n",
    "│     • 5-point scale: 1 (wrong) to 5 (perfect)                │\n",
    "│     • Target: >4.0 average score                             │\n",
    "│                                                              │\n",
    "│  2. Latency (User experience)                                │\n",
    "│     • p50: <500ms, p95: <2s, p99: <5s                        │\n",
    "│                                                              │\n",
    "│  3. Cost per Query                                           │\n",
    "│     • Embedding + LLM API costs                              │\n",
    "│     • Target: <$0.01 per query                               │\n",
    "│                                                              │\n",
    "│  4. User Satisfaction (CSAT)                                 │\n",
    "│     • Thumbs up/down feedback                                │\n",
    "│     • Target: >80% positive                                  │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Implementation: Comprehensive Evaluation Suite**\n",
    "\n",
    "```python\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Single evaluation result.\"\"\"\n",
    "    query: str\n",
    "    retrieved_docs: List[str]\n",
    "    relevant_docs: List[str]\n",
    "    generated_answer: str\n",
    "    ground_truth: str\n",
    "    metrics: Dict[str, float]\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive RAG evaluation framework.\n",
    "    \n",
    "    Implements:\n",
    "    - Retrieval metrics (P@k, R@k, MRR, NDCG)\n",
    "    - Generation metrics (faithfulness, relevance)\n",
    "    - End-to-end correctness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "        self.openai_client = rag_system.openai_client\n",
    "    \n",
    "    def precision_at_k(\n",
    "        self,\n",
    "        retrieved: List[str],\n",
    "        relevant: List[str],\n",
    "        k: int = 5\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Precision@k: Fraction of retrieved docs that are relevant.\n",
    "        \n",
    "        Example:\n",
    "            retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']\n",
    "            relevant = ['doc1', 'doc3', 'doc7']\n",
    "            P@5 = 2/5 = 0.40 (doc1 and doc3 are relevant)\n",
    "        \"\"\"\n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevant_set = set(relevant)\n",
    "        \n",
    "        relevant_retrieved = sum(1 for doc in retrieved_k if doc in relevant_set)\n",
    "        \n",
    "        return relevant_retrieved / k if k > 0 else 0.0\n",
    "    \n",
    "    def recall_at_k(\n",
    "        self,\n",
    "        retrieved: List[str],\n",
    "        relevant: List[str],\n",
    "        k: int = 5\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Recall@k: Fraction of relevant docs that were retrieved.\n",
    "        \n",
    "        Example:\n",
    "            retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']\n",
    "            relevant = ['doc1', 'doc3', 'doc7']\n",
    "            R@5 = 2/3 = 0.67 (found doc1 and doc3, missing doc7)\n",
    "        \"\"\"\n",
    "        retrieved_k = set(retrieved[:k])\n",
    "        relevant_set = set(relevant)\n",
    "        \n",
    "        relevant_retrieved = len(retrieved_k & relevant_set)\n",
    "        \n",
    "        return relevant_retrieved / len(relevant_set) if relevant_set else 0.0\n",
    "    \n",
    "    def mean_reciprocal_rank(\n",
    "        self,\n",
    "        retrieved: List[str],\n",
    "        relevant: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        MRR: Reciprocal of rank of first relevant document.\n",
    "        \n",
    "        Example:\n",
    "            retrieved = ['doc2', 'doc1', 'doc3']  # doc1 is relevant\n",
    "            MRR = 1/2 = 0.50 (first relevant at position 2)\n",
    "        \"\"\"\n",
    "        relevant_set = set(relevant)\n",
    "        \n",
    "        for rank, doc in enumerate(retrieved, start=1):\n",
    "            if doc in relevant_set:\n",
    "                return 1.0 / rank\n",
    "        \n",
    "        return 0.0  # No relevant doc found\n",
    "    \n",
    "    def ndcg_at_k(\n",
    "        self,\n",
    "        retrieved: List[str],\n",
    "        relevance_scores: Dict[str, int],  # doc_id -> relevance (0-3)\n",
    "        k: int = 5\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        NDCG@k: Normalized Discounted Cumulative Gain.\n",
    "        \n",
    "        Accounts for:\n",
    "        - Position (earlier is better)\n",
    "        - Relevance grade (highly relevant > somewhat relevant)\n",
    "        \n",
    "        Example:\n",
    "            retrieved = ['doc1', 'doc2', 'doc3']\n",
    "            relevance_scores = {'doc1': 3, 'doc2': 1, 'doc3': 2}\n",
    "            DCG = (2^3-1)/log2(2) + (2^1-1)/log2(3) + (2^2-1)/log2(4)\n",
    "                = 7/1 + 1/1.58 + 3/2 = 7 + 0.63 + 1.5 = 9.13\n",
    "            IDCG (ideal) = 7 + 1.5 + 0.63 = 9.13\n",
    "            NDCG = DCG / IDCG = 1.0 (perfect ranking)\n",
    "        \"\"\"\n",
    "        retrieved_k = retrieved[:k]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for i, doc in enumerate(retrieved_k, start=1):\n",
    "            rel = relevance_scores.get(doc, 0)\n",
    "            dcg += (2**rel - 1) / np.log2(i + 1)\n",
    "        \n",
    "        # Calculate IDCG (ideal ranking)\n",
    "        ideal_ranking = sorted(relevance_scores.values(), reverse=True)[:k]\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_ranking, start=1):\n",
    "            idcg += (2**rel - 1) / np.log2(i + 1)\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    def evaluate_faithfulness(\n",
    "        self,\n",
    "        answer: str,\n",
    "        context: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Faithfulness: Answer is grounded in context (no hallucination).\n",
    "        \n",
    "        Method:\n",
    "        - LLM judges if each statement in answer is supported by context\n",
    "        - Returns score 0-1\n",
    "        \"\"\"\n",
    "        context_text = \"\\n\\n\".join(context)\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate if the answer is faithful to the context (no hallucination).\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question: Is every statement in the answer supported by the context?\n",
    "Answer with a score from 0 (completely unfaithful) to 10 (perfectly faithful).\n",
    "Respond with ONLY a number.\n",
    "\n",
    "Score:\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.choices[0].message.content.strip())\n",
    "            return min(score / 10.0, 1.0)  # Normalize to 0-1\n",
    "        except:\n",
    "            return 0.5  # Default if parsing fails\n",
    "    \n",
    "    def evaluate_relevance(\n",
    "        self,\n",
    "        query: str,\n",
    "        answer: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Answer Relevance: Does answer address the question?\n",
    "        \n",
    "        Method:\n",
    "        - Compute embeddings for query and answer\n",
    "        - Calculate cosine similarity\n",
    "        \"\"\"\n",
    "        query_embedding = np.array(self.rag.get_embedding(query))\n",
    "        answer_embedding = np.array(self.rag.get_embedding(answer))\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(query_embedding, answer_embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(answer_embedding)\n",
    "        )\n",
    "        \n",
    "        return float(similarity)\n",
    "    \n",
    "    def evaluate_correctness(\n",
    "        self,\n",
    "        answer: str,\n",
    "        ground_truth: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Correctness: LLM-judged correctness compared to ground truth.\n",
    "        \n",
    "        Returns: Score 0-1\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Compare the generated answer to the ground truth answer.\n",
    "\n",
    "Ground Truth:\n",
    "{ground_truth}\n",
    "\n",
    "Generated Answer:\n",
    "{answer}\n",
    "\n",
    "Rate the correctness on a scale of 0-10:\n",
    "- 10: Perfect match, all key information present\n",
    "- 7-9: Mostly correct, minor details missing\n",
    "- 4-6: Partially correct, some key info missing\n",
    "- 1-3: Mostly incorrect\n",
    "- 0: Completely wrong\n",
    "\n",
    "Respond with ONLY a number.\n",
    "\n",
    "Score:\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.choices[0].message.content.strip())\n",
    "            return min(score / 10.0, 1.0)\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    def evaluate_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        relevant_doc_ids: List[str],\n",
    "        ground_truth_answer: str,\n",
    "        top_k: int = 5\n",
    "    ) -> EvalResult:\n",
    "        \"\"\"\n",
    "        Complete evaluation for a single query.\n",
    "        \n",
    "        Returns all metrics.\n",
    "        \"\"\"\n",
    "        # Retrieve documents\n",
    "        retrieved_docs = self.rag.search(query, top_k=top_k)\n",
    "        retrieved_ids = [doc['id'] for doc in retrieved_docs]\n",
    "        retrieved_contents = [doc['content'] for doc in retrieved_docs]\n",
    "        \n",
    "        # Generate answer\n",
    "        result = self.rag.generate_answer(query, retrieved_docs)\n",
    "        generated_answer = result['answer']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            # Retrieval metrics\n",
    "            'precision@5': self.precision_at_k(retrieved_ids, relevant_doc_ids, k=5),\n",
    "            'recall@5': self.recall_at_k(retrieved_ids, relevant_doc_ids, k=5),\n",
    "            'mrr': self.mean_reciprocal_rank(retrieved_ids, relevant_doc_ids),\n",
    "            \n",
    "            # Generation metrics\n",
    "            'faithfulness': self.evaluate_faithfulness(generated_answer, retrieved_contents),\n",
    "            'relevance': self.evaluate_relevance(query, generated_answer),\n",
    "            'correctness': self.evaluate_correctness(generated_answer, ground_truth_answer)\n",
    "        }\n",
    "        \n",
    "        return EvalResult(\n",
    "            query=query,\n",
    "            retrieved_docs=retrieved_ids,\n",
    "            relevant_docs=relevant_doc_ids,\n",
    "            generated_answer=generated_answer,\n",
    "            ground_truth=ground_truth_answer,\n",
    "            metrics=metrics\n",
    "        )\n",
    "    \n",
    "    def evaluate_dataset(\n",
    "        self,\n",
    "        test_cases: List[Dict]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate RAG system on test dataset.\n",
    "        \n",
    "        Args:\n",
    "            test_cases: List of {\n",
    "                'query': str,\n",
    "                'relevant_docs': List[str],\n",
    "                'ground_truth': str\n",
    "            }\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated metrics across all test cases\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, case in enumerate(test_cases, 1):\n",
    "            print(f\"Evaluating {i}/{len(test_cases)}: {case['query'][:50]}...\")\n",
    "            \n",
    "            result = self.evaluate_query(\n",
    "                query=case['query'],\n",
    "                relevant_doc_ids=case['relevant_docs'],\n",
    "                ground_truth_answer=case['ground_truth']\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        all_metrics = [r.metrics for r in results]\n",
    "        \n",
    "        aggregated = {}\n",
    "        for metric_name in all_metrics[0].keys():\n",
    "            values = [m[metric_name] for m in all_metrics]\n",
    "            aggregated[metric_name] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values)\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'aggregated': aggregated,\n",
    "            'num_queries': len(test_cases)\n",
    "        }\n",
    "\n",
    "# Example test dataset\n",
    "test_cases = [\n",
    "    {\n",
    "        'query': 'Which tables contain customer PII?',\n",
    "        'relevant_docs': ['table_customers', 'table_orders'],\n",
    "        'ground_truth': 'The customers table contains PII including email and phone. It is in the dwh.customers schema and owned by data-engineering.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'How often is sales data updated?',\n",
    "        'relevant_docs': ['table_sales', 'pipeline_sales_etl'],\n",
    "        'ground_truth': 'Sales data is updated daily at 3 AM via the ventas_daily_etl pipeline.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = RAGEvaluator(rag)\n",
    "eval_results = evaluator.evaluate_dataset(test_cases)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== RAG Evaluation Results ===\")\n",
    "for metric, stats in eval_results['aggregated'].items():\n",
    "    print(f\"{metric}: {stats['mean']:.3f} ± {stats['std']:.3f} (min={stats['min']:.3f}, max={stats['max']:.3f})\")\n",
    "```\n",
    "\n",
    "**A/B Testing RAG Systems:**\n",
    "\n",
    "```python\n",
    "class RAGComparison:\n",
    "    \"\"\"\n",
    "    Compare two RAG systems (e.g., different retrieval methods).\n",
    "    \n",
    "    Use case:\n",
    "    - Test hybrid search vs pure vector search\n",
    "    - Compare different embedding models\n",
    "    - Evaluate LLM model upgrades (GPT-4 vs GPT-4o)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        system_a: RAGSystem,\n",
    "        system_b: RAGSystem,\n",
    "        system_a_name: str = \"System A\",\n",
    "        system_b_name: str = \"System B\"\n",
    "    ):\n",
    "        self.system_a = system_a\n",
    "        self.system_b = system_b\n",
    "        self.system_a_name = system_a_name\n",
    "        self.system_b_name = system_b_name\n",
    "        \n",
    "        self.evaluator_a = RAGEvaluator(system_a)\n",
    "        self.evaluator_b = RAGEvaluator(system_b)\n",
    "    \n",
    "    def compare(\n",
    "        self,\n",
    "        test_cases: List[Dict]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Run both systems on same test set, compare metrics.\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {self.system_a_name}...\")\n",
    "        results_a = self.evaluator_a.evaluate_dataset(test_cases)\n",
    "        \n",
    "        print(f\"\\nEvaluating {self.system_b_name}...\")\n",
    "        results_b = self.evaluator_b.evaluate_dataset(test_cases)\n",
    "        \n",
    "        # Calculate improvements\n",
    "        comparison = {}\n",
    "        \n",
    "        for metric in results_a['aggregated'].keys():\n",
    "            mean_a = results_a['aggregated'][metric]['mean']\n",
    "            mean_b = results_b['aggregated'][metric]['mean']\n",
    "            \n",
    "            improvement = ((mean_b - mean_a) / mean_a) * 100 if mean_a > 0 else 0\n",
    "            \n",
    "            comparison[metric] = {\n",
    "                self.system_a_name: mean_a,\n",
    "                self.system_b_name: mean_b,\n",
    "                'improvement_pct': improvement,\n",
    "                'better': self.system_b_name if mean_b > mean_a else self.system_a_name\n",
    "            }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def print_comparison(self, comparison: Dict):\n",
    "        \"\"\"Pretty print comparison results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"  {self.system_a_name} vs {self.system_b_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for metric, stats in comparison.items():\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(f\"  {self.system_a_name}: {stats[self.system_a_name]:.3f}\")\n",
    "            print(f\"  {self.system_b_name}: {stats[self.system_b_name]:.3f}\")\n",
    "            \n",
    "            if stats['improvement_pct'] > 0:\n",
    "                print(f\"  ✅ {stats[self.system_b_name]} is {stats['improvement_pct']:.1f}% better\")\n",
    "            elif stats['improvement_pct'] < 0:\n",
    "                print(f\"  ❌ {stats[self.system_b_name]} is {abs(stats['improvement_pct']):.1f}% worse\")\n",
    "            else:\n",
    "                print(f\"  ➖ No difference\")\n",
    "\n",
    "# Example: Compare vector search vs hybrid search\n",
    "rag_vector = RAGSystem(collection_name=\"docs_vector\")\n",
    "rag_hybrid = HybridRAG(rag_vector)\n",
    "\n",
    "comparison = RAGComparison(\n",
    "    rag_vector,\n",
    "    rag_hybrid,\n",
    "    system_a_name=\"Vector Search\",\n",
    "    system_b_name=\"Hybrid Search\"\n",
    ")\n",
    "\n",
    "results = comparison.compare(test_cases)\n",
    "comparison.print_comparison(results)\n",
    "```\n",
    "\n",
    "**Continuous Monitoring Dashboard:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class RAGMonitor:\n",
    "    \"\"\"\n",
    "    Production monitoring for RAG system.\n",
    "    \n",
    "    Tracks:\n",
    "    - Query latency (p50, p95, p99)\n",
    "    - Retrieval quality (avg precision/recall)\n",
    "    - User feedback (thumbs up/down)\n",
    "    - Cost per query\n",
    "    - Error rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_log = []\n",
    "    \n",
    "    def log_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_retrieved: int,\n",
    "        latency_ms: float,\n",
    "        cost_usd: float,\n",
    "        user_feedback: Optional[str] = None,  # 'positive' or 'negative'\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Log single query metrics.\"\"\"\n",
    "        self.metrics_log.append({\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'query': query,\n",
    "            'num_retrieved': num_retrieved,\n",
    "            'latency_ms': latency_ms,\n",
    "            'cost_usd': cost_usd,\n",
    "            'user_feedback': user_feedback,\n",
    "            'error': error\n",
    "        })\n",
    "    \n",
    "    def get_summary(self, last_n_hours: int = 24) -> Dict:\n",
    "        \"\"\"Generate summary for last N hours.\"\"\"\n",
    "        cutoff = datetime.utcnow() - timedelta(hours=last_n_hours)\n",
    "        \n",
    "        recent_logs = [\n",
    "            log for log in self.metrics_log\n",
    "            if datetime.fromisoformat(log['timestamp']) > cutoff\n",
    "        ]\n",
    "        \n",
    "        if not recent_logs:\n",
    "            return {}\n",
    "        \n",
    "        latencies = [log['latency_ms'] for log in recent_logs]\n",
    "        costs = [log['cost_usd'] for log in recent_logs]\n",
    "        \n",
    "        feedbacks = [log.get('user_feedback') for log in recent_logs if log.get('user_feedback')]\n",
    "        positive_feedback = sum(1 for f in feedbacks if f == 'positive')\n",
    "        \n",
    "        errors = sum(1 for log in recent_logs if log.get('error'))\n",
    "        \n",
    "        return {\n",
    "            'time_window': f'Last {last_n_hours} hours',\n",
    "            'total_queries': len(recent_logs),\n",
    "            'latency': {\n",
    "                'p50': np.percentile(latencies, 50),\n",
    "                'p95': np.percentile(latencies, 95),\n",
    "                'p99': np.percentile(latencies, 99)\n",
    "            },\n",
    "            'cost': {\n",
    "                'total': sum(costs),\n",
    "                'avg_per_query': np.mean(costs)\n",
    "            },\n",
    "            'user_satisfaction': {\n",
    "                'total_feedback': len(feedbacks),\n",
    "                'positive_rate': positive_feedback / len(feedbacks) if feedbacks else 0\n",
    "            },\n",
    "            'error_rate': errors / len(recent_logs) if recent_logs else 0\n",
    "        }\n",
    "    \n",
    "    def export_to_prometheus(self) -> str:\n",
    "        \"\"\"Export metrics in Prometheus format.\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        metrics = f\"\"\"\n",
    "# HELP rag_queries_total Total number of queries\n",
    "# TYPE rag_queries_total counter\n",
    "rag_queries_total {summary['total_queries']}\n",
    "\n",
    "# HELP rag_latency_ms Query latency percentiles\n",
    "# TYPE rag_latency_ms gauge\n",
    "rag_latency_ms{{quantile=\"0.5\"}} {summary['latency']['p50']}\n",
    "rag_latency_ms{{quantile=\"0.95\"}} {summary['latency']['p95']}\n",
    "rag_latency_ms{{quantile=\"0.99\"}} {summary['latency']['p99']}\n",
    "\n",
    "# HELP rag_cost_usd Total cost in USD\n",
    "# TYPE rag_cost_usd counter\n",
    "rag_cost_usd {summary['cost']['total']}\n",
    "\n",
    "# HELP rag_user_satisfaction User satisfaction rate (0-1)\n",
    "# TYPE rag_user_satisfaction gauge\n",
    "rag_user_satisfaction {summary['user_satisfaction']['positive_rate']}\n",
    "\n",
    "# HELP rag_error_rate Error rate (0-1)\n",
    "# TYPE rag_error_rate gauge\n",
    "rag_error_rate {summary['error_rate']}\n",
    "\"\"\"\n",
    "        return metrics\n",
    "\n",
    "# Usage in production\n",
    "monitor = RAGMonitor()\n",
    "\n",
    "# In your RAG query endpoint\n",
    "def handle_query(query: str):\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = rag.query(query)\n",
    "        latency = (time.time() - start) * 1000  # ms\n",
    "        \n",
    "        # Estimate cost (embedding + LLM)\n",
    "        cost = 0.00002 * len(query.split()) + 0.00001 * len(result['answer'].split())\n",
    "        \n",
    "        monitor.log_query(\n",
    "            query=query,\n",
    "            num_retrieved=result['context_used'],\n",
    "            latency_ms=latency,\n",
    "            cost_usd=cost\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        monitor.log_query(\n",
    "            query=query,\n",
    "            num_retrieved=0,\n",
    "            latency_ms=0,\n",
    "            cost_usd=0,\n",
    "            error=str(e)\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# Get daily summary\n",
    "summary = monitor.get_summary(last_n_hours=24)\n",
    "print(json.dumps(summary, indent=2))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed6433",
   "metadata": {},
   "source": [
    "### 🚀 **Production RAG: Multi-Source Knowledge Base**\n",
    "\n",
    "**Enterprise RAG Architecture:**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│  DATA SOURCES (Multi-format ingestion)                        │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │\n",
    "│  │ Confluence  │  │   GitHub    │  │     DBT     │          │\n",
    "│  │   (API)     │  │   (Git)     │  │ (manifest)  │          │\n",
    "│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘          │\n",
    "│         │                │                │                   │\n",
    "│  ┌──────┴──────┐  ┌──────┴──────┐  ┌──────┴──────┐          │\n",
    "│  │   Airflow   │  │  Snowflake  │  │ Looker/     │          │\n",
    "│  │    DAGs     │  │  INFORMATION│  │  Tableau    │          │\n",
    "│  │ (Docstring) │  │   _SCHEMA   │  │  (Metadata) │          │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘          │\n",
    "│                                                                │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │\n",
    "│  │   Slack     │  │   Jira      │  │  Runbooks   │          │\n",
    "│  │  (Search)   │  │   (API)     │  │  (Markdown) │          │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘          │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "                            │\n",
    "                            ▼\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│  INGESTION PIPELINE (Orchestrated by Airflow)                 │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│  DAG: knowledge_base_sync (runs every 6 hours)                │\n",
    "│                                                                │\n",
    "│  ┌──────────────────────────────────────────────────────────┐ │\n",
    "│  │ Task 1: Extract from Sources                             │ │\n",
    "│  │  ├─ Confluence: Get pages updated in last 6h            │ │\n",
    "│  │  ├─ GitHub: Pull README/wiki commits                    │ │\n",
    "│  │  ├─ DBT: Parse manifest.json for table docs             │ │\n",
    "│  │  └─ Snowflake: Query INFORMATION_SCHEMA                 │ │\n",
    "│  └──────────────────┬───────────────────────────────────────┘ │\n",
    "│                     │                                          │\n",
    "│  ┌──────────────────┴───────────────────────────────────────┐ │\n",
    "│  │ Task 2: Transform & Enrich                               │ │\n",
    "│  │  ├─ Parse Markdown/HTML to plain text                   │ │\n",
    "│  │  ├─ Extract metadata (owner, last_updated, tags)        │ │\n",
    "│  │  ├─ Chunk documents (800 tokens, 200 overlap)           │ │\n",
    "│  │  ├─ Deduplicate by content hash                         │ │\n",
    "│  │  └─ Add source lineage (URL, file path)                 │ │\n",
    "│  └──────────────────┬───────────────────────────────────────┘ │\n",
    "│                     │                                          │\n",
    "│  ┌──────────────────┴───────────────────────────────────────┐ │\n",
    "│  │ Task 3: Generate Embeddings                              │ │\n",
    "│  │  ├─ Batch embed (100 docs per API call)                 │ │\n",
    "│  │  ├─ Model: text-embedding-3-small (OpenAI)              │ │\n",
    "│  │  ├─ Cost tracking: $0.02 per 1M tokens                  │ │\n",
    "│  │  └─ Retry logic: exponential backoff                    │ │\n",
    "│  └──────────────────┬───────────────────────────────────────┘ │\n",
    "│                     │                                          │\n",
    "│  ┌──────────────────┴───────────────────────────────────────┐ │\n",
    "│  │ Task 4: Load to Vector Store                             │ │\n",
    "│  │  ├─ Upsert to Pinecone (production) or ChromaDB (dev)   │ │\n",
    "│  │  ├─ Organize into collections by source type            │ │\n",
    "│  │  ├─ Index metadata for filtering                        │ │\n",
    "│  │  └─ Validation: sample queries for smoke test           │ │\n",
    "│  └──────────────────────────────────────────────────────────┘ │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "                            │\n",
    "                            ▼\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│  VECTOR DATABASE (Pinecone for production scale)              │\n",
    "│  ├─ Collections: schemas, pipelines, metrics, runbooks        │\n",
    "│  ├─ Total vectors: ~500K (50K per collection)                 │\n",
    "│  ├─ Metadata filters: source, owner, tags, last_updated       │\n",
    "│  └─ SLA: p95 latency <50ms, 99.9% uptime                      │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Complete Production Implementation:**\n",
    "\n",
    "```python\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from git import Repo\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class DataSource:\n",
    "    \"\"\"Configuration for a data source.\"\"\"\n",
    "    name: str\n",
    "    type: str  # 'confluence', 'github', 'dbt', 'snowflake'\n",
    "    config: Dict\n",
    "    sync_frequency_hours: int = 6\n",
    "\n",
    "class MultiSourceRAG:\n",
    "    \"\"\"\n",
    "    Production RAG system with multi-source ingestion.\n",
    "    \n",
    "    Features:\n",
    "    - Syncs from 8+ data sources\n",
    "    - Incremental updates (only changed docs)\n",
    "    - Metadata-rich indexing\n",
    "    - Source attribution in answers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_db: str = \"pinecone\",  # or \"chromadb\"\n",
    "        pinecone_api_key: Optional[str] = None\n",
    "    ):\n",
    "        if vector_db == \"pinecone\":\n",
    "            import pinecone\n",
    "            pinecone.init(api_key=pinecone_api_key)\n",
    "            self.index = pinecone.Index(\"data-knowledge-base\")\n",
    "        else:\n",
    "            self.chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "            self.index = self.chroma_client.get_or_create_collection(\"data_docs\")\n",
    "        \n",
    "        self.openai_client = OpenAI()\n",
    "        \n",
    "        # Track last sync times\n",
    "        self.last_sync = {}\n",
    "    \n",
    "    def sync_confluence(self, base_url: str, space_key: str, api_token: str):\n",
    "        \"\"\"\n",
    "        Sync from Confluence space.\n",
    "        \n",
    "        Extracts:\n",
    "        - Page title, content, labels\n",
    "        - Last updated timestamp\n",
    "        - Page URL for source attribution\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Get pages updated since last sync\n",
    "        last_sync_time = self.last_sync.get('confluence', datetime.utcnow() - timedelta(hours=24))\n",
    "        \n",
    "        # Confluence REST API\n",
    "        url = f\"{base_url}/rest/api/content\"\n",
    "        params = {\n",
    "            \"spaceKey\": space_key,\n",
    "            \"expand\": \"body.storage,version,metadata.labels\",\n",
    "            \"limit\": 100,\n",
    "            \"orderby\": \"lastmodified\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        pages = response.json()['results']\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for page in pages:\n",
    "            # Check if updated since last sync\n",
    "            updated = datetime.fromisoformat(page['version']['when'].rstrip('Z'))\n",
    "            if updated < last_sync_time:\n",
    "                continue\n",
    "            \n",
    "            # Parse HTML content\n",
    "            from bs4 import BeautifulSoup\n",
    "            soup = BeautifulSoup(page['body']['storage']['value'], 'html.parser')\n",
    "            content = soup.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            doc = Document(\n",
    "                id=f\"confluence_{page['id']}\",\n",
    "                content=f\"# {page['title']}\\n\\n{content}\",\n",
    "                source=f\"{base_url}/wiki{page['_links']['webui']}\",\n",
    "                metadata={\n",
    "                    'type': 'confluence_page',\n",
    "                    'title': page['title'],\n",
    "                    'space': space_key,\n",
    "                    'last_updated': updated.isoformat(),\n",
    "                    'labels': [label['name'] for label in page['metadata']['labels']['results']]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"✅ Synced {len(documents)} Confluence pages\")\n",
    "        self.last_sync['confluence'] = datetime.utcnow()\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def sync_github_repo(self, repo_url: str, branch: str = \"main\"):\n",
    "        \"\"\"\n",
    "        Sync README and wiki from GitHub repo.\n",
    "        \n",
    "        Extracts:\n",
    "        - README.md\n",
    "        - All wiki pages\n",
    "        - Code docstrings (Python files)\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        \n",
    "        # Clone repo to temp directory\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        try:\n",
    "            repo = Repo.clone_from(repo_url, temp_dir, branch=branch, depth=1)\n",
    "            \n",
    "            documents = []\n",
    "            \n",
    "            # Index README\n",
    "            readme_path = Path(temp_dir) / \"README.md\"\n",
    "            if readme_path.exists():\n",
    "                content = readme_path.read_text()\n",
    "                \n",
    "                doc = Document(\n",
    "                    id=f\"github_readme_{hashlib.md5(repo_url.encode()).hexdigest()}\",\n",
    "                    content=content,\n",
    "                    source=f\"{repo_url}/blob/{branch}/README.md\",\n",
    "                    metadata={\n",
    "                        'type': 'github_readme',\n",
    "                        'repo': repo_url,\n",
    "                        'branch': branch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                documents.append(doc)\n",
    "            \n",
    "            # Index Python docstrings\n",
    "            for py_file in Path(temp_dir).rglob(\"*.py\"):\n",
    "                if \".git\" in str(py_file):\n",
    "                    continue\n",
    "                \n",
    "                # Extract docstrings using AST\n",
    "                docstrings = self._extract_docstrings(py_file)\n",
    "                \n",
    "                for func_name, docstring in docstrings.items():\n",
    "                    doc = Document(\n",
    "                        id=f\"github_docstring_{hashlib.md5(str(py_file).encode()).hexdigest()}_{func_name}\",\n",
    "                        content=f\"Function: {func_name}\\n\\n{docstring}\",\n",
    "                        source=f\"{repo_url}/blob/{branch}/{py_file.relative_to(temp_dir)}\",\n",
    "                        metadata={\n",
    "                            'type': 'code_docstring',\n",
    "                            'repo': repo_url,\n",
    "                            'file': str(py_file.relative_to(temp_dir)),\n",
    "                            'function': func_name\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    documents.append(doc)\n",
    "            \n",
    "            print(f\"✅ Synced {len(documents)} GitHub documents\")\n",
    "            return documents\n",
    "        \n",
    "        finally:\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    def _extract_docstrings(self, py_file: Path) -> Dict[str, str]:\n",
    "        \"\"\"Extract function docstrings from Python file.\"\"\"\n",
    "        import ast\n",
    "        \n",
    "        try:\n",
    "            with open(py_file) as f:\n",
    "                tree = ast.parse(f.read())\n",
    "            \n",
    "            docstrings = {}\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    docstring = ast.get_docstring(node)\n",
    "                    if docstring:\n",
    "                        docstrings[node.name] = docstring\n",
    "            \n",
    "            return docstrings\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {}\n",
    "    \n",
    "    def sync_dbt_docs(self, manifest_path: str, catalog_path: str):\n",
    "        \"\"\"\n",
    "        Sync from DBT manifest and catalog.\n",
    "        \n",
    "        Extracts:\n",
    "        - Table descriptions\n",
    "        - Column descriptions\n",
    "        - Tests\n",
    "        - Lineage (upstream/downstream)\n",
    "        \"\"\"\n",
    "        with open(manifest_path) as f:\n",
    "            manifest = json.load(f)\n",
    "        \n",
    "        with open(catalog_path) as f:\n",
    "            catalog = json.load(f)\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for node_id, node in manifest['nodes'].items():\n",
    "            if node['resource_type'] not in ['model', 'source']:\n",
    "                continue\n",
    "            \n",
    "            # Build comprehensive documentation\n",
    "            content_parts = [\n",
    "                f\"# {node['name']}\",\n",
    "                f\"\\n**Type:** {node['resource_type']}\",\n",
    "                f\"**Schema:** {node['schema']}\",\n",
    "                f\"**Database:** {node['database']}\",\n",
    "                f\"\\n## Description\\n{node.get('description', 'No description provided')}\",\n",
    "            ]\n",
    "            \n",
    "            # Add column information\n",
    "            if node_id in catalog['nodes']:\n",
    "                cat_node = catalog['nodes'][node_id]\n",
    "                content_parts.append(\"\\n## Columns\")\n",
    "                \n",
    "                for col_name, col_info in cat_node['columns'].items():\n",
    "                    content_parts.append(\n",
    "                        f\"\\n- **{col_name}** ({col_info['type']}): {col_info.get('comment', '')}\"\n",
    "                    )\n",
    "            \n",
    "            # Add tests\n",
    "            if 'tests' in node:\n",
    "                content_parts.append(\"\\n## Tests\")\n",
    "                for test in node['tests']:\n",
    "                    content_parts.append(f\"- {test}\")\n",
    "            \n",
    "            content = \"\\n\".join(content_parts)\n",
    "            \n",
    "            doc = Document(\n",
    "                id=f\"dbt_{node_id}\",\n",
    "                content=content,\n",
    "                source=f\"dbt:///{node['original_file_path']}\",\n",
    "                metadata={\n",
    "                    'type': 'dbt_model',\n",
    "                    'schema': node['schema'],\n",
    "                    'database': node['database'],\n",
    "                    'materialization': node.get('config', {}).get('materialized'),\n",
    "                    'tags': node.get('tags', [])\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"✅ Synced {len(documents)} DBT models\")\n",
    "        return documents\n",
    "    \n",
    "    def sync_snowflake_schema(\n",
    "        self,\n",
    "        connection_params: Dict,\n",
    "        databases: List[str]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sync table/view metadata from Snowflake INFORMATION_SCHEMA.\n",
    "        \n",
    "        Extracts:\n",
    "        - Table descriptions (COMMENT)\n",
    "        - Column names and types\n",
    "        - Row counts\n",
    "        - Last updated timestamp\n",
    "        \"\"\"\n",
    "        import snowflake.connector\n",
    "        \n",
    "        conn = snowflake.connector.connect(**connection_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for database in databases:\n",
    "            # Query INFORMATION_SCHEMA\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                t.table_schema,\n",
    "                t.table_name,\n",
    "                t.table_type,\n",
    "                t.row_count,\n",
    "                t.comment,\n",
    "                LISTAGG(c.column_name || ' (' || c.data_type || ')', ', ') AS columns\n",
    "            FROM {database}.INFORMATION_SCHEMA.TABLES t\n",
    "            LEFT JOIN {database}.INFORMATION_SCHEMA.COLUMNS c\n",
    "                ON t.table_schema = c.table_schema \n",
    "                AND t.table_name = c.table_name\n",
    "            WHERE t.table_schema NOT IN ('INFORMATION_SCHEMA')\n",
    "            GROUP BY 1,2,3,4,5\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(query)\n",
    "            \n",
    "            for row in cursor:\n",
    "                schema, table, table_type, row_count, comment, columns = row\n",
    "                \n",
    "                content = f\"\"\"\n",
    "# {database}.{schema}.{table}\n",
    "\n",
    "**Type:** {table_type}\n",
    "**Row Count:** {row_count:,}\n",
    "\n",
    "## Description\n",
    "{comment or 'No description'}\n",
    "\n",
    "## Columns\n",
    "{columns}\n",
    "\"\"\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    id=f\"snowflake_{database}_{schema}_{table}\",\n",
    "                    content=content,\n",
    "                    source=f\"snowflake://{database}/{schema}/{table}\",\n",
    "                    metadata={\n",
    "                        'type': 'snowflake_table',\n",
    "                        'database': database,\n",
    "                        'schema': schema,\n",
    "                        'table': table,\n",
    "                        'row_count': row_count,\n",
    "                        'table_type': table_type\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                documents.append(doc)\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"✅ Synced {len(documents)} Snowflake tables\")\n",
    "        return documents\n",
    "    \n",
    "    def full_sync(self, sources: List[DataSource]):\n",
    "        \"\"\"\n",
    "        Sync all configured data sources.\n",
    "        \n",
    "        Returns: Total documents indexed\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        \n",
    "        for source in sources:\n",
    "            print(f\"\\n📥 Syncing {source.name} ({source.type})...\")\n",
    "            \n",
    "            try:\n",
    "                if source.type == 'confluence':\n",
    "                    docs = self.sync_confluence(**source.config)\n",
    "                elif source.type == 'github':\n",
    "                    docs = self.sync_github_repo(**source.config)\n",
    "                elif source.type == 'dbt':\n",
    "                    docs = self.sync_dbt_docs(**source.config)\n",
    "                elif source.type == 'snowflake':\n",
    "                    docs = self.sync_snowflake_schema(**source.config)\n",
    "                else:\n",
    "                    print(f\"⚠️ Unknown source type: {source.type}\")\n",
    "                    continue\n",
    "                \n",
    "                all_documents.extend(docs)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error syncing {source.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Index all documents\n",
    "        print(f\"\\n📊 Indexing {len(all_documents)} documents...\")\n",
    "        \n",
    "        rag = RAGSystem()\n",
    "        for doc in all_documents:\n",
    "            rag.index_document(doc)\n",
    "        \n",
    "        print(f\"\\n✅ Full sync complete: {len(all_documents)} documents indexed\")\n",
    "        \n",
    "        return len(all_documents)\n",
    "\n",
    "# Configuration\n",
    "sources = [\n",
    "    DataSource(\n",
    "        name=\"Engineering Wiki\",\n",
    "        type=\"confluence\",\n",
    "        config={\n",
    "            \"base_url\": \"https://company.atlassian.net\",\n",
    "            \"space_key\": \"ENG\",\n",
    "            \"api_token\": os.getenv(\"CONFLUENCE_TOKEN\")\n",
    "        }\n",
    "    ),\n",
    "    DataSource(\n",
    "        name=\"Data Platform Repo\",\n",
    "        type=\"github\",\n",
    "        config={\n",
    "            \"repo_url\": \"https://github.com/company/data-platform\",\n",
    "            \"branch\": \"main\"\n",
    "        }\n",
    "    ),\n",
    "    DataSource(\n",
    "        name=\"DBT Documentation\",\n",
    "        type=\"dbt\",\n",
    "        config={\n",
    "            \"manifest_path\": \"/path/to/target/manifest.json\",\n",
    "            \"catalog_path\": \"/path/to/target/catalog.json\"\n",
    "        }\n",
    "    ),\n",
    "    DataSource(\n",
    "        name=\"Snowflake Warehouse\",\n",
    "        type=\"snowflake\",\n",
    "        config={\n",
    "            \"connection_params\": {\n",
    "                \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "                \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "                \"account\": \"company.us-east-1\",\n",
    "                \"warehouse\": \"ANALYTICS_WH\"\n",
    "            },\n",
    "            \"databases\": [\"ANALYTICS\", \"RAW\"]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run sync\n",
    "multi_rag = MultiSourceRAG(vector_db=\"pinecone\")\n",
    "total_docs = multi_rag.full_sync(sources)\n",
    "```\n",
    "\n",
    "**Slack Bot Integration:**\n",
    "\n",
    "```python\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "\n",
    "class SlackRAGBot:\n",
    "    \"\"\"\n",
    "    Slack bot for querying data documentation.\n",
    "    \n",
    "    Commands:\n",
    "    - @databot What tables have customer PII?\n",
    "    - /ask How is monthly revenue calculated?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag = rag_system\n",
    "        \n",
    "        # Initialize Slack app\n",
    "        self.app = App(token=os.getenv(\"SLACK_BOT_TOKEN\"))\n",
    "        \n",
    "        # Register handlers\n",
    "        self.app.message(self.handle_mention)\n",
    "        self.app.command(\"/ask\")(self.handle_command)\n",
    "        \n",
    "        # Track usage\n",
    "        self.usage_log = []\n",
    "    \n",
    "    def handle_mention(self, message, say):\n",
    "        \"\"\"Handle @databot mentions.\"\"\"\n",
    "        query = message['text'].split('>', 1)[1].strip()  # Remove @mention\n",
    "        user = message['user']\n",
    "        \n",
    "        # Show typing indicator\n",
    "        say(f\"🤔 Searching documentation...\")\n",
    "        \n",
    "        # Query RAG\n",
    "        result = self.rag.query(query)\n",
    "        \n",
    "        # Format response with sources\n",
    "        response = f\"*Answer:*\\n{result['answer']}\\n\\n\"\n",
    "        response += \"*Sources:*\\n\"\n",
    "        \n",
    "        for source in result['sources']:\n",
    "            response += f\"• <{source['source']}|{source['citation']}>\\n\"\n",
    "        \n",
    "        say(response)\n",
    "        \n",
    "        # Log usage\n",
    "        self.usage_log.append({\n",
    "            'user': user,\n",
    "            'query': query,\n",
    "            'timestamp': datetime.utcnow()\n",
    "        })\n",
    "    \n",
    "    def handle_command(self, ack, command, respond):\n",
    "        \"\"\"Handle /ask slash command.\"\"\"\n",
    "        ack()\n",
    "        \n",
    "        query = command['text']\n",
    "        \n",
    "        # Query RAG\n",
    "        result = self.rag.query(query)\n",
    "        \n",
    "        # Format as rich message block\n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": f\"*Question:* {query}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"divider\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": f\"*Answer:*\\n{result['answer']}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"context\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": f\"Confidence: {result['confidence']:.0%}\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"actions\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"button\",\n",
    "                        \"text\": {\"type\": \"plain_text\", \"text\": \"👍 Helpful\"},\n",
    "                        \"value\": \"positive\",\n",
    "                        \"action_id\": \"feedback_positive\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"button\",\n",
    "                        \"text\": {\"type\": \"plain_text\", \"text\": \"👎 Not helpful\"},\n",
    "                        \"value\": \"negative\",\n",
    "                        \"action_id\": \"feedback_negative\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        respond(blocks=blocks)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start Slack bot.\"\"\"\n",
    "        handler = SocketModeHandler(self.app, os.getenv(\"SLACK_APP_TOKEN\"))\n",
    "        print(\"⚡ Slack bot is running!\")\n",
    "        handler.start()\n",
    "\n",
    "# Deploy bot\n",
    "bot = SlackRAGBot(rag)\n",
    "bot.start()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4e1fa",
   "metadata": {},
   "source": [
    "## 1. Setup ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb openai langchain\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Cliente persistente\n",
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "# Crear colección\n",
    "collection = client.get_or_create_collection(\n",
    "    name='data_docs',\n",
    "    metadata={'description': 'Documentación técnica de datos'}\n",
    ")\n",
    "\n",
    "print(f'Colección creada: {collection.name}')\n",
    "print(f'Documentos: {collection.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912f4e5",
   "metadata": {},
   "source": [
    "## 2. Ingestión de documentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Genera embedding con OpenAI.\"\"\"\n",
    "    resp = client_openai.embeddings.create(\n",
    "        model='text-embedding-ada-002',\n",
    "        input=text\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "docs = [\n",
    "    {\n",
    "        'id': 'tabla_ventas',\n",
    "        'text': '''\n",
    "Tabla: ventas\n",
    "Esquema: dwh.ventas\n",
    "Descripción: Transacciones de ventas diarias desde 2020.\n",
    "Columnas:\n",
    "- venta_id (BIGINT, PK): identificador único\n",
    "- fecha (DATE): fecha de la transacción\n",
    "- producto_id (INT, FK): referencia a dim_productos\n",
    "- cantidad (INT): unidades vendidas\n",
    "- total (DECIMAL): monto en USD\n",
    "Frecuencia: actualización diaria a las 3 AM\n",
    "Owner: equipo-analytics\n",
    "        ''',\n",
    "        'metadata': {'type': 'schema', 'owner': 'analytics'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'pipeline_ventas',\n",
    "        'text': '''\n",
    "Pipeline: ventas_daily_etl\n",
    "Descripción: procesa ventas del día anterior\n",
    "Pasos:\n",
    "1. Extracción de S3 (bucket: raw-data/ventas/)\n",
    "2. Validación con Great Expectations\n",
    "3. Deduplicación por venta_id\n",
    "4. Enriquecimiento con datos de productos\n",
    "5. Carga a Redshift\n",
    "Dependencias: dim_productos debe estar actualizado\n",
    "Alertas: email a data-eng si falla\n",
    "        ''',\n",
    "        'metadata': {'type': 'pipeline', 'owner': 'data-eng'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'metrica_revenue',\n",
    "        'text': '''\n",
    "Métrica: monthly_revenue\n",
    "Definición: SUM(total) de ventas agrupado por mes\n",
    "Fórmula: SELECT DATE_TRUNC('month', fecha) mes, SUM(total) revenue FROM ventas GROUP BY 1\n",
    "Business owner: CFO\n",
    "Dashboards: Tableau (Revenue Overview)\n",
    "        ''',\n",
    "        'metadata': {'type': 'metric', 'owner': 'finance'}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Agregar a ChromaDB\n",
    "for doc in docs:\n",
    "    embedding = get_embedding(doc['text'])\n",
    "    collection.add(\n",
    "        ids=[doc['id']],\n",
    "        documents=[doc['text']],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[doc['metadata']]\n",
    "    )\n",
    "\n",
    "print(f'✅ {len(docs)} documentos indexados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceeedb0",
   "metadata": {},
   "source": [
    "## 3. Búsqueda semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 3) -> list:\n",
    "    \"\"\"Busca documentos relevantes.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "pregunta = '¿Qué tabla contiene información de ventas?'\n",
    "resultados = semantic_search(pregunta)\n",
    "\n",
    "print(f'Pregunta: {pregunta}\\n')\n",
    "for i, doc in enumerate(resultados['documents'][0]):\n",
    "    print(f'Resultado {i+1}:')\n",
    "    print(doc[:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1942ee3",
   "metadata": {},
   "source": [
    "## 4. RAG: respuesta con contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03111440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question: str) -> str:\n",
    "    \"\"\"Responde usando RAG.\"\"\"\n",
    "    # 1. Buscar contexto relevante\n",
    "    results = semantic_search(question, top_k=2)\n",
    "    context = '\\n\\n'.join(results['documents'][0])\n",
    "    \n",
    "    # 2. Prompt con contexto\n",
    "    prompt = f'''\n",
    "Eres un experto en ingeniería de datos. Responde la pregunta usando SOLO la información del contexto.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta (menciona la fuente si es relevante):\n",
    "'''\n",
    "    \n",
    "    # 3. Generar respuesta\n",
    "    resp = client_openai.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "preguntas = [\n",
    "    '¿Cuál es el esquema de la tabla de ventas?',\n",
    "    '¿A qué hora se actualiza la data de ventas?',\n",
    "    '¿Qué pipeline procesa las ventas?',\n",
    "    '¿Cómo se calcula el monthly revenue?'\n",
    "]\n",
    "\n",
    "for q in preguntas:\n",
    "    answer = rag_answer(q)\n",
    "    print(f'❓ {q}')\n",
    "    print(f'✅ {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd0647",
   "metadata": {},
   "source": [
    "## 5. RAG con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed49f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Embeddings y vectorstore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = Chroma(\n",
    "    persist_directory='./chroma_db',\n",
    "    embedding_function=embeddings,\n",
    "    collection_name='data_docs'\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({'query': '¿Quién es el owner de la tabla ventas?'})\n",
    "print('Respuesta:', result['result'])\n",
    "print('\\nFuentes:')\n",
    "for doc in result['source_documents']:\n",
    "    print('-', doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da038e49",
   "metadata": {},
   "source": [
    "## 6. Chunking avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b95679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para documentos largos\n",
    "long_doc = '''\n",
    "# Data Warehouse - Guía Completa\n",
    "\n",
    "## Arquitectura\n",
    "Redshift cluster con 5 nodos dc2.large.\n",
    "Schemas: raw, staging, dwh, analytics.\n",
    "\n",
    "## Tablas principales\n",
    "- ventas: transacciones diarias\n",
    "- clientes: información demográfica\n",
    "- productos: catálogo completo\n",
    "\n",
    "## Pipelines\n",
    "Airflow con 15 DAGs ejecutándose diariamente.\n",
    "'''\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n\\n', '\\n', '. ', ' ']\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(long_doc)\n",
    "print(f'Documento dividido en {len(chunks)} chunks:\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16ccaf",
   "metadata": {},
   "source": [
    "## 7. Filtrado por metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1253cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar solo pipelines\n",
    "results_pipeline = collection.query(\n",
    "    query_embeddings=[get_embedding('automatización de datos')],\n",
    "    n_results=5,\n",
    "    where={'type': 'pipeline'}\n",
    ")\n",
    "\n",
    "print('Pipelines encontrados:')\n",
    "for doc in results_pipeline['documents'][0]:\n",
    "    print('-', doc.split('\\n')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95d08f",
   "metadata": {},
   "source": [
    "## 8. Buenas prácticas RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800db9b8",
   "metadata": {},
   "source": [
    "- **Chunking inteligente**: divide por secciones lógicas (headers, párrafos).\n",
    "- **Metadatos ricos**: agrega source, timestamp, owner, versión.\n",
    "- **Híbrido**: combina búsqueda semántica + keyword search.\n",
    "- **Re-ranking**: usa modelos como Cohere Rerank para mejorar resultados.\n",
    "- **Cache**: guarda respuestas frecuentes.\n",
    "- **Actualización**: sincroniza vectorstore con cambios en docs.\n",
    "- **Monitoreo**: loggea queries, latencia, quality de respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6f6b2",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfb600",
   "metadata": {},
   "source": [
    "1. Indexa tu documentación real de data warehouse en ChromaDB.\n",
    "2. Construye un chatbot Slack que responda preguntas sobre esquemas.\n",
    "3. Implementa hybrid search (semántico + BM25) con LangChain.\n",
    "4. Crea un dashboard Streamlit para explorar el vectorstore."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
