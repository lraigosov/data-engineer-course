{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4fc791",
   "metadata": {},
   "source": [
    "# 🚀 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG\n",
    "\n",
    "**Objetivo**: construir un chatbot empresarial que permita consultar datos usando lenguaje natural, combinando RAG (documentación) y NL2SQL (queries).\n",
    "\n",
    "## Alcance del Proyecto\n",
    "\n",
    "- **Duración**: 4-6 horas\n",
    "- **Dificultad**: Alta\n",
    "- **Stack**: OpenAI, ChromaDB, LangChain, SQLite/PostgreSQL, Streamlit\n",
    "\n",
    "## Funcionalidades\n",
    "\n",
    "1. ✅ Indexar documentación de esquemas y métricas\n",
    "2. ✅ Responder preguntas sobre estructura de datos (RAG)\n",
    "3. ✅ Ejecutar queries en lenguaje natural (NL2SQL)\n",
    "4. ✅ Visualizar resultados en tablas/gráficos\n",
    "5. ✅ Historial de conversación\n",
    "6. ✅ Validación de seguridad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1ba73",
   "metadata": {},
   "source": [
    "### 🎯 **Arquitectura del Data Chatbot: RAG + NL2SQL Híbrido**\n",
    "\n",
    "**Componentes del Sistema:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                  DATA CHATBOT ARCHITECTURE                       │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  USER INTERFACE (Streamlit)                                     │\n",
    "│  ┌──────────────────────────────────────┐                      │\n",
    "│  │ • Natural language input             │                      │\n",
    "│  │ • Results display (tables, charts)   │                      │\n",
    "│  │ • Conversation history               │                      │\n",
    "│  │ • Export buttons (CSV, Excel)        │                      │\n",
    "│  └──────────────────────────────────────┘                      │\n",
    "│                    ↓                                             │\n",
    "│  INTENT CLASSIFIER (GPT-3.5)                                    │\n",
    "│  ┌──────────────────────────────────────┐                      │\n",
    "│  │ Input: \"What's our revenue by region?\"                      │\n",
    "│  │ Output: QUERY (needs SQL execution)  │                      │\n",
    "│  │                                       │                      │\n",
    "│  │ Input: \"What columns does sales have?\"                      │\n",
    "│  │ Output: SCHEMA (use RAG)             │                      │\n",
    "│  └──────────────────────────────────────┘                      │\n",
    "│          ↓                      ↓                                │\n",
    "│  ┌──────────────┐      ┌──────────────┐                        │\n",
    "│  │  RAG PATH    │      │  SQL PATH    │                        │\n",
    "│  └──────────────┘      └──────────────┘                        │\n",
    "│          ↓                      ↓                                │\n",
    "│  RETRIEVAL (ChromaDB)   NL2SQL (GPT-4)                         │\n",
    "│  ┌───────────────────┐  ┌───────────────────┐                 │\n",
    "│  │ • Vector search   │  │ • Schema context  │                 │\n",
    "│  │ • Top-K docs      │  │ • SQL generation  │                 │\n",
    "│  │ • Rerank (opt)    │  │ • Safety check    │                 │\n",
    "│  └───────────────────┘  └───────────────────┘                 │\n",
    "│          ↓                      ↓                                │\n",
    "│  GENERATION (GPT-4)     EXECUTION (SQLite/Postgres)            │\n",
    "│  ┌───────────────────┐  ┌───────────────────┐                 │\n",
    "│  │ • Context + Q     │  │ • Execute query   │                 │\n",
    "│  │ • Generate answer │  │ • Return DataFrame│                 │\n",
    "│  └───────────────────┘  └───────────────────┘                 │\n",
    "│          ↓                      ↓                                │\n",
    "│  ┌──────────────────────────────────────────┐                 │\n",
    "│  │       RESPONSE FORMATTING                 │                 │\n",
    "│  │ • Text answer (RAG)                       │                 │\n",
    "│  │ • Table + Chart (SQL)                     │                 │\n",
    "│  │ • Error handling                          │                 │\n",
    "│  └──────────────────────────────────────────┘                 │\n",
    "│                    ↓                                             │\n",
    "│  OBSERVABILITY & CACHING                                        │\n",
    "│  ┌──────────────────────────────────────────┐                 │\n",
    "│  │ • Query logs (audit trail)                │                 │\n",
    "│  │ • Performance metrics                     │                 │\n",
    "│  │ • Redis cache (frequent queries)          │                 │\n",
    "│  │ • User feedback collection                │                 │\n",
    "│  └──────────────────────────────────────────┘                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Design Decisions & Trade-offs:**\n",
    "\n",
    "| Decision | Rationale | Trade-off |\n",
    "|----------|-----------|-----------|\n",
    "| **Intent Classification First** | Ruta óptima: RAG es 10x más rápido que SQL | Clasificación errónea → respuesta subóptima (mitigado con GPT-3.5 accuracy >95%) |\n",
    "| **ChromaDB (embedded)** | Simple setup, no infra externa | Escala limitada (millones de docs → usar Pinecone/Weaviate) |\n",
    "| **GPT-4 para NL2SQL** | Mejor accuracy en SQL complejo (JOINs, subqueries) | 10x más caro que GPT-3.5 (mitigado con caching) |\n",
    "| **SQLite** | Zero-config, perfecto para demo | Producción → Postgres/MySQL con connection pooling |\n",
    "| **Streamlit** | Prototipo rápido, sin frontend | Limitado concurrency (producción → FastAPI + React) |\n",
    "\n",
    "**Intent Classification Strategy:**\n",
    "\n",
    "```python\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Intent(BaseModel):\n",
    "    \"\"\"Clasificación de intención con confidence\"\"\"\n",
    "    type: Literal['SCHEMA', 'QUERY', 'AGGREGATION', 'COMPARISON', 'TREND', 'AMBIGUOUS']\n",
    "    confidence: float  # 0-1\n",
    "    reasoning: str\n",
    "\n",
    "def classify_intent_advanced(question: str, conversation_history: list = None) -> Intent:\n",
    "    \"\"\"\n",
    "    Clasificador avanzado con contexto conversacional.\n",
    "    \n",
    "    Types:\n",
    "    - SCHEMA: Preguntas sobre estructura (\"¿Qué columnas...?\", \"¿Cómo se define...?\")\n",
    "    - QUERY: Consulta directa (\"¿Cuánto...?\", \"¿Quién es...?\")\n",
    "    - AGGREGATION: Requiere GROUP BY (\"Top 5\", \"Por categoría\")\n",
    "    - COMPARISON: Requiere múltiples queries (\"Compara X vs Y\")\n",
    "    - TREND: Serie temporal (\"Evolución\", \"Tendencia\")\n",
    "    - AMBIGUOUS: Necesita aclaración\n",
    "    \"\"\"\n",
    "    \n",
    "    # Contexto conversacional (últimas 3 interacciones)\n",
    "    context = \"\"\n",
    "    if conversation_history:\n",
    "        recent = conversation_history[-3:]\n",
    "        context = \"\\n\".join([f\"Q: {h['question']}\\nA: {h['answer_type']}\" for h in recent])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Clasifica esta pregunta considerando el historial de conversación.\n",
    "    \n",
    "    Historial reciente:\n",
    "    {context or 'Primera pregunta'}\n",
    "    \n",
    "    Pregunta actual: \"{question}\"\n",
    "    \n",
    "    Clasifica en:\n",
    "    - SCHEMA: Preguntas sobre definiciones, estructura de datos\n",
    "    - QUERY: Consultas directas que requieren SQL\n",
    "    - AGGREGATION: Análisis agregado (totales, promedios, top N)\n",
    "    - COMPARISON: Comparaciones entre entidades\n",
    "    - TREND: Análisis temporal (evolución, tendencias)\n",
    "    - AMBIGUOUS: Pregunta poco clara, necesita aclaración\n",
    "    \n",
    "    Responde en JSON:\n",
    "    {{\n",
    "        \"type\": \"...\",\n",
    "        \"confidence\": 0.95,\n",
    "        \"reasoning\": \"Breve explicación\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.1,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return Intent(**json.loads(response.choices[0].message.content))\n",
    "\n",
    "# Ejemplo\n",
    "intent = classify_intent_advanced(\n",
    "    \"¿Cómo han evolucionado las ventas de Laptops en el último trimestre?\",\n",
    "    conversation_history=[\n",
    "        {'question': '¿Qué productos vendemos?', 'answer_type': 'SCHEMA'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Tipo: {intent.type}\")\n",
    "print(f\"Confianza: {intent.confidence:.2%}\")\n",
    "print(f\"Razonamiento: {intent.reasoning}\")\n",
    "\n",
    "# Output:\n",
    "# Tipo: TREND\n",
    "# Confianza: 92%\n",
    "# Razonamiento: Pregunta sobre evolución temporal requiere serie temporal y filtro por producto\n",
    "```\n",
    "\n",
    "**Routing Logic (Production-Ready):**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class QueryPlan:\n",
    "    \"\"\"Plan de ejecución para la query\"\"\"\n",
    "    route: Literal['RAG', 'SQL', 'HYBRID', 'CLARIFY']\n",
    "    steps: list[str]\n",
    "    estimated_cost_usd: float\n",
    "    estimated_latency_sec: float\n",
    "\n",
    "def create_query_plan(intent: Intent, question: str) -> QueryPlan:\n",
    "    \"\"\"\n",
    "    Genera plan de ejecución óptimo basado en intención.\n",
    "    \n",
    "    Estrategias:\n",
    "    - RAG: Rápido (0.5s), barato ($0.001), para preguntas conceptuales\n",
    "    - SQL: Medio (2s), medio ($0.01), para queries directas\n",
    "    - HYBRID: Lento (4s), caro ($0.02), para análisis complejos\n",
    "    - CLARIFY: Pedir aclaración al usuario\n",
    "    \"\"\"\n",
    "    \n",
    "    if intent.type == 'SCHEMA':\n",
    "        return QueryPlan(\n",
    "            route='RAG',\n",
    "            steps=['Retrieve docs from ChromaDB', 'Generate answer with GPT-4'],\n",
    "            estimated_cost_usd=0.001,\n",
    "            estimated_latency_sec=0.5\n",
    "        )\n",
    "    \n",
    "    elif intent.type == 'QUERY' and intent.confidence > 0.85:\n",
    "        return QueryPlan(\n",
    "            route='SQL',\n",
    "            steps=['Generate SQL', 'Validate safety', 'Execute', 'Format results'],\n",
    "            estimated_cost_usd=0.01,\n",
    "            estimated_latency_sec=2.0\n",
    "        )\n",
    "    \n",
    "    elif intent.type in ['AGGREGATION', 'COMPARISON', 'TREND']:\n",
    "        return QueryPlan(\n",
    "            route='HYBRID',\n",
    "            steps=[\n",
    "                'Retrieve schema docs (RAG)',\n",
    "                'Generate complex SQL with context',\n",
    "                'Execute and analyze results',\n",
    "                'Generate insights with LLM'\n",
    "            ],\n",
    "            estimated_cost_usd=0.02,\n",
    "            estimated_latency_sec=4.0\n",
    "        )\n",
    "    \n",
    "    else:  # AMBIGUOUS or low confidence\n",
    "        return QueryPlan(\n",
    "            route='CLARIFY',\n",
    "            steps=['Ask user for clarification'],\n",
    "            estimated_cost_usd=0.0,\n",
    "            estimated_latency_sec=0.1\n",
    "        )\n",
    "\n",
    "def execute_plan(plan: QueryPlan, question: str, context: Dict[str, Any]) -> Dict:\n",
    "    \"\"\"Ejecuta el plan según la ruta\"\"\"\n",
    "    \n",
    "    if plan.route == 'RAG':\n",
    "        return execute_rag(question)\n",
    "    \n",
    "    elif plan.route == 'SQL':\n",
    "        return execute_sql(question, context)\n",
    "    \n",
    "    elif plan.route == 'HYBRID':\n",
    "        # Paso 1: RAG para contexto\n",
    "        schema_context = rag_search(question, top_k=3)\n",
    "        \n",
    "        # Paso 2: SQL mejorado con contexto\n",
    "        sql_result = execute_sql_with_context(question, schema_context)\n",
    "        \n",
    "        # Paso 3: LLM analiza resultados\n",
    "        insights = generate_insights(sql_result, question)\n",
    "        \n",
    "        return {\n",
    "            'type': 'HYBRID',\n",
    "            'data': sql_result['data'],\n",
    "            'sql': sql_result['sql'],\n",
    "            'insights': insights,\n",
    "            'plan': plan\n",
    "        }\n",
    "    \n",
    "    else:  # CLARIFY\n",
    "        return {\n",
    "            'type': 'CLARIFY',\n",
    "            'message': generate_clarification_question(question, context)\n",
    "        }\n",
    "\n",
    "# Ejemplo de flujo completo\n",
    "question = \"Compare sales performance between regions\"\n",
    "intent = classify_intent_advanced(question)\n",
    "plan = create_query_plan(intent, question)\n",
    "result = execute_plan(plan, question, context={'user_id': 'analyst_001'})\n",
    "\n",
    "print(f\"Plan: {plan.route}\")\n",
    "print(f\"Cost: ${plan.estimated_cost_usd:.4f}\")\n",
    "print(f\"Latency: {plan.estimated_latency_sec:.1f}s\")\n",
    "```\n",
    "\n",
    "**Error Recovery Patterns:**\n",
    "\n",
    "```python\n",
    "class ChatbotError(Exception):\n",
    "    \"\"\"Base exception para errores del chatbot\"\"\"\n",
    "    pass\n",
    "\n",
    "class SQLGenerationError(ChatbotError):\n",
    "    \"\"\"Error en generación de SQL\"\"\"\n",
    "    pass\n",
    "\n",
    "class ExecutionError(ChatbotError):\n",
    "    \"\"\"Error en ejecución de query\"\"\"\n",
    "    pass\n",
    "\n",
    "def chatbot_with_recovery(question: str, max_retries: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Chatbot con recuperación automática de errores.\n",
    "    \n",
    "    Estrategias:\n",
    "    1. SQL inválido → Regenerar con error message\n",
    "    2. Timeout → Sugerir query más simple\n",
    "    3. No results → Sugerir alternativas\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            # Intent classification\n",
    "            intent = classify_intent_advanced(question)\n",
    "            \n",
    "            if intent.confidence < 0.7:\n",
    "                # Low confidence → Ask for clarification\n",
    "                return {\n",
    "                    'type': 'CLARIFY',\n",
    "                    'message': f\"No estoy seguro de entender. ¿Quieres decir...?\",\n",
    "                    'suggestions': generate_suggestions(question)\n",
    "                }\n",
    "            \n",
    "            # Create and execute plan\n",
    "            plan = create_query_plan(intent, question)\n",
    "            result = execute_plan(plan, question, {})\n",
    "            \n",
    "            # Success\n",
    "            return result\n",
    "        \n",
    "        except SQLGenerationError as e:\n",
    "            if attempt < max_retries:\n",
    "                # Regenerar con más contexto\n",
    "                question_improved = f\"{question}. Error anterior: {str(e)}. Por favor genera SQL válido.\"\n",
    "                continue\n",
    "            else:\n",
    "                return {\n",
    "                    'type': 'ERROR',\n",
    "                    'message': 'No pude generar una consulta válida. ¿Puedes reformular la pregunta?'\n",
    "                }\n",
    "        \n",
    "        except ExecutionError as e:\n",
    "            if 'timeout' in str(e).lower():\n",
    "                return {\n",
    "                    'type': 'ERROR',\n",
    "                    'message': 'La consulta tomó demasiado tiempo. Intenta con un rango de fechas más pequeño.',\n",
    "                    'suggestion': 'Por ejemplo: \"Ventas del último mes\"'\n",
    "                }\n",
    "            elif 'no results' in str(e).lower():\n",
    "                return {\n",
    "                    'type': 'INFO',\n",
    "                    'message': 'No encontré resultados para esta consulta.',\n",
    "                    'suggestions': [\n",
    "                        'Intenta expandir el rango de fechas',\n",
    "                        'Verifica los filtros aplicados',\n",
    "                        'Revisa la ortografía de nombres'\n",
    "                    ]\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Unexpected error → Log and fallback to safe response\n",
    "            logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                'type': 'ERROR',\n",
    "                'message': 'Ocurrió un error inesperado. Por favor intenta nuevamente.',\n",
    "                'error_id': str(uuid.uuid4())  # For support tracking\n",
    "            }\n",
    "    \n",
    "    # Max retries exceeded\n",
    "    return {\n",
    "        'type': 'ERROR',\n",
    "        'message': 'No pude procesar tu pregunta después de varios intentos.'\n",
    "    }\n",
    "```\n",
    "\n",
    "**Conversational Context Management:**\n",
    "\n",
    "```python\n",
    "from collections import deque\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Maneja contexto conversacional para follow-up questions\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 10):\n",
    "        self.history = deque(maxlen=max_history)\n",
    "        self.last_sql = None\n",
    "        self.last_dataframe = None\n",
    "    \n",
    "    def add_interaction(self, question: str, response: Dict):\n",
    "        \"\"\"Añade interacción al historial\"\"\"\n",
    "        self.history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'question': question,\n",
    "            'response_type': response['type'],\n",
    "            'summary': self._summarize_response(response)\n",
    "        })\n",
    "        \n",
    "        # Guardar último SQL/DataFrame para follow-ups\n",
    "        if response['type'] == 'SQL':\n",
    "            self.last_sql = response.get('sql')\n",
    "            self.last_dataframe = response.get('data')\n",
    "    \n",
    "    def resolve_references(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Resuelve referencias pronominales (it, that, those, etc.)\n",
    "        \n",
    "        Ejemplos:\n",
    "        Q1: \"What are total sales by region?\"\n",
    "        Q2: \"Show me the top 3\" → \"Show me the top 3 regions by total sales\"\n",
    "        \"\"\"\n",
    "        \n",
    "        pronouns = ['it', 'that', 'those', 'them', 'this', 'these']\n",
    "        \n",
    "        if any(p in question.lower() for p in pronouns) and self.history:\n",
    "            last_q = self.history[-1]['question']\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Resuelve la referencia en esta pregunta usando el contexto.\n",
    "            \n",
    "            Pregunta anterior: \"{last_q}\"\n",
    "            Pregunta actual: \"{question}\"\n",
    "            \n",
    "            Reescribe la pregunta actual sin pronombres, haciendo explícito a qué se refiere.\n",
    "            \n",
    "            Pregunta resuelta:\n",
    "            \"\"\"\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model='gpt-3.5-turbo',\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        return question\n",
    "    \n",
    "    def suggest_follow_ups(self) -> list[str]:\n",
    "        \"\"\"Genera sugerencias de preguntas de seguimiento\"\"\"\n",
    "        \n",
    "        if not self.last_dataframe or len(self.last_dataframe) == 0:\n",
    "            return []\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # Análisis del DataFrame\n",
    "        df = self.last_dataframe\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if numeric_cols:\n",
    "            suggestions.append(f\"Muéstrame un gráfico de {numeric_cols[0]}\")\n",
    "            suggestions.append(f\"¿Cuál es el promedio de {numeric_cols[0]}?\")\n",
    "        \n",
    "        if 'fecha' in df.columns or 'date' in df.columns:\n",
    "            suggestions.append(\"¿Cuál es la tendencia en el tiempo?\")\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            suggestions.append(\"Muéstrame solo los top 5\")\n",
    "        \n",
    "        return suggestions[:3]  # Max 3 sugerencias\n",
    "\n",
    "# Uso\n",
    "conv = ConversationManager()\n",
    "\n",
    "q1 = \"What are total sales by category?\"\n",
    "r1 = chatbot_with_recovery(q1)\n",
    "conv.add_interaction(q1, r1)\n",
    "\n",
    "q2 = \"Show me the top 3\"  # Referencia ambigua\n",
    "q2_resolved = conv.resolve_references(q2)  # → \"Show me the top 3 categories by total sales\"\n",
    "r2 = chatbot_with_recovery(q2_resolved)\n",
    "conv.add_interaction(q2_resolved, r2)\n",
    "\n",
    "# Sugerencias\n",
    "suggestions = conv.suggest_follow_ups()\n",
    "print(\"Preguntas sugeridas:\")\n",
    "for s in suggestions:\n",
    "    print(f\"  • {s}\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebaca39",
   "metadata": {},
   "source": [
    "## Parte 1: Setup del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afaf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai chromadb langchain streamlit pandas plotly sqlalchemy\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Configuración\n",
    "os.environ['OPENAI_API_KEY'] = 'tu-api-key'  # Reemplazar\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print('✅ Setup completo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdec757",
   "metadata": {},
   "source": [
    "## Parte 2: Base de datos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear BD SQLite con datos de ejemplo\n",
    "conn = sqlite3.connect('empresa.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabla ventas\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS ventas (\n",
    "    venta_id INTEGER PRIMARY KEY,\n",
    "    fecha DATE,\n",
    "    producto TEXT,\n",
    "    categoria TEXT,\n",
    "    cantidad INTEGER,\n",
    "    precio_unitario REAL,\n",
    "    total REAL,\n",
    "    region TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Datos de ejemplo\n",
    "ventas_data = [\n",
    "    (1, '2024-01-15', 'Laptop Pro', 'Electrónica', 2, 1200, 2400, 'Norte'),\n",
    "    (2, '2024-01-16', 'Mouse Wireless', 'Accesorios', 10, 25, 250, 'Sur'),\n",
    "    (3, '2024-01-17', 'Teclado Mecánico', 'Accesorios', 5, 80, 400, 'Norte'),\n",
    "    (4, '2024-01-18', 'Monitor 27\"', 'Electrónica', 3, 350, 1050, 'Este'),\n",
    "    (5, '2024-01-19', 'Laptop Pro', 'Electrónica', 1, 1200, 1200, 'Oeste'),\n",
    "    (6, '2024-01-20', 'Webcam HD', 'Accesorios', 8, 60, 480, 'Norte')\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT OR REPLACE INTO ventas VALUES (?,?,?,?,?,?,?,?)', ventas_data)\n",
    "conn.commit()\n",
    "\n",
    "# Verificar\n",
    "df_ventas = pd.read_sql_query('SELECT * FROM ventas LIMIT 3', conn)\n",
    "print('✅ Base de datos creada\\n')\n",
    "print(df_ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c488754",
   "metadata": {},
   "source": [
    "## Parte 3: Indexar documentación (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed60ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB para RAG\n",
    "chroma_client = chromadb.PersistentClient(path='./chatbot_db')\n",
    "collection = chroma_client.get_or_create_collection(name='data_docs')\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    resp = client.embeddings.create(\n",
    "        model='text-embedding-ada-002',\n",
    "        input=text\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "# Documentación a indexar\n",
    "docs = [\n",
    "    {\n",
    "        'id': 'tabla_ventas',\n",
    "        'text': '''\n",
    "Tabla: ventas\n",
    "Descripción: Registro de todas las transacciones de venta de productos.\n",
    "Columnas:\n",
    "- venta_id: ID único de la venta\n",
    "- fecha: Fecha de la transacción\n",
    "- producto: Nombre del producto vendido\n",
    "- categoria: Categoría (Electrónica, Accesorios)\n",
    "- cantidad: Unidades vendidas\n",
    "- precio_unitario: Precio por unidad en USD\n",
    "- total: Monto total (cantidad * precio_unitario)\n",
    "- region: Región geográfica (Norte, Sur, Este, Oeste)\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        'id': 'metrica_revenue',\n",
    "        'text': '''\n",
    "Métrica: Revenue Total\n",
    "Definición: Suma del campo 'total' de todas las ventas.\n",
    "Cálculo: SELECT SUM(total) FROM ventas\n",
    "Uso: Dashboard ejecutivo, reportes mensuales\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        'id': 'producto_top',\n",
    "        'text': '''\n",
    "Análisis: Top Productos\n",
    "Query: SELECT producto, SUM(cantidad) as unidades, SUM(total) as revenue \n",
    "       FROM ventas GROUP BY producto ORDER BY revenue DESC\n",
    "Insight: Identifica productos más rentables\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Indexar\n",
    "for doc in docs:\n",
    "    collection.add(\n",
    "        ids=[doc['id']],\n",
    "        documents=[doc['text']],\n",
    "        embeddings=[get_embedding(doc['text'])]\n",
    "    )\n",
    "\n",
    "print(f'✅ {len(docs)} documentos indexados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86089301",
   "metadata": {},
   "source": [
    "## Parte 4: Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a8e03",
   "metadata": {},
   "source": [
    "### 🔐 **Security & Governance: Acceso Seguro a Datos**\n",
    "\n",
    "**Security Layers:**\n",
    "\n",
    "```python\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                  SECURITY ARCHITECTURE                        │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│                                                               │\n",
    "│  1️⃣ AUTHENTICATION (WHO are you?)                            │\n",
    "│     • JWT tokens                                             │\n",
    "│     • OAuth2 (SSO)                                           │\n",
    "│     • API keys (machine-to-machine)                          │\n",
    "│                                                               │\n",
    "│  2️⃣ AUTHORIZATION (WHAT can you access?)                     │\n",
    "│     • Role-Based Access Control (RBAC)                       │\n",
    "│     • Row-Level Security (RLS)                               │\n",
    "│     • Column masking (PII)                                   │\n",
    "│                                                               │\n",
    "│  3️⃣ SQL INJECTION PREVENTION                                 │\n",
    "│     • Whitelist validation                                   │\n",
    "│     • Parameterized queries                                  │\n",
    "│     • Read-only user                                         │\n",
    "│                                                               │\n",
    "│  4️⃣ PROMPT INJECTION PREVENTION                              │\n",
    "│     • System message isolation                               │\n",
    "│     • Output validation                                      │\n",
    "│     • Jailbreak detection                                    │\n",
    "│                                                               │\n",
    "│  5️⃣ AUDIT & MONITORING                                       │\n",
    "│     • Query logging                                          │\n",
    "│     • Sensitive data access tracking                         │\n",
    "│     • Anomaly detection                                      │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**SQL Injection Prevention (Defense in Depth):**\n",
    "\n",
    "```python\n",
    "from typing import Set\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "class SQLValidator:\n",
    "    \"\"\"Valida seguridad de SQL generado por LLM\"\"\"\n",
    "    \n",
    "    # Whitelist de operaciones permitidas\n",
    "    ALLOWED_KEYWORDS = {\n",
    "        'SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT',\n",
    "        'HAVING', 'AS', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'ON', 'AND', 'OR'\n",
    "    }\n",
    "    \n",
    "    # Blacklist de operaciones peligrosas\n",
    "    DANGEROUS_KEYWORDS = {\n",
    "        'INSERT', 'UPDATE', 'DELETE', 'DROP', 'ALTER', 'CREATE', 'TRUNCATE',\n",
    "        'EXEC', 'EXECUTE', 'GRANT', 'REVOKE', 'SCRIPT', 'xp_', 'sp_'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, allowed_tables: Set[str], allowed_columns: Dict[str, Set[str]]):\n",
    "        self.allowed_tables = allowed_tables\n",
    "        self.allowed_columns = allowed_columns  # {table: {col1, col2, ...}}\n",
    "    \n",
    "    def validate(self, sql: str) -> tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valida SQL en múltiples niveles.\n",
    "        \n",
    "        Returns:\n",
    "            (is_valid, error_message)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Level 1: Blacklist check (fast)\n",
    "        sql_upper = sql.upper()\n",
    "        for keyword in self.DANGEROUS_KEYWORDS:\n",
    "            if keyword in sql_upper:\n",
    "                return False, f\"Dangerous keyword detected: {keyword}\"\n",
    "        \n",
    "        # Level 2: Whitelist check\n",
    "        parsed = sqlparse.parse(sql)[0]\n",
    "        for token in parsed.tokens:\n",
    "            if token.ttype is Keyword or token.ttype is DML:\n",
    "                keyword = token.value.upper()\n",
    "                if keyword not in self.ALLOWED_KEYWORDS:\n",
    "                    return False, f\"Keyword not allowed: {keyword}\"\n",
    "        \n",
    "        # Level 3: Table validation\n",
    "        tables_used = self._extract_tables(parsed)\n",
    "        for table in tables_used:\n",
    "            if table not in self.allowed_tables:\n",
    "                return False, f\"Table not allowed: {table}\"\n",
    "        \n",
    "        # Level 4: Column validation (optional, más estricto)\n",
    "        # columns_used = self._extract_columns(parsed)\n",
    "        # for table, columns in columns_used.items():\n",
    "        #     for col in columns:\n",
    "        #         if col not in self.allowed_columns.get(table, set()):\n",
    "        #             return False, f\"Column not allowed: {table}.{col}\"\n",
    "        \n",
    "        # Level 5: Complexity check (prevent expensive queries)\n",
    "        if sql_upper.count('JOIN') > 3:\n",
    "            return False, \"Too many JOINs (max 3)\"\n",
    "        \n",
    "        if 'UNION' in sql_upper and sql_upper.count('UNION') > 2:\n",
    "            return False, \"Too many UNIONs (max 2)\"\n",
    "        \n",
    "        # Level 6: Injection patterns\n",
    "        injection_patterns = [\n",
    "            r\";\\s*DROP\",\n",
    "            r\"OR\\s+1\\s*=\\s*1\",\n",
    "            r\"--\\s*$\",\n",
    "            r\"\\/\\*.*\\*\\/\",\n",
    "            r\"xp_cmdshell\",\n",
    "            r\"WAITFOR\\s+DELAY\"\n",
    "        ]\n",
    "        \n",
    "        import re\n",
    "        for pattern in injection_patterns:\n",
    "            if re.search(pattern, sql, re.IGNORECASE):\n",
    "                return False, f\"Injection pattern detected: {pattern}\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def _extract_tables(self, parsed) -> Set[str]:\n",
    "        \"\"\"Extrae nombres de tablas del SQL\"\"\"\n",
    "        tables = set()\n",
    "        \n",
    "        from_seen = False\n",
    "        for token in parsed.tokens:\n",
    "            if from_seen:\n",
    "                if isinstance(token, IdentifierList):\n",
    "                    for identifier in token.get_identifiers():\n",
    "                        tables.add(identifier.get_real_name())\n",
    "                elif isinstance(token, Identifier):\n",
    "                    tables.add(token.get_real_name())\n",
    "                from_seen = False\n",
    "            \n",
    "            if token.ttype is Keyword and token.value.upper() == 'FROM':\n",
    "                from_seen = True\n",
    "        \n",
    "        return tables\n",
    "\n",
    "# Uso\n",
    "validator = SQLValidator(\n",
    "    allowed_tables={'ventas', 'productos', 'clientes'},\n",
    "    allowed_columns={\n",
    "        'ventas': {'venta_id', 'fecha', 'producto', 'total', 'region'},\n",
    "        'clientes': {'cliente_id', 'nombre', 'email'}  # Note: no SSN, credit_card, etc\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test cases\n",
    "test_queries = [\n",
    "    \"SELECT * FROM ventas WHERE region='Norte'\",  # ✅ Valid\n",
    "    \"SELECT * FROM ventas; DROP TABLE ventas;\",   # ❌ SQL injection\n",
    "    \"SELECT * FROM usuarios WHERE 1=1 OR 1=1\",     # ❌ Injection pattern\n",
    "    \"SELECT * FROM ventas JOIN productos ON ventas.producto=productos.producto_id\",  # ✅ Valid JOIN\n",
    "]\n",
    "\n",
    "for sql in test_queries:\n",
    "    is_valid, error = validator.validate(sql)\n",
    "    print(f\"{'✅' if is_valid else '❌'} {sql[:50]}...\")\n",
    "    if error:\n",
    "        print(f\"   Error: {error}\")\n",
    "```\n",
    "\n",
    "**Prompt Injection Prevention:**\n",
    "\n",
    "```python\n",
    "class PromptInjectionDetector:\n",
    "    \"\"\"Detecta intentos de prompt injection\"\"\"\n",
    "    \n",
    "    # Patrones comunes de jailbreak\n",
    "    JAILBREAK_PATTERNS = [\n",
    "        r\"ignore (previous|above) instructions\",\n",
    "        r\"disregard (previous|above|all)\",\n",
    "        r\"forget (what|everything) (you|i) (told|said)\",\n",
    "        r\"you are now\",\n",
    "        r\"pretend (you|to) (are|be)\",\n",
    "        r\"roleplay\",\n",
    "        r\"DAN mode\",\n",
    "        r\"developer mode\",\n",
    "        r\"system: \",\n",
    "        r\"<\\|im_start\\|>\",\n",
    "        r\"PWNED\",\n",
    "    ]\n",
    "    \n",
    "    def detect(self, user_input: str) -> tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Detecta prompt injection.\n",
    "        \n",
    "        Returns:\n",
    "            (is_safe, detected_pattern)\n",
    "        \"\"\"\n",
    "        \n",
    "        input_lower = user_input.lower()\n",
    "        \n",
    "        # Check jailbreak patterns\n",
    "        for pattern in self.JAILBREAK_PATTERNS:\n",
    "            if re.search(pattern, input_lower):\n",
    "                return False, pattern\n",
    "        \n",
    "        # Check for system role impersonation\n",
    "        if user_input.strip().startswith(('system:', 'assistant:', 'user:')):\n",
    "            return False, \"Role impersonation\"\n",
    "        \n",
    "        # Check for excessive prompt tokens (> 1000 words might be malicious)\n",
    "        if len(user_input.split()) > 1000:\n",
    "            return False, \"Excessive input length\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def sanitize(self, user_input: str) -> str:\n",
    "        \"\"\"Sanitiza input del usuario\"\"\"\n",
    "        \n",
    "        # Remove potential role markers\n",
    "        sanitized = re.sub(r'^(system|assistant|user):\\s*', '', user_input, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Escape special tokens (if using specific LLM)\n",
    "        sanitized = sanitized.replace('<|im_start|>', '').replace('<|im_end|>', '')\n",
    "        \n",
    "        # Trim excessive whitespace\n",
    "        sanitized = ' '.join(sanitized.split())\n",
    "        \n",
    "        return sanitized\n",
    "\n",
    "def secure_chatbot(user_input: str, user_context: Dict) -> Dict:\n",
    "    \"\"\"Chatbot con validación de seguridad\"\"\"\n",
    "    \n",
    "    # 1. Prompt injection detection\n",
    "    detector = PromptInjectionDetector()\n",
    "    is_safe, pattern = detector.detect(user_input)\n",
    "    \n",
    "    if not is_safe:\n",
    "        logger.warning(f\"Prompt injection detected: {pattern} from user {user_context['user_id']}\")\n",
    "        return {\n",
    "            'type': 'ERROR',\n",
    "            'message': 'Input validation failed. Please rephrase your question.'\n",
    "        }\n",
    "    \n",
    "    # 2. Sanitize input\n",
    "    sanitized_input = detector.sanitize(user_input)\n",
    "    \n",
    "    # 3. Classify intent\n",
    "    intent = classify_intent_advanced(sanitized_input)\n",
    "    \n",
    "    # 4. If SQL route, validate generated SQL\n",
    "    if intent.type in ['QUERY', 'AGGREGATION']:\n",
    "        sql = nl_to_sql(sanitized_input)\n",
    "        \n",
    "        validator = SQLValidator(\n",
    "            allowed_tables=user_context.get('allowed_tables', {'ventas'}),\n",
    "            allowed_columns=user_context.get('allowed_columns', {})\n",
    "        )\n",
    "        \n",
    "        is_valid, error = validator.validate(sql)\n",
    "        \n",
    "        if not is_valid:\n",
    "            logger.error(f\"Invalid SQL generated: {error}\")\n",
    "            return {\n",
    "                'type': 'ERROR',\n",
    "                'message': 'Query validation failed. Please try a different question.'\n",
    "            }\n",
    "        \n",
    "        # Execute with read-only user\n",
    "        result = execute_sql_readonly(sql, user_context)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        # RAG path (also validate output)\n",
    "        answer = rag_answer(sanitized_input)\n",
    "        \n",
    "        # Check if answer leaks sensitive info\n",
    "        if contains_pii(answer):\n",
    "            answer = mask_pii(answer)\n",
    "        \n",
    "        return {'type': 'RAG', 'answer': answer}\n",
    "```\n",
    "\n",
    "**Role-Based Access Control (RBAC):**\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "\n",
    "class Role(Enum):\n",
    "    VIEWER = 'viewer'       # Read-only, limited tables\n",
    "    ANALYST = 'analyst'     # Read all tables, no PII\n",
    "    ADMIN = 'admin'         # Full access\n",
    "    DATA_SCIENTIST = 'data_scientist'  # Read all + export\n",
    "\n",
    "class Permission(Enum):\n",
    "    READ_SALES = 'read:sales'\n",
    "    READ_CUSTOMERS = 'read:customers'\n",
    "    READ_PII = 'read:pii'\n",
    "    EXPORT_DATA = 'export:data'\n",
    "    EXECUTE_JOINS = 'execute:joins'\n",
    "\n",
    "# Role → Permissions mapping\n",
    "ROLE_PERMISSIONS = {\n",
    "    Role.VIEWER: {\n",
    "        Permission.READ_SALES,\n",
    "    },\n",
    "    Role.ANALYST: {\n",
    "        Permission.READ_SALES,\n",
    "        Permission.READ_CUSTOMERS,\n",
    "        Permission.EXECUTE_JOINS,\n",
    "    },\n",
    "    Role.DATA_SCIENTIST: {\n",
    "        Permission.READ_SALES,\n",
    "        Permission.READ_CUSTOMERS,\n",
    "        Permission.EXECUTE_JOINS,\n",
    "        Permission.EXPORT_DATA,\n",
    "    },\n",
    "    Role.ADMIN: {\n",
    "        Permission.READ_SALES,\n",
    "        Permission.READ_CUSTOMERS,\n",
    "        Permission.READ_PII,\n",
    "        Permission.EXECUTE_JOINS,\n",
    "        Permission.EXPORT_DATA,\n",
    "    }\n",
    "}\n",
    "\n",
    "class AccessControlManager:\n",
    "    \"\"\"Gestiona acceso basado en roles\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id: str, role: Role):\n",
    "        self.user_id = user_id\n",
    "        self.role = role\n",
    "        self.permissions = ROLE_PERMISSIONS[role]\n",
    "    \n",
    "    def can(self, permission: Permission) -> bool:\n",
    "        \"\"\"Verifica si el usuario tiene el permiso\"\"\"\n",
    "        return permission in self.permissions\n",
    "    \n",
    "    def get_allowed_tables(self) -> Set[str]:\n",
    "        \"\"\"Retorna tablas permitidas según rol\"\"\"\n",
    "        tables = set()\n",
    "        \n",
    "        if Permission.READ_SALES in self.permissions:\n",
    "            tables.add('ventas')\n",
    "            tables.add('productos')\n",
    "        \n",
    "        if Permission.READ_CUSTOMERS in self.permissions:\n",
    "            tables.add('clientes')\n",
    "        \n",
    "        # PII tables solo para admin\n",
    "        if Permission.READ_PII in self.permissions:\n",
    "            tables.add('empleados_salarios')\n",
    "            tables.add('clientes_detalles')\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def apply_row_level_security(self, sql: str) -> str:\n",
    "        \"\"\"\n",
    "        Aplica Row-Level Security (RLS) al SQL.\n",
    "        \n",
    "        Ejemplo: Si user_role=VIEWER y region='Norte',\n",
    "                 solo puede ver ventas de región Norte.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Viewer: solo su región\n",
    "        if self.role == Role.VIEWER:\n",
    "            user_region = self._get_user_region(self.user_id)\n",
    "            if 'WHERE' in sql.upper():\n",
    "                sql = sql.replace('WHERE', f\"WHERE region='{user_region}' AND\", 1)\n",
    "            else:\n",
    "                sql += f\" WHERE region='{user_region}'\"\n",
    "        \n",
    "        return sql\n",
    "    \n",
    "    def mask_columns(self, df: pd.DataFrame, table: str) -> pd.DataFrame:\n",
    "        \"\"\"Enmascara columnas sensibles según permisos\"\"\"\n",
    "        \n",
    "        # Si no tiene permiso READ_PII, enmascarar datos sensibles\n",
    "        if Permission.READ_PII not in self.permissions:\n",
    "            pii_columns = {\n",
    "                'clientes': ['email', 'telefono', 'direccion'],\n",
    "                'empleados': ['ssn', 'salario', 'cuenta_bancaria']\n",
    "            }\n",
    "            \n",
    "            for col in pii_columns.get(table, []):\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].apply(lambda x: '***MASKED***')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_user_region(self, user_id: str) -> str:\n",
    "        \"\"\"Obtiene región del usuario (desde DB o cache)\"\"\"\n",
    "        # Placeholder - en producción consultar DB\n",
    "        return 'Norte'\n",
    "\n",
    "def require_permission(permission: Permission):\n",
    "    \"\"\"Decorator para proteger endpoints\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Obtener ACL del contexto (inyectado por FastAPI Depends)\n",
    "            acl = kwargs.get('acl')\n",
    "            if not acl or not acl.can(permission):\n",
    "                raise HTTPException(403, f\"Permission denied: {permission.value}\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Uso en chatbot\n",
    "def secure_query_execution(question: str, user_id: str, role: Role) -> Dict:\n",
    "    \"\"\"Ejecuta query con control de acceso\"\"\"\n",
    "    \n",
    "    # Setup ACL\n",
    "    acl = AccessControlManager(user_id, role)\n",
    "    \n",
    "    # Generate SQL\n",
    "    sql = nl_to_sql(question)\n",
    "    \n",
    "    # Validate tables\n",
    "    validator = SQLValidator(\n",
    "        allowed_tables=acl.get_allowed_tables(),\n",
    "        allowed_columns={}  # Configure based on role\n",
    "    )\n",
    "    \n",
    "    is_valid, error = validator.validate(sql)\n",
    "    if not is_valid:\n",
    "        return {'type': 'ERROR', 'message': f'Access denied: {error}'}\n",
    "    \n",
    "    # Apply RLS\n",
    "    sql_with_rls = acl.apply_row_level_security(sql)\n",
    "    \n",
    "    # Execute\n",
    "    df = pd.read_sql_query(sql_with_rls, conn)\n",
    "    \n",
    "    # Mask PII\n",
    "    df_masked = acl.mask_columns(df, table='clientes')  # Detect table from SQL\n",
    "    \n",
    "    # Check export permission\n",
    "    can_export = acl.can(Permission.EXPORT_DATA)\n",
    "    \n",
    "    return {\n",
    "        'type': 'SQL',\n",
    "        'sql': sql_with_rls,\n",
    "        'data': df_masked,\n",
    "        'can_export': can_export\n",
    "    }\n",
    "\n",
    "# Ejemplo\n",
    "result = secure_query_execution(\n",
    "    question=\"Show me all customers\",\n",
    "    user_id='analyst_001',\n",
    "    role=Role.ANALYST\n",
    ")\n",
    "\n",
    "# ANALYST puede ver clientes, pero email/teléfono están enmascarados\n",
    "print(result['data'])\n",
    "#    cliente_id        nombre              email        telefono\n",
    "# 0           1    Empresa ABC  ***MASKED***   ***MASKED***\n",
    "```\n",
    "\n",
    "**Audit Logging:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class AuditLogger:\n",
    "    \"\"\"Logs de auditoría para compliance\"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "        self._create_audit_table()\n",
    "    \n",
    "    def _create_audit_table(self):\n",
    "        \"\"\"Crea tabla de auditoría\"\"\"\n",
    "        self.db.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS audit_log (\n",
    "                log_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                timestamp TEXT,\n",
    "                user_id TEXT,\n",
    "                user_role TEXT,\n",
    "                action TEXT,\n",
    "                question TEXT,\n",
    "                sql_generated TEXT,\n",
    "                tables_accessed TEXT,\n",
    "                rows_returned INTEGER,\n",
    "                execution_time_ms INTEGER,\n",
    "                success BOOLEAN,\n",
    "                error_message TEXT,\n",
    "                ip_address TEXT,\n",
    "                session_id TEXT\n",
    "            )\n",
    "        ''')\n",
    "    \n",
    "    def log_query(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        user_role: str,\n",
    "        question: str,\n",
    "        sql: Optional[str],\n",
    "        tables: Set[str],\n",
    "        rows_returned: int,\n",
    "        execution_time_ms: int,\n",
    "        success: bool,\n",
    "        error: Optional[str] = None,\n",
    "        ip_address: Optional[str] = None,\n",
    "        session_id: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Registra ejecución de query\"\"\"\n",
    "        \n",
    "        self.db.execute('''\n",
    "            INSERT INTO audit_log VALUES (\n",
    "                NULL, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?\n",
    "            )\n",
    "        ''', (\n",
    "            datetime.now().isoformat(),\n",
    "            user_id,\n",
    "            user_role,\n",
    "            'QUERY',\n",
    "            question,\n",
    "            sql,\n",
    "            json.dumps(list(tables)),\n",
    "            rows_returned,\n",
    "            execution_time_ms,\n",
    "            success,\n",
    "            error,\n",
    "            ip_address,\n",
    "            session_id\n",
    "        ))\n",
    "        \n",
    "        self.db.commit()\n",
    "    \n",
    "    def get_user_activity(self, user_id: str, days: int = 7) -> pd.DataFrame:\n",
    "        \"\"\"Obtiene actividad de un usuario\"\"\"\n",
    "        return pd.read_sql_query(f'''\n",
    "            SELECT \n",
    "                timestamp,\n",
    "                question,\n",
    "                tables_accessed,\n",
    "                rows_returned,\n",
    "                success\n",
    "            FROM audit_log\n",
    "            WHERE user_id = ?\n",
    "              AND timestamp >= datetime('now', '-{days} days')\n",
    "            ORDER BY timestamp DESC\n",
    "        ''', self.db, params=(user_id,))\n",
    "    \n",
    "    def detect_anomalies(self) -> pd.DataFrame:\n",
    "        \"\"\"Detecta actividad sospechosa\"\"\"\n",
    "        return pd.read_sql_query('''\n",
    "            SELECT \n",
    "                user_id,\n",
    "                COUNT(*) as query_count,\n",
    "                SUM(rows_returned) as total_rows,\n",
    "                COUNT(DISTINCT tables_accessed) as unique_tables\n",
    "            FROM audit_log\n",
    "            WHERE timestamp >= datetime('now', '-1 hour')\n",
    "            GROUP BY user_id\n",
    "            HAVING query_count > 100  -- Anomaly threshold\n",
    "               OR total_rows > 1000000  -- Data exfiltration?\n",
    "        ''', self.db)\n",
    "\n",
    "# Uso integrado\n",
    "audit = AuditLogger(conn)\n",
    "\n",
    "def chatbot_with_audit(question: str, user_context: Dict) -> Dict:\n",
    "    \"\"\"Chatbot con auditoría completa\"\"\"\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = secure_query_execution(\n",
    "            question=question,\n",
    "            user_id=user_context['user_id'],\n",
    "            role=user_context['role']\n",
    "        )\n",
    "        \n",
    "        execution_time = int((time.time() - start) * 1000)\n",
    "        \n",
    "        # Log successful execution\n",
    "        audit.log_query(\n",
    "            user_id=user_context['user_id'],\n",
    "            user_role=user_context['role'].value,\n",
    "            question=question,\n",
    "            sql=result.get('sql'),\n",
    "            tables={'ventas'},  # Extract from SQL in production\n",
    "            rows_returned=len(result.get('data', [])),\n",
    "            execution_time_ms=execution_time,\n",
    "            success=True,\n",
    "            ip_address=user_context.get('ip'),\n",
    "            session_id=user_context.get('session_id')\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        execution_time = int((time.time() - start) * 1000)\n",
    "        \n",
    "        # Log error\n",
    "        audit.log_query(\n",
    "            user_id=user_context['user_id'],\n",
    "            user_role=user_context['role'].value,\n",
    "            question=question,\n",
    "            sql=None,\n",
    "            tables=set(),\n",
    "            rows_returned=0,\n",
    "            execution_time_ms=execution_time,\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )\n",
    "        \n",
    "        raise\n",
    "\n",
    "# Monitoring dashboard\n",
    "anomalies = audit.detect_anomalies()\n",
    "if not anomalies.empty:\n",
    "    print(\"⚠️  Suspicious activity detected:\")\n",
    "    print(anomalies)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_search(question: str, top_k: int = 2):\n",
    "    \"\"\"Busca contexto relevante.\"\"\"\n",
    "    query_emb = get_embedding(question)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_emb],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return '\\n\\n'.join(results['documents'][0])\n",
    "\n",
    "def rag_answer(question: str):\n",
    "    \"\"\"Responde usando RAG.\"\"\"\n",
    "    context = rag_search(question)\n",
    "    \n",
    "    prompt = f'''\n",
    "Eres un asistente de datos experto. Responde basándote SOLO en el contexto.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Test\n",
    "q1 = '¿Qué columnas tiene la tabla ventas?'\n",
    "print(f'❓ {q1}')\n",
    "print(f'✅ {rag_answer(q1)}\\n')\n",
    "\n",
    "q2 = '¿Cómo se calcula el revenue total?'\n",
    "print(f'❓ {q2}')\n",
    "print(f'✅ {rag_answer(q2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6845b",
   "metadata": {},
   "source": [
    "## Parte 5: Sistema NL2SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a71d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema():\n",
    "    \"\"\"Obtiene esquema de la BD.\"\"\"\n",
    "    return '''\n",
    "Tabla: ventas\n",
    "Columnas:\n",
    "- venta_id INTEGER\n",
    "- fecha DATE\n",
    "- producto TEXT\n",
    "- categoria TEXT\n",
    "- cantidad INTEGER\n",
    "- precio_unitario REAL\n",
    "- total REAL\n",
    "- region TEXT\n",
    "'''\n",
    "\n",
    "def is_safe_query(query: str):\n",
    "    \"\"\"Valida seguridad.\"\"\"\n",
    "    dangerous = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'ALTER', 'CREATE']\n",
    "    return not any(kw in query.upper() for kw in dangerous)\n",
    "\n",
    "def nl_to_sql(question: str):\n",
    "    \"\"\"Convierte pregunta a SQL.\"\"\"\n",
    "    schema = get_schema()\n",
    "    \n",
    "    prompt = f'''\n",
    "Esquema de base de datos:\n",
    "{schema}\n",
    "\n",
    "Convierte esta pregunta a SQL (SQLite):\n",
    "{question}\n",
    "\n",
    "Reglas:\n",
    "- Solo SELECT\n",
    "- Devuelve SOLO el SQL, sin explicaciones\n",
    "\n",
    "SQL:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```sql','').replace('```','').strip()\n",
    "\n",
    "def execute_nl_query(question: str):\n",
    "    \"\"\"Ejecuta query desde lenguaje natural.\"\"\"\n",
    "    try:\n",
    "        sql = nl_to_sql(question)\n",
    "        \n",
    "        if not is_safe_query(sql):\n",
    "            return {'error': 'Query no segura detectada'}\n",
    "        \n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        \n",
    "        return {\n",
    "            'sql': sql,\n",
    "            'data': df,\n",
    "            'rows': len(df)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test\n",
    "q3 = '¿Cuántas ventas hubo en total?'\n",
    "result = execute_nl_query(q3)\n",
    "print(f'❓ {q3}')\n",
    "print(f'SQL generado: {result[\"sql\"]}')\n",
    "print(f'Resultado:\\n{result[\"data\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84551f",
   "metadata": {},
   "source": [
    "## Parte 6: Chatbot unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(question: str):\n",
    "    \"\"\"Clasifica la intención de la pregunta.\"\"\"\n",
    "    prompt = f'''\n",
    "Clasifica esta pregunta en UNA categoría:\n",
    "- SCHEMA: pregunta sobre estructura de datos, definiciones\n",
    "- QUERY: requiere ejecutar una consulta SQL\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Responde solo con: SCHEMA o QUERY\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def chatbot(question: str):\n",
    "    \"\"\"Chatbot inteligente que elige RAG o NL2SQL.\"\"\"\n",
    "    intent = classify_intent(question)\n",
    "    \n",
    "    if intent == 'SCHEMA':\n",
    "        return {\n",
    "            'type': 'RAG',\n",
    "            'answer': rag_answer(question)\n",
    "        }\n",
    "    else:  # QUERY\n",
    "        result = execute_nl_query(question)\n",
    "        if 'error' in result:\n",
    "            return {'type': 'ERROR', 'answer': result['error']}\n",
    "        return {\n",
    "            'type': 'SQL',\n",
    "            'sql': result['sql'],\n",
    "            'data': result['data'],\n",
    "            'rows': result['rows']\n",
    "        }\n",
    "\n",
    "# Tests\n",
    "preguntas = [\n",
    "    '¿Qué información tiene la tabla ventas?',\n",
    "    '¿Cuál fue el total de ventas por región?',\n",
    "    '¿Cómo se define el revenue?',\n",
    "    'Top 3 productos por ingresos'\n",
    "]\n",
    "\n",
    "for q in preguntas:\n",
    "    print(f'\\n❓ {q}')\n",
    "    response = chatbot(q)\n",
    "    print(f'Tipo: {response[\"type\"]}')\n",
    "    if response['type'] == 'SQL':\n",
    "        print(f'SQL: {response[\"sql\"]}')\n",
    "        print(response['data'])\n",
    "    else:\n",
    "        print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048c846",
   "metadata": {},
   "source": [
    "### ⚡ **Performance Optimization: Caching & Query Optimization**\n",
    "\n",
    "**Multi-Layer Caching Strategy:**\n",
    "\n",
    "```python\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                    CACHING ARCHITECTURE                         │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  L1: EMBEDDINGS CACHE (Persistent, Disk)                       │\n",
    "│      • ChromaDB stores embeddings                              │\n",
    "│      • Avoids re-embedding same questions                      │\n",
    "│      • ~$0.0001 saved per cached query                         │\n",
    "│                                                                 │\n",
    "│  L2: INTENT CLASSIFICATION CACHE (Redis, 1 hour TTL)           │\n",
    "│      • Cache question → intent mapping                         │\n",
    "│      • Key: hash(question) → Intent object                     │\n",
    "│      • Hit rate: ~40% for common questions                     │\n",
    "│      • ~0.5s saved per hit                                     │\n",
    "│                                                                 │\n",
    "│  L3: SQL GENERATION CACHE (Redis, 24 hour TTL)                 │\n",
    "│      • Cache question → SQL mapping                            │\n",
    "│      • Invalidate on schema changes                            │\n",
    "│      • Hit rate: ~60% for repetitive queries                   │\n",
    "│      • ~2s + $0.01 saved per hit                               │\n",
    "│                                                                 │\n",
    "│  L4: QUERY RESULTS CACHE (Redis, 5 min TTL)                    │\n",
    "│      • Cache SQL → DataFrame                                   │\n",
    "│      • Short TTL (data freshness)                              │\n",
    "│      • Hit rate: ~30% for dashboards                           │\n",
    "│      • ~5s saved per hit (DB query time)                       │\n",
    "│                                                                 │\n",
    "│  L5: RAG ANSWER CACHE (Redis, 1 week TTL)                      │\n",
    "│      • Cache schema questions → answers                        │\n",
    "│      • Long TTL (schema rarely changes)                        │\n",
    "│      • Hit rate: ~70% for documentation queries                │\n",
    "│      • ~1s + $0.005 saved per hit                              │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Redis-Based Caching Implementation:**\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import hashlib\n",
    "import pickle\n",
    "from typing import Optional, Any\n",
    "from functools import wraps\n",
    "\n",
    "class CacheManager:\n",
    "    \"\"\"Multi-layer cache manager con Redis\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_url: str = 'redis://localhost:6379/0'):\n",
    "        self.redis = redis.from_url(redis_url, decode_responses=False)\n",
    "        \n",
    "        # TTL por tipo de cache (segundos)\n",
    "        self.ttl_config = {\n",
    "            'intent': 3600,        # 1 hour\n",
    "            'sql': 86400,          # 24 hours\n",
    "            'results': 300,        # 5 minutes\n",
    "            'rag': 604800,         # 1 week\n",
    "        }\n",
    "    \n",
    "    def _make_key(self, cache_type: str, query: str, context: Dict = None) -> str:\n",
    "        \"\"\"Genera cache key determinístico\"\"\"\n",
    "        # Include context in key for user-specific caching\n",
    "        key_parts = [cache_type, query]\n",
    "        if context:\n",
    "            key_parts.append(json.dumps(context, sort_keys=True))\n",
    "        \n",
    "        key_string = '|'.join(key_parts)\n",
    "        key_hash = hashlib.sha256(key_string.encode()).hexdigest()[:16]\n",
    "        \n",
    "        return f\"chatbot:{cache_type}:{key_hash}\"\n",
    "    \n",
    "    def get(self, cache_type: str, query: str, context: Dict = None) -> Optional[Any]:\n",
    "        \"\"\"Obtiene valor del cache\"\"\"\n",
    "        key = self._make_key(cache_type, query, context)\n",
    "        \n",
    "        cached = self.redis.get(key)\n",
    "        if cached:\n",
    "            # Deserialize\n",
    "            return pickle.loads(cached)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def set(self, cache_type: str, query: str, value: Any, context: Dict = None):\n",
    "        \"\"\"Guarda valor en cache con TTL\"\"\"\n",
    "        key = self._make_key(cache_type, query, context)\n",
    "        ttl = self.ttl_config.get(cache_type, 3600)\n",
    "        \n",
    "        # Serialize\n",
    "        serialized = pickle.dumps(value)\n",
    "        \n",
    "        self.redis.setex(key, ttl, serialized)\n",
    "    \n",
    "    def invalidate(self, cache_type: str, pattern: str = '*'):\n",
    "        \"\"\"Invalida cache por tipo\"\"\"\n",
    "        keys_pattern = f\"chatbot:{cache_type}:{pattern}\"\n",
    "        \n",
    "        for key in self.redis.scan_iter(match=keys_pattern):\n",
    "            self.redis.delete(key)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Estadísticas de cache\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for cache_type in self.ttl_config.keys():\n",
    "            pattern = f\"chatbot:{cache_type}:*\"\n",
    "            keys = list(self.redis.scan_iter(match=pattern))\n",
    "            stats[cache_type] = {\n",
    "                'keys': len(keys),\n",
    "                'ttl': self.ttl_config[cache_type]\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Decorator para cachear funciones\n",
    "def cached(cache_type: str):\n",
    "    \"\"\"Decorator para cachear automáticamente\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Extract query from args\n",
    "            query = args[0] if args else kwargs.get('question', kwargs.get('query'))\n",
    "            \n",
    "            # Check cache\n",
    "            cache_mgr = kwargs.get('cache_manager') or CacheManager()\n",
    "            cached_result = cache_mgr.get(cache_type, query)\n",
    "            \n",
    "            if cached_result is not None:\n",
    "                print(f\"✅ Cache HIT [{cache_type}]: {query[:50]}...\")\n",
    "                return cached_result\n",
    "            \n",
    "            print(f\"❌ Cache MISS [{cache_type}]: {query[:50]}...\")\n",
    "            \n",
    "            # Execute function\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Store in cache\n",
    "            cache_mgr.set(cache_type, query, result)\n",
    "            \n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Aplicar caching a funciones existentes\n",
    "cache = CacheManager()\n",
    "\n",
    "@cached('intent')\n",
    "def classify_intent_cached(question: str, cache_manager=None):\n",
    "    \"\"\"Intent classification con cache\"\"\"\n",
    "    return classify_intent_advanced(question)\n",
    "\n",
    "@cached('sql')\n",
    "def nl_to_sql_cached(question: str, cache_manager=None):\n",
    "    \"\"\"NL2SQL con cache\"\"\"\n",
    "    return nl_to_sql(question)\n",
    "\n",
    "@cached('rag')\n",
    "def rag_answer_cached(question: str, cache_manager=None):\n",
    "    \"\"\"RAG con cache\"\"\"\n",
    "    return rag_answer(question)\n",
    "\n",
    "# Chatbot con caching completo\n",
    "def chatbot_with_caching(question: str) -> Dict:\n",
    "    \"\"\"Chatbot optimizado con multi-layer caching\"\"\"\n",
    "    \n",
    "    # Layer 2: Intent cache\n",
    "    intent = classify_intent_cached(question, cache_manager=cache)\n",
    "    \n",
    "    if intent.type == 'SCHEMA':\n",
    "        # Layer 5: RAG answer cache\n",
    "        answer = rag_answer_cached(question, cache_manager=cache)\n",
    "        return {'type': 'RAG', 'answer': answer}\n",
    "    \n",
    "    else:\n",
    "        # Layer 3: SQL generation cache\n",
    "        sql = nl_to_sql_cached(question, cache_manager=cache)\n",
    "        \n",
    "        # Layer 4: Query results cache\n",
    "        cache_key = f\"results:{sql}\"\n",
    "        cached_df = cache.get('results', cache_key)\n",
    "        \n",
    "        if cached_df is not None:\n",
    "            print(f\"✅ Cache HIT [results]\")\n",
    "            return {\n",
    "                'type': 'SQL',\n",
    "                'sql': sql,\n",
    "                'data': cached_df,\n",
    "                'from_cache': True\n",
    "            }\n",
    "        \n",
    "        # Execute query\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        \n",
    "        # Cache results\n",
    "        cache.set('results', cache_key, df)\n",
    "        \n",
    "        return {\n",
    "            'type': 'SQL',\n",
    "            'sql': sql,\n",
    "            'data': df,\n",
    "            'from_cache': False\n",
    "        }\n",
    "\n",
    "# Performance comparison\n",
    "import time\n",
    "\n",
    "def benchmark_caching():\n",
    "    \"\"\"Compara performance con y sin cache\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"What are total sales by region?\",\n",
    "        \"What columns does sales table have?\",\n",
    "        \"Show me top 5 products by revenue\"\n",
    "    ] * 3  # Repetir para demostrar cache hits\n",
    "    \n",
    "    # Sin cache\n",
    "    start = time.time()\n",
    "    for q in questions:\n",
    "        _ = chatbot(q)  # Función original\n",
    "    time_no_cache = time.time() - start\n",
    "    \n",
    "    # Con cache\n",
    "    start = time.time()\n",
    "    for q in questions:\n",
    "        _ = chatbot_with_caching(q)\n",
    "    time_with_cache = time.time() - start\n",
    "    \n",
    "    print(f\"\\n📊 CACHING PERFORMANCE:\")\n",
    "    print(f\"Without cache: {time_no_cache:.2f}s\")\n",
    "    print(f\"With cache: {time_with_cache:.2f}s\")\n",
    "    print(f\"Speedup: {time_no_cache/time_with_cache:.1f}x\")\n",
    "    \n",
    "    # Cost savings\n",
    "    cost_per_query = 0.01  # Average\n",
    "    queries_cached = len(questions) * 0.6  # Assuming 60% hit rate\n",
    "    cost_saved = queries_cached * cost_per_query\n",
    "    \n",
    "    print(f\"Cost saved: ${cost_saved:.2f}\")\n",
    "\n",
    "# Cache stats dashboard\n",
    "stats = cache.get_stats()\n",
    "print(\"\\n📈 CACHE STATISTICS:\")\n",
    "for cache_type, data in stats.items():\n",
    "    print(f\"{cache_type}: {data['keys']} keys, TTL={data['ttl']}s\")\n",
    "```\n",
    "\n",
    "**Query Optimization Strategies:**\n",
    "\n",
    "```python\n",
    "class QueryOptimizer:\n",
    "    \"\"\"Optimiza SQL generado por LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "    \n",
    "    def optimize(self, sql: str) -> tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Optimiza SQL y retorna versión mejorada + explicación.\n",
    "        \n",
    "        Optimizations:\n",
    "        1. Add LIMIT if missing (prevent full table scans)\n",
    "        2. Suggest indexes for WHERE/JOIN columns\n",
    "        3. Replace SELECT * with explicit columns\n",
    "        4. Rewrite subqueries as JOINs when possible\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizations = []\n",
    "        optimized_sql = sql\n",
    "        \n",
    "        # 1. Add LIMIT if missing (safety)\n",
    "        if 'LIMIT' not in sql.upper():\n",
    "            optimized_sql += ' LIMIT 10000'\n",
    "            optimizations.append({\n",
    "                'type': 'LIMIT_ADDED',\n",
    "                'reason': 'Prevent full table scan (max 10K rows)',\n",
    "                'impact': 'High'\n",
    "            })\n",
    "        \n",
    "        # 2. Check for SELECT *\n",
    "        if 'SELECT *' in sql.upper():\n",
    "            # Suggest explicit columns (would need schema context)\n",
    "            optimizations.append({\n",
    "                'type': 'SELECT_STAR',\n",
    "                'reason': 'SELECT * retrieves unnecessary columns',\n",
    "                'suggestion': 'Specify only needed columns',\n",
    "                'impact': 'Medium'\n",
    "            })\n",
    "        \n",
    "        # 3. Analyze EXPLAIN QUERY PLAN\n",
    "        explain_result = self.db.execute(f'EXPLAIN QUERY PLAN {optimized_sql}').fetchall()\n",
    "        \n",
    "        for row in explain_result:\n",
    "            plan_detail = row[3]  # SQLite EXPLAIN format\n",
    "            \n",
    "            # Detect full table scan\n",
    "            if 'SCAN TABLE' in plan_detail and 'USING INDEX' not in plan_detail:\n",
    "                table_name = plan_detail.split('SCAN TABLE')[1].split()[0]\n",
    "                optimizations.append({\n",
    "                    'type': 'FULL_TABLE_SCAN',\n",
    "                    'reason': f'Full table scan on {table_name}',\n",
    "                    'suggestion': f'Consider adding index on WHERE clause columns',\n",
    "                    'impact': 'High'\n",
    "                })\n",
    "        \n",
    "        # 4. Execution time estimate\n",
    "        start = time.time()\n",
    "        self.db.execute(optimized_sql)\n",
    "        execution_time = time.time() - start\n",
    "        \n",
    "        if execution_time > 5.0:\n",
    "            optimizations.append({\n",
    "                'type': 'SLOW_QUERY',\n",
    "                'reason': f'Query took {execution_time:.2f}s (threshold: 5s)',\n",
    "                'suggestion': 'Consider adding indexes or simplifying query',\n",
    "                'impact': 'Critical'\n",
    "            })\n",
    "        \n",
    "        return optimized_sql, {\n",
    "            'optimizations': optimizations,\n",
    "            'execution_time_sec': execution_time,\n",
    "            'explain_plan': explain_result\n",
    "        }\n",
    "    \n",
    "    def suggest_indexes(self, sql: str) -> list[str]:\n",
    "        \"\"\"Sugiere índices basados en el SQL\"\"\"\n",
    "        \n",
    "        # Parse WHERE clause\n",
    "        import sqlparse\n",
    "        parsed = sqlparse.parse(sql)[0]\n",
    "        \n",
    "        where_columns = self._extract_where_columns(parsed)\n",
    "        \n",
    "        suggestions = []\n",
    "        for table, columns in where_columns.items():\n",
    "            for col in columns:\n",
    "                suggestions.append(f\"CREATE INDEX idx_{table}_{col} ON {table}({col});\")\n",
    "        \n",
    "        return suggestions\n",
    "    \n",
    "    def _extract_where_columns(self, parsed) -> Dict[str, Set[str]]:\n",
    "        \"\"\"Extrae columnas usadas en WHERE clause\"\"\"\n",
    "        # Simplified - production would use proper SQL parsing\n",
    "        where_columns = {}\n",
    "        \n",
    "        # Placeholder logic\n",
    "        # In production: parse AST to extract WHERE conditions\n",
    "        \n",
    "        return where_columns\n",
    "\n",
    "# Uso en chatbot\n",
    "def chatbot_with_optimization(question: str) -> Dict:\n",
    "    \"\"\"Chatbot con optimización de queries\"\"\"\n",
    "    \n",
    "    sql = nl_to_sql_cached(question, cache_manager=cache)\n",
    "    \n",
    "    # Optimize SQL\n",
    "    optimizer = QueryOptimizer(conn)\n",
    "    optimized_sql, opt_info = optimizer.optimize(sql)\n",
    "    \n",
    "    # Execute optimized SQL\n",
    "    df = pd.read_sql_query(optimized_sql, conn)\n",
    "    \n",
    "    # Suggest indexes if slow\n",
    "    suggestions = []\n",
    "    if opt_info['execution_time_sec'] > 2.0:\n",
    "        suggestions = optimizer.suggest_indexes(optimized_sql)\n",
    "    \n",
    "    return {\n",
    "        'type': 'SQL',\n",
    "        'sql': optimized_sql,\n",
    "        'data': df,\n",
    "        'optimization_info': opt_info,\n",
    "        'index_suggestions': suggestions\n",
    "    }\n",
    "\n",
    "# Ejemplo\n",
    "result = chatbot_with_optimization(\"Show me all sales from last year\")\n",
    "\n",
    "print(f\"SQL: {result['sql']}\")\n",
    "print(f\"Execution time: {result['optimization_info']['execution_time_sec']:.2f}s\")\n",
    "print(f\"Optimizations applied: {len(result['optimization_info']['optimizations'])}\")\n",
    "\n",
    "if result['index_suggestions']:\n",
    "    print(\"\\n💡 Index suggestions:\")\n",
    "    for suggestion in result['index_suggestions']:\n",
    "        print(f\"   {suggestion}\")\n",
    "```\n",
    "\n",
    "**Concurrent Request Handling:**\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "class AsyncChatbot:\n",
    "    \"\"\"Chatbot con procesamiento paralelo\"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers: int = 10):\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.cache = CacheManager()\n",
    "    \n",
    "    async def process_question(self, question: str, user_context: Dict) -> Dict:\n",
    "        \"\"\"Procesa una pregunta de forma asíncrona\"\"\"\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Run blocking chatbot function in thread pool\n",
    "        result = await loop.run_in_executor(\n",
    "            self.executor,\n",
    "            chatbot_with_caching,\n",
    "            question\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def process_batch(self, questions: List[str], user_context: Dict) -> List[Dict]:\n",
    "        \"\"\"Procesa múltiples preguntas en paralelo\"\"\"\n",
    "        \n",
    "        tasks = [\n",
    "            self.process_question(q, user_context)\n",
    "            for q in questions\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Uso con FastAPI\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "async_chatbot = AsyncChatbot(max_workers=10)\n",
    "\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    user_id: str\n",
    "\n",
    "class BatchRequest(BaseModel):\n",
    "    questions: List[str]\n",
    "    user_id: str\n",
    "\n",
    "@app.post('/v1/chat')\n",
    "async def chat_endpoint(request: QuestionRequest):\n",
    "    \"\"\"Endpoint asíncrono para preguntas individuales\"\"\"\n",
    "    \n",
    "    result = await async_chatbot.process_question(\n",
    "        question=request.question,\n",
    "        user_context={'user_id': request.user_id}\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "@app.post('/v1/chat/batch')\n",
    "async def batch_chat_endpoint(request: BatchRequest):\n",
    "    \"\"\"Endpoint para procesar múltiples preguntas en paralelo\"\"\"\n",
    "    \n",
    "    results = await async_chatbot.process_batch(\n",
    "        questions=request.questions,\n",
    "        user_context={'user_id': request.user_id}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'count': len(results),\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# Benchmark parallelism\n",
    "async def benchmark_parallel():\n",
    "    questions = [\n",
    "        \"What are total sales?\",\n",
    "        \"Top 5 products by revenue?\",\n",
    "        \"Sales by region?\",\n",
    "        \"Average order value?\",\n",
    "        \"Customer count by segment?\"\n",
    "    ]\n",
    "    \n",
    "    # Sequential\n",
    "    start = time.time()\n",
    "    for q in questions:\n",
    "        _ = chatbot_with_caching(q)\n",
    "    seq_time = time.time() - start\n",
    "    \n",
    "    # Parallel\n",
    "    start = time.time()\n",
    "    results = await async_chatbot.process_batch(questions, {})\n",
    "    par_time = time.time() - start\n",
    "    \n",
    "    print(f\"Sequential: {seq_time:.2f}s\")\n",
    "    print(f\"Parallel: {par_time:.2f}s\")\n",
    "    print(f\"Speedup: {seq_time/par_time:.1f}x\")\n",
    "\n",
    "# asyncio.run(benchmark_parallel())\n",
    "```\n",
    "\n",
    "**Performance Monitoring Dashboard:**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Métricas de performance del chatbot\"\"\"\n",
    "    total_requests: int\n",
    "    cache_hits: int\n",
    "    cache_misses: int\n",
    "    avg_latency_ms: float\n",
    "    p95_latency_ms: float\n",
    "    p99_latency_ms: float\n",
    "    error_rate: float\n",
    "    cost_total_usd: float\n",
    "    cost_per_request_usd: float\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitorea performance del chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def record(self, request_type: str, latency_ms: float, from_cache: bool, cost_usd: float, success: bool):\n",
    "        \"\"\"Registra métrica de una request\"\"\"\n",
    "        self.metrics['latency'].append(latency_ms)\n",
    "        self.metrics['cache_hit'].append(1 if from_cache else 0)\n",
    "        self.metrics['cost'].append(cost_usd)\n",
    "        self.metrics['success'].append(1 if success else 0)\n",
    "        self.metrics['type'].append(request_type)\n",
    "    \n",
    "    def get_metrics(self) -> PerformanceMetrics:\n",
    "        \"\"\"Calcula métricas agregadas\"\"\"\n",
    "        \n",
    "        total = len(self.metrics['latency'])\n",
    "        cache_hits = sum(self.metrics['cache_hit'])\n",
    "        \n",
    "        latencies = sorted(self.metrics['latency'])\n",
    "        p95_idx = int(len(latencies) * 0.95)\n",
    "        p99_idx = int(len(latencies) * 0.99)\n",
    "        \n",
    "        return PerformanceMetrics(\n",
    "            total_requests=total,\n",
    "            cache_hits=cache_hits,\n",
    "            cache_misses=total - cache_hits,\n",
    "            avg_latency_ms=np.mean(self.metrics['latency']),\n",
    "            p95_latency_ms=latencies[p95_idx] if latencies else 0,\n",
    "            p99_latency_ms=latencies[p99_idx] if latencies else 0,\n",
    "            error_rate=(total - sum(self.metrics['success'])) / total if total > 0 else 0,\n",
    "            cost_total_usd=sum(self.metrics['cost']),\n",
    "            cost_per_request_usd=np.mean(self.metrics['cost'])\n",
    "        )\n",
    "    \n",
    "    def create_dashboard(self) -> go.Figure:\n",
    "        \"\"\"Genera dashboard de performance\"\"\"\n",
    "        \n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Latency Distribution', 'Cache Hit Rate', 'Cost Over Time', 'Request Types']\n",
    "        )\n",
    "        \n",
    "        # Latency histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.metrics['latency'], name='Latency (ms)'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Cache hit rate\n",
    "        cache_hit_rate = sum(self.metrics['cache_hit']) / len(self.metrics['cache_hit']) * 100\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode='gauge+number',\n",
    "                value=cache_hit_rate,\n",
    "                title={'text': 'Cache Hit Rate (%)'},\n",
    "                gauge={'axis': {'range': [0, 100]}}\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Cost over time\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=np.cumsum(self.metrics['cost']), mode='lines', name='Cumulative Cost'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Request types pie\n",
    "        type_counts = pd.Series(self.metrics['type']).value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=type_counts.index, values=type_counts.values),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=800, showlegend=False, title_text=\"Chatbot Performance Dashboard\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Integración con chatbot\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "def chatbot_monitored(question: str) -> Dict:\n",
    "    \"\"\"Chatbot con monitoreo de performance\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = chatbot_with_caching(question)\n",
    "        \n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        from_cache = result.get('from_cache', False)\n",
    "        cost = 0.0 if from_cache else 0.01  # Estimate\n",
    "        \n",
    "        monitor.record(\n",
    "            request_type=result['type'],\n",
    "            latency_ms=latency_ms,\n",
    "            from_cache=from_cache,\n",
    "            cost_usd=cost,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        monitor.record(\n",
    "            request_type='ERROR',\n",
    "            latency_ms=latency_ms,\n",
    "            from_cache=False,\n",
    "            cost_usd=0.0,\n",
    "            success=False\n",
    "        )\n",
    "        \n",
    "        raise\n",
    "\n",
    "# Generar reporte\n",
    "metrics = monitor.get_metrics()\n",
    "print(f\"\"\"\n",
    "📊 PERFORMANCE REPORT\n",
    "━━━━━━━━━━━━━━━━━━━━━\n",
    "Total Requests:     {metrics.total_requests}\n",
    "Cache Hit Rate:     {metrics.cache_hits / metrics.total_requests * 100:.1f}%\n",
    "Avg Latency:        {metrics.avg_latency_ms:.0f}ms\n",
    "P95 Latency:        {metrics.p95_latency_ms:.0f}ms\n",
    "P99 Latency:        {metrics.p99_latency_ms:.0f}ms\n",
    "Error Rate:         {metrics.error_rate * 100:.2f}%\n",
    "Total Cost:         ${metrics.cost_total_usd:.2f}\n",
    "Cost per Request:   ${metrics.cost_per_request_usd:.4f}\n",
    "\"\"\")\n",
    "\n",
    "# dashboard = monitor.create_dashboard()\n",
    "# dashboard.show()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab567a1",
   "metadata": {},
   "source": [
    "## Parte 7: Interfaz Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a55071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como app.py y ejecutar: streamlit run app.py\n",
    "\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from chatbot import chatbot  # Importar función\n",
    "\n",
    "st.set_page_config(page_title='Data Chatbot', page_icon='🤖', layout='wide')\n",
    "\n",
    "st.title('🤖 Chatbot de Consulta de Datos')\n",
    "st.markdown('Pregunta sobre tus datos en lenguaje natural')\n",
    "\n",
    "# Historial en session_state\n",
    "if 'history' not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "\n",
    "# Input\n",
    "question = st.text_input('Tu pregunta:', placeholder='Ej: ¿Cuáles son las ventas totales por categoría?')\n",
    "\n",
    "if st.button('Consultar') and question:\n",
    "    with st.spinner('Procesando...'):\n",
    "        response = chatbot(question)\n",
    "        st.session_state.history.append({'q': question, 'r': response})\n",
    "        \n",
    "        if response['type'] == 'SQL':\n",
    "            st.success(f\"SQL ejecutado: `{response['sql']}`\")\n",
    "            st.dataframe(response['data'])\n",
    "            \n",
    "            # Visualización automática si es numérico\n",
    "            if len(response['data'].columns) == 2:\n",
    "                fig = px.bar(response['data'], x=response['data'].columns[0], y=response['data'].columns[1])\n",
    "                st.plotly_chart(fig)\n",
    "        else:\n",
    "            st.info(response['answer'])\n",
    "\n",
    "# Historial\n",
    "if st.session_state.history:\n",
    "    st.markdown('---')\n",
    "    st.subheader('📜 Historial')\n",
    "    for item in reversed(st.session_state.history[-5:]):\n",
    "        st.markdown(f\"**Q:** {item['q']}\")\n",
    "        if item['r']['type'] == 'SQL':\n",
    "            st.code(item['r']['sql'])\n",
    "        st.markdown('---')\n",
    "'''\n",
    "\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print('✅ App Streamlit guardada en app.py')\n",
    "print('Ejecuta: streamlit run app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f573a",
   "metadata": {},
   "source": [
    "## Parte 8: Mejoras y extensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f439c",
   "metadata": {},
   "source": [
    "### 🚀 **Production Deployment & Scaling Strategy**\n",
    "\n",
    "**Deployment Architecture (Cloud-Native):**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────┐\n",
    "│              PRODUCTION DEPLOYMENT ARCHITECTURE                   │\n",
    "├──────────────────────────────────────────────────────────────────┤\n",
    "│                                                                   │\n",
    "│  EDGE LAYER (CDN + Load Balancer)                               │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │ CloudFlare / CloudFront            │                         │\n",
    "│  │  • Static assets caching           │                         │\n",
    "│  │  • DDoS protection                 │                         │\n",
    "│  │  • SSL termination                 │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "│                    ↓                                              │\n",
    "│  APPLICATION LAYER (Kubernetes)                                  │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │ FastAPI Pods (autoscale 3-20)      │                         │\n",
    "│  │  • Uvicorn workers: 4 per pod      │                         │\n",
    "│  │  • Resources: 1 CPU, 2GB RAM       │                         │\n",
    "│  │  • Health checks: /health           │                         │\n",
    "│  │  • Horizontal Pod Autoscaler        │                         │\n",
    "│  │    (target: 70% CPU)                │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "│                    ↓                                              │\n",
    "│  CACHING LAYER (Redis Cluster)                                   │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │ Redis (HA, 3 nodes)                │                         │\n",
    "│  │  • Intent cache (1h TTL)            │                         │\n",
    "│  │  • SQL cache (24h TTL)              │                         │\n",
    "│  │  • Results cache (5min TTL)         │                         │\n",
    "│  │  • Persistence: AOF + RDB           │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "│                    ↓                                              │\n",
    "│  VECTOR STORE (Pinecone / Weaviate)                             │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │ Production vector DB                │                         │\n",
    "│  │  • 10M+ vectors capacity            │                         │\n",
    "│  │  • Automatic backups                │                         │\n",
    "│  │  • Replication: 2x                  │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "│                    ↓                                              │\n",
    "│  DATA LAYER (PostgreSQL RDS)                                     │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │ AWS RDS PostgreSQL (read replicas)  │                         │\n",
    "│  │  • Primary: writes + reads          │                         │\n",
    "│  │  • Replica 1: chatbot reads         │                         │\n",
    "│  │  • Replica 2: analytics reads       │                         │\n",
    "│  │  • Connection pooling (PgBouncer)   │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "│                                                                   │\n",
    "│  OBSERVABILITY (Prometheus + Grafana)                            │\n",
    "│  ┌────────────────────────────────────┐                         │\n",
    "│  │  • Metrics: latency, errors, cost   │                         │\n",
    "│  │  • Logs: ELK Stack (ES, Logstash)   │                         │\n",
    "│  │  • Tracing: Jaeger (distributed)    │                         │\n",
    "│  │  • Alerts: PagerDuty integration    │                         │\n",
    "│  └────────────────────────────────────┘                         │\n",
    "└──────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Kubernetes Deployment Manifest:**\n",
    "\n",
    "```yaml\n",
    "# chatbot-deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: chatbot-api\n",
    "  namespace: data-platform\n",
    "spec:\n",
    "  replicas: 3  # Minimum replicas\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: chatbot-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: chatbot-api\n",
    "        version: v1.2.0\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fastapi\n",
    "        image: myregistry.io/chatbot-api:v1.2.0\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "        env:\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: openai-secrets\n",
    "              key: api-key\n",
    "        - name: REDIS_URL\n",
    "          value: \"redis://redis-cluster:6379/0\"\n",
    "        - name: DATABASE_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: db-secrets\n",
    "              key: connection-string\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "        \n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: chatbot-api\n",
    "  namespace: data-platform\n",
    "spec:\n",
    "  selector:\n",
    "    app: chatbot-api\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: chatbot-hpa\n",
    "  namespace: data-platform\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: chatbot-api\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 20\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: http_requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"1000\"\n",
    "```\n",
    "\n",
    "**Health Checks & Circuit Breaker:**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, Response, status\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class HealthChecker:\n",
    "    \"\"\"Verifica salud del servicio\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "    \n",
    "    def check_health(self) -> tuple[bool, Dict]:\n",
    "        \"\"\"Comprehensive health check\"\"\"\n",
    "        \n",
    "        checks = {}\n",
    "        is_healthy = True\n",
    "        \n",
    "        # 1. Redis connectivity\n",
    "        try:\n",
    "            cache.redis.ping()\n",
    "            checks['redis'] = {'status': 'UP', 'latency_ms': 1}\n",
    "        except Exception as e:\n",
    "            checks['redis'] = {'status': 'DOWN', 'error': str(e)}\n",
    "            is_healthy = False\n",
    "        \n",
    "        # 2. Database connectivity\n",
    "        try:\n",
    "            conn.execute('SELECT 1')\n",
    "            checks['database'] = {'status': 'UP'}\n",
    "        except Exception as e:\n",
    "            checks['database'] = {'status': 'DOWN', 'error': str(e)}\n",
    "            is_healthy = False\n",
    "        \n",
    "        # 3. OpenAI API\n",
    "        try:\n",
    "            # Quick test with minimal cost\n",
    "            client.models.list()\n",
    "            checks['openai'] = {'status': 'UP'}\n",
    "        except Exception as e:\n",
    "            checks['openai'] = {'status': 'DOWN', 'error': str(e)}\n",
    "            is_healthy = False\n",
    "        \n",
    "        # 4. System resources\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_percent = psutil.virtual_memory().percent\n",
    "        \n",
    "        checks['system'] = {\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'memory_percent': memory_percent,\n",
    "            'status': 'UP' if cpu_percent < 90 and memory_percent < 90 else 'DEGRADED'\n",
    "        }\n",
    "        \n",
    "        # 5. Error rate\n",
    "        error_rate = self.error_count / max(self.request_count, 1)\n",
    "        checks['error_rate'] = {\n",
    "            'value': error_rate,\n",
    "            'threshold': 0.05,\n",
    "            'status': 'UP' if error_rate < 0.05 else 'DEGRADED'\n",
    "        }\n",
    "        \n",
    "        return is_healthy, checks\n",
    "\n",
    "health_checker = HealthChecker()\n",
    "\n",
    "@app.get('/health')\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint (for liveness probe)\"\"\"\n",
    "    is_healthy, checks = health_checker.check_health()\n",
    "    \n",
    "    status_code = 200 if is_healthy else 503\n",
    "    \n",
    "    return Response(\n",
    "        content=json.dumps({\n",
    "            'status': 'UP' if is_healthy else 'DOWN',\n",
    "            'uptime_seconds': (datetime.now() - health_checker.start_time).total_seconds(),\n",
    "            'checks': checks\n",
    "        }),\n",
    "        status_code=status_code,\n",
    "        media_type='application/json'\n",
    "    )\n",
    "\n",
    "@app.get('/ready')\n",
    "def readiness_check():\n",
    "    \"\"\"Readiness check (for readiness probe)\"\"\"\n",
    "    # Simpler check - can service handle requests?\n",
    "    try:\n",
    "        cache.redis.ping()\n",
    "        return {'status': 'READY'}\n",
    "    except:\n",
    "        return Response(status_code=503, content='{\"status\": \"NOT_READY\"}')\n",
    "\n",
    "@app.get('/metrics')\n",
    "def metrics_endpoint():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    \n",
    "    return Response(\n",
    "        content=generate_latest(),\n",
    "        media_type=CONTENT_TYPE_LATEST\n",
    "    )\n",
    "```\n",
    "\n",
    "**Circuit Breaker Pattern:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = 'closed'      # Normal operation\n",
    "    OPEN = 'open'          # Failures exceeded threshold, rejecting requests\n",
    "    HALF_OPEN = 'half_open'  # Testing if service recovered\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker para external services (OpenAI, Database).\n",
    "    \n",
    "    Prevents cascading failures cuando un servicio está caído.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        failure_threshold: int = 5,\n",
    "        timeout_seconds: int = 60,\n",
    "        half_open_max_calls: int = 3\n",
    "    ):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "        self.half_open_max_calls = half_open_max_calls\n",
    "        \n",
    "        self.state = CircuitState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.half_open_calls = 0\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
    "        \n",
    "        if self.state == CircuitState.OPEN:\n",
    "            # Check if timeout expired\n",
    "            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout_seconds):\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "                self.half_open_calls = 0\n",
    "            else:\n",
    "                raise CircuitBreakerOpenError(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        if self.state == CircuitState.HALF_OPEN:\n",
    "            if self.half_open_calls >= self.half_open_max_calls:\n",
    "                raise CircuitBreakerOpenError(\"Half-open limit reached\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Success - reset or close circuit\n",
    "            if self.state == CircuitState.HALF_OPEN:\n",
    "                self.state = CircuitState.CLOSED\n",
    "                self.failure_count = 0\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = datetime.now()\n",
    "            \n",
    "            if self.state == CircuitState.HALF_OPEN:\n",
    "                # Failed during half-open, go back to open\n",
    "                self.state = CircuitState.OPEN\n",
    "            elif self.failure_count >= self.failure_threshold:\n",
    "                # Threshold exceeded, open circuit\n",
    "                self.state = CircuitState.OPEN\n",
    "            \n",
    "            raise e\n",
    "\n",
    "class CircuitBreakerOpenError(Exception):\n",
    "    pass\n",
    "\n",
    "# Uso\n",
    "openai_circuit = CircuitBreaker(failure_threshold=5, timeout_seconds=60)\n",
    "db_circuit = CircuitBreaker(failure_threshold=3, timeout_seconds=30)\n",
    "\n",
    "def chatbot_with_circuit_breaker(question: str) -> Dict:\n",
    "    \"\"\"Chatbot con circuit breaker protection\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Try OpenAI with circuit breaker\n",
    "        intent = openai_circuit.call(classify_intent_advanced, question)\n",
    "        \n",
    "        if intent.type == 'QUERY':\n",
    "            # Try database with circuit breaker\n",
    "            sql = openai_circuit.call(nl_to_sql, question)\n",
    "            df = db_circuit.call(pd.read_sql_query, sql, conn)\n",
    "            \n",
    "            return {'type': 'SQL', 'data': df}\n",
    "        else:\n",
    "            answer = openai_circuit.call(rag_answer, question)\n",
    "            return {'type': 'RAG', 'answer': answer}\n",
    "    \n",
    "    except CircuitBreakerOpenError as e:\n",
    "        # Service unavailable - return cached or degraded response\n",
    "        return {\n",
    "            'type': 'ERROR',\n",
    "            'message': 'Service temporarily unavailable. Please try again later.',\n",
    "            'retry_after_seconds': 60\n",
    "        }\n",
    "```\n",
    "\n",
    "**Scaling Strategy & Cost Optimization:**\n",
    "\n",
    "```python\n",
    "class CostOptimizer:\n",
    "    \"\"\"Optimiza costos de operación\"\"\"\n",
    "    \n",
    "    # Pricing (as of 2024)\n",
    "    PRICING = {\n",
    "        'gpt-4': {'input': 0.03, 'output': 0.06},  # per 1K tokens\n",
    "        'gpt-3.5-turbo': {'input': 0.0015, 'output': 0.002},\n",
    "        'text-embedding-ada-002': 0.0001,  # per 1K tokens\n",
    "        'redis': 0.05,  # per GB-hour\n",
    "        'postgres': 0.10,  # per GB-hour\n",
    "        'compute': 0.05,  # per vCPU-hour\n",
    "    }\n",
    "    \n",
    "    def estimate_monthly_cost(\n",
    "        self,\n",
    "        daily_requests: int,\n",
    "        cache_hit_rate: float = 0.6,\n",
    "        avg_tokens_per_request: int = 500\n",
    "    ) -> Dict:\n",
    "        \"\"\"Estima costo mensual\"\"\"\n",
    "        \n",
    "        monthly_requests = daily_requests * 30\n",
    "        \n",
    "        # LLM costs (considering cache)\n",
    "        llm_requests = monthly_requests * (1 - cache_hit_rate)\n",
    "        \n",
    "        # Assume 30% use GPT-4, 70% use GPT-3.5\n",
    "        gpt4_requests = llm_requests * 0.3\n",
    "        gpt35_requests = llm_requests * 0.7\n",
    "        \n",
    "        gpt4_cost = (gpt4_requests * avg_tokens_per_request / 1000) * (\n",
    "            self.PRICING['gpt-4']['input'] + self.PRICING['gpt-4']['output']\n",
    "        )\n",
    "        \n",
    "        gpt35_cost = (gpt35_requests * avg_tokens_per_request / 1000) * (\n",
    "            self.PRICING['gpt-3.5-turbo']['input'] + self.PRICING['gpt-3.5-turbo']['output']\n",
    "        )\n",
    "        \n",
    "        llm_cost = gpt4_cost + gpt35_cost\n",
    "        \n",
    "        # Embeddings (only for new documents)\n",
    "        embedding_cost = 100  # Assume 100K tokens/month for new docs\n",
    "        \n",
    "        # Infrastructure\n",
    "        redis_cost = 4 * 720 * self.PRICING['redis']  # 4GB Redis\n",
    "        postgres_cost = 20 * 720 * self.PRICING['postgres']  # 20GB Postgres\n",
    "        compute_cost = 10 * 720 * self.PRICING['compute']  # 10 vCPUs average\n",
    "        \n",
    "        infra_cost = redis_cost + postgres_cost + compute_cost\n",
    "        \n",
    "        total = llm_cost + embedding_cost + infra_cost\n",
    "        \n",
    "        return {\n",
    "            'llm_cost': llm_cost,\n",
    "            'embedding_cost': embedding_cost,\n",
    "            'infrastructure_cost': infra_cost,\n",
    "            'total_monthly_usd': total,\n",
    "            'cost_per_request_usd': total / monthly_requests,\n",
    "            'breakdown': {\n",
    "                'gpt-4': gpt4_cost,\n",
    "                'gpt-3.5-turbo': gpt35_cost,\n",
    "                'embeddings': embedding_cost,\n",
    "                'redis': redis_cost,\n",
    "                'postgres': postgres_cost,\n",
    "                'compute': compute_cost\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def recommend_optimizations(self, current_config: Dict) -> List[str]:\n",
    "        \"\"\"Recomienda optimizaciones de costo\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if current_config['cache_hit_rate'] < 0.5:\n",
    "            recommendations.append(\n",
    "                \"⬆️ Increase cache TTL → 50% hit rate → save ~$500/month\"\n",
    "            )\n",
    "        \n",
    "        if current_config['gpt4_usage_pct'] > 0.5:\n",
    "            recommendations.append(\n",
    "                \"⬇️ Route more queries to GPT-3.5 → 20x cheaper → save ~$1000/month\"\n",
    "            )\n",
    "        \n",
    "        if current_config['avg_tokens_per_request'] > 1000:\n",
    "            recommendations.append(\n",
    "                \"✂️ Optimize prompts to reduce tokens → save ~$300/month\"\n",
    "            )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Ejemplo\n",
    "optimizer = CostOptimizer()\n",
    "\n",
    "cost_estimate = optimizer.estimate_monthly_cost(\n",
    "    daily_requests=10000,\n",
    "    cache_hit_rate=0.6,\n",
    "    avg_tokens_per_request=500\n",
    ")\n",
    "\n",
    "print(\"💰 MONTHLY COST ESTIMATE:\")\n",
    "print(f\"Total: ${cost_estimate['total_monthly_usd']:,.2f}\")\n",
    "print(f\"Cost per request: ${cost_estimate['cost_per_request_usd']:.4f}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "for component, cost in cost_estimate['breakdown'].items():\n",
    "    print(f\"  {component}: ${cost:,.2f}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = optimizer.recommend_optimizations({\n",
    "    'cache_hit_rate': 0.45,\n",
    "    'gpt4_usage_pct': 0.60,\n",
    "    'avg_tokens_per_request': 1200\n",
    "})\n",
    "\n",
    "print(\"\\n💡 OPTIMIZATION RECOMMENDATIONS:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "```\n",
    "\n",
    "**Disaster Recovery & Backup Strategy:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "class BackupManager:\n",
    "    \"\"\"Gestiona backups y disaster recovery\"\"\"\n",
    "    \n",
    "    def __init__(self, s3_bucket: str):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.bucket = s3_bucket\n",
    "    \n",
    "    def backup_vector_store(self):\n",
    "        \"\"\"Backup de ChromaDB/Pinecone a S3\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Export ChromaDB\n",
    "        collection = chroma_client.get_collection('data_docs')\n",
    "        data = collection.get(include=['documents', 'embeddings', 'metadatas'])\n",
    "        \n",
    "        # Save to S3\n",
    "        backup_key = f'backups/vector_store_{timestamp}.json'\n",
    "        self.s3.put_object(\n",
    "            Bucket=self.bucket,\n",
    "            Key=backup_key,\n",
    "            Body=json.dumps(data)\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Vector store backed up to s3://{self.bucket}/{backup_key}\")\n",
    "    \n",
    "    def backup_database(self):\n",
    "        \"\"\"Backup de PostgreSQL a S3\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Use pg_dump\n",
    "        import subprocess\n",
    "        dump_file = f'/tmp/db_backup_{timestamp}.sql'\n",
    "        \n",
    "        subprocess.run([\n",
    "            'pg_dump',\n",
    "            '-h', os.getenv('DB_HOST'),\n",
    "            '-U', os.getenv('DB_USER'),\n",
    "            '-d', os.getenv('DB_NAME'),\n",
    "            '-f', dump_file\n",
    "        ], check=True)\n",
    "        \n",
    "        # Upload to S3\n",
    "        with open(dump_file, 'rb') as f:\n",
    "            self.s3.upload_fileobj(f, self.bucket, f'backups/database_{timestamp}.sql')\n",
    "        \n",
    "        print(f\"✅ Database backed up to S3\")\n",
    "    \n",
    "    def restore_from_backup(self, backup_date: str):\n",
    "        \"\"\"Restaura desde backup\"\"\"\n",
    "        \n",
    "        # Restore vector store\n",
    "        vector_key = f'backups/vector_store_{backup_date}.json'\n",
    "        obj = self.s3.get_object(Bucket=self.bucket, Key=vector_key)\n",
    "        data = json.loads(obj['Body'].read())\n",
    "        \n",
    "        # Re-index\n",
    "        collection = chroma_client.get_or_create_collection('data_docs')\n",
    "        collection.add(\n",
    "            ids=data['ids'],\n",
    "            documents=data['documents'],\n",
    "            embeddings=data['embeddings'],\n",
    "            metadatas=data['metadatas']\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Vector store restored from {backup_date}\")\n",
    "\n",
    "# Scheduled backups (use Airflow or Kubernetes CronJob)\n",
    "backup_mgr = BackupManager(s3_bucket='chatbot-backups')\n",
    "\n",
    "# Daily backups\n",
    "# Schedule: 0 2 * * * (2 AM daily)\n",
    "backup_mgr.backup_vector_store()\n",
    "backup_mgr.backup_database()\n",
    "```\n",
    "\n",
    "**Production Readiness Checklist:**\n",
    "\n",
    "```markdown\n",
    "## 🚀 PRODUCTION READINESS CHECKLIST\n",
    "\n",
    "### 1. Performance ✅\n",
    "- [ ] Latency p95 < 3s\n",
    "- [ ] Cache hit rate > 50%\n",
    "- [ ] Error rate < 1%\n",
    "- [ ] Horizontal scaling tested (3-20 pods)\n",
    "- [ ] Load testing completed (10K concurrent users)\n",
    "\n",
    "### 2. Security ✅\n",
    "- [ ] SQL injection prevention validated\n",
    "- [ ] Prompt injection detection active\n",
    "- [ ] RBAC implemented and tested\n",
    "- [ ] Audit logging enabled\n",
    "- [ ] PII masking working\n",
    "- [ ] Rate limiting configured\n",
    "\n",
    "### 3. Observability ✅\n",
    "- [ ] Metrics exported to Prometheus\n",
    "- [ ] Grafana dashboards created\n",
    "- [ ] Distributed tracing (Jaeger)\n",
    "- [ ] Centralized logging (ELK)\n",
    "- [ ] Alerts configured (PagerDuty)\n",
    "\n",
    "### 4. Reliability ✅\n",
    "- [ ] Health checks (liveness + readiness)\n",
    "- [ ] Circuit breakers for external services\n",
    "- [ ] Graceful degradation on failures\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Chaos engineering tests passed\n",
    "\n",
    "### 5. Data & Backups ✅\n",
    "- [ ] Daily vector store backups\n",
    "- [ ] Database backups (automated)\n",
    "- [ ] Disaster recovery plan documented\n",
    "- [ ] Restore tested (RTO < 1 hour)\n",
    "\n",
    "### 6. Cost Optimization ✅\n",
    "- [ ] Multi-layer caching active\n",
    "- [ ] Model routing (GPT-3.5 vs GPT-4)\n",
    "- [ ] Token optimization (prompts < 500 tokens)\n",
    "- [ ] Resource limits set\n",
    "- [ ] Cost monitoring with alerts\n",
    "\n",
    "### 7. Documentation ✅\n",
    "- [ ] API documentation (OpenAPI/Swagger)\n",
    "- [ ] Runbooks for common issues\n",
    "- [ ] Architecture diagrams\n",
    "- [ ] Onboarding guide for new team members\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fa29d",
   "metadata": {},
   "source": [
    "### Siguientes pasos\n",
    "\n",
    "1. **Caché**: implementa Redis para queries frecuentes\n",
    "2. **Feedback loop**: permite al usuario marcar respuestas correctas/incorrectas\n",
    "3. **Multi-tabla**: soporta JOINs automáticos\n",
    "4. **Exportación**: permite descargar resultados en CSV/Excel\n",
    "5. **Auditoría**: loggea todas las queries ejecutadas\n",
    "6. **Permisos**: implementa autenticación y control de acceso\n",
    "7. **Visualizaciones**: genera gráficos automáticos según el tipo de datos\n",
    "8. **Sugerencias**: recomienda preguntas frecuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cc333",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f1e58",
   "metadata": {},
   "source": [
    "**Criterios**:\n",
    "\n",
    "- ✅ Sistema RAG funcional (20%)\n",
    "- ✅ NL2SQL con validación de seguridad (25%)\n",
    "- ✅ Clasificación correcta de intención (15%)\n",
    "- ✅ Interfaz funcional (20%)\n",
    "- ✅ Manejo de errores (10%)\n",
    "- ✅ Código limpio y documentado (10%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
