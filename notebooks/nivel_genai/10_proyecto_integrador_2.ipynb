{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bfcc57",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Proyecto Integrador 2: Plataforma Self-Service con GenAI\n",
    "\n",
    "**Objetivo**: construir una plataforma que permita a usuarios no t√©cnicos generar pipelines ETL, documentaci√≥n, y reportes usando IA generativa.\n",
    "\n",
    "## Alcance del Proyecto\n",
    "\n",
    "- **Duraci√≥n**: 6-8 horas\n",
    "- **Dificultad**: Muy Alta\n",
    "- **Stack**: OpenAI, LangGraph, Airflow, Great Expectations, Streamlit\n",
    "\n",
    "## Funcionalidades\n",
    "\n",
    "1. ‚úÖ Generaci√≥n de pipelines ETL desde descripci√≥n en lenguaje natural\n",
    "2. ‚úÖ Validaci√≥n autom√°tica del c√≥digo generado\n",
    "3. ‚úÖ Creaci√≥n de tests unitarios\n",
    "4. ‚úÖ Generaci√≥n de documentaci√≥n t√©cnica\n",
    "5. ‚úÖ Dashboard de monitoreo\n",
    "6. ‚úÖ Sistema de aprobaci√≥n y deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17119fed",
   "metadata": {},
   "source": [
    "## Parte 1: Arquitectura del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541588eb",
   "metadata": {},
   "source": [
    "```\n",
    "User Input (NL)\n",
    "    ‚Üì\n",
    "[1. Parser] ‚Üí extrae requisitos\n",
    "    ‚Üì\n",
    "[2. Generator] ‚Üí genera c√≥digo ETL\n",
    "    ‚Üì\n",
    "[3. Validator] ‚Üí valida sintaxis, seguridad\n",
    "    ‚Üì\n",
    "[4. Tester] ‚Üí genera y ejecuta tests\n",
    "    ‚Üì\n",
    "[5. Documenter] ‚Üí crea README\n",
    "    ‚Üì\n",
    "[6. Reviewer] ‚Üí revisi√≥n humana\n",
    "    ‚Üì\n",
    "[7. Deployer] ‚Üí despliega a producci√≥n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73695e2",
   "metadata": {},
   "source": [
    "## Parte 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langgraph streamlit pytest great-expectations\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print('‚úÖ Setup completo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82669c94",
   "metadata": {},
   "source": [
    "## Parte 3: Parser de requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed71996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements(user_input: str) -> dict:\n",
    "    \"\"\"Extrae requisitos estructurados desde lenguaje natural.\"\"\"\n",
    "    prompt = f'''\n",
    "Extrae los requisitos de este pipeline ETL en formato JSON:\n",
    "\n",
    "Input del usuario:\n",
    "{user_input}\n",
    "\n",
    "Devuelve JSON con:\n",
    "{{\n",
    "  \"pipeline_name\": \"nombre_descriptivo\",\n",
    "  \"source\": {{\"type\": \"csv/api/db\", \"details\": \"...\"}},\n",
    "  \"transformations\": [\"lista de transformaciones\"],\n",
    "  \"destination\": {{\"type\": \"csv/db/s3\", \"details\": \"...\"}},\n",
    "  \"schedule\": \"daily/hourly/manual\",\n",
    "  \"validations\": [\"reglas de calidad\"]\n",
    "}}\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Test\n",
    "user_request = '''\n",
    "Necesito un pipeline que:\n",
    "1. Lea ventas.csv de S3 bucket \"raw-data\"\n",
    "2. Filtre solo ventas de los √∫ltimos 30 d√≠as\n",
    "3. Agregue una columna \"mes\" (YYYY-MM)\n",
    "4. Calcule total por mes y categor√≠a\n",
    "5. Escriba a PostgreSQL tabla \"ventas_mensual\"\n",
    "6. Ejecute diariamente a las 2 AM\n",
    "7. Valide que no haya nulos en \"total\"\n",
    "'''\n",
    "\n",
    "requirements = parse_requirements(user_request)\n",
    "print('Requisitos extra√≠dos:')\n",
    "print(json.dumps(requirements, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c981f2",
   "metadata": {},
   "source": [
    "## Parte 4: Generador de c√≥digo ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etl_pipeline(requirements: dict) -> str:\n",
    "    \"\"\"Genera c√≥digo Python completo del pipeline.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un pipeline ETL en Python basado en estos requisitos:\n",
    "\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El c√≥digo debe:\n",
    "- Usar pandas, boto3 (si S3), sqlalchemy (si DB)\n",
    "- Incluir manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Type hints\n",
    "- Docstrings\n",
    "- Funci√≥n main() ejecutable\n",
    "\n",
    "C√≥digo Python:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "etl_code = generate_etl_pipeline(requirements)\n",
    "print('C√≥digo generado (preview):')\n",
    "print(etl_code[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596f94e",
   "metadata": {},
   "source": [
    "## Parte 5: Validador de c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba195c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_code(code: str) -> dict:\n",
    "    \"\"\"Valida sintaxis y seguridad.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. Validar sintaxis Python\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        issues.append({'type': 'SYNTAX', 'message': str(e)})\n",
    "    \n",
    "    # 2. Detectar patrones inseguros\n",
    "    dangerous_patterns = ['eval(', 'exec(', 'os.system(', '__import__']\n",
    "    for pattern in dangerous_patterns:\n",
    "        if pattern in code:\n",
    "            issues.append({'type': 'SECURITY', 'message': f'Patr√≥n peligroso detectado: {pattern}'})\n",
    "    \n",
    "    # 3. Verificar imports necesarios\n",
    "    required_imports = ['pandas', 'logging']\n",
    "    for imp in required_imports:\n",
    "        if f'import {imp}' not in code:\n",
    "            issues.append({'type': 'MISSING_IMPORT', 'message': f'Falta import: {imp}'})\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "validation = validate_code(etl_code)\n",
    "print(f\"\\nValidaci√≥n: {'‚úÖ Aprobado' if validation['valid'] else '‚ùå Con errores'}\")\n",
    "if validation['issues']:\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"- [{issue['type']}] {issue['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b44e",
   "metadata": {},
   "source": [
    "## Parte 6: Generador de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    \"\"\"Genera tests unitarios con pytest.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera tests unitarios con pytest para este c√≥digo ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "C√≥digo:\n",
    "{code[:1000]}...\n",
    "\n",
    "Genera tests para:\n",
    "1. Lectura de datos (mock de fuente)\n",
    "2. Transformaciones\n",
    "3. Validaciones de calidad\n",
    "4. Escritura (mock de destino)\n",
    "5. Manejo de errores\n",
    "\n",
    "C√≥digo de tests (pytest):\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "test_code = generate_tests(etl_code, requirements)\n",
    "print('Tests generados (preview):')\n",
    "print(test_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ae2b",
   "metadata": {},
   "source": [
    "## Parte 7: Generador de documentaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88df093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(requirements: dict, code: str) -> str:\n",
    "    \"\"\"Genera README.md completo.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un README.md completo para este pipeline ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "Incluye:\n",
    "- Descripci√≥n general\n",
    "- Arquitectura (diagrama ASCII)\n",
    "- Requisitos (dependencias)\n",
    "- Configuraci√≥n\n",
    "- C√≥mo ejecutar\n",
    "- Monitoreo y troubleshooting\n",
    "- Contacto/owner\n",
    "\n",
    "Markdown:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "documentation = generate_documentation(requirements, etl_code)\n",
    "print('Documentaci√≥n generada (preview):')\n",
    "print(documentation[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89813a88",
   "metadata": {},
   "source": [
    "## Parte 8: DAG de Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f80052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_airflow_dag(requirements: dict, etl_code: str) -> str:\n",
    "    \"\"\"Genera DAG de Airflow.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un DAG de Airflow para este pipeline:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El DAG debe:\n",
    "- Nombre descriptivo\n",
    "- Schedule seg√∫n requisitos\n",
    "- Tasks: extract, validate, transform, load\n",
    "- Retry logic\n",
    "- Alertas por email si falla\n",
    "\n",
    "C√≥digo Python del DAG:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "dag_code = generate_airflow_dag(requirements, etl_code)\n",
    "print('DAG de Airflow (preview):')\n",
    "print(dag_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ea627",
   "metadata": {},
   "source": [
    "## Parte 9: Workflow con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    user_input: str\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    state['requirements'] = parse_requirements(state['user_input'])\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    state['validation'] = validate_code(state['code'])\n",
    "    if not state['validation']['valid']:\n",
    "        state['errors'].extend([i['message'] for i in state['validation']['issues']])\n",
    "    return state\n",
    "\n",
    "def test_node(state: PipelineState) -> PipelineState:\n",
    "    if state['validation']['valid']:\n",
    "        state['tests'] = generate_tests(state['code'], state['requirements'])\n",
    "    return state\n",
    "\n",
    "def document_node(state: PipelineState) -> PipelineState:\n",
    "    state['docs'] = generate_documentation(state['requirements'], state['code'])\n",
    "    state['dag'] = generate_airflow_dag(state['requirements'], state['code'])\n",
    "    return state\n",
    "\n",
    "def review_node(state: PipelineState) -> PipelineState:\n",
    "    # En producci√≥n: human-in-the-loop\n",
    "    state['approved'] = state['validation']['valid']\n",
    "    return state\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "workflow.add_node('parse', parse_node)\n",
    "workflow.add_node('generate', generate_node)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('test', test_node)\n",
    "workflow.add_node('document', document_node)\n",
    "workflow.add_node('review', review_node)\n",
    "\n",
    "workflow.set_entry_point('parse')\n",
    "workflow.add_edge('parse', 'generate')\n",
    "workflow.add_edge('generate', 'validate')\n",
    "workflow.add_edge('validate', 'test')\n",
    "workflow.add_edge('test', 'document')\n",
    "workflow.add_edge('document', 'review')\n",
    "workflow.add_edge('review', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar workflow completo\n",
    "initial_state = {\n",
    "    'user_input': user_request,\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print('\\nüéâ Pipeline generado completamente\\n')\n",
    "print(f\"Aprobado: {'‚úÖ' if final_state['approved'] else '‚ùå'}\")\n",
    "print(f\"Errores: {len(final_state['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4610a91",
   "metadata": {},
   "source": [
    "## Parte 10: Interfaz Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como platform_app.py\n",
    "\n",
    "streamlit_app = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from pipeline_generator import app, PipelineState  # Importar workflow\n",
    "\n",
    "st.set_page_config(page_title='GenAI Data Platform', page_icon='üèóÔ∏è', layout='wide')\n",
    "\n",
    "st.title('üèóÔ∏è Plataforma Self-Service de Pipelines')\n",
    "st.markdown('Genera pipelines ETL completos usando lenguaje natural')\n",
    "\n",
    "# Input\n",
    "user_input = st.text_area(\n",
    "    'Describe tu pipeline:',\n",
    "    height=200,\n",
    "    placeholder='Ej: Necesito procesar datos de ventas desde S3, agregar por mes, y cargar a Redshift...'\n",
    ")\n",
    "\n",
    "if st.button('üöÄ Generar Pipeline', type='primary'):\n",
    "    if not user_input:\n",
    "        st.warning('Por favor describe el pipeline')\n",
    "    else:\n",
    "        with st.spinner('Generando pipeline completo...'):\n",
    "            initial = {\n",
    "                'user_input': user_input,\n",
    "                'requirements': {}, 'code': '', 'validation': {},\n",
    "                'tests': '', 'docs': '', 'dag': '',\n",
    "                'approved': False, 'errors': []\n",
    "            }\n",
    "            \n",
    "            result = app.invoke(initial)\n",
    "            \n",
    "            # Tabs\n",
    "            tab1, tab2, tab3, tab4, tab5 = st.tabs(['üìã Requisitos', 'üíª C√≥digo', 'üß™ Tests', 'üìÑ Docs', 'üõ´ DAG'])\n",
    "            \n",
    "            with tab1:\n",
    "                st.json(result['requirements'])\n",
    "            \n",
    "            with tab2:\n",
    "                st.code(result['code'], language='python')\n",
    "                st.download_button('Descargar c√≥digo', result['code'], file_name='pipeline.py')\n",
    "            \n",
    "            with tab3:\n",
    "                st.code(result['tests'], language='python')\n",
    "            \n",
    "            with tab4:\n",
    "                st.markdown(result['docs'])\n",
    "            \n",
    "            with tab5:\n",
    "                st.code(result['dag'], language='python')\n",
    "            \n",
    "            # Validaci√≥n\n",
    "            if result['approved']:\n",
    "                st.success('‚úÖ Pipeline aprobado y listo para deployment')\n",
    "            else:\n",
    "                st.error(f\"‚ùå Pipeline con errores: {result['errors']}\")\n",
    "'''\n",
    "\n",
    "with open('platform_app.py', 'w') as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print('‚úÖ Plataforma Streamlit guardada en platform_app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4251",
   "metadata": {},
   "source": [
    "## Parte 11: Sistema de deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def deploy_pipeline(code: str, dag: str, tests: str, pipeline_name: str):\n",
    "    \"\"\"Despliega pipeline a producci√≥n.\"\"\"\n",
    "    # 1. Crear estructura de directorios\n",
    "    base_path = f'./pipelines/{pipeline_name}'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # 2. Guardar archivos\n",
    "    with open(f'{base_path}/pipeline.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    with open(f'{base_path}/test_pipeline.py', 'w') as f:\n",
    "        f.write(tests)\n",
    "    \n",
    "    with open(f'{base_path}/dag.py', 'w') as f:\n",
    "        f.write(dag)\n",
    "    \n",
    "    # 3. Ejecutar tests\n",
    "    test_result = subprocess.run(\n",
    "        ['pytest', f'{base_path}/test_pipeline.py', '-v'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if test_result.returncode != 0:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': 'Tests fallidos',\n",
    "            'output': test_result.stdout\n",
    "        }\n",
    "    \n",
    "    # 4. Copiar DAG a Airflow\n",
    "    # airflow_dags_path = '/opt/airflow/dags/'\n",
    "    # shutil.copy(f'{base_path}/dag.py', f'{airflow_dags_path}/{pipeline_name}.py')\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'message': f'Pipeline {pipeline_name} desplegado exitosamente',\n",
    "        'path': base_path\n",
    "    }\n",
    "\n",
    "# Deployment\n",
    "if final_state['approved']:\n",
    "    deploy_result = deploy_pipeline(\n",
    "        code=final_state['code'],\n",
    "        dag=final_state['dag'],\n",
    "        tests=final_state['tests'],\n",
    "        pipeline_name=final_state['requirements']['pipeline_name']\n",
    "    )\n",
    "    print(deploy_result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cf2e",
   "metadata": {},
   "source": [
    "## Parte 12: Monitoreo y observabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_dashboard(pipeline_name: str):\n",
    "    \"\"\"Genera dashboard de monitoreo.\"\"\"\n",
    "    dashboard_code = f'''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "st.title('üìä Monitoreo: {pipeline_name}')\n",
    "\n",
    "# M√©tricas simuladas\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Ejecuciones exitosas', '95%', '‚Üë 2%')\n",
    "col2.metric('Tiempo promedio', '12.5 min', '‚Üì 1.2 min')\n",
    "col3.metric('Registros procesados', '1.2M', '‚Üë 15K')\n",
    "col4.metric('Errores', '3', '‚Üì 5')\n",
    "\n",
    "# Gr√°fico de ejecuciones\n",
    "df_runs = pd.DataFrame({{\n",
    "    'fecha': pd.date_range('2024-01-01', periods=30),\n",
    "    'duracion': [10 + i*0.2 for i in range(30)],\n",
    "    'status': ['success'] * 28 + ['failed'] * 2\n",
    "}}})\n",
    "\n",
    "fig = px.line(df_runs, x='fecha', y='duracion', color='status')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "# Logs recientes\n",
    "st.subheader('Logs recientes')\n",
    "st.text_area('', value='2024-01-15 02:00 - [INFO] Pipeline iniciado\\\\n2024-01-15 02:05 - [INFO] 10000 registros procesados', height=200)\n",
    "'''\n",
    "    \n",
    "    return dashboard_code\n",
    "\n",
    "monitoring_dash = generate_monitoring_dashboard(requirements['pipeline_name'])\n",
    "print('Dashboard de monitoreo generado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b578ed",
   "metadata": {},
   "source": [
    "## Parte 13: Mejoras futuras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179193c",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "**Fase 2**:\n",
    "- Multi-agente: equipo de agentes especializados (data engineer, QA, DevOps)\n",
    "- Cost estimation: predecir costos de infraestructura\n",
    "- A/B testing: comparar versiones de pipelines\n",
    "- Auto-scaling: ajustar recursos seg√∫n carga\n",
    "\n",
    "**Fase 3**:\n",
    "- Self-healing: detecci√≥n y correcci√≥n autom√°tica de errores\n",
    "- Optimization: sugerencias de mejora basadas en m√©tricas\n",
    "- Federated learning: aprender de pipelines de otros equipos\n",
    "- Natural language alerting: alertas explicadas en lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dba29",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27d809",
   "metadata": {},
   "source": [
    "**Criterios**:\n",
    "\n",
    "- ‚úÖ Parsing correcto de requisitos (15%)\n",
    "- ‚úÖ Generaci√≥n de c√≥digo funcional (25%)\n",
    "- ‚úÖ Validaci√≥n y seguridad (15%)\n",
    "- ‚úÖ Tests completos (15%)\n",
    "- ‚úÖ Documentaci√≥n clara (10%)\n",
    "- ‚úÖ Workflow orquestado (10%)\n",
    "- ‚úÖ Interfaz usable (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d050fa4",
   "metadata": {},
   "source": [
    "## Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0905e",
   "metadata": {},
   "source": [
    "Has construido una plataforma completa de self-service que:\n",
    "\n",
    "‚úÖ Democratiza el acceso a la ingenier√≠a de datos\n",
    "‚úÖ Reduce tiempo de desarrollo de d√≠as a minutos\n",
    "‚úÖ Mantiene est√°ndares de calidad y seguridad\n",
    "‚úÖ Genera documentaci√≥n autom√°ticamente\n",
    "‚úÖ Facilita deployment y monitoreo\n",
    "\n",
    "**¬°Felicitaciones por completar el m√≥dulo de GenAI!** üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
