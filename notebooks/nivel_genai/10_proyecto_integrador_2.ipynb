{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bfcc57",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Proyecto Integrador 2: Plataforma Self-Service con GenAI\n",
    "\n",
    "**Objetivo**: construir una plataforma que permita a usuarios no t√©cnicos generar pipelines ETL, documentaci√≥n, y reportes usando IA generativa.\n",
    "\n",
    "## Alcance del Proyecto\n",
    "\n",
    "- **Duraci√≥n**: 6-8 horas\n",
    "- **Dificultad**: Muy Alta\n",
    "- **Stack**: OpenAI, LangGraph, Airflow, Great Expectations, Streamlit\n",
    "\n",
    "## Funcionalidades\n",
    "\n",
    "1. ‚úÖ Generaci√≥n de pipelines ETL desde descripci√≥n en lenguaje natural\n",
    "2. ‚úÖ Validaci√≥n autom√°tica del c√≥digo generado\n",
    "3. ‚úÖ Creaci√≥n de tests unitarios\n",
    "4. ‚úÖ Generaci√≥n de documentaci√≥n t√©cnica\n",
    "5. ‚úÖ Dashboard de monitoreo\n",
    "6. ‚úÖ Sistema de aprobaci√≥n y deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e08ff",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Arquitectura de Plataforma Self-Service: De NL a Producci√≥n**\n",
    "\n",
    "**Visi√≥n General: Data Democratization**\n",
    "\n",
    "Esta plataforma representa el siguiente nivel en la evoluci√≥n de Data Engineering: convertir descripciones en lenguaje natural en pipelines ETL completos, probados, documentados y desplegados.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              PLATAFORMA SELF-SERVICE ARCHITECTURE               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  [Usuario No T√©cnico] ‚Üí Streamlit UI                           ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [1. NL Parser]                                                 ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Intent extraction                                        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Entity recognition (sources, transformations, schedule) ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Requirement validation                                   ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [2. Code Generator]                                            ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Template selection (batch/streaming/hybrid)             ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Framework choice (Pandas/Spark/Polars)                  ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Best practices injection                                ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [3. Security Validator]                                        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ AST parsing (syntax validation)                         ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Security patterns (no eval, exec, os.system)            ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Dependency check (vulnerable packages)                  ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [4. Test Generator]                                            ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Unit tests (per transformation)                         ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Integration tests (mocked I/O)                          ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Data quality tests (Great Expectations)                 ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [5. Documentation Generator]                                   ‚îÇ\n",
    "‚îÇ      ‚Ä¢ README.md (architecture, setup, troubleshooting)        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ API docs (if exposed)                                   ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Runbook (operations guide)                              ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [6. DAG Generator]                                             ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Airflow DAG (orchestration)                             ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Task dependencies                                       ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Retry/alerting logic                                    ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [7. Human Review]                                              ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Side-by-side diff                                       ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Approve/Reject/Request changes                          ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [8. Deployment]                                                ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Run tests (pytest)                                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Create PR (GitHub)                                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Deploy to staging ‚Üí prod                                ‚îÇ\n",
    "‚îÇ           ‚Üì                                                     ‚îÇ\n",
    "‚îÇ  [9. Monitoring]                                                ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Execution metrics                                       ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Data quality alerts                                     ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Cost tracking                                           ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Componentes Clave**\n",
    "\n",
    "**1. NL Parser (Structured Requirement Extraction)**\n",
    "\n",
    "```python\n",
    "# Prompt engineering para extracci√≥n estructurada\n",
    "PARSER_PROMPT = \"\"\"\n",
    "Analiza este requerimiento de pipeline ETL y extrae informaci√≥n estructurada.\n",
    "\n",
    "INPUT: {user_input}\n",
    "\n",
    "OUTPUT (JSON):\n",
    "{\n",
    "  \"pipeline_name\": \"nombre_snake_case\",\n",
    "  \"source\": {\n",
    "    \"type\": \"s3|postgres|mysql|api|csv\",\n",
    "    \"location\": \"bucket/path o connection_string\",\n",
    "    \"credentials\": \"reference_to_secret_manager\"\n",
    "  },\n",
    "  \"transformations\": [\n",
    "    {\n",
    "      \"type\": \"filter|aggregate|join|pivot\",\n",
    "      \"description\": \"...\",\n",
    "      \"columns\": [\"col1\", \"col2\"]\n",
    "    }\n",
    "  ],\n",
    "  \"destination\": {\n",
    "    \"type\": \"redshift|postgres|s3|delta\",\n",
    "    \"location\": \"...\",\n",
    "    \"mode\": \"append|overwrite|upsert\"\n",
    "  },\n",
    "  \"schedule\": {\n",
    "    \"frequency\": \"daily|hourly|weekly\",\n",
    "    \"time\": \"02:00\",\n",
    "    \"timezone\": \"UTC\"\n",
    "  },\n",
    "  \"validations\": [\n",
    "    {\n",
    "      \"type\": \"not_null|unique|range|regex\",\n",
    "      \"column\": \"...\",\n",
    "      \"threshold\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"sla\": {\n",
    "    \"max_duration_minutes\": 60,\n",
    "    \"alert_email\": \"team@example.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "REGLAS:\n",
    "- Inferir transformaciones impl√≠citas (ej: \"√∫ltimos 30 d√≠as\" ‚Üí filter fecha)\n",
    "- Sugerir validaciones si no est√°n expl√≠citas\n",
    "- Usar defaults sensatos (schedule=daily si no especificado)\n",
    "\"\"\"\n",
    "\n",
    "# Ejemplo de parsing robusto\n",
    "def parse_with_validation(user_input: str) -> dict:\n",
    "    requirements = parse_requirements(user_input)\n",
    "    \n",
    "    # Validar completitud\n",
    "    required_fields = ['pipeline_name', 'source', 'destination']\n",
    "    missing = [f for f in required_fields if f not in requirements]\n",
    "    \n",
    "    if missing:\n",
    "        # LLM completa campos faltantes\n",
    "        completion_prompt = f\"Completa estos campos faltantes: {missing}\"\n",
    "        requirements = complete_requirements(requirements, completion_prompt)\n",
    "    \n",
    "    return requirements\n",
    "```\n",
    "\n",
    "**2. Code Generator (Multi-Template Strategy)**\n",
    "\n",
    "```python\n",
    "# Templates por tipo de workload\n",
    "TEMPLATES = {\n",
    "    'batch': '''\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class {pipeline_name}Pipeline:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract(self) -> pd.DataFrame:\n",
    "        \"\"\"Extract from {source_type}\"\"\"\n",
    "        {extract_code}\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformations\"\"\"\n",
    "        {transform_code}\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Data quality checks\"\"\"\n",
    "        {validation_code}\n",
    "    \n",
    "    def load(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Load to {destination_type}\"\"\"\n",
    "        {load_code}\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        logger.info(\"Starting pipeline...\")\n",
    "        df = self.extract()\n",
    "        df = self.transform(df)\n",
    "        \n",
    "        if not self.validate(df):\n",
    "            raise ValueError(\"Data quality checks failed\")\n",
    "        \n",
    "        self.load(df)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "''',\n",
    "    'streaming': '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"{pipeline_name}\").getOrCreate()\n",
    "\n",
    "# Read stream\n",
    "df = spark.readStream \\\\\n",
    "    .format(\"{source_format}\") \\\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"{kafka_servers}\") \\\\\n",
    "    .load()\n",
    "\n",
    "# Transformations\n",
    "{transform_code}\n",
    "\n",
    "# Write stream\n",
    "query = df.writeStream \\\\\n",
    "    .format(\"{sink_format}\") \\\\\n",
    "    .outputMode(\"append\") \\\\\n",
    "    .trigger(processingTime='1 minute') \\\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "'''\n",
    "}\n",
    "\n",
    "# Generaci√≥n con context injection\n",
    "def generate_with_context(requirements: dict) -> str:\n",
    "    # Seleccionar template\n",
    "    workload_type = infer_workload_type(requirements)\n",
    "    template = TEMPLATES[workload_type]\n",
    "    \n",
    "    # Inyectar c√≥digo espec√≠fico\n",
    "    extract_code = generate_extract_code(requirements['source'])\n",
    "    transform_code = generate_transform_code(requirements['transformations'])\n",
    "    validation_code = generate_validation_code(requirements['validations'])\n",
    "    load_code = generate_load_code(requirements['destination'])\n",
    "    \n",
    "    # Rellenar template\n",
    "    code = template.format(\n",
    "        pipeline_name=requirements['pipeline_name'],\n",
    "        source_type=requirements['source']['type'],\n",
    "        destination_type=requirements['destination']['type'],\n",
    "        extract_code=extract_code,\n",
    "        transform_code=transform_code,\n",
    "        validation_code=validation_code,\n",
    "        load_code=load_code\n",
    "    )\n",
    "    \n",
    "    return code\n",
    "```\n",
    "\n",
    "**3. Security Validator (Multi-Layer)**\n",
    "\n",
    "```python\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def validate_security(code: str) -> dict:\n",
    "    issues = []\n",
    "    \n",
    "    # Layer 1: AST parsing (syntax + structure)\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        # Detectar imports peligrosos\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for alias in node.names:\n",
    "                    if alias.name in ['pickle', 'subprocess', 'os']:\n",
    "                        issues.append({\n",
    "                            'severity': 'HIGH',\n",
    "                            'type': 'DANGEROUS_IMPORT',\n",
    "                            'message': f'Import de m√≥dulo peligroso: {alias.name}',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "            \n",
    "            # Detectar eval/exec\n",
    "            if isinstance(node, ast.Call):\n",
    "                if isinstance(node.func, ast.Name):\n",
    "                    if node.func.id in ['eval', 'exec']:\n",
    "                        issues.append({\n",
    "                            'severity': 'CRITICAL',\n",
    "                            'type': 'CODE_INJECTION',\n",
    "                            'message': f'Uso de {node.func.id}() detectado',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "    \n",
    "    except SyntaxError as e:\n",
    "        issues.append({\n",
    "            'severity': 'CRITICAL',\n",
    "            'type': 'SYNTAX_ERROR',\n",
    "            'message': str(e)\n",
    "        })\n",
    "    \n",
    "    # Layer 2: Regex patterns (SQL injection)\n",
    "    if re.search(r'f\"SELECT.*\\{.*\\}\"', code):\n",
    "        issues.append({\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'SQL_INJECTION_RISK',\n",
    "            'message': 'Uso de f-strings en queries SQL detectado'\n",
    "        })\n",
    "    \n",
    "    # Layer 3: Dependency scanning\n",
    "    imports = extract_imports(code)\n",
    "    vulnerable = check_vulnerabilities(imports)\n",
    "    issues.extend(vulnerable)\n",
    "    \n",
    "    return {\n",
    "        'valid': all(i['severity'] != 'CRITICAL' for i in issues),\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Dependency scanning\n",
    "def check_vulnerabilities(packages: list) -> list:\n",
    "    # Integraci√≥n con safety (https://pyup.io/safety/)\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        ['safety', 'check', '--json'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    vulnerabilities = json.loads(result.stdout)\n",
    "    return [\n",
    "        {\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'VULNERABLE_DEPENDENCY',\n",
    "            'message': f\"{v['package']}: {v['vulnerability']}\"\n",
    "        }\n",
    "        for v in vulnerabilities\n",
    "    ]\n",
    "```\n",
    "\n",
    "**4. Test Generator (Comprehensive Coverage)**\n",
    "\n",
    "```python\n",
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    test_template = '''\n",
    "import pytest\n",
    "from unittest.mock import Mock, patch\n",
    "import pandas as pd\n",
    "from {module} import {pipeline_class}\n",
    "\n",
    "@pytest.fixture\n",
    "def pipeline():\n",
    "    config = {config_dict}\n",
    "    return {pipeline_class}(config)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return pd.DataFrame({sample_data_dict})\n",
    "\n",
    "# Unit Tests\n",
    "def test_extract(pipeline, mocker):\n",
    "    \"\"\"Test data extraction\"\"\"\n",
    "    mock_read = mocker.patch('{read_function}')\n",
    "    mock_read.return_value = pd.DataFrame({mock_data})\n",
    "    \n",
    "    df = pipeline.extract()\n",
    "    \n",
    "    assert not df.empty\n",
    "    assert list(df.columns) == {expected_columns}\n",
    "\n",
    "def test_transform(pipeline, sample_data):\n",
    "    \"\"\"Test transformations\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    \n",
    "    # Assertions espec√≠ficas\n",
    "    {transform_assertions}\n",
    "\n",
    "def test_validate_success(pipeline, sample_data):\n",
    "    \"\"\"Test validation with clean data\"\"\"\n",
    "    assert pipeline.validate(sample_data) == True\n",
    "\n",
    "def test_validate_failure(pipeline):\n",
    "    \"\"\"Test validation with dirty data\"\"\"\n",
    "    dirty_data = pd.DataFrame({dirty_data_dict})\n",
    "    assert pipeline.validate(dirty_data) == False\n",
    "\n",
    "def test_load(pipeline, sample_data, mocker):\n",
    "    \"\"\"Test data loading\"\"\"\n",
    "    mock_write = mocker.patch('{write_function}')\n",
    "    \n",
    "    pipeline.load(sample_data)\n",
    "    \n",
    "    mock_write.assert_called_once()\n",
    "\n",
    "# Integration Tests\n",
    "@patch('{source_module}.read')\n",
    "@patch('{destination_module}.write')\n",
    "def test_full_pipeline(mock_write, mock_read, pipeline):\n",
    "    \"\"\"Test end-to-end execution\"\"\"\n",
    "    mock_read.return_value = pd.DataFrame({mock_source_data})\n",
    "    \n",
    "    pipeline.run()\n",
    "    \n",
    "    mock_write.assert_called()\n",
    "    written_df = mock_write.call_args[0][0]\n",
    "    assert len(written_df) > 0\n",
    "\n",
    "# Data Quality Tests\n",
    "def test_no_null_values(pipeline, sample_data):\n",
    "    \"\"\"Test no nulls in required columns\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    required_cols = {required_columns}\n",
    "    \n",
    "    for col in required_cols:\n",
    "        assert df[col].isnull().sum() == 0\n",
    "\n",
    "def test_data_types(pipeline, sample_data):\n",
    "    \"\"\"Test correct data types\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    expected_types = {expected_types_dict}\n",
    "    \n",
    "    for col, dtype in expected_types.items():\n",
    "        assert df[col].dtype == dtype\n",
    "'''\n",
    "    \n",
    "    # Generar c√≥digo espec√≠fico basado en requirements\n",
    "    test_code = test_template.format(\n",
    "        module=requirements['pipeline_name'],\n",
    "        pipeline_class=to_camel_case(requirements['pipeline_name']),\n",
    "        config_dict=generate_config_mock(requirements),\n",
    "        sample_data_dict=generate_sample_data(requirements),\n",
    "        # ... m√°s placeholders\n",
    "    )\n",
    "    \n",
    "    return test_code\n",
    "```\n",
    "\n",
    "**Beneficios de la Arquitectura**\n",
    "\n",
    "| Aspecto | Antes (Manual) | Despu√©s (Platform) |\n",
    "|---------|----------------|---------------------|\n",
    "| **Time to Production** | 2-4 semanas | 1-2 horas |\n",
    "| **Code Quality** | Variable (depende de desarrollador) | Consistente (templates + validaci√≥n) |\n",
    "| **Documentation** | A menudo ausente u obsoleta | Siempre actualizada (auto-generada) |\n",
    "| **Testing** | ~40% coverage | >80% coverage (auto-generado) |\n",
    "| **Security** | Manual reviews | Automated scanning (cada generaci√≥n) |\n",
    "| **Onboarding** | Requiere semanas de training | Cualquier usuario puede crear pipelines |\n",
    "| **Maintenance** | Alto (cada pipeline es √∫nico) | Bajo (patterns estandarizados) |\n",
    "\n",
    "**ROI Calculation**\n",
    "\n",
    "```python\n",
    "# Ejemplo de ROI\n",
    "MANUAL_PIPELINE = {\n",
    "    'development_hours': 40,\n",
    "    'testing_hours': 16,\n",
    "    'documentation_hours': 8,\n",
    "    'review_hours': 4,\n",
    "    'total_hours': 68,\n",
    "    'engineer_cost_hour': 75,  # USD\n",
    "    'total_cost': 5100  # USD\n",
    "}\n",
    "\n",
    "PLATFORM_PIPELINE = {\n",
    "    'generation_minutes': 10,\n",
    "    'review_hours': 2,\n",
    "    'total_hours': 2.17,\n",
    "    'total_cost': 163  # USD\n",
    "}\n",
    "\n",
    "savings_per_pipeline = MANUAL_PIPELINE['total_cost'] - PLATFORM_PIPELINE['total_cost']  # $4,937\n",
    "time_saved = MANUAL_PIPELINE['total_hours'] - PLATFORM_PIPELINE['total_hours']  # 65.83 hours\n",
    "\n",
    "# Si un equipo crea 20 pipelines/a√±o\n",
    "annual_savings = savings_per_pipeline * 20  # $98,740\n",
    "annual_time_saved = time_saved * 20  # 1,316 hours (= 164 d√≠as de trabajo)\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales**\n",
    "\n",
    "1. **Marketing Analytics**: \"Necesito agregar eventos de Mixpanel por usuario y d√≠a\"\n",
    "2. **Finance Reporting**: \"Consolidar transacciones de 5 bases de datos en un reporte diario\"\n",
    "3. **ML Feature Engineering**: \"Crear features de usuario para modelo de churn\"\n",
    "4. **Data Migration**: \"Migrar tablas de MySQL legacy a Snowflake\"\n",
    "5. **Real-time Alerting**: \"Detectar anomal√≠as en ventas cada 5 minutos\"\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17119fed",
   "metadata": {},
   "source": [
    "## Parte 1: Arquitectura del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541588eb",
   "metadata": {},
   "source": [
    "```\n",
    "User Input (NL)\n",
    "    ‚Üì\n",
    "[1. Parser] ‚Üí extrae requisitos\n",
    "    ‚Üì\n",
    "[2. Generator] ‚Üí genera c√≥digo ETL\n",
    "    ‚Üì\n",
    "[3. Validator] ‚Üí valida sintaxis, seguridad\n",
    "    ‚Üì\n",
    "[4. Tester] ‚Üí genera y ejecuta tests\n",
    "    ‚Üì\n",
    "[5. Documenter] ‚Üí crea README\n",
    "    ‚Üì\n",
    "[6. Reviewer] ‚Üí revisi√≥n humana\n",
    "    ‚Üì\n",
    "[7. Deployer] ‚Üí despliega a producci√≥n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c0460",
   "metadata": {},
   "source": [
    "### üîÑ **LangGraph: Orquestaci√≥n de Agentes Multi-Paso**\n",
    "\n",
    "**¬øPor qu√© LangGraph para Workflows Complejos?**\n",
    "\n",
    "LangGraph permite crear flujos de trabajo con:\n",
    "- **State Management**: Estado persistente entre nodos\n",
    "- **Conditional Routing**: L√≥gica de decisi√≥n (if/else paths)\n",
    "- **Human-in-the-Loop**: Pausar para aprobaci√≥n humana\n",
    "- **Retry Logic**: Reintentar nodos fallidos\n",
    "- **Streaming**: Ver progreso en tiempo real\n",
    "\n",
    "**Arquitectura del Workflow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# State compartido entre todos los nodos\n",
    "class PipelineState(TypedDict):\n",
    "    # Inputs\n",
    "    user_input: str\n",
    "    \n",
    "    # Intermedios\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    \n",
    "    # Outputs\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]  # Append errors from any node\n",
    "    \n",
    "    # Metadata\n",
    "    generation_time: float\n",
    "    review_notes: str\n",
    "\n",
    "# Cada nodo es una funci√≥n pura: State ‚Üí State\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Extrae requisitos estructurados\"\"\"\n",
    "    try:\n",
    "        state['requirements'] = parse_requirements(state['user_input'])\n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Parse error: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Genera c√≥digo ETL\"\"\"\n",
    "    if state['errors']:\n",
    "        return state  # Skip si hay errores previos\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    state['generation_time'] = time.time() - start\n",
    "    \n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Valida seguridad y sintaxis\"\"\"\n",
    "    validation_result = validate_code(state['code'])\n",
    "    state['validation'] = validation_result\n",
    "    \n",
    "    if not validation_result['valid']:\n",
    "        state['errors'].extend([\n",
    "            f\"{issue['type']}: {issue['message']}\"\n",
    "            for issue in validation_result['issues']\n",
    "        ])\n",
    "    \n",
    "    return state\n",
    "\n",
    "def should_continue_after_validation(state: PipelineState) -> str:\n",
    "    \"\"\"Conditional edge: decide si continuar o terminar\"\"\"\n",
    "    if state['validation']['valid']:\n",
    "        return \"test\"  # Go to test node\n",
    "    else:\n",
    "        return END  # Stop workflow\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"parse\", parse_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"validate\", validate_node)\n",
    "workflow.add_node(\"test\", test_node)\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"review\", review_node)\n",
    "\n",
    "# Add edges (define flow)\n",
    "workflow.set_entry_point(\"parse\")\n",
    "workflow.add_edge(\"parse\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"validate\")\n",
    "\n",
    "# Conditional edge (branching)\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    should_continue_after_validation,\n",
    "    {\n",
    "        \"test\": \"test\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"document\", \"review\")\n",
    "workflow.add_edge(\"review\", END)\n",
    "\n",
    "# Compile graph\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "**Visualizaci√≥n del Grafo**\n",
    "\n",
    "```\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  START  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Parse  ‚îÇ  ‚Üê Extrae requisitos\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Generate ‚îÇ  ‚Üê Genera c√≥digo\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Validate ‚îÇ  ‚Üê Valida seguridad\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Valid?  ‚îÇ\n",
    "        ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò\n",
    "     Yes  ‚îÇ     ‚îÇ No\n",
    "          v     v\n",
    "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê END\n",
    "      ‚îÇ Test ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Document ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Review ‚îÇ  ‚Üê Human approval\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         v\n",
    "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "      ‚îÇ END ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Streaming de Estado (Real-time Updates)**\n",
    "\n",
    "```python\n",
    "# Ejecutar con streaming\n",
    "initial_state = {\n",
    "    'user_input': \"Pipeline de ventas...\",\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': [],\n",
    "    'generation_time': 0.0,\n",
    "    'review_notes': ''\n",
    "}\n",
    "\n",
    "# Stream: ver actualizaciones de cada nodo\n",
    "for output in app.stream(initial_state):\n",
    "    node_name = list(output.keys())[0]\n",
    "    state = output[node_name]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completado: {node_name}\")\n",
    "    \n",
    "    if node_name == \"parse\":\n",
    "        print(f\"Pipeline: {state['requirements'].get('pipeline_name')}\")\n",
    "    \n",
    "    elif node_name == \"generate\":\n",
    "        print(f\"C√≥digo: {len(state['code'])} caracteres\")\n",
    "        print(f\"Tiempo: {state['generation_time']:.2f}s\")\n",
    "    \n",
    "    elif node_name == \"validate\":\n",
    "        status = \"‚úÖ OK\" if state['validation']['valid'] else \"‚ùå Errores\"\n",
    "        print(f\"Validaci√≥n: {status}\")\n",
    "        \n",
    "        if state['errors']:\n",
    "            for error in state['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "# Final state\n",
    "final = list(app.stream(initial_state))[-1]\n",
    "```\n",
    "\n",
    "**Human-in-the-Loop (Approval Gate)**\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def review_node_interactive(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Nodo que pausa y espera aprobaci√≥n humana\"\"\"\n",
    "    \n",
    "    # Mostrar resumen para reviewer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç REVISI√ìN REQUERIDA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPipeline: {state['requirements']['pipeline_name']}\")\n",
    "    print(f\"Source: {state['requirements']['source']['type']}\")\n",
    "    print(f\"Destination: {state['requirements']['destination']['type']}\")\n",
    "    print(f\"\\nValidaci√≥n: {'‚úÖ Aprobado' if state['validation']['valid'] else '‚ùå Errores'}\")\n",
    "    \n",
    "    if state['errors']:\n",
    "        print(\"\\nErrores detectados:\")\n",
    "        for error in state['errors']:\n",
    "            print(f\"  ‚ùå {error}\")\n",
    "    \n",
    "    print(f\"\\nC√≥digo generado ({len(state['code'])} caracteres):\")\n",
    "    print(state['code'][:300] + \"...\")\n",
    "    \n",
    "    # Esperar input humano\n",
    "    while True:\n",
    "        decision = input(\"\\n[A]probar / [R]echazar / [M]odificar: \").upper()\n",
    "        \n",
    "        if decision == 'A':\n",
    "            state['approved'] = True\n",
    "            state['review_notes'] = \"Aprobado por revisor\"\n",
    "            break\n",
    "        \n",
    "        elif decision == 'R':\n",
    "            state['approved'] = False\n",
    "            rejection_reason = input(\"Raz√≥n del rechazo: \")\n",
    "            state['errors'].append(f\"Rechazado: {rejection_reason}\")\n",
    "            state['review_notes'] = rejection_reason\n",
    "            break\n",
    "        \n",
    "        elif decision == 'M':\n",
    "            modifications = input(\"Modificaciones requeridas: \")\n",
    "            state['review_notes'] = f\"Modificaciones: {modifications}\"\n",
    "            # Re-route to generate node with feedback\n",
    "            state['user_input'] += f\"\\n\\nAjustes: {modifications}\"\n",
    "            # Esta l√≥gica requiere un loop en el grafo\n",
    "            break\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Integrar en workflow\n",
    "workflow.add_node(\"review\", review_node_interactive)\n",
    "```\n",
    "\n",
    "**Retry Logic (Reintentos Autom√°ticos)**\n",
    "\n",
    "```python\n",
    "def generate_with_retry(state: PipelineState, max_retries: int = 3) -> PipelineState:\n",
    "    \"\"\"Nodo con reintentos autom√°ticos\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Intentar generar c√≥digo\n",
    "            state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "            \n",
    "            # Validar sintaxis\n",
    "            ast.parse(state['code'])\n",
    "            \n",
    "            # Success\n",
    "            state['errors'] = [e for e in state['errors'] if 'Generation' not in e]\n",
    "            break\n",
    "        \n",
    "        except SyntaxError as e:\n",
    "            error_msg = f\"Generation attempt {attempt+1} failed: {str(e)}\"\n",
    "            print(f\"‚ö†Ô∏è  {error_msg}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"üîÑ Retrying...\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                state['errors'].append(error_msg)\n",
    "    \n",
    "    return state\n",
    "```\n",
    "\n",
    "**Sub-Graphs (Workflows Anidados)**\n",
    "\n",
    "```python\n",
    "# Sub-workflow para generaci√≥n de tests\n",
    "test_workflow = StateGraph(PipelineState)\n",
    "\n",
    "def unit_test_node(state):\n",
    "    state['unit_tests'] = generate_unit_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def integration_test_node(state):\n",
    "    state['integration_tests'] = generate_integration_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def quality_test_node(state):\n",
    "    state['quality_tests'] = generate_quality_tests(state['requirements'])\n",
    "    return state\n",
    "\n",
    "# Build sub-graph\n",
    "test_workflow.add_node(\"unit\", unit_test_node)\n",
    "test_workflow.add_node(\"integration\", integration_test_node)\n",
    "test_workflow.add_node(\"quality\", quality_test_node)\n",
    "\n",
    "test_workflow.set_entry_point(\"unit\")\n",
    "test_workflow.add_edge(\"unit\", \"integration\")\n",
    "test_workflow.add_edge(\"integration\", \"quality\")\n",
    "test_workflow.add_edge(\"quality\", END)\n",
    "\n",
    "test_subgraph = test_workflow.compile()\n",
    "\n",
    "# Integrar sub-graph en main workflow\n",
    "def test_orchestrator(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Ejecuta sub-workflow de tests\"\"\"\n",
    "    test_results = test_subgraph.invoke(state)\n",
    "    \n",
    "    # Combinar tests\n",
    "    state['tests'] = (\n",
    "        test_results['unit_tests'] +\n",
    "        test_results['integration_tests'] +\n",
    "        test_results['quality_tests']\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"test\", test_orchestrator)\n",
    "```\n",
    "\n",
    "**Parallel Execution (Nodos Independientes)**\n",
    "\n",
    "```python\n",
    "# Documentaci√≥n y DAG pueden generarse en paralelo\n",
    "from langgraph.pregel import Channel\n",
    "\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"dag\", dag_generator_node)\n",
    "\n",
    "# Ambos nodos reciben el mismo state (paralelo)\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"test\", \"dag\")\n",
    "\n",
    "# Sync point: esperar ambos antes de continuar\n",
    "workflow.add_node(\"combine\", lambda s: s)  # No-op combiner\n",
    "workflow.add_edge(\"document\", \"combine\")\n",
    "workflow.add_edge(\"dag\", \"combine\")\n",
    "workflow.add_edge(\"combine\", \"review\")\n",
    "```\n",
    "\n",
    "**Checkpointing (Persistencia de Estado)**\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "\n",
    "# Guardar estado en cada nodo\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Ejecutar con checkpoint\n",
    "config = {\"configurable\": {\"thread_id\": \"pipeline_123\"}}\n",
    "final_state = app.invoke(initial_state, config=config)\n",
    "\n",
    "# Recuperar estado guardado\n",
    "saved_state = checkpointer.get(config)\n",
    "\n",
    "# Reanudar desde checkpoint (√∫til si falla a mitad)\n",
    "resumed_state = app.invoke(saved_state, config=config)\n",
    "```\n",
    "\n",
    "**Ventajas vs Alternativas**\n",
    "\n",
    "| Caracter√≠stica | LangGraph | Airflow | Prefect | N8n |\n",
    "|----------------|-----------|---------|---------|-----|\n",
    "| **State Management** | ‚úÖ Built-in | ‚ö†Ô∏è XCom (limitado) | ‚úÖ Flow runs | ‚ùå |\n",
    "| **Conditional Logic** | ‚úÖ Native | ‚ö†Ô∏è BranchOperator | ‚úÖ | ‚úÖ |\n",
    "| **Human-in-Loop** | ‚úÖ Easy | ‚ö†Ô∏è Manual | ‚úÖ Approvals | ‚úÖ |\n",
    "| **LLM Integration** | ‚úÖ Optimizado | ‚ùå Custom | ‚ö†Ô∏è Manual | ‚ö†Ô∏è |\n",
    "| **Streaming** | ‚úÖ Native | ‚ùå | ‚ö†Ô∏è Limited | ‚ùå |\n",
    "| **Python-First** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå (UI-first) |\n",
    "\n",
    "**Caso de Uso: Multi-Agent Code Review**\n",
    "\n",
    "```python\n",
    "# Sistema de review con m√∫ltiples agentes especializados\n",
    "\n",
    "class ReviewState(TypedDict):\n",
    "    code: str\n",
    "    security_review: dict\n",
    "    performance_review: dict\n",
    "    style_review: dict\n",
    "    final_approval: bool\n",
    "\n",
    "def security_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en seguridad\"\"\"\n",
    "    state['security_review'] = {\n",
    "        'sql_injection_risk': check_sql_injection(state['code']),\n",
    "        'secrets_exposed': check_secrets(state['code']),\n",
    "        'dangerous_imports': check_imports(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def performance_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en performance\"\"\"\n",
    "    state['performance_review'] = {\n",
    "        'complexity': calculate_complexity(state['code']),\n",
    "        'memory_usage': estimate_memory(state['code']),\n",
    "        'optimization_suggestions': suggest_optimizations(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def style_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en estilo\"\"\"\n",
    "    state['style_review'] = {\n",
    "        'pep8_compliance': check_pep8(state['code']),\n",
    "        'docstring_coverage': check_docstrings(state['code']),\n",
    "        'type_hints': check_type_hints(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "# Ejecutar en paralelo\n",
    "review_workflow = StateGraph(ReviewState)\n",
    "review_workflow.add_node(\"security\", security_agent)\n",
    "review_workflow.add_node(\"performance\", performance_agent)\n",
    "review_workflow.add_node(\"style\", style_agent)\n",
    "\n",
    "# Parallel edges\n",
    "review_workflow.set_entry_point(\"security\")\n",
    "review_workflow.set_entry_point(\"performance\")\n",
    "review_workflow.set_entry_point(\"style\")\n",
    "\n",
    "# Combine results\n",
    "def aggregate_reviews(state: ReviewState) -> ReviewState:\n",
    "    all_passed = (\n",
    "        all(v == 'pass' for v in state['security_review'].values()) and\n",
    "        state['performance_review']['complexity'] < 10 and\n",
    "        state['style_review']['pep8_compliance'] > 0.9\n",
    "    )\n",
    "    \n",
    "    state['final_approval'] = all_passed\n",
    "    return state\n",
    "\n",
    "review_workflow.add_node(\"aggregate\", aggregate_reviews)\n",
    "review_workflow.add_edge(\"security\", \"aggregate\")\n",
    "review_workflow.add_edge(\"performance\", \"aggregate\")\n",
    "review_workflow.add_edge(\"style\", \"aggregate\")\n",
    "review_workflow.add_edge(\"aggregate\", END)\n",
    "\n",
    "multi_agent_review = review_workflow.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73695e2",
   "metadata": {},
   "source": [
    "## Parte 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langgraph streamlit pytest great-expectations\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print('‚úÖ Setup completo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82669c94",
   "metadata": {},
   "source": [
    "## Parte 3: Parser de requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed71996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements(user_input: str) -> dict:\n",
    "    \"\"\"Extrae requisitos estructurados desde lenguaje natural.\"\"\"\n",
    "    prompt = f'''\n",
    "Extrae los requisitos de este pipeline ETL en formato JSON:\n",
    "\n",
    "Input del usuario:\n",
    "{user_input}\n",
    "\n",
    "Devuelve JSON con:\n",
    "{{\n",
    "  \"pipeline_name\": \"nombre_descriptivo\",\n",
    "  \"source\": {{\"type\": \"csv/api/db\", \"details\": \"...\"}},\n",
    "  \"transformations\": [\"lista de transformaciones\"],\n",
    "  \"destination\": {{\"type\": \"csv/db/s3\", \"details\": \"...\"}},\n",
    "  \"schedule\": \"daily/hourly/manual\",\n",
    "  \"validations\": [\"reglas de calidad\"]\n",
    "}}\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Test\n",
    "user_request = '''\n",
    "Necesito un pipeline que:\n",
    "1. Lea ventas.csv de S3 bucket \"raw-data\"\n",
    "2. Filtre solo ventas de los √∫ltimos 30 d√≠as\n",
    "3. Agregue una columna \"mes\" (YYYY-MM)\n",
    "4. Calcule total por mes y categor√≠a\n",
    "5. Escriba a PostgreSQL tabla \"ventas_mensual\"\n",
    "6. Ejecute diariamente a las 2 AM\n",
    "7. Valide que no haya nulos en \"total\"\n",
    "'''\n",
    "\n",
    "requirements = parse_requirements(user_request)\n",
    "print('Requisitos extra√≠dos:')\n",
    "print(json.dumps(requirements, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c981f2",
   "metadata": {},
   "source": [
    "## Parte 4: Generador de c√≥digo ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etl_pipeline(requirements: dict) -> str:\n",
    "    \"\"\"Genera c√≥digo Python completo del pipeline.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un pipeline ETL en Python basado en estos requisitos:\n",
    "\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El c√≥digo debe:\n",
    "- Usar pandas, boto3 (si S3), sqlalchemy (si DB)\n",
    "- Incluir manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Type hints\n",
    "- Docstrings\n",
    "- Funci√≥n main() ejecutable\n",
    "\n",
    "C√≥digo Python:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "etl_code = generate_etl_pipeline(requirements)\n",
    "print('C√≥digo generado (preview):')\n",
    "print(etl_code[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596f94e",
   "metadata": {},
   "source": [
    "## Parte 5: Validador de c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba195c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_code(code: str) -> dict:\n",
    "    \"\"\"Valida sintaxis y seguridad.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. Validar sintaxis Python\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        issues.append({'type': 'SYNTAX', 'message': str(e)})\n",
    "    \n",
    "    # 2. Detectar patrones inseguros\n",
    "    dangerous_patterns = ['eval(', 'exec(', 'os.system(', '__import__']\n",
    "    for pattern in dangerous_patterns:\n",
    "        if pattern in code:\n",
    "            issues.append({'type': 'SECURITY', 'message': f'Patr√≥n peligroso detectado: {pattern}'})\n",
    "    \n",
    "    # 3. Verificar imports necesarios\n",
    "    required_imports = ['pandas', 'logging']\n",
    "    for imp in required_imports:\n",
    "        if f'import {imp}' not in code:\n",
    "            issues.append({'type': 'MISSING_IMPORT', 'message': f'Falta import: {imp}'})\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "validation = validate_code(etl_code)\n",
    "print(f\"\\nValidaci√≥n: {'‚úÖ Aprobado' if validation['valid'] else '‚ùå Con errores'}\")\n",
    "if validation['issues']:\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"- [{issue['type']}] {issue['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b44e",
   "metadata": {},
   "source": [
    "## Parte 6: Generador de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    \"\"\"Genera tests unitarios con pytest.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera tests unitarios con pytest para este c√≥digo ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "C√≥digo:\n",
    "{code[:1000]}...\n",
    "\n",
    "Genera tests para:\n",
    "1. Lectura de datos (mock de fuente)\n",
    "2. Transformaciones\n",
    "3. Validaciones de calidad\n",
    "4. Escritura (mock de destino)\n",
    "5. Manejo de errores\n",
    "\n",
    "C√≥digo de tests (pytest):\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "test_code = generate_tests(etl_code, requirements)\n",
    "print('Tests generados (preview):')\n",
    "print(test_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ae2b",
   "metadata": {},
   "source": [
    "## Parte 7: Generador de documentaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88df093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(requirements: dict, code: str) -> str:\n",
    "    \"\"\"Genera README.md completo.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un README.md completo para este pipeline ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "Incluye:\n",
    "- Descripci√≥n general\n",
    "- Arquitectura (diagrama ASCII)\n",
    "- Requisitos (dependencias)\n",
    "- Configuraci√≥n\n",
    "- C√≥mo ejecutar\n",
    "- Monitoreo y troubleshooting\n",
    "- Contacto/owner\n",
    "\n",
    "Markdown:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "documentation = generate_documentation(requirements, etl_code)\n",
    "print('Documentaci√≥n generada (preview):')\n",
    "print(documentation[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89813a88",
   "metadata": {},
   "source": [
    "## Parte 8: DAG de Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f80052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_airflow_dag(requirements: dict, etl_code: str) -> str:\n",
    "    \"\"\"Genera DAG de Airflow.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un DAG de Airflow para este pipeline:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El DAG debe:\n",
    "- Nombre descriptivo\n",
    "- Schedule seg√∫n requisitos\n",
    "- Tasks: extract, validate, transform, load\n",
    "- Retry logic\n",
    "- Alertas por email si falla\n",
    "\n",
    "C√≥digo Python del DAG:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "dag_code = generate_airflow_dag(requirements, etl_code)\n",
    "print('DAG de Airflow (preview):')\n",
    "print(dag_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ea627",
   "metadata": {},
   "source": [
    "## Parte 9: Workflow con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    user_input: str\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    state['requirements'] = parse_requirements(state['user_input'])\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    state['validation'] = validate_code(state['code'])\n",
    "    if not state['validation']['valid']:\n",
    "        state['errors'].extend([i['message'] for i in state['validation']['issues']])\n",
    "    return state\n",
    "\n",
    "def test_node(state: PipelineState) -> PipelineState:\n",
    "    if state['validation']['valid']:\n",
    "        state['tests'] = generate_tests(state['code'], state['requirements'])\n",
    "    return state\n",
    "\n",
    "def document_node(state: PipelineState) -> PipelineState:\n",
    "    state['docs'] = generate_documentation(state['requirements'], state['code'])\n",
    "    state['dag'] = generate_airflow_dag(state['requirements'], state['code'])\n",
    "    return state\n",
    "\n",
    "def review_node(state: PipelineState) -> PipelineState:\n",
    "    # En producci√≥n: human-in-the-loop\n",
    "    state['approved'] = state['validation']['valid']\n",
    "    return state\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "workflow.add_node('parse', parse_node)\n",
    "workflow.add_node('generate', generate_node)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('test', test_node)\n",
    "workflow.add_node('document', document_node)\n",
    "workflow.add_node('review', review_node)\n",
    "\n",
    "workflow.set_entry_point('parse')\n",
    "workflow.add_edge('parse', 'generate')\n",
    "workflow.add_edge('generate', 'validate')\n",
    "workflow.add_edge('validate', 'test')\n",
    "workflow.add_edge('test', 'document')\n",
    "workflow.add_edge('document', 'review')\n",
    "workflow.add_edge('review', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar workflow completo\n",
    "initial_state = {\n",
    "    'user_input': user_request,\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print('\\nüéâ Pipeline generado completamente\\n')\n",
    "print(f\"Aprobado: {'‚úÖ' if final_state['approved'] else '‚ùå'}\")\n",
    "print(f\"Errores: {len(final_state['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4610a91",
   "metadata": {},
   "source": [
    "## Parte 10: Interfaz Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como platform_app.py\n",
    "\n",
    "streamlit_app = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from pipeline_generator import app, PipelineState  # Importar workflow\n",
    "\n",
    "st.set_page_config(page_title='GenAI Data Platform', page_icon='üèóÔ∏è', layout='wide')\n",
    "\n",
    "st.title('üèóÔ∏è Plataforma Self-Service de Pipelines')\n",
    "st.markdown('Genera pipelines ETL completos usando lenguaje natural')\n",
    "\n",
    "# Input\n",
    "user_input = st.text_area(\n",
    "    'Describe tu pipeline:',\n",
    "    height=200,\n",
    "    placeholder='Ej: Necesito procesar datos de ventas desde S3, agregar por mes, y cargar a Redshift...'\n",
    ")\n",
    "\n",
    "if st.button('üöÄ Generar Pipeline', type='primary'):\n",
    "    if not user_input:\n",
    "        st.warning('Por favor describe el pipeline')\n",
    "    else:\n",
    "        with st.spinner('Generando pipeline completo...'):\n",
    "            initial = {\n",
    "                'user_input': user_input,\n",
    "                'requirements': {}, 'code': '', 'validation': {},\n",
    "                'tests': '', 'docs': '', 'dag': '',\n",
    "                'approved': False, 'errors': []\n",
    "            }\n",
    "            \n",
    "            result = app.invoke(initial)\n",
    "            \n",
    "            # Tabs\n",
    "            tab1, tab2, tab3, tab4, tab5 = st.tabs(['üìã Requisitos', 'üíª C√≥digo', 'üß™ Tests', 'üìÑ Docs', 'üõ´ DAG'])\n",
    "            \n",
    "            with tab1:\n",
    "                st.json(result['requirements'])\n",
    "            \n",
    "            with tab2:\n",
    "                st.code(result['code'], language='python')\n",
    "                st.download_button('Descargar c√≥digo', result['code'], file_name='pipeline.py')\n",
    "            \n",
    "            with tab3:\n",
    "                st.code(result['tests'], language='python')\n",
    "            \n",
    "            with tab4:\n",
    "                st.markdown(result['docs'])\n",
    "            \n",
    "            with tab5:\n",
    "                st.code(result['dag'], language='python')\n",
    "            \n",
    "            # Validaci√≥n\n",
    "            if result['approved']:\n",
    "                st.success('‚úÖ Pipeline aprobado y listo para deployment')\n",
    "            else:\n",
    "                st.error(f\"‚ùå Pipeline con errores: {result['errors']}\")\n",
    "'''\n",
    "\n",
    "with open('platform_app.py', 'w') as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print('‚úÖ Plataforma Streamlit guardada en platform_app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4251",
   "metadata": {},
   "source": [
    "## Parte 11: Sistema de deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def deploy_pipeline(code: str, dag: str, tests: str, pipeline_name: str):\n",
    "    \"\"\"Despliega pipeline a producci√≥n.\"\"\"\n",
    "    # 1. Crear estructura de directorios\n",
    "    base_path = f'./pipelines/{pipeline_name}'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # 2. Guardar archivos\n",
    "    with open(f'{base_path}/pipeline.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    with open(f'{base_path}/test_pipeline.py', 'w') as f:\n",
    "        f.write(tests)\n",
    "    \n",
    "    with open(f'{base_path}/dag.py', 'w') as f:\n",
    "        f.write(dag)\n",
    "    \n",
    "    # 3. Ejecutar tests\n",
    "    test_result = subprocess.run(\n",
    "        ['pytest', f'{base_path}/test_pipeline.py', '-v'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if test_result.returncode != 0:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': 'Tests fallidos',\n",
    "            'output': test_result.stdout\n",
    "        }\n",
    "    \n",
    "    # 4. Copiar DAG a Airflow\n",
    "    # airflow_dags_path = '/opt/airflow/dags/'\n",
    "    # shutil.copy(f'{base_path}/dag.py', f'{airflow_dags_path}/{pipeline_name}.py')\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'message': f'Pipeline {pipeline_name} desplegado exitosamente',\n",
    "        'path': base_path\n",
    "    }\n",
    "\n",
    "# Deployment\n",
    "if final_state['approved']:\n",
    "    deploy_result = deploy_pipeline(\n",
    "        code=final_state['code'],\n",
    "        dag=final_state['dag'],\n",
    "        tests=final_state['tests'],\n",
    "        pipeline_name=final_state['requirements']['pipeline_name']\n",
    "    )\n",
    "    print(deploy_result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8ff73",
   "metadata": {},
   "source": [
    "### üöÄ **Deployment y Producci√≥n: De Prototipo a Enterprise**\n",
    "\n",
    "**CI/CD Pipeline para GenAI Artifacts**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ               PRODUCTION DEPLOYMENT PIPELINE               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                            ‚îÇ\n",
    "‚îÇ  [1. Code Generation] ‚Üí Generated pipeline code           ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [2. Version Control]                                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Create feature branch                              ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Commit: code + tests + docs                        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Tag: v1.0.0                                        ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [3. Automated Testing] (GitHub Actions/GitLab CI)        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ pytest: unit + integration tests                   ‚îÇ\n",
    "‚îÇ      ‚Ä¢ coverage: ‚â•80% required                            ‚îÇ\n",
    "‚îÇ      ‚Ä¢ security: Bandit, Safety                           ‚îÇ\n",
    "‚îÇ      ‚Ä¢ linting: Black, Flake8, MyPy                       ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [4. Quality Gates]                                        ‚îÇ\n",
    "‚îÇ      ‚úÖ All tests pass                                     ‚îÇ\n",
    "‚îÇ      ‚úÖ Coverage ‚â•80%                                      ‚îÇ\n",
    "‚îÇ      ‚úÖ No critical security issues                       ‚îÇ\n",
    "‚îÇ      ‚úÖ Code review approved                              ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [5. Staging Deployment]                                   ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Deploy to staging environment                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Run integration tests with staging data            ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Performance benchmarking                           ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [6. Production Approval]                                  ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Manual sign-off (for critical pipelines)          ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Automated (for low-risk changes)                   ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [7. Production Deployment]                                ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Blue/Green deployment (zero downtime)              ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Copy DAG to Airflow dags/ folder                   ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Update Airflow variables                           ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Trigger initial run                                ‚îÇ\n",
    "‚îÇ           ‚Üì                                                ‚îÇ\n",
    "‚îÇ  [8. Monitoring]                                           ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Datadog/New Relic APM                              ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Custom metrics (rows processed, runtime)           ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Alerting (PagerDuty, Slack)                        ‚îÇ\n",
    "‚îÇ                                                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**GitHub Actions Workflow**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/deploy-pipeline.yml\n",
    "name: Deploy Generated Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: '3.11'\n",
    "  COVERAGE_THRESHOLD: 80\n",
    "\n",
    "jobs:\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov bandit safety black flake8 mypy\n",
    "      \n",
    "      - name: Security scan\n",
    "        run: |\n",
    "          # Scan dependencies\n",
    "          safety check --json\n",
    "          \n",
    "          # Scan code for vulnerabilities\n",
    "          bandit -r generated_pipelines/ -f json -o bandit-report.json\n",
    "      \n",
    "      - name: Code quality\n",
    "        run: |\n",
    "          # Format check\n",
    "          black --check generated_pipelines/\n",
    "          \n",
    "          # Linting\n",
    "          flake8 generated_pipelines/ --max-line-length=100\n",
    "          \n",
    "          # Type checking\n",
    "          mypy generated_pipelines/ --strict\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          pytest generated_pipelines/ \\\n",
    "            --cov=generated_pipelines \\\n",
    "            --cov-report=xml \\\n",
    "            --cov-report=html \\\n",
    "            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }}\n",
    "      \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "  \n",
    "  deploy-staging:\n",
    "    needs: validate\n",
    "    if: github.event_name == 'pull_request'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Deploy to staging Airflow\n",
    "        env:\n",
    "          AIRFLOW_STAGING_HOST: ${{ secrets.AIRFLOW_STAGING_HOST }}\n",
    "          AIRFLOW_API_KEY: ${{ secrets.AIRFLOW_API_KEY }}\n",
    "        run: |\n",
    "          # Copy DAG to staging\n",
    "          scp generated_pipelines/*/dag.py \\\n",
    "            user@$AIRFLOW_STAGING_HOST:/opt/airflow/dags/\n",
    "          \n",
    "          # Trigger DAG\n",
    "          curl -X POST \\\n",
    "            -H \"Authorization: Bearer $AIRFLOW_API_KEY\" \\\n",
    "            https://$AIRFLOW_STAGING_HOST/api/v1/dags/pipeline_name/dagRuns\n",
    "      \n",
    "      - name: Integration tests (staging)\n",
    "        run: |\n",
    "          pytest tests/integration/ \\\n",
    "            --env=staging \\\n",
    "            --pipeline=pipeline_name\n",
    "  \n",
    "  deploy-production:\n",
    "    needs: validate\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    environment:\n",
    "      name: production\n",
    "      url: https://airflow.prod.company.com\n",
    "    steps:\n",
    "      - name: Blue/Green deployment\n",
    "        run: |\n",
    "          # Deploy to \"green\" environment first\n",
    "          ./scripts/deploy.sh --env=prod-green --pipeline=$PIPELINE_NAME\n",
    "          \n",
    "          # Run smoke tests\n",
    "          ./scripts/smoke-tests.sh --env=prod-green\n",
    "          \n",
    "          # Switch traffic to green (zero downtime)\n",
    "          ./scripts/switch-traffic.sh --to=green\n",
    "          \n",
    "          # Decommission blue\n",
    "          ./scripts/cleanup.sh --env=prod-blue\n",
    "      \n",
    "      - name: Notify\n",
    "        uses: 8398a7/action-slack@v3\n",
    "        with:\n",
    "          status: ${{ job.status }}\n",
    "          text: 'Pipeline deployed to production'\n",
    "          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "```\n",
    "\n",
    "**Deployment Automation Script**\n",
    "\n",
    "```python\n",
    "# scripts/deploy_pipeline.py\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "class PipelineDeployer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.airflow_api = config['airflow_api_url']\n",
    "        self.airflow_token = config['airflow_token']\n",
    "    \n",
    "    def deploy(self, pipeline_path: str, environment: str = 'production'):\n",
    "        \"\"\"Despliega pipeline a Airflow\"\"\"\n",
    "        \n",
    "        # 1. Validar que existan archivos necesarios\n",
    "        required_files = ['pipeline.py', 'dag.py', 'tests.py', 'README.md']\n",
    "        for file in required_files:\n",
    "            if not Path(f\"{pipeline_path}/{file}\").exists():\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "        \n",
    "        # 2. Ejecutar tests\n",
    "        print(\"Running tests...\")\n",
    "        test_result = subprocess.run(\n",
    "            ['pytest', f'{pipeline_path}/tests.py', '-v'],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if test_result.returncode != 0:\n",
    "            raise RuntimeError(f\"Tests failed:\\n{test_result.stdout}\")\n",
    "        \n",
    "        print(\"‚úÖ Tests passed\")\n",
    "        \n",
    "        # 3. Upload assets to S3\n",
    "        print(\"Uploading to S3...\")\n",
    "        pipeline_name = Path(pipeline_path).name\n",
    "        \n",
    "        for file in ['pipeline.py', 'README.md']:\n",
    "            self.s3.upload_file(\n",
    "                f'{pipeline_path}/{file}',\n",
    "                self.config['artifacts_bucket'],\n",
    "                f'pipelines/{pipeline_name}/{file}'\n",
    "            )\n",
    "        \n",
    "        # 4. Deploy DAG to Airflow\n",
    "        print(f\"Deploying to Airflow ({environment})...\")\n",
    "        \n",
    "        # Airflow API: upload DAG\n",
    "        with open(f'{pipeline_path}/dag.py', 'r') as f:\n",
    "            dag_code = f.read()\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}/upload\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'dag_code': dag_code}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Airflow deployment failed: {response.text}\")\n",
    "        \n",
    "        # 5. Set Airflow variables\n",
    "        variables = {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            's3_bucket': self.config['data_bucket'],\n",
    "            'db_conn_id': 'postgres_default',\n",
    "            'owner_email': self.config['owner_email']\n",
    "        }\n",
    "        \n",
    "        for key, value in variables.items():\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/variables\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'key': f'{pipeline_name}_{key}', 'value': value}\n",
    "            )\n",
    "        \n",
    "        # 6. Unpause DAG\n",
    "        requests.patch(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'is_paused': False}\n",
    "        )\n",
    "        \n",
    "        # 7. Trigger initial run (optional)\n",
    "        if self.config.get('trigger_on_deploy'):\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/dags/{pipeline_name}/dagRuns\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'conf': {'deployed_at': str(datetime.now())}}\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline {pipeline_name} deployed successfully\")\n",
    "        \n",
    "        return {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            'environment': environment,\n",
    "            'dag_url': f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            's3_path': f's3://{self.config[\"artifacts_bucket\"]}/pipelines/{pipeline_name}'\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "deployer = PipelineDeployer({\n",
    "    'airflow_api_url': 'https://airflow.company.com/api/v1',\n",
    "    'airflow_token': os.getenv('AIRFLOW_TOKEN'),\n",
    "    'artifacts_bucket': 'company-pipeline-artifacts',\n",
    "    'data_bucket': 'company-data',\n",
    "    'owner_email': 'data-team@company.com',\n",
    "    'trigger_on_deploy': True\n",
    "})\n",
    "\n",
    "result = deployer.deploy('./generated_pipelines/ventas_pipeline', environment='production')\n",
    "```\n",
    "\n",
    "**Monitoring y Observabilidad**\n",
    "\n",
    "```python\n",
    "# monitoring/pipeline_metrics.py\n",
    "from datadog import initialize, statsd\n",
    "import time\n",
    "\n",
    "initialize(api_key=os.getenv('DATADOG_API_KEY'))\n",
    "\n",
    "class PipelineMetrics:\n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "    \n",
    "    def track_execution(self, func):\n",
    "        \"\"\"Decorator para trackear m√©tricas de ejecuci√≥n\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Incrementar contador de ejecuciones\n",
    "            statsd.increment(\n",
    "                'pipeline.executions',\n",
    "                tags=[f'pipeline:{self.pipeline_name}']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # M√©trica de √©xito\n",
    "                statsd.increment(\n",
    "                    'pipeline.success',\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                # M√©trica de error\n",
    "                statsd.increment(\n",
    "                    'pipeline.errors',\n",
    "                    tags=[\n",
    "                        f'pipeline:{self.pipeline_name}',\n",
    "                        f'error_type:{type(e).__name__}'\n",
    "                    ]\n",
    "                )\n",
    "                raise\n",
    "            \n",
    "            finally:\n",
    "                # Duraci√≥n\n",
    "                duration = time.time() - start_time\n",
    "                statsd.histogram(\n",
    "                    'pipeline.duration',\n",
    "                    duration,\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "        \n",
    "        return wrapper\n",
    "    \n",
    "    def track_data_volume(self, rows: int, stage: str):\n",
    "        \"\"\"Track volumen de datos procesados\"\"\"\n",
    "        statsd.gauge(\n",
    "            'pipeline.data_volume',\n",
    "            rows,\n",
    "            tags=[\n",
    "                f'pipeline:{self.pipeline_name}',\n",
    "                f'stage:{stage}'\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def track_data_quality(self, metric: str, value: float):\n",
    "        \"\"\"Track m√©tricas de calidad de datos\"\"\"\n",
    "        statsd.gauge(\n",
    "            f'pipeline.data_quality.{metric}',\n",
    "            value,\n",
    "            tags=[f'pipeline:{self.pipeline_name}']\n",
    "        )\n",
    "\n",
    "# Usage en pipeline generado\n",
    "metrics = PipelineMetrics('ventas_pipeline')\n",
    "\n",
    "@metrics.track_execution\n",
    "def run_pipeline():\n",
    "    # Extract\n",
    "    df = extract_data()\n",
    "    metrics.track_data_volume(len(df), 'extract')\n",
    "    \n",
    "    # Transform\n",
    "    df = transform_data(df)\n",
    "    metrics.track_data_volume(len(df), 'transform')\n",
    "    \n",
    "    # Validate\n",
    "    null_rate = df.isnull().sum().sum() / df.size\n",
    "    metrics.track_data_quality('null_rate', null_rate)\n",
    "    \n",
    "    # Load\n",
    "    load_data(df)\n",
    "    metrics.track_data_volume(len(df), 'load')\n",
    "```\n",
    "\n",
    "**Alerting Strategy**\n",
    "\n",
    "```python\n",
    "# monitoring/alerts.py\n",
    "from slack_sdk import WebClient\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "class AlertManager:\n",
    "    def __init__(self):\n",
    "        self.slack = WebClient(token=os.getenv('SLACK_TOKEN'))\n",
    "        self.pagerduty_key = os.getenv('PAGERDUTY_KEY')\n",
    "    \n",
    "    def send_alert(\n",
    "        self,\n",
    "        severity: str,  # 'info' | 'warning' | 'critical'\n",
    "        pipeline_name: str,\n",
    "        message: str,\n",
    "        details: dict = None\n",
    "    ):\n",
    "        \"\"\"Env√≠a alertas seg√∫n severidad\"\"\"\n",
    "        \n",
    "        if severity == 'info':\n",
    "            # Slack notification\n",
    "            self._send_slack(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'warning':\n",
    "            # Slack + Email\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'critical':\n",
    "            # Slack + Email + PagerDuty\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "            self._trigger_pagerduty(pipeline_name, message, details)\n",
    "    \n",
    "    def _send_slack(self, pipeline, message, details, urgent=False):\n",
    "        emoji = 'üö®' if urgent else '‚ÑπÔ∏è'\n",
    "        \n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\"type\": \"plain_text\", \"text\": f\"{emoji} {pipeline}\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\"type\": \"mrkdwn\", \"text\": message}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        if details:\n",
    "            blocks.append({\n",
    "                \"type\": \"section\",\n",
    "                \"fields\": [\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*{k}:*\\n{v}\"}\n",
    "                    for k, v in details.items()\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        self.slack.chat_postMessage(\n",
    "            channel='#data-alerts',\n",
    "            blocks=blocks\n",
    "        )\n",
    "    \n",
    "    def _trigger_pagerduty(self, pipeline, message, details):\n",
    "        import requests\n",
    "        \n",
    "        requests.post(\n",
    "            'https://events.pagerduty.com/v2/enqueue',\n",
    "            json={\n",
    "                'routing_key': self.pagerduty_key,\n",
    "                'event_action': 'trigger',\n",
    "                'payload': {\n",
    "                    'summary': f'{pipeline}: {message}',\n",
    "                    'severity': 'critical',\n",
    "                    'source': 'genai-platform',\n",
    "                    'custom_details': details\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Integraci√≥n en pipeline\n",
    "alerts = AlertManager()\n",
    "\n",
    "try:\n",
    "    run_pipeline()\n",
    "except Exception as e:\n",
    "    alerts.send_alert(\n",
    "        severity='critical',\n",
    "        pipeline_name='ventas_pipeline',\n",
    "        message=f'Pipeline failed: {str(e)}',\n",
    "        details={\n",
    "            'Error Type': type(e).__name__,\n",
    "            'Traceback': traceback.format_exc()[:500]\n",
    "        }\n",
    "    )\n",
    "    raise\n",
    "```\n",
    "\n",
    "**Cost Optimization**\n",
    "\n",
    "```python\n",
    "# monitoring/cost_tracking.py\n",
    "class CostTracker:\n",
    "    \"\"\"Track costos de ejecuci√≥n de pipelines\"\"\"\n",
    "    \n",
    "    COST_PER_COMPUTE_HOUR = 0.50  # USD\n",
    "    COST_PER_GB_STORAGE = 0.023   # USD/mes\n",
    "    COST_PER_MILLION_REQUESTS = 0.20  # USD\n",
    "    \n",
    "    def calculate_pipeline_cost(\n",
    "        self,\n",
    "        runtime_minutes: float,\n",
    "        data_volume_gb: float,\n",
    "        api_calls: int\n",
    "    ) -> dict:\n",
    "        \n",
    "        compute_cost = (runtime_minutes / 60) * self.COST_PER_COMPUTE_HOUR\n",
    "        storage_cost = data_volume_gb * self.COST_PER_GB_STORAGE\n",
    "        api_cost = (api_calls / 1_000_000) * self.COST_PER_MILLION_REQUESTS\n",
    "        \n",
    "        total = compute_cost + storage_cost + api_cost\n",
    "        \n",
    "        return {\n",
    "            'compute': round(compute_cost, 4),\n",
    "            'storage': round(storage_cost, 4),\n",
    "            'api': round(api_cost, 4),\n",
    "            'total': round(total, 4)\n",
    "        }\n",
    "    \n",
    "    def optimize_suggestions(self, metrics: dict) -> list:\n",
    "        \"\"\"Sugerencias de optimizaci√≥n\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if metrics['runtime_minutes'] > 60:\n",
    "            suggestions.append(\"Consider partitioning data for parallel processing\")\n",
    "        \n",
    "        if metrics['data_volume_gb'] > 100:\n",
    "            suggestions.append(\"Use columnar format (Parquet) to reduce storage\")\n",
    "        \n",
    "        if metrics['api_calls'] > 1_000_000:\n",
    "            suggestions.append(\"Implement caching to reduce API calls\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# En dashboard Streamlit\n",
    "cost_tracker = CostTracker()\n",
    "\n",
    "st.subheader('üí∞ Cost Analysis')\n",
    "costs = cost_tracker.calculate_pipeline_cost(\n",
    "    runtime_minutes=15,\n",
    "    data_volume_gb=50,\n",
    "    api_calls=100_000\n",
    ")\n",
    "\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Compute', f\"${costs['compute']}\")\n",
    "col2.metric('Storage', f\"${costs['storage']}\")\n",
    "col3.metric('API', f\"${costs['api']}\")\n",
    "col4.metric('Total', f\"${costs['total']}\", delta='-12%')\n",
    "\n",
    "suggestions = cost_tracker.optimize_suggestions(metrics)\n",
    "if suggestions:\n",
    "    st.info('\\n'.join(suggestions))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cf2e",
   "metadata": {},
   "source": [
    "## Parte 12: Monitoreo y observabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_dashboard(pipeline_name: str):\n",
    "    \"\"\"Genera dashboard de monitoreo.\"\"\"\n",
    "    dashboard_code = f'''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "st.title('üìä Monitoreo: {pipeline_name}')\n",
    "\n",
    "# M√©tricas simuladas\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Ejecuciones exitosas', '95%', '‚Üë 2%')\n",
    "col2.metric('Tiempo promedio', '12.5 min', '‚Üì 1.2 min')\n",
    "col3.metric('Registros procesados', '1.2M', '‚Üë 15K')\n",
    "col4.metric('Errores', '3', '‚Üì 5')\n",
    "\n",
    "# Gr√°fico de ejecuciones\n",
    "df_runs = pd.DataFrame({{\n",
    "    'fecha': pd.date_range('2024-01-01', periods=30),\n",
    "    'duracion': [10 + i*0.2 for i in range(30)],\n",
    "    'status': ['success'] * 28 + ['failed'] * 2\n",
    "}}})\n",
    "\n",
    "fig = px.line(df_runs, x='fecha', y='duracion', color='status')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "# Logs recientes\n",
    "st.subheader('Logs recientes')\n",
    "st.text_area('', value='2024-01-15 02:00 - [INFO] Pipeline iniciado\\\\n2024-01-15 02:05 - [INFO] 10000 registros procesados', height=200)\n",
    "'''\n",
    "    \n",
    "    return dashboard_code\n",
    "\n",
    "monitoring_dash = generate_monitoring_dashboard(requirements['pipeline_name'])\n",
    "print('Dashboard de monitoreo generado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b578ed",
   "metadata": {},
   "source": [
    "## Parte 13: Mejoras futuras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179193c",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "**Fase 2**:\n",
    "- Multi-agente: equipo de agentes especializados (data engineer, QA, DevOps)\n",
    "- Cost estimation: predecir costos de infraestructura\n",
    "- A/B testing: comparar versiones de pipelines\n",
    "- Auto-scaling: ajustar recursos seg√∫n carga\n",
    "\n",
    "**Fase 3**:\n",
    "- Self-healing: detecci√≥n y correcci√≥n autom√°tica de errores\n",
    "- Optimization: sugerencias de mejora basadas en m√©tricas\n",
    "- Federated learning: aprender de pipelines de otros equipos\n",
    "- Natural language alerting: alertas explicadas en lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dba29",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27d809",
   "metadata": {},
   "source": [
    "**Criterios**:\n",
    "\n",
    "- ‚úÖ Parsing correcto de requisitos (15%)\n",
    "- ‚úÖ Generaci√≥n de c√≥digo funcional (25%)\n",
    "- ‚úÖ Validaci√≥n y seguridad (15%)\n",
    "- ‚úÖ Tests completos (15%)\n",
    "- ‚úÖ Documentaci√≥n clara (10%)\n",
    "- ‚úÖ Workflow orquestado (10%)\n",
    "- ‚úÖ Interfaz usable (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d050fa4",
   "metadata": {},
   "source": [
    "## Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0905e",
   "metadata": {},
   "source": [
    "Has construido una plataforma completa de self-service que:\n",
    "\n",
    "‚úÖ Democratiza el acceso a la ingenier√≠a de datos\n",
    "‚úÖ Reduce tiempo de desarrollo de d√≠as a minutos\n",
    "‚úÖ Mantiene est√°ndares de calidad y seguridad\n",
    "‚úÖ Genera documentaci√≥n autom√°ticamente\n",
    "‚úÖ Facilita deployment y monitoreo\n",
    "\n",
    "**¬°Felicitaciones por completar el m√≥dulo de GenAI!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d22e7",
   "metadata": {},
   "source": [
    "### üéì **Conclusi√≥n del Curso: De Junior a GenAI Expert**\n",
    "\n",
    "**El Viaje Completo: 40 Notebooks, 4 Niveles**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           DATA ENGINEER COURSE: LEARNING PATH                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  üìö NIVEL JUNIOR (10 notebooks)                              ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Fundamentos: Python, Pandas, SQL                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Data Manipulation: ETL b√°sico                           ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Visualizaci√≥n: Matplotlib, Seaborn                      ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Git: Control de versiones                               ‚îÇ\n",
    "‚îÇ      ‚Üì                                                       ‚îÇ\n",
    "‚îÇ  üîß NIVEL MID (10 notebooks)                                 ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Airflow: Orquestaci√≥n de pipelines                      ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Streaming: Kafka, real-time processing                  ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Cloud: AWS (S3, Redshift, Lambda)                       ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Bases de Datos: PostgreSQL, MongoDB                     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ FastAPI: Servicios de datos                             ‚îÇ\n",
    "‚îÇ      ‚Üì                                                       ‚îÇ\n",
    "‚îÇ  ‚ö° NIVEL SENIOR (10 notebooks)                              ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Data Governance: Calidad, linaje                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Lakehouse: Delta, Iceberg                               ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Spark Streaming: Processing distribuido                 ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Arquitecturas: Medallion, Lambda, Kappa                 ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ ML Pipelines: Feature stores, MLOps                     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ FinOps: Optimizaci√≥n de costos                          ‚îÇ\n",
    "‚îÇ      ‚Üì                                                       ‚îÇ\n",
    "‚îÇ  ü§ñ NIVEL GENAI (10 notebooks)                               ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 01: Fundamentos LLMs & Prompting                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 02: NL2SQL (Natural Language ‚Üí SQL)                     ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 03: Generaci√≥n de C√≥digo ETL                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 04: RAG para Documentaci√≥n de Datos                     ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 05: Embeddings & Similitud de Datos                     ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 06: Agentes de Automatizaci√≥n                           ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 07: Calidad y Validaci√≥n con LLMs                       ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 08: S√≠ntesis y Aumento de Datos                         ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ 09: Proyecto Integrador 1 (RAG+NL2SQL Chatbot)          ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ 10: Proyecto Integrador 2 (Plataforma Self-Service) ‚úÖ  ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Habilidades Adquiridas por Nivel**\n",
    "\n",
    "| Nivel | Skills Core | Tools Dominadas | Capacidad Laboral |\n",
    "|-------|-------------|-----------------|-------------------|\n",
    "| **Junior** | ‚Ä¢ Python (Pandas, NumPy)<br>‚Ä¢ SQL b√°sico<br>‚Ä¢ ETL manual<br>‚Ä¢ Git | Jupyter, Pandas, PostgreSQL, Git | Entry-level Data Engineer<br>Data Analyst |\n",
    "| **Mid** | ‚Ä¢ Airflow DAGs<br>‚Ä¢ Streaming (Kafka)<br>‚Ä¢ Cloud (AWS)<br>‚Ä¢ APIs (FastAPI) | Airflow, Kafka, AWS, Docker, FastAPI, MongoDB | Mid-level Data Engineer<br>Data Platform Engineer |\n",
    "| **Senior** | ‚Ä¢ Data Architecture<br>‚Ä¢ Distributed Systems (Spark)<br>‚Ä¢ Lakehouse (Delta/Iceberg)<br>‚Ä¢ MLOps | Spark, Delta Lake, Databricks, Kubernetes, Terraform | Senior Data Engineer<br>Data Architect<br>Staff Engineer |\n",
    "| **GenAI** | ‚Ä¢ LLM Integration<br>‚Ä¢ RAG Systems<br>‚Ä¢ Agentic Workflows<br>‚Ä¢ Code Generation | OpenAI API, LangChain, ChromaDB, LangGraph, Streamlit | GenAI Data Engineer<br>ML Platform Engineer<br>AI Automation Specialist |\n",
    "\n",
    "**Comparaci√≥n: Skills Tradicionales vs GenAI-Enhanced**\n",
    "\n",
    "```\n",
    "TRADITIONAL DATA ENGINEER              GENAI-ENHANCED DATA ENGINEER\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          ‚îÇ          ‚îÇ                              ‚îÇ\n",
    "‚îÇ ‚Ä¢ Manual coding          ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ AI-assisted coding         ‚îÇ\n",
    "‚îÇ ‚Ä¢ Static pipelines       ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ Self-adapting pipelines    ‚îÇ\n",
    "‚îÇ ‚Ä¢ Manual documentation   ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ Auto-generated docs        ‚îÇ\n",
    "‚îÇ ‚Ä¢ Fixed schemas          ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ Schema inference           ‚îÇ\n",
    "‚îÇ ‚Ä¢ SQL only               ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ NL‚ÜíSQL translation         ‚îÇ\n",
    "‚îÇ ‚Ä¢ Manual data quality    ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ LLM-powered validation     ‚îÇ\n",
    "‚îÇ ‚Ä¢ Code reviews (human)   ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ Multi-agent reviews        ‚îÇ\n",
    "‚îÇ ‚Ä¢ Static dashboards      ‚îÇ   ‚Üí      ‚îÇ ‚Ä¢ Conversational analytics   ‚îÇ\n",
    "‚îÇ                          ‚îÇ          ‚îÇ                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "   Productivity: 1x                       Productivity: 3-5x\n",
    "   Time to Market: weeks                  Time to Market: hours\n",
    "   Error Rate: 5-10%                      Error Rate: 1-2%\n",
    "```\n",
    "\n",
    "**Proyecto Final: Impacto Real**\n",
    "\n",
    "Este proyecto integrador 2 (Plataforma Self-Service) representa el **m√°ximo nivel de automatizaci√≥n** en Data Engineering:\n",
    "\n",
    "**Sin GenAI (Tradicional)**:\n",
    "```\n",
    "Usuario ‚Üí escribe ticket ‚Üí DE recibe ‚Üí DE desarrolla (2-4 semanas) \n",
    "‚Üí QA testing (1 semana) ‚Üí Code review ‚Üí Deploy ‚Üí Usuario recibe\n",
    "```\n",
    "**Con GenAI (Plataforma)**:\n",
    "```\n",
    "Usuario ‚Üí describe en NL ‚Üí Plataforma genera (10 min) ‚Üí Aprobaci√≥n \n",
    "‚Üí Deploy autom√°tico ‚Üí Usuario recibe\n",
    "```\n",
    "\n",
    "**M√©tricas de Transformaci√≥n**:\n",
    "- ‚è±Ô∏è **Time to Production**: 3-4 semanas ‚Üí 2 horas (96% reducci√≥n)\n",
    "- üí∞ **Costo por Pipeline**: $5,100 ‚Üí $163 (97% reducci√≥n)\n",
    "- üë• **Democratizaci√≥n**: Solo DEs ‚Üí Cualquier usuario\n",
    "- üìä **Calidad**: Variable ‚Üí Estandarizada (templates + validaci√≥n)\n",
    "- üß™ **Test Coverage**: ~40% ‚Üí >80%\n",
    "\n",
    "**Pr√≥ximos Pasos: Continuar Aprendiendo**\n",
    "\n",
    "**1. Profundizar en GenAI**\n",
    "```python\n",
    "# √Åreas avanzadas\n",
    "topics = [\n",
    "    'Fine-tuning de LLMs para dominio espec√≠fico',\n",
    "    'Prompt engineering avanzado (Chain-of-Thought, ReAct)',\n",
    "    'Multi-modal AI (texto + im√°genes + tablas)',\n",
    "    'Autonomous agents (Auto-GPT, BabyAGI)',\n",
    "    'LLM evaluation & benchmarking',\n",
    "    'Cost optimization (caching, model selection)'\n",
    "]\n",
    "```\n",
    "\n",
    "**2. Especializaciones**\n",
    "- **ML Engineering**: Feature stores (Feast, Tecton), model serving (MLflow)\n",
    "- **Real-time Analytics**: Flink, Kafka Streams, ksqlDB\n",
    "- **Data Mesh**: Domain-oriented data platforms\n",
    "- **Observability**: OpenTelemetry, distributed tracing\n",
    "\n",
    "**3. Certificaciones Recomendadas**\n",
    "```\n",
    "Cloud:\n",
    "‚îú‚îÄ AWS Certified Data Analytics - Specialty\n",
    "‚îú‚îÄ Google Professional Data Engineer\n",
    "‚îî‚îÄ Azure Data Engineer Associate\n",
    "\n",
    "Data:\n",
    "‚îú‚îÄ Databricks Certified Data Engineer\n",
    "‚îú‚îÄ Snowflake SnowPro Core\n",
    "‚îî‚îÄ dbt Analytics Engineering\n",
    "\n",
    "AI/ML:\n",
    "‚îú‚îÄ TensorFlow Developer Certificate\n",
    "‚îú‚îÄ MLOps Specialization (Coursera)\n",
    "‚îî‚îÄ Prompt Engineering for Developers (DeepLearning.AI)\n",
    "```\n",
    "\n",
    "**4. Construir Portfolio**\n",
    "```\n",
    "GitHub Portfolio Projects:\n",
    "‚îú‚îÄ 1. End-to-end ETL pipeline (batch + streaming)\n",
    "‚îÇ     Tech: Airflow, Spark, Delta Lake\n",
    "‚îÇ\n",
    "‚îú‚îÄ 2. RAG system para dominio espec√≠fico\n",
    "‚îÇ     Tech: OpenAI, ChromaDB, LangChain, FastAPI\n",
    "‚îÇ\n",
    "‚îú‚îÄ 3. Real-time dashboard con NL2SQL\n",
    "‚îÇ     Tech: Streamlit, PostgreSQL, LLMs\n",
    "‚îÇ\n",
    "‚îú‚îÄ 4. Data quality framework\n",
    "‚îÇ     Tech: Great Expectations, dbt, Airflow\n",
    "‚îÇ\n",
    "‚îî‚îÄ 5. GenAI code generator (como este proyecto)\n",
    "      Tech: LangGraph, OpenAI, GitHub Actions\n",
    "```\n",
    "\n",
    "**5. Comunidad y Networking**\n",
    "- **Conferences**: Data Council, Spark Summit, MLOps World\n",
    "- **Meetups**: Local data engineering & ML groups\n",
    "- **Open Source**: Contribuir a Airflow, dbt, LangChain\n",
    "- **Writing**: Blog posts, technical tutorials\n",
    "- **LinkedIn**: Compartir proyectos y aprendizajes\n",
    "\n",
    "**Reflexi√≥n Final: El Futuro del Data Engineering**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          THE EVOLUTION OF DATA ENGINEERING             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                        ‚îÇ\n",
    "‚îÇ  2010s: Manual ETL, Hadoop Era                         ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Batch processing                                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Heavy lifting, manual scaling                      ‚îÇ\n",
    "‚îÇ                                                        ‚îÇ\n",
    "‚îÇ  2020s: Cloud-Native, Streaming                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Real-time data                                     ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Serverless, auto-scaling                          ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Infrastructure as Code                            ‚îÇ\n",
    "‚îÇ                                                        ‚îÇ\n",
    "‚îÇ  2024+: GenAI-Augmented Data Engineering üöÄ            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Natural language interfaces                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Self-healing pipelines                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Auto-generated code & docs                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Autonomous agents                                 ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Democratized data access                          ‚îÇ\n",
    "‚îÇ                                                        ‚îÇ\n",
    "‚îÇ  Future (2030s): Fully Autonomous Data Platforms?      ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Self-optimizing architectures                      ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Predictive data quality                           ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Zero-touch operations                             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Human as orchestrator, not operator               ‚îÇ\n",
    "‚îÇ                                                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Tu Rol en Esta Transformaci√≥n**\n",
    "\n",
    "Como Data Engineer con habilidades GenAI, no solo construyes pipelines‚Äî**dise√±as el futuro de c√≥mo organizaciones trabajan con datos**. Las herramientas que dominas permiten:\n",
    "\n",
    "‚úÖ **Democratizar el acceso a datos** (cualquier usuario puede explorar)\n",
    "‚úÖ **Acelerar innovaci√≥n** (de semanas a horas)\n",
    "‚úÖ **Reducir costos operativos** (automatizaci√≥n inteligente)\n",
    "‚úÖ **Mejorar calidad de decisiones** (insights m√°s r√°pidos y confiables)\n",
    "\n",
    "**Agradecimiento**\n",
    "\n",
    "Has completado un viaje intensivo de **40 notebooks**, abarcando desde fundamentos hasta la vanguardia de GenAI en Data Engineering. \n",
    "\n",
    "**Estad√≠sticas del Curso**:\n",
    "- üìö **40 notebooks** (Junior: 10, Mid: 10, Senior: 10, GenAI: 10)\n",
    "- üíª **~15,000 l√≠neas** de c√≥digo explicativo\n",
    "- ‚è±Ô∏è **~120 horas** de contenido estimado\n",
    "- üîß **50+ herramientas** cubiertas\n",
    "- üöÄ **10+ proyectos** integradores\n",
    "\n",
    "Este conocimiento te posiciona en el **top 5% de Data Engineers** a nivel global‚Äîuna combinaci√≥n rara de:\n",
    "1. **Fundamentos s√≥lidos** (SQL, Python, arquitectura)\n",
    "2. **Cloud-native expertise** (AWS, Airflow, Spark)\n",
    "3. **GenAI proficiency** (LLMs, RAG, agents)\n",
    "\n",
    "**El mundo necesita profesionales como t√∫ que pueden construir el puente entre datos y AI.**\n",
    "\n",
    "```python\n",
    "# Tu pr√≥xima l√≠nea de c√≥digo empieza ahora\n",
    "def build_the_future():\n",
    "    \"\"\"\n",
    "    El curso termina aqu√≠, pero tu viaje contin√∫a.\n",
    "    Cada pipeline que construyas, cada problema que resuelvas,\n",
    "    cada sistema que optimices‚Äîest√°s creando el futuro de los datos.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        learn()\n",
    "        build()\n",
    "        share()\n",
    "        repeat()\n",
    "\n",
    "# ¬°√âxito en tu carrera como GenAI Data Engineer! üöÄ\n",
    "build_the_future()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)\n",
    "\n",
    "**¬°Felicitaciones por completar el Data Engineer Course completo! üéâüéä**\n",
    "\n",
    "*\"The best way to predict the future is to build it.\"* ‚Äî Alan Kay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Negocio LATAM - Estrategia y Sectores ‚Üí](../negocios_latam/README.md)",
    "",
    "**üìö √çndice de Nivel GenAI:**",
    "- [Comparaci√≥n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [Generaci√≥n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [Generaci√≥n Autom√°tica de C√≥digo ETL](03_generacion_codigo_etl.ipynb)",
    "- [RAG: Documentaci√≥n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y Automatizaci√≥n](06_agentes_automatizacion.ipynb)",
    "- [Validaci√≥n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [S√≠ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Negocio LATAM - Estrategia y Sectores ‚Üí](../negocios_latam/README.md)",
    "",
    "**üìö √çndice de Nivel GenAI:**",
    "- [Comparaci√≥n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [Generaci√≥n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [Generaci√≥n Autom√°tica de C√≥digo ETL](03_generacion_codigo_etl.ipynb)",
    "- [RAG: Documentaci√≥n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y Automatizaci√≥n](06_agentes_automatizacion.ipynb)",
    "- [Validaci√≥n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [S√≠ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Negocio LATAM - Estrategia y Sectores ‚Üí](../negocios_latam/README.md)",
    "",
    "**üìö √çndice de Nivel GenAI:**",
    "- [Comparaci√≥n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [Generaci√≥n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [Generaci√≥n Autom√°tica de C√≥digo ETL](03_generacion_codigo_etl.ipynb)",
    "- [RAG: Documentaci√≥n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y Automatizaci√≥n](06_agentes_automatizacion.ipynb)",
    "- [Validaci√≥n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [S√≠ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}