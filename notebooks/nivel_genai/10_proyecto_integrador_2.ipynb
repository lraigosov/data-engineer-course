{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bfcc57",
   "metadata": {},
   "source": [
    "# 🏗️ Proyecto Integrador 2: Plataforma Self-Service con GenAI\n",
    "\n",
    "**Objetivo**: construir una plataforma que permita a usuarios no técnicos generar pipelines ETL, documentación, y reportes usando IA generativa.\n",
    "\n",
    "## Alcance del Proyecto\n",
    "\n",
    "- **Duración**: 6-8 horas\n",
    "- **Dificultad**: Muy Alta\n",
    "- **Stack**: OpenAI, LangGraph, Airflow, Great Expectations, Streamlit\n",
    "\n",
    "## Funcionalidades\n",
    "\n",
    "1. ✅ Generación de pipelines ETL desde descripción en lenguaje natural\n",
    "2. ✅ Validación automática del código generado\n",
    "3. ✅ Creación de tests unitarios\n",
    "4. ✅ Generación de documentación técnica\n",
    "5. ✅ Dashboard de monitoreo\n",
    "6. ✅ Sistema de aprobación y deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e08ff",
   "metadata": {},
   "source": [
    "### 🏗️ **Arquitectura de Plataforma Self-Service: De NL a Producción**\n",
    "\n",
    "**Visión General: Data Democratization**\n",
    "\n",
    "Esta plataforma representa el siguiente nivel en la evolución de Data Engineering: convertir descripciones en lenguaje natural en pipelines ETL completos, probados, documentados y desplegados.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│              PLATAFORMA SELF-SERVICE ARCHITECTURE               │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  [Usuario No Técnico] → Streamlit UI                           │\n",
    "│           ↓                                                     │\n",
    "│  [1. NL Parser]                                                 │\n",
    "│      • Intent extraction                                        │\n",
    "│      • Entity recognition (sources, transformations, schedule) │\n",
    "│      • Requirement validation                                   │\n",
    "│           ↓                                                     │\n",
    "│  [2. Code Generator]                                            │\n",
    "│      • Template selection (batch/streaming/hybrid)             │\n",
    "│      • Framework choice (Pandas/Spark/Polars)                  │\n",
    "│      • Best practices injection                                │\n",
    "│           ↓                                                     │\n",
    "│  [3. Security Validator]                                        │\n",
    "│      • AST parsing (syntax validation)                         │\n",
    "│      • Security patterns (no eval, exec, os.system)            │\n",
    "│      • Dependency check (vulnerable packages)                  │\n",
    "│           ↓                                                     │\n",
    "│  [4. Test Generator]                                            │\n",
    "│      • Unit tests (per transformation)                         │\n",
    "│      • Integration tests (mocked I/O)                          │\n",
    "│      • Data quality tests (Great Expectations)                 │\n",
    "│           ↓                                                     │\n",
    "│  [5. Documentation Generator]                                   │\n",
    "│      • README.md (architecture, setup, troubleshooting)        │\n",
    "│      • API docs (if exposed)                                   │\n",
    "│      • Runbook (operations guide)                              │\n",
    "│           ↓                                                     │\n",
    "│  [6. DAG Generator]                                             │\n",
    "│      • Airflow DAG (orchestration)                             │\n",
    "│      • Task dependencies                                       │\n",
    "│      • Retry/alerting logic                                    │\n",
    "│           ↓                                                     │\n",
    "│  [7. Human Review]                                              │\n",
    "│      • Side-by-side diff                                       │\n",
    "│      • Approve/Reject/Request changes                          │\n",
    "│           ↓                                                     │\n",
    "│  [8. Deployment]                                                │\n",
    "│      • Run tests (pytest)                                      │\n",
    "│      • Create PR (GitHub)                                      │\n",
    "│      • Deploy to staging → prod                                │\n",
    "│           ↓                                                     │\n",
    "│  [9. Monitoring]                                                │\n",
    "│      • Execution metrics                                       │\n",
    "│      • Data quality alerts                                     │\n",
    "│      • Cost tracking                                           │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Componentes Clave**\n",
    "\n",
    "**1. NL Parser (Structured Requirement Extraction)**\n",
    "\n",
    "```python\n",
    "# Prompt engineering para extracción estructurada\n",
    "PARSER_PROMPT = \"\"\"\n",
    "Analiza este requerimiento de pipeline ETL y extrae información estructurada.\n",
    "\n",
    "INPUT: {user_input}\n",
    "\n",
    "OUTPUT (JSON):\n",
    "{\n",
    "  \"pipeline_name\": \"nombre_snake_case\",\n",
    "  \"source\": {\n",
    "    \"type\": \"s3|postgres|mysql|api|csv\",\n",
    "    \"location\": \"bucket/path o connection_string\",\n",
    "    \"credentials\": \"reference_to_secret_manager\"\n",
    "  },\n",
    "  \"transformations\": [\n",
    "    {\n",
    "      \"type\": \"filter|aggregate|join|pivot\",\n",
    "      \"description\": \"...\",\n",
    "      \"columns\": [\"col1\", \"col2\"]\n",
    "    }\n",
    "  ],\n",
    "  \"destination\": {\n",
    "    \"type\": \"redshift|postgres|s3|delta\",\n",
    "    \"location\": \"...\",\n",
    "    \"mode\": \"append|overwrite|upsert\"\n",
    "  },\n",
    "  \"schedule\": {\n",
    "    \"frequency\": \"daily|hourly|weekly\",\n",
    "    \"time\": \"02:00\",\n",
    "    \"timezone\": \"UTC\"\n",
    "  },\n",
    "  \"validations\": [\n",
    "    {\n",
    "      \"type\": \"not_null|unique|range|regex\",\n",
    "      \"column\": \"...\",\n",
    "      \"threshold\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"sla\": {\n",
    "    \"max_duration_minutes\": 60,\n",
    "    \"alert_email\": \"team@example.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "REGLAS:\n",
    "- Inferir transformaciones implícitas (ej: \"últimos 30 días\" → filter fecha)\n",
    "- Sugerir validaciones si no están explícitas\n",
    "- Usar defaults sensatos (schedule=daily si no especificado)\n",
    "\"\"\"\n",
    "\n",
    "# Ejemplo de parsing robusto\n",
    "def parse_with_validation(user_input: str) -> dict:\n",
    "    requirements = parse_requirements(user_input)\n",
    "    \n",
    "    # Validar completitud\n",
    "    required_fields = ['pipeline_name', 'source', 'destination']\n",
    "    missing = [f for f in required_fields if f not in requirements]\n",
    "    \n",
    "    if missing:\n",
    "        # LLM completa campos faltantes\n",
    "        completion_prompt = f\"Completa estos campos faltantes: {missing}\"\n",
    "        requirements = complete_requirements(requirements, completion_prompt)\n",
    "    \n",
    "    return requirements\n",
    "```\n",
    "\n",
    "**2. Code Generator (Multi-Template Strategy)**\n",
    "\n",
    "```python\n",
    "# Templates por tipo de workload\n",
    "TEMPLATES = {\n",
    "    'batch': '''\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class {pipeline_name}Pipeline:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract(self) -> pd.DataFrame:\n",
    "        \"\"\"Extract from {source_type}\"\"\"\n",
    "        {extract_code}\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformations\"\"\"\n",
    "        {transform_code}\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Data quality checks\"\"\"\n",
    "        {validation_code}\n",
    "    \n",
    "    def load(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Load to {destination_type}\"\"\"\n",
    "        {load_code}\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        logger.info(\"Starting pipeline...\")\n",
    "        df = self.extract()\n",
    "        df = self.transform(df)\n",
    "        \n",
    "        if not self.validate(df):\n",
    "            raise ValueError(\"Data quality checks failed\")\n",
    "        \n",
    "        self.load(df)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "''',\n",
    "    'streaming': '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"{pipeline_name}\").getOrCreate()\n",
    "\n",
    "# Read stream\n",
    "df = spark.readStream \\\\\n",
    "    .format(\"{source_format}\") \\\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"{kafka_servers}\") \\\\\n",
    "    .load()\n",
    "\n",
    "# Transformations\n",
    "{transform_code}\n",
    "\n",
    "# Write stream\n",
    "query = df.writeStream \\\\\n",
    "    .format(\"{sink_format}\") \\\\\n",
    "    .outputMode(\"append\") \\\\\n",
    "    .trigger(processingTime='1 minute') \\\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "'''\n",
    "}\n",
    "\n",
    "# Generación con context injection\n",
    "def generate_with_context(requirements: dict) -> str:\n",
    "    # Seleccionar template\n",
    "    workload_type = infer_workload_type(requirements)\n",
    "    template = TEMPLATES[workload_type]\n",
    "    \n",
    "    # Inyectar código específico\n",
    "    extract_code = generate_extract_code(requirements['source'])\n",
    "    transform_code = generate_transform_code(requirements['transformations'])\n",
    "    validation_code = generate_validation_code(requirements['validations'])\n",
    "    load_code = generate_load_code(requirements['destination'])\n",
    "    \n",
    "    # Rellenar template\n",
    "    code = template.format(\n",
    "        pipeline_name=requirements['pipeline_name'],\n",
    "        source_type=requirements['source']['type'],\n",
    "        destination_type=requirements['destination']['type'],\n",
    "        extract_code=extract_code,\n",
    "        transform_code=transform_code,\n",
    "        validation_code=validation_code,\n",
    "        load_code=load_code\n",
    "    )\n",
    "    \n",
    "    return code\n",
    "```\n",
    "\n",
    "**3. Security Validator (Multi-Layer)**\n",
    "\n",
    "```python\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def validate_security(code: str) -> dict:\n",
    "    issues = []\n",
    "    \n",
    "    # Layer 1: AST parsing (syntax + structure)\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        # Detectar imports peligrosos\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for alias in node.names:\n",
    "                    if alias.name in ['pickle', 'subprocess', 'os']:\n",
    "                        issues.append({\n",
    "                            'severity': 'HIGH',\n",
    "                            'type': 'DANGEROUS_IMPORT',\n",
    "                            'message': f'Import de módulo peligroso: {alias.name}',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "            \n",
    "            # Detectar eval/exec\n",
    "            if isinstance(node, ast.Call):\n",
    "                if isinstance(node.func, ast.Name):\n",
    "                    if node.func.id in ['eval', 'exec']:\n",
    "                        issues.append({\n",
    "                            'severity': 'CRITICAL',\n",
    "                            'type': 'CODE_INJECTION',\n",
    "                            'message': f'Uso de {node.func.id}() detectado',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "    \n",
    "    except SyntaxError as e:\n",
    "        issues.append({\n",
    "            'severity': 'CRITICAL',\n",
    "            'type': 'SYNTAX_ERROR',\n",
    "            'message': str(e)\n",
    "        })\n",
    "    \n",
    "    # Layer 2: Regex patterns (SQL injection)\n",
    "    if re.search(r'f\"SELECT.*\\{.*\\}\"', code):\n",
    "        issues.append({\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'SQL_INJECTION_RISK',\n",
    "            'message': 'Uso de f-strings en queries SQL detectado'\n",
    "        })\n",
    "    \n",
    "    # Layer 3: Dependency scanning\n",
    "    imports = extract_imports(code)\n",
    "    vulnerable = check_vulnerabilities(imports)\n",
    "    issues.extend(vulnerable)\n",
    "    \n",
    "    return {\n",
    "        'valid': all(i['severity'] != 'CRITICAL' for i in issues),\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Dependency scanning\n",
    "def check_vulnerabilities(packages: list) -> list:\n",
    "    # Integración con safety (https://pyup.io/safety/)\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        ['safety', 'check', '--json'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    vulnerabilities = json.loads(result.stdout)\n",
    "    return [\n",
    "        {\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'VULNERABLE_DEPENDENCY',\n",
    "            'message': f\"{v['package']}: {v['vulnerability']}\"\n",
    "        }\n",
    "        for v in vulnerabilities\n",
    "    ]\n",
    "```\n",
    "\n",
    "**4. Test Generator (Comprehensive Coverage)**\n",
    "\n",
    "```python\n",
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    test_template = '''\n",
    "import pytest\n",
    "from unittest.mock import Mock, patch\n",
    "import pandas as pd\n",
    "from {module} import {pipeline_class}\n",
    "\n",
    "@pytest.fixture\n",
    "def pipeline():\n",
    "    config = {config_dict}\n",
    "    return {pipeline_class}(config)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return pd.DataFrame({sample_data_dict})\n",
    "\n",
    "# Unit Tests\n",
    "def test_extract(pipeline, mocker):\n",
    "    \"\"\"Test data extraction\"\"\"\n",
    "    mock_read = mocker.patch('{read_function}')\n",
    "    mock_read.return_value = pd.DataFrame({mock_data})\n",
    "    \n",
    "    df = pipeline.extract()\n",
    "    \n",
    "    assert not df.empty\n",
    "    assert list(df.columns) == {expected_columns}\n",
    "\n",
    "def test_transform(pipeline, sample_data):\n",
    "    \"\"\"Test transformations\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    \n",
    "    # Assertions específicas\n",
    "    {transform_assertions}\n",
    "\n",
    "def test_validate_success(pipeline, sample_data):\n",
    "    \"\"\"Test validation with clean data\"\"\"\n",
    "    assert pipeline.validate(sample_data) == True\n",
    "\n",
    "def test_validate_failure(pipeline):\n",
    "    \"\"\"Test validation with dirty data\"\"\"\n",
    "    dirty_data = pd.DataFrame({dirty_data_dict})\n",
    "    assert pipeline.validate(dirty_data) == False\n",
    "\n",
    "def test_load(pipeline, sample_data, mocker):\n",
    "    \"\"\"Test data loading\"\"\"\n",
    "    mock_write = mocker.patch('{write_function}')\n",
    "    \n",
    "    pipeline.load(sample_data)\n",
    "    \n",
    "    mock_write.assert_called_once()\n",
    "\n",
    "# Integration Tests\n",
    "@patch('{source_module}.read')\n",
    "@patch('{destination_module}.write')\n",
    "def test_full_pipeline(mock_write, mock_read, pipeline):\n",
    "    \"\"\"Test end-to-end execution\"\"\"\n",
    "    mock_read.return_value = pd.DataFrame({mock_source_data})\n",
    "    \n",
    "    pipeline.run()\n",
    "    \n",
    "    mock_write.assert_called()\n",
    "    written_df = mock_write.call_args[0][0]\n",
    "    assert len(written_df) > 0\n",
    "\n",
    "# Data Quality Tests\n",
    "def test_no_null_values(pipeline, sample_data):\n",
    "    \"\"\"Test no nulls in required columns\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    required_cols = {required_columns}\n",
    "    \n",
    "    for col in required_cols:\n",
    "        assert df[col].isnull().sum() == 0\n",
    "\n",
    "def test_data_types(pipeline, sample_data):\n",
    "    \"\"\"Test correct data types\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    expected_types = {expected_types_dict}\n",
    "    \n",
    "    for col, dtype in expected_types.items():\n",
    "        assert df[col].dtype == dtype\n",
    "'''\n",
    "    \n",
    "    # Generar código específico basado en requirements\n",
    "    test_code = test_template.format(\n",
    "        module=requirements['pipeline_name'],\n",
    "        pipeline_class=to_camel_case(requirements['pipeline_name']),\n",
    "        config_dict=generate_config_mock(requirements),\n",
    "        sample_data_dict=generate_sample_data(requirements),\n",
    "        # ... más placeholders\n",
    "    )\n",
    "    \n",
    "    return test_code\n",
    "```\n",
    "\n",
    "**Beneficios de la Arquitectura**\n",
    "\n",
    "| Aspecto | Antes (Manual) | Después (Platform) |\n",
    "|---------|----------------|---------------------|\n",
    "| **Time to Production** | 2-4 semanas | 1-2 horas |\n",
    "| **Code Quality** | Variable (depende de desarrollador) | Consistente (templates + validación) |\n",
    "| **Documentation** | A menudo ausente u obsoleta | Siempre actualizada (auto-generada) |\n",
    "| **Testing** | ~40% coverage | >80% coverage (auto-generado) |\n",
    "| **Security** | Manual reviews | Automated scanning (cada generación) |\n",
    "| **Onboarding** | Requiere semanas de training | Cualquier usuario puede crear pipelines |\n",
    "| **Maintenance** | Alto (cada pipeline es único) | Bajo (patterns estandarizados) |\n",
    "\n",
    "**ROI Calculation**\n",
    "\n",
    "```python\n",
    "# Ejemplo de ROI\n",
    "MANUAL_PIPELINE = {\n",
    "    'development_hours': 40,\n",
    "    'testing_hours': 16,\n",
    "    'documentation_hours': 8,\n",
    "    'review_hours': 4,\n",
    "    'total_hours': 68,\n",
    "    'engineer_cost_hour': 75,  # USD\n",
    "    'total_cost': 5100  # USD\n",
    "}\n",
    "\n",
    "PLATFORM_PIPELINE = {\n",
    "    'generation_minutes': 10,\n",
    "    'review_hours': 2,\n",
    "    'total_hours': 2.17,\n",
    "    'total_cost': 163  # USD\n",
    "}\n",
    "\n",
    "savings_per_pipeline = MANUAL_PIPELINE['total_cost'] - PLATFORM_PIPELINE['total_cost']  # $4,937\n",
    "time_saved = MANUAL_PIPELINE['total_hours'] - PLATFORM_PIPELINE['total_hours']  # 65.83 hours\n",
    "\n",
    "# Si un equipo crea 20 pipelines/año\n",
    "annual_savings = savings_per_pipeline * 20  # $98,740\n",
    "annual_time_saved = time_saved * 20  # 1,316 hours (= 164 días de trabajo)\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales**\n",
    "\n",
    "1. **Marketing Analytics**: \"Necesito agregar eventos de Mixpanel por usuario y día\"\n",
    "2. **Finance Reporting**: \"Consolidar transacciones de 5 bases de datos en un reporte diario\"\n",
    "3. **ML Feature Engineering**: \"Crear features de usuario para modelo de churn\"\n",
    "4. **Data Migration**: \"Migrar tablas de MySQL legacy a Snowflake\"\n",
    "5. **Real-time Alerting**: \"Detectar anomalías en ventas cada 5 minutos\"\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17119fed",
   "metadata": {},
   "source": [
    "## Parte 1: Arquitectura del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541588eb",
   "metadata": {},
   "source": [
    "```\n",
    "User Input (NL)\n",
    "    ↓\n",
    "[1. Parser] → extrae requisitos\n",
    "    ↓\n",
    "[2. Generator] → genera código ETL\n",
    "    ↓\n",
    "[3. Validator] → valida sintaxis, seguridad\n",
    "    ↓\n",
    "[4. Tester] → genera y ejecuta tests\n",
    "    ↓\n",
    "[5. Documenter] → crea README\n",
    "    ↓\n",
    "[6. Reviewer] → revisión humana\n",
    "    ↓\n",
    "[7. Deployer] → despliega a producción\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c0460",
   "metadata": {},
   "source": [
    "### 🔄 **LangGraph: Orquestación de Agentes Multi-Paso**\n",
    "\n",
    "**¿Por qué LangGraph para Workflows Complejos?**\n",
    "\n",
    "LangGraph permite crear flujos de trabajo con:\n",
    "- **State Management**: Estado persistente entre nodos\n",
    "- **Conditional Routing**: Lógica de decisión (if/else paths)\n",
    "- **Human-in-the-Loop**: Pausar para aprobación humana\n",
    "- **Retry Logic**: Reintentar nodos fallidos\n",
    "- **Streaming**: Ver progreso en tiempo real\n",
    "\n",
    "**Arquitectura del Workflow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# State compartido entre todos los nodos\n",
    "class PipelineState(TypedDict):\n",
    "    # Inputs\n",
    "    user_input: str\n",
    "    \n",
    "    # Intermedios\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    \n",
    "    # Outputs\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]  # Append errors from any node\n",
    "    \n",
    "    # Metadata\n",
    "    generation_time: float\n",
    "    review_notes: str\n",
    "\n",
    "# Cada nodo es una función pura: State → State\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Extrae requisitos estructurados\"\"\"\n",
    "    try:\n",
    "        state['requirements'] = parse_requirements(state['user_input'])\n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Parse error: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Genera código ETL\"\"\"\n",
    "    if state['errors']:\n",
    "        return state  # Skip si hay errores previos\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    state['generation_time'] = time.time() - start\n",
    "    \n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Valida seguridad y sintaxis\"\"\"\n",
    "    validation_result = validate_code(state['code'])\n",
    "    state['validation'] = validation_result\n",
    "    \n",
    "    if not validation_result['valid']:\n",
    "        state['errors'].extend([\n",
    "            f\"{issue['type']}: {issue['message']}\"\n",
    "            for issue in validation_result['issues']\n",
    "        ])\n",
    "    \n",
    "    return state\n",
    "\n",
    "def should_continue_after_validation(state: PipelineState) -> str:\n",
    "    \"\"\"Conditional edge: decide si continuar o terminar\"\"\"\n",
    "    if state['validation']['valid']:\n",
    "        return \"test\"  # Go to test node\n",
    "    else:\n",
    "        return END  # Stop workflow\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"parse\", parse_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"validate\", validate_node)\n",
    "workflow.add_node(\"test\", test_node)\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"review\", review_node)\n",
    "\n",
    "# Add edges (define flow)\n",
    "workflow.set_entry_point(\"parse\")\n",
    "workflow.add_edge(\"parse\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"validate\")\n",
    "\n",
    "# Conditional edge (branching)\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    should_continue_after_validation,\n",
    "    {\n",
    "        \"test\": \"test\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"document\", \"review\")\n",
    "workflow.add_edge(\"review\", END)\n",
    "\n",
    "# Compile graph\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "**Visualización del Grafo**\n",
    "\n",
    "```\n",
    "        ┌─────────┐\n",
    "        │  START  │\n",
    "        └────┬────┘\n",
    "             │\n",
    "             v\n",
    "        ┌─────────┐\n",
    "        │  Parse  │  ← Extrae requisitos\n",
    "        └────┬────┘\n",
    "             │\n",
    "             v\n",
    "        ┌──────────┐\n",
    "        │ Generate │  ← Genera código\n",
    "        └────┬─────┘\n",
    "             │\n",
    "             v\n",
    "        ┌──────────┐\n",
    "        │ Validate │  ← Valida seguridad\n",
    "        └────┬─────┘\n",
    "             │\n",
    "        ┌────┴────┐\n",
    "        │ Valid?  │\n",
    "        └─┬─────┬─┘\n",
    "     Yes  │     │ No\n",
    "          v     v\n",
    "      ┌──────┐ END\n",
    "      │ Test │\n",
    "      └──┬───┘\n",
    "         │\n",
    "         v\n",
    "    ┌──────────┐\n",
    "    │ Document │\n",
    "    └────┬─────┘\n",
    "         │\n",
    "         v\n",
    "    ┌────────┐\n",
    "    │ Review │  ← Human approval\n",
    "    └────┬───┘\n",
    "         │\n",
    "         v\n",
    "      ┌─────┐\n",
    "      │ END │\n",
    "      └─────┘\n",
    "```\n",
    "\n",
    "**Streaming de Estado (Real-time Updates)**\n",
    "\n",
    "```python\n",
    "# Ejecutar con streaming\n",
    "initial_state = {\n",
    "    'user_input': \"Pipeline de ventas...\",\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': [],\n",
    "    'generation_time': 0.0,\n",
    "    'review_notes': ''\n",
    "}\n",
    "\n",
    "# Stream: ver actualizaciones de cada nodo\n",
    "for output in app.stream(initial_state):\n",
    "    node_name = list(output.keys())[0]\n",
    "    state = output[node_name]\n",
    "    \n",
    "    print(f\"\\n✅ Completado: {node_name}\")\n",
    "    \n",
    "    if node_name == \"parse\":\n",
    "        print(f\"Pipeline: {state['requirements'].get('pipeline_name')}\")\n",
    "    \n",
    "    elif node_name == \"generate\":\n",
    "        print(f\"Código: {len(state['code'])} caracteres\")\n",
    "        print(f\"Tiempo: {state['generation_time']:.2f}s\")\n",
    "    \n",
    "    elif node_name == \"validate\":\n",
    "        status = \"✅ OK\" if state['validation']['valid'] else \"❌ Errores\"\n",
    "        print(f\"Validación: {status}\")\n",
    "        \n",
    "        if state['errors']:\n",
    "            for error in state['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "# Final state\n",
    "final = list(app.stream(initial_state))[-1]\n",
    "```\n",
    "\n",
    "**Human-in-the-Loop (Approval Gate)**\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def review_node_interactive(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Nodo que pausa y espera aprobación humana\"\"\"\n",
    "    \n",
    "    # Mostrar resumen para reviewer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🔍 REVISIÓN REQUERIDA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPipeline: {state['requirements']['pipeline_name']}\")\n",
    "    print(f\"Source: {state['requirements']['source']['type']}\")\n",
    "    print(f\"Destination: {state['requirements']['destination']['type']}\")\n",
    "    print(f\"\\nValidación: {'✅ Aprobado' if state['validation']['valid'] else '❌ Errores'}\")\n",
    "    \n",
    "    if state['errors']:\n",
    "        print(\"\\nErrores detectados:\")\n",
    "        for error in state['errors']:\n",
    "            print(f\"  ❌ {error}\")\n",
    "    \n",
    "    print(f\"\\nCódigo generado ({len(state['code'])} caracteres):\")\n",
    "    print(state['code'][:300] + \"...\")\n",
    "    \n",
    "    # Esperar input humano\n",
    "    while True:\n",
    "        decision = input(\"\\n[A]probar / [R]echazar / [M]odificar: \").upper()\n",
    "        \n",
    "        if decision == 'A':\n",
    "            state['approved'] = True\n",
    "            state['review_notes'] = \"Aprobado por revisor\"\n",
    "            break\n",
    "        \n",
    "        elif decision == 'R':\n",
    "            state['approved'] = False\n",
    "            rejection_reason = input(\"Razón del rechazo: \")\n",
    "            state['errors'].append(f\"Rechazado: {rejection_reason}\")\n",
    "            state['review_notes'] = rejection_reason\n",
    "            break\n",
    "        \n",
    "        elif decision == 'M':\n",
    "            modifications = input(\"Modificaciones requeridas: \")\n",
    "            state['review_notes'] = f\"Modificaciones: {modifications}\"\n",
    "            # Re-route to generate node with feedback\n",
    "            state['user_input'] += f\"\\n\\nAjustes: {modifications}\"\n",
    "            # Esta lógica requiere un loop en el grafo\n",
    "            break\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Integrar en workflow\n",
    "workflow.add_node(\"review\", review_node_interactive)\n",
    "```\n",
    "\n",
    "**Retry Logic (Reintentos Automáticos)**\n",
    "\n",
    "```python\n",
    "def generate_with_retry(state: PipelineState, max_retries: int = 3) -> PipelineState:\n",
    "    \"\"\"Nodo con reintentos automáticos\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Intentar generar código\n",
    "            state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "            \n",
    "            # Validar sintaxis\n",
    "            ast.parse(state['code'])\n",
    "            \n",
    "            # Success\n",
    "            state['errors'] = [e for e in state['errors'] if 'Generation' not in e]\n",
    "            break\n",
    "        \n",
    "        except SyntaxError as e:\n",
    "            error_msg = f\"Generation attempt {attempt+1} failed: {str(e)}\"\n",
    "            print(f\"⚠️  {error_msg}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"🔄 Retrying...\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                state['errors'].append(error_msg)\n",
    "    \n",
    "    return state\n",
    "```\n",
    "\n",
    "**Sub-Graphs (Workflows Anidados)**\n",
    "\n",
    "```python\n",
    "# Sub-workflow para generación de tests\n",
    "test_workflow = StateGraph(PipelineState)\n",
    "\n",
    "def unit_test_node(state):\n",
    "    state['unit_tests'] = generate_unit_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def integration_test_node(state):\n",
    "    state['integration_tests'] = generate_integration_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def quality_test_node(state):\n",
    "    state['quality_tests'] = generate_quality_tests(state['requirements'])\n",
    "    return state\n",
    "\n",
    "# Build sub-graph\n",
    "test_workflow.add_node(\"unit\", unit_test_node)\n",
    "test_workflow.add_node(\"integration\", integration_test_node)\n",
    "test_workflow.add_node(\"quality\", quality_test_node)\n",
    "\n",
    "test_workflow.set_entry_point(\"unit\")\n",
    "test_workflow.add_edge(\"unit\", \"integration\")\n",
    "test_workflow.add_edge(\"integration\", \"quality\")\n",
    "test_workflow.add_edge(\"quality\", END)\n",
    "\n",
    "test_subgraph = test_workflow.compile()\n",
    "\n",
    "# Integrar sub-graph en main workflow\n",
    "def test_orchestrator(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Ejecuta sub-workflow de tests\"\"\"\n",
    "    test_results = test_subgraph.invoke(state)\n",
    "    \n",
    "    # Combinar tests\n",
    "    state['tests'] = (\n",
    "        test_results['unit_tests'] +\n",
    "        test_results['integration_tests'] +\n",
    "        test_results['quality_tests']\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"test\", test_orchestrator)\n",
    "```\n",
    "\n",
    "**Parallel Execution (Nodos Independientes)**\n",
    "\n",
    "```python\n",
    "# Documentación y DAG pueden generarse en paralelo\n",
    "from langgraph.pregel import Channel\n",
    "\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"dag\", dag_generator_node)\n",
    "\n",
    "# Ambos nodos reciben el mismo state (paralelo)\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"test\", \"dag\")\n",
    "\n",
    "# Sync point: esperar ambos antes de continuar\n",
    "workflow.add_node(\"combine\", lambda s: s)  # No-op combiner\n",
    "workflow.add_edge(\"document\", \"combine\")\n",
    "workflow.add_edge(\"dag\", \"combine\")\n",
    "workflow.add_edge(\"combine\", \"review\")\n",
    "```\n",
    "\n",
    "**Checkpointing (Persistencia de Estado)**\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "\n",
    "# Guardar estado en cada nodo\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Ejecutar con checkpoint\n",
    "config = {\"configurable\": {\"thread_id\": \"pipeline_123\"}}\n",
    "final_state = app.invoke(initial_state, config=config)\n",
    "\n",
    "# Recuperar estado guardado\n",
    "saved_state = checkpointer.get(config)\n",
    "\n",
    "# Reanudar desde checkpoint (útil si falla a mitad)\n",
    "resumed_state = app.invoke(saved_state, config=config)\n",
    "```\n",
    "\n",
    "**Ventajas vs Alternativas**\n",
    "\n",
    "| Característica | LangGraph | Airflow | Prefect | N8n |\n",
    "|----------------|-----------|---------|---------|-----|\n",
    "| **State Management** | ✅ Built-in | ⚠️ XCom (limitado) | ✅ Flow runs | ❌ |\n",
    "| **Conditional Logic** | ✅ Native | ⚠️ BranchOperator | ✅ | ✅ |\n",
    "| **Human-in-Loop** | ✅ Easy | ⚠️ Manual | ✅ Approvals | ✅ |\n",
    "| **LLM Integration** | ✅ Optimizado | ❌ Custom | ⚠️ Manual | ⚠️ |\n",
    "| **Streaming** | ✅ Native | ❌ | ⚠️ Limited | ❌ |\n",
    "| **Python-First** | ✅ | ✅ | ✅ | ❌ (UI-first) |\n",
    "\n",
    "**Caso de Uso: Multi-Agent Code Review**\n",
    "\n",
    "```python\n",
    "# Sistema de review con múltiples agentes especializados\n",
    "\n",
    "class ReviewState(TypedDict):\n",
    "    code: str\n",
    "    security_review: dict\n",
    "    performance_review: dict\n",
    "    style_review: dict\n",
    "    final_approval: bool\n",
    "\n",
    "def security_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en seguridad\"\"\"\n",
    "    state['security_review'] = {\n",
    "        'sql_injection_risk': check_sql_injection(state['code']),\n",
    "        'secrets_exposed': check_secrets(state['code']),\n",
    "        'dangerous_imports': check_imports(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def performance_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en performance\"\"\"\n",
    "    state['performance_review'] = {\n",
    "        'complexity': calculate_complexity(state['code']),\n",
    "        'memory_usage': estimate_memory(state['code']),\n",
    "        'optimization_suggestions': suggest_optimizations(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def style_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en estilo\"\"\"\n",
    "    state['style_review'] = {\n",
    "        'pep8_compliance': check_pep8(state['code']),\n",
    "        'docstring_coverage': check_docstrings(state['code']),\n",
    "        'type_hints': check_type_hints(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "# Ejecutar en paralelo\n",
    "review_workflow = StateGraph(ReviewState)\n",
    "review_workflow.add_node(\"security\", security_agent)\n",
    "review_workflow.add_node(\"performance\", performance_agent)\n",
    "review_workflow.add_node(\"style\", style_agent)\n",
    "\n",
    "# Parallel edges\n",
    "review_workflow.set_entry_point(\"security\")\n",
    "review_workflow.set_entry_point(\"performance\")\n",
    "review_workflow.set_entry_point(\"style\")\n",
    "\n",
    "# Combine results\n",
    "def aggregate_reviews(state: ReviewState) -> ReviewState:\n",
    "    all_passed = (\n",
    "        all(v == 'pass' for v in state['security_review'].values()) and\n",
    "        state['performance_review']['complexity'] < 10 and\n",
    "        state['style_review']['pep8_compliance'] > 0.9\n",
    "    )\n",
    "    \n",
    "    state['final_approval'] = all_passed\n",
    "    return state\n",
    "\n",
    "review_workflow.add_node(\"aggregate\", aggregate_reviews)\n",
    "review_workflow.add_edge(\"security\", \"aggregate\")\n",
    "review_workflow.add_edge(\"performance\", \"aggregate\")\n",
    "review_workflow.add_edge(\"style\", \"aggregate\")\n",
    "review_workflow.add_edge(\"aggregate\", END)\n",
    "\n",
    "multi_agent_review = review_workflow.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73695e2",
   "metadata": {},
   "source": [
    "## Parte 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langgraph streamlit pytest great-expectations\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print('✅ Setup completo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82669c94",
   "metadata": {},
   "source": [
    "## Parte 3: Parser de requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed71996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements(user_input: str) -> dict:\n",
    "    \"\"\"Extrae requisitos estructurados desde lenguaje natural.\"\"\"\n",
    "    prompt = f'''\n",
    "Extrae los requisitos de este pipeline ETL en formato JSON:\n",
    "\n",
    "Input del usuario:\n",
    "{user_input}\n",
    "\n",
    "Devuelve JSON con:\n",
    "{{\n",
    "  \"pipeline_name\": \"nombre_descriptivo\",\n",
    "  \"source\": {{\"type\": \"csv/api/db\", \"details\": \"...\"}},\n",
    "  \"transformations\": [\"lista de transformaciones\"],\n",
    "  \"destination\": {{\"type\": \"csv/db/s3\", \"details\": \"...\"}},\n",
    "  \"schedule\": \"daily/hourly/manual\",\n",
    "  \"validations\": [\"reglas de calidad\"]\n",
    "}}\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Test\n",
    "user_request = '''\n",
    "Necesito un pipeline que:\n",
    "1. Lea ventas.csv de S3 bucket \"raw-data\"\n",
    "2. Filtre solo ventas de los últimos 30 días\n",
    "3. Agregue una columna \"mes\" (YYYY-MM)\n",
    "4. Calcule total por mes y categoría\n",
    "5. Escriba a PostgreSQL tabla \"ventas_mensual\"\n",
    "6. Ejecute diariamente a las 2 AM\n",
    "7. Valide que no haya nulos en \"total\"\n",
    "'''\n",
    "\n",
    "requirements = parse_requirements(user_request)\n",
    "print('Requisitos extraídos:')\n",
    "print(json.dumps(requirements, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c981f2",
   "metadata": {},
   "source": [
    "## Parte 4: Generador de código ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etl_pipeline(requirements: dict) -> str:\n",
    "    \"\"\"Genera código Python completo del pipeline.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un pipeline ETL en Python basado en estos requisitos:\n",
    "\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El código debe:\n",
    "- Usar pandas, boto3 (si S3), sqlalchemy (si DB)\n",
    "- Incluir manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Type hints\n",
    "- Docstrings\n",
    "- Función main() ejecutable\n",
    "\n",
    "Código Python:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "etl_code = generate_etl_pipeline(requirements)\n",
    "print('Código generado (preview):')\n",
    "print(etl_code[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596f94e",
   "metadata": {},
   "source": [
    "## Parte 5: Validador de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba195c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_code(code: str) -> dict:\n",
    "    \"\"\"Valida sintaxis y seguridad.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. Validar sintaxis Python\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        issues.append({'type': 'SYNTAX', 'message': str(e)})\n",
    "    \n",
    "    # 2. Detectar patrones inseguros\n",
    "    dangerous_patterns = ['eval(', 'exec(', 'os.system(', '__import__']\n",
    "    for pattern in dangerous_patterns:\n",
    "        if pattern in code:\n",
    "            issues.append({'type': 'SECURITY', 'message': f'Patrón peligroso detectado: {pattern}'})\n",
    "    \n",
    "    # 3. Verificar imports necesarios\n",
    "    required_imports = ['pandas', 'logging']\n",
    "    for imp in required_imports:\n",
    "        if f'import {imp}' not in code:\n",
    "            issues.append({'type': 'MISSING_IMPORT', 'message': f'Falta import: {imp}'})\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "validation = validate_code(etl_code)\n",
    "print(f\"\\nValidación: {'✅ Aprobado' if validation['valid'] else '❌ Con errores'}\")\n",
    "if validation['issues']:\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"- [{issue['type']}] {issue['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b44e",
   "metadata": {},
   "source": [
    "## Parte 6: Generador de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    \"\"\"Genera tests unitarios con pytest.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera tests unitarios con pytest para este código ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "Código:\n",
    "{code[:1000]}...\n",
    "\n",
    "Genera tests para:\n",
    "1. Lectura de datos (mock de fuente)\n",
    "2. Transformaciones\n",
    "3. Validaciones de calidad\n",
    "4. Escritura (mock de destino)\n",
    "5. Manejo de errores\n",
    "\n",
    "Código de tests (pytest):\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "test_code = generate_tests(etl_code, requirements)\n",
    "print('Tests generados (preview):')\n",
    "print(test_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ae2b",
   "metadata": {},
   "source": [
    "## Parte 7: Generador de documentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88df093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(requirements: dict, code: str) -> str:\n",
    "    \"\"\"Genera README.md completo.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un README.md completo para este pipeline ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "Incluye:\n",
    "- Descripción general\n",
    "- Arquitectura (diagrama ASCII)\n",
    "- Requisitos (dependencias)\n",
    "- Configuración\n",
    "- Cómo ejecutar\n",
    "- Monitoreo y troubleshooting\n",
    "- Contacto/owner\n",
    "\n",
    "Markdown:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "documentation = generate_documentation(requirements, etl_code)\n",
    "print('Documentación generada (preview):')\n",
    "print(documentation[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89813a88",
   "metadata": {},
   "source": [
    "## Parte 8: DAG de Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f80052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_airflow_dag(requirements: dict, etl_code: str) -> str:\n",
    "    \"\"\"Genera DAG de Airflow.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un DAG de Airflow para este pipeline:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El DAG debe:\n",
    "- Nombre descriptivo\n",
    "- Schedule según requisitos\n",
    "- Tasks: extract, validate, transform, load\n",
    "- Retry logic\n",
    "- Alertas por email si falla\n",
    "\n",
    "Código Python del DAG:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "dag_code = generate_airflow_dag(requirements, etl_code)\n",
    "print('DAG de Airflow (preview):')\n",
    "print(dag_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ea627",
   "metadata": {},
   "source": [
    "## Parte 9: Workflow con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    user_input: str\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    state['requirements'] = parse_requirements(state['user_input'])\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    state['validation'] = validate_code(state['code'])\n",
    "    if not state['validation']['valid']:\n",
    "        state['errors'].extend([i['message'] for i in state['validation']['issues']])\n",
    "    return state\n",
    "\n",
    "def test_node(state: PipelineState) -> PipelineState:\n",
    "    if state['validation']['valid']:\n",
    "        state['tests'] = generate_tests(state['code'], state['requirements'])\n",
    "    return state\n",
    "\n",
    "def document_node(state: PipelineState) -> PipelineState:\n",
    "    state['docs'] = generate_documentation(state['requirements'], state['code'])\n",
    "    state['dag'] = generate_airflow_dag(state['requirements'], state['code'])\n",
    "    return state\n",
    "\n",
    "def review_node(state: PipelineState) -> PipelineState:\n",
    "    # En producción: human-in-the-loop\n",
    "    state['approved'] = state['validation']['valid']\n",
    "    return state\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "workflow.add_node('parse', parse_node)\n",
    "workflow.add_node('generate', generate_node)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('test', test_node)\n",
    "workflow.add_node('document', document_node)\n",
    "workflow.add_node('review', review_node)\n",
    "\n",
    "workflow.set_entry_point('parse')\n",
    "workflow.add_edge('parse', 'generate')\n",
    "workflow.add_edge('generate', 'validate')\n",
    "workflow.add_edge('validate', 'test')\n",
    "workflow.add_edge('test', 'document')\n",
    "workflow.add_edge('document', 'review')\n",
    "workflow.add_edge('review', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar workflow completo\n",
    "initial_state = {\n",
    "    'user_input': user_request,\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print('\\n🎉 Pipeline generado completamente\\n')\n",
    "print(f\"Aprobado: {'✅' if final_state['approved'] else '❌'}\")\n",
    "print(f\"Errores: {len(final_state['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4610a91",
   "metadata": {},
   "source": [
    "## Parte 10: Interfaz Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como platform_app.py\n",
    "\n",
    "streamlit_app = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from pipeline_generator import app, PipelineState  # Importar workflow\n",
    "\n",
    "st.set_page_config(page_title='GenAI Data Platform', page_icon='🏗️', layout='wide')\n",
    "\n",
    "st.title('🏗️ Plataforma Self-Service de Pipelines')\n",
    "st.markdown('Genera pipelines ETL completos usando lenguaje natural')\n",
    "\n",
    "# Input\n",
    "user_input = st.text_area(\n",
    "    'Describe tu pipeline:',\n",
    "    height=200,\n",
    "    placeholder='Ej: Necesito procesar datos de ventas desde S3, agregar por mes, y cargar a Redshift...'\n",
    ")\n",
    "\n",
    "if st.button('🚀 Generar Pipeline', type='primary'):\n",
    "    if not user_input:\n",
    "        st.warning('Por favor describe el pipeline')\n",
    "    else:\n",
    "        with st.spinner('Generando pipeline completo...'):\n",
    "            initial = {\n",
    "                'user_input': user_input,\n",
    "                'requirements': {}, 'code': '', 'validation': {},\n",
    "                'tests': '', 'docs': '', 'dag': '',\n",
    "                'approved': False, 'errors': []\n",
    "            }\n",
    "            \n",
    "            result = app.invoke(initial)\n",
    "            \n",
    "            # Tabs\n",
    "            tab1, tab2, tab3, tab4, tab5 = st.tabs(['📋 Requisitos', '💻 Código', '🧪 Tests', '📄 Docs', '🛫 DAG'])\n",
    "            \n",
    "            with tab1:\n",
    "                st.json(result['requirements'])\n",
    "            \n",
    "            with tab2:\n",
    "                st.code(result['code'], language='python')\n",
    "                st.download_button('Descargar código', result['code'], file_name='pipeline.py')\n",
    "            \n",
    "            with tab3:\n",
    "                st.code(result['tests'], language='python')\n",
    "            \n",
    "            with tab4:\n",
    "                st.markdown(result['docs'])\n",
    "            \n",
    "            with tab5:\n",
    "                st.code(result['dag'], language='python')\n",
    "            \n",
    "            # Validación\n",
    "            if result['approved']:\n",
    "                st.success('✅ Pipeline aprobado y listo para deployment')\n",
    "            else:\n",
    "                st.error(f\"❌ Pipeline con errores: {result['errors']}\")\n",
    "'''\n",
    "\n",
    "with open('platform_app.py', 'w') as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print('✅ Plataforma Streamlit guardada en platform_app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4251",
   "metadata": {},
   "source": [
    "## Parte 11: Sistema de deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def deploy_pipeline(code: str, dag: str, tests: str, pipeline_name: str):\n",
    "    \"\"\"Despliega pipeline a producción.\"\"\"\n",
    "    # 1. Crear estructura de directorios\n",
    "    base_path = f'./pipelines/{pipeline_name}'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # 2. Guardar archivos\n",
    "    with open(f'{base_path}/pipeline.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    with open(f'{base_path}/test_pipeline.py', 'w') as f:\n",
    "        f.write(tests)\n",
    "    \n",
    "    with open(f'{base_path}/dag.py', 'w') as f:\n",
    "        f.write(dag)\n",
    "    \n",
    "    # 3. Ejecutar tests\n",
    "    test_result = subprocess.run(\n",
    "        ['pytest', f'{base_path}/test_pipeline.py', '-v'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if test_result.returncode != 0:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': 'Tests fallidos',\n",
    "            'output': test_result.stdout\n",
    "        }\n",
    "    \n",
    "    # 4. Copiar DAG a Airflow\n",
    "    # airflow_dags_path = '/opt/airflow/dags/'\n",
    "    # shutil.copy(f'{base_path}/dag.py', f'{airflow_dags_path}/{pipeline_name}.py')\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'message': f'Pipeline {pipeline_name} desplegado exitosamente',\n",
    "        'path': base_path\n",
    "    }\n",
    "\n",
    "# Deployment\n",
    "if final_state['approved']:\n",
    "    deploy_result = deploy_pipeline(\n",
    "        code=final_state['code'],\n",
    "        dag=final_state['dag'],\n",
    "        tests=final_state['tests'],\n",
    "        pipeline_name=final_state['requirements']['pipeline_name']\n",
    "    )\n",
    "    print(deploy_result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8ff73",
   "metadata": {},
   "source": [
    "### 🚀 **Deployment y Producción: De Prototipo a Enterprise**\n",
    "\n",
    "**CI/CD Pipeline para GenAI Artifacts**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│               PRODUCTION DEPLOYMENT PIPELINE               │\n",
    "├────────────────────────────────────────────────────────────┤\n",
    "│                                                            │\n",
    "│  [1. Code Generation] → Generated pipeline code           │\n",
    "│           ↓                                                │\n",
    "│  [2. Version Control]                                      │\n",
    "│      • Create feature branch                              │\n",
    "│      • Commit: code + tests + docs                        │\n",
    "│      • Tag: v1.0.0                                        │\n",
    "│           ↓                                                │\n",
    "│  [3. Automated Testing] (GitHub Actions/GitLab CI)        │\n",
    "│      • pytest: unit + integration tests                   │\n",
    "│      • coverage: ≥80% required                            │\n",
    "│      • security: Bandit, Safety                           │\n",
    "│      • linting: Black, Flake8, MyPy                       │\n",
    "│           ↓                                                │\n",
    "│  [4. Quality Gates]                                        │\n",
    "│      ✅ All tests pass                                     │\n",
    "│      ✅ Coverage ≥80%                                      │\n",
    "│      ✅ No critical security issues                       │\n",
    "│      ✅ Code review approved                              │\n",
    "│           ↓                                                │\n",
    "│  [5. Staging Deployment]                                   │\n",
    "│      • Deploy to staging environment                      │\n",
    "│      • Run integration tests with staging data            │\n",
    "│      • Performance benchmarking                           │\n",
    "│           ↓                                                │\n",
    "│  [6. Production Approval]                                  │\n",
    "│      • Manual sign-off (for critical pipelines)          │\n",
    "│      • Automated (for low-risk changes)                   │\n",
    "│           ↓                                                │\n",
    "│  [7. Production Deployment]                                │\n",
    "│      • Blue/Green deployment (zero downtime)              │\n",
    "│      • Copy DAG to Airflow dags/ folder                   │\n",
    "│      • Update Airflow variables                           │\n",
    "│      • Trigger initial run                                │\n",
    "│           ↓                                                │\n",
    "│  [8. Monitoring]                                           │\n",
    "│      • Datadog/New Relic APM                              │\n",
    "│      • Custom metrics (rows processed, runtime)           │\n",
    "│      • Alerting (PagerDuty, Slack)                        │\n",
    "│                                                            │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**GitHub Actions Workflow**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/deploy-pipeline.yml\n",
    "name: Deploy Generated Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: '3.11'\n",
    "  COVERAGE_THRESHOLD: 80\n",
    "\n",
    "jobs:\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov bandit safety black flake8 mypy\n",
    "      \n",
    "      - name: Security scan\n",
    "        run: |\n",
    "          # Scan dependencies\n",
    "          safety check --json\n",
    "          \n",
    "          # Scan code for vulnerabilities\n",
    "          bandit -r generated_pipelines/ -f json -o bandit-report.json\n",
    "      \n",
    "      - name: Code quality\n",
    "        run: |\n",
    "          # Format check\n",
    "          black --check generated_pipelines/\n",
    "          \n",
    "          # Linting\n",
    "          flake8 generated_pipelines/ --max-line-length=100\n",
    "          \n",
    "          # Type checking\n",
    "          mypy generated_pipelines/ --strict\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          pytest generated_pipelines/ \\\n",
    "            --cov=generated_pipelines \\\n",
    "            --cov-report=xml \\\n",
    "            --cov-report=html \\\n",
    "            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }}\n",
    "      \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "  \n",
    "  deploy-staging:\n",
    "    needs: validate\n",
    "    if: github.event_name == 'pull_request'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Deploy to staging Airflow\n",
    "        env:\n",
    "          AIRFLOW_STAGING_HOST: ${{ secrets.AIRFLOW_STAGING_HOST }}\n",
    "          AIRFLOW_API_KEY: ${{ secrets.AIRFLOW_API_KEY }}\n",
    "        run: |\n",
    "          # Copy DAG to staging\n",
    "          scp generated_pipelines/*/dag.py \\\n",
    "            user@$AIRFLOW_STAGING_HOST:/opt/airflow/dags/\n",
    "          \n",
    "          # Trigger DAG\n",
    "          curl -X POST \\\n",
    "            -H \"Authorization: Bearer $AIRFLOW_API_KEY\" \\\n",
    "            https://$AIRFLOW_STAGING_HOST/api/v1/dags/pipeline_name/dagRuns\n",
    "      \n",
    "      - name: Integration tests (staging)\n",
    "        run: |\n",
    "          pytest tests/integration/ \\\n",
    "            --env=staging \\\n",
    "            --pipeline=pipeline_name\n",
    "  \n",
    "  deploy-production:\n",
    "    needs: validate\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    environment:\n",
    "      name: production\n",
    "      url: https://airflow.prod.company.com\n",
    "    steps:\n",
    "      - name: Blue/Green deployment\n",
    "        run: |\n",
    "          # Deploy to \"green\" environment first\n",
    "          ./scripts/deploy.sh --env=prod-green --pipeline=$PIPELINE_NAME\n",
    "          \n",
    "          # Run smoke tests\n",
    "          ./scripts/smoke-tests.sh --env=prod-green\n",
    "          \n",
    "          # Switch traffic to green (zero downtime)\n",
    "          ./scripts/switch-traffic.sh --to=green\n",
    "          \n",
    "          # Decommission blue\n",
    "          ./scripts/cleanup.sh --env=prod-blue\n",
    "      \n",
    "      - name: Notify\n",
    "        uses: 8398a7/action-slack@v3\n",
    "        with:\n",
    "          status: ${{ job.status }}\n",
    "          text: 'Pipeline deployed to production'\n",
    "          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "```\n",
    "\n",
    "**Deployment Automation Script**\n",
    "\n",
    "```python\n",
    "# scripts/deploy_pipeline.py\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "class PipelineDeployer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.airflow_api = config['airflow_api_url']\n",
    "        self.airflow_token = config['airflow_token']\n",
    "    \n",
    "    def deploy(self, pipeline_path: str, environment: str = 'production'):\n",
    "        \"\"\"Despliega pipeline a Airflow\"\"\"\n",
    "        \n",
    "        # 1. Validar que existan archivos necesarios\n",
    "        required_files = ['pipeline.py', 'dag.py', 'tests.py', 'README.md']\n",
    "        for file in required_files:\n",
    "            if not Path(f\"{pipeline_path}/{file}\").exists():\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "        \n",
    "        # 2. Ejecutar tests\n",
    "        print(\"Running tests...\")\n",
    "        test_result = subprocess.run(\n",
    "            ['pytest', f'{pipeline_path}/tests.py', '-v'],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if test_result.returncode != 0:\n",
    "            raise RuntimeError(f\"Tests failed:\\n{test_result.stdout}\")\n",
    "        \n",
    "        print(\"✅ Tests passed\")\n",
    "        \n",
    "        # 3. Upload assets to S3\n",
    "        print(\"Uploading to S3...\")\n",
    "        pipeline_name = Path(pipeline_path).name\n",
    "        \n",
    "        for file in ['pipeline.py', 'README.md']:\n",
    "            self.s3.upload_file(\n",
    "                f'{pipeline_path}/{file}',\n",
    "                self.config['artifacts_bucket'],\n",
    "                f'pipelines/{pipeline_name}/{file}'\n",
    "            )\n",
    "        \n",
    "        # 4. Deploy DAG to Airflow\n",
    "        print(f\"Deploying to Airflow ({environment})...\")\n",
    "        \n",
    "        # Airflow API: upload DAG\n",
    "        with open(f'{pipeline_path}/dag.py', 'r') as f:\n",
    "            dag_code = f.read()\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}/upload\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'dag_code': dag_code}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Airflow deployment failed: {response.text}\")\n",
    "        \n",
    "        # 5. Set Airflow variables\n",
    "        variables = {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            's3_bucket': self.config['data_bucket'],\n",
    "            'db_conn_id': 'postgres_default',\n",
    "            'owner_email': self.config['owner_email']\n",
    "        }\n",
    "        \n",
    "        for key, value in variables.items():\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/variables\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'key': f'{pipeline_name}_{key}', 'value': value}\n",
    "            )\n",
    "        \n",
    "        # 6. Unpause DAG\n",
    "        requests.patch(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'is_paused': False}\n",
    "        )\n",
    "        \n",
    "        # 7. Trigger initial run (optional)\n",
    "        if self.config.get('trigger_on_deploy'):\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/dags/{pipeline_name}/dagRuns\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'conf': {'deployed_at': str(datetime.now())}}\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ Pipeline {pipeline_name} deployed successfully\")\n",
    "        \n",
    "        return {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            'environment': environment,\n",
    "            'dag_url': f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            's3_path': f's3://{self.config[\"artifacts_bucket\"]}/pipelines/{pipeline_name}'\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "deployer = PipelineDeployer({\n",
    "    'airflow_api_url': 'https://airflow.company.com/api/v1',\n",
    "    'airflow_token': os.getenv('AIRFLOW_TOKEN'),\n",
    "    'artifacts_bucket': 'company-pipeline-artifacts',\n",
    "    'data_bucket': 'company-data',\n",
    "    'owner_email': 'data-team@company.com',\n",
    "    'trigger_on_deploy': True\n",
    "})\n",
    "\n",
    "result = deployer.deploy('./generated_pipelines/ventas_pipeline', environment='production')\n",
    "```\n",
    "\n",
    "**Monitoring y Observabilidad**\n",
    "\n",
    "```python\n",
    "# monitoring/pipeline_metrics.py\n",
    "from datadog import initialize, statsd\n",
    "import time\n",
    "\n",
    "initialize(api_key=os.getenv('DATADOG_API_KEY'))\n",
    "\n",
    "class PipelineMetrics:\n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "    \n",
    "    def track_execution(self, func):\n",
    "        \"\"\"Decorator para trackear métricas de ejecución\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Incrementar contador de ejecuciones\n",
    "            statsd.increment(\n",
    "                'pipeline.executions',\n",
    "                tags=[f'pipeline:{self.pipeline_name}']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # Métrica de éxito\n",
    "                statsd.increment(\n",
    "                    'pipeline.success',\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Métrica de error\n",
    "                statsd.increment(\n",
    "                    'pipeline.errors',\n",
    "                    tags=[\n",
    "                        f'pipeline:{self.pipeline_name}',\n",
    "                        f'error_type:{type(e).__name__}'\n",
    "                    ]\n",
    "                )\n",
    "                raise\n",
    "            \n",
    "            finally:\n",
    "                # Duración\n",
    "                duration = time.time() - start_time\n",
    "                statsd.histogram(\n",
    "                    'pipeline.duration',\n",
    "                    duration,\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "        \n",
    "        return wrapper\n",
    "    \n",
    "    def track_data_volume(self, rows: int, stage: str):\n",
    "        \"\"\"Track volumen de datos procesados\"\"\"\n",
    "        statsd.gauge(\n",
    "            'pipeline.data_volume',\n",
    "            rows,\n",
    "            tags=[\n",
    "                f'pipeline:{self.pipeline_name}',\n",
    "                f'stage:{stage}'\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def track_data_quality(self, metric: str, value: float):\n",
    "        \"\"\"Track métricas de calidad de datos\"\"\"\n",
    "        statsd.gauge(\n",
    "            f'pipeline.data_quality.{metric}',\n",
    "            value,\n",
    "            tags=[f'pipeline:{self.pipeline_name}']\n",
    "        )\n",
    "\n",
    "# Usage en pipeline generado\n",
    "metrics = PipelineMetrics('ventas_pipeline')\n",
    "\n",
    "@metrics.track_execution\n",
    "def run_pipeline():\n",
    "    # Extract\n",
    "    df = extract_data()\n",
    "    metrics.track_data_volume(len(df), 'extract')\n",
    "    \n",
    "    # Transform\n",
    "    df = transform_data(df)\n",
    "    metrics.track_data_volume(len(df), 'transform')\n",
    "    \n",
    "    # Validate\n",
    "    null_rate = df.isnull().sum().sum() / df.size\n",
    "    metrics.track_data_quality('null_rate', null_rate)\n",
    "    \n",
    "    # Load\n",
    "    load_data(df)\n",
    "    metrics.track_data_volume(len(df), 'load')\n",
    "```\n",
    "\n",
    "**Alerting Strategy**\n",
    "\n",
    "```python\n",
    "# monitoring/alerts.py\n",
    "from slack_sdk import WebClient\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "class AlertManager:\n",
    "    def __init__(self):\n",
    "        self.slack = WebClient(token=os.getenv('SLACK_TOKEN'))\n",
    "        self.pagerduty_key = os.getenv('PAGERDUTY_KEY')\n",
    "    \n",
    "    def send_alert(\n",
    "        self,\n",
    "        severity: str,  # 'info' | 'warning' | 'critical'\n",
    "        pipeline_name: str,\n",
    "        message: str,\n",
    "        details: dict = None\n",
    "    ):\n",
    "        \"\"\"Envía alertas según severidad\"\"\"\n",
    "        \n",
    "        if severity == 'info':\n",
    "            # Slack notification\n",
    "            self._send_slack(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'warning':\n",
    "            # Slack + Email\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'critical':\n",
    "            # Slack + Email + PagerDuty\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "            self._trigger_pagerduty(pipeline_name, message, details)\n",
    "    \n",
    "    def _send_slack(self, pipeline, message, details, urgent=False):\n",
    "        emoji = '🚨' if urgent else 'ℹ️'\n",
    "        \n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\"type\": \"plain_text\", \"text\": f\"{emoji} {pipeline}\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\"type\": \"mrkdwn\", \"text\": message}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        if details:\n",
    "            blocks.append({\n",
    "                \"type\": \"section\",\n",
    "                \"fields\": [\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*{k}:*\\n{v}\"}\n",
    "                    for k, v in details.items()\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        self.slack.chat_postMessage(\n",
    "            channel='#data-alerts',\n",
    "            blocks=blocks\n",
    "        )\n",
    "    \n",
    "    def _trigger_pagerduty(self, pipeline, message, details):\n",
    "        import requests\n",
    "        \n",
    "        requests.post(\n",
    "            'https://events.pagerduty.com/v2/enqueue',\n",
    "            json={\n",
    "                'routing_key': self.pagerduty_key,\n",
    "                'event_action': 'trigger',\n",
    "                'payload': {\n",
    "                    'summary': f'{pipeline}: {message}',\n",
    "                    'severity': 'critical',\n",
    "                    'source': 'genai-platform',\n",
    "                    'custom_details': details\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Integración en pipeline\n",
    "alerts = AlertManager()\n",
    "\n",
    "try:\n",
    "    run_pipeline()\n",
    "except Exception as e:\n",
    "    alerts.send_alert(\n",
    "        severity='critical',\n",
    "        pipeline_name='ventas_pipeline',\n",
    "        message=f'Pipeline failed: {str(e)}',\n",
    "        details={\n",
    "            'Error Type': type(e).__name__,\n",
    "            'Traceback': traceback.format_exc()[:500]\n",
    "        }\n",
    "    )\n",
    "    raise\n",
    "```\n",
    "\n",
    "**Cost Optimization**\n",
    "\n",
    "```python\n",
    "# monitoring/cost_tracking.py\n",
    "class CostTracker:\n",
    "    \"\"\"Track costos de ejecución de pipelines\"\"\"\n",
    "    \n",
    "    COST_PER_COMPUTE_HOUR = 0.50  # USD\n",
    "    COST_PER_GB_STORAGE = 0.023   # USD/mes\n",
    "    COST_PER_MILLION_REQUESTS = 0.20  # USD\n",
    "    \n",
    "    def calculate_pipeline_cost(\n",
    "        self,\n",
    "        runtime_minutes: float,\n",
    "        data_volume_gb: float,\n",
    "        api_calls: int\n",
    "    ) -> dict:\n",
    "        \n",
    "        compute_cost = (runtime_minutes / 60) * self.COST_PER_COMPUTE_HOUR\n",
    "        storage_cost = data_volume_gb * self.COST_PER_GB_STORAGE\n",
    "        api_cost = (api_calls / 1_000_000) * self.COST_PER_MILLION_REQUESTS\n",
    "        \n",
    "        total = compute_cost + storage_cost + api_cost\n",
    "        \n",
    "        return {\n",
    "            'compute': round(compute_cost, 4),\n",
    "            'storage': round(storage_cost, 4),\n",
    "            'api': round(api_cost, 4),\n",
    "            'total': round(total, 4)\n",
    "        }\n",
    "    \n",
    "    def optimize_suggestions(self, metrics: dict) -> list:\n",
    "        \"\"\"Sugerencias de optimización\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if metrics['runtime_minutes'] > 60:\n",
    "            suggestions.append(\"Consider partitioning data for parallel processing\")\n",
    "        \n",
    "        if metrics['data_volume_gb'] > 100:\n",
    "            suggestions.append(\"Use columnar format (Parquet) to reduce storage\")\n",
    "        \n",
    "        if metrics['api_calls'] > 1_000_000:\n",
    "            suggestions.append(\"Implement caching to reduce API calls\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# En dashboard Streamlit\n",
    "cost_tracker = CostTracker()\n",
    "\n",
    "st.subheader('💰 Cost Analysis')\n",
    "costs = cost_tracker.calculate_pipeline_cost(\n",
    "    runtime_minutes=15,\n",
    "    data_volume_gb=50,\n",
    "    api_calls=100_000\n",
    ")\n",
    "\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Compute', f\"${costs['compute']}\")\n",
    "col2.metric('Storage', f\"${costs['storage']}\")\n",
    "col3.metric('API', f\"${costs['api']}\")\n",
    "col4.metric('Total', f\"${costs['total']}\", delta='-12%')\n",
    "\n",
    "suggestions = cost_tracker.optimize_suggestions(metrics)\n",
    "if suggestions:\n",
    "    st.info('\\n'.join(suggestions))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cf2e",
   "metadata": {},
   "source": [
    "## Parte 12: Monitoreo y observabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_dashboard(pipeline_name: str):\n",
    "    \"\"\"Genera dashboard de monitoreo.\"\"\"\n",
    "    dashboard_code = f'''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "st.title('📊 Monitoreo: {pipeline_name}')\n",
    "\n",
    "# Métricas simuladas\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Ejecuciones exitosas', '95%', '↑ 2%')\n",
    "col2.metric('Tiempo promedio', '12.5 min', '↓ 1.2 min')\n",
    "col3.metric('Registros procesados', '1.2M', '↑ 15K')\n",
    "col4.metric('Errores', '3', '↓ 5')\n",
    "\n",
    "# Gráfico de ejecuciones\n",
    "df_runs = pd.DataFrame({{\n",
    "    'fecha': pd.date_range('2024-01-01', periods=30),\n",
    "    'duracion': [10 + i*0.2 for i in range(30)],\n",
    "    'status': ['success'] * 28 + ['failed'] * 2\n",
    "}}})\n",
    "\n",
    "fig = px.line(df_runs, x='fecha', y='duracion', color='status')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "# Logs recientes\n",
    "st.subheader('Logs recientes')\n",
    "st.text_area('', value='2024-01-15 02:00 - [INFO] Pipeline iniciado\\\\n2024-01-15 02:05 - [INFO] 10000 registros procesados', height=200)\n",
    "'''\n",
    "    \n",
    "    return dashboard_code\n",
    "\n",
    "monitoring_dash = generate_monitoring_dashboard(requirements['pipeline_name'])\n",
    "print('Dashboard de monitoreo generado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b578ed",
   "metadata": {},
   "source": [
    "## Parte 13: Mejoras futuras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179193c",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "**Fase 2**:\n",
    "- Multi-agente: equipo de agentes especializados (data engineer, QA, DevOps)\n",
    "- Cost estimation: predecir costos de infraestructura\n",
    "- A/B testing: comparar versiones de pipelines\n",
    "- Auto-scaling: ajustar recursos según carga\n",
    "\n",
    "**Fase 3**:\n",
    "- Self-healing: detección y corrección automática de errores\n",
    "- Optimization: sugerencias de mejora basadas en métricas\n",
    "- Federated learning: aprender de pipelines de otros equipos\n",
    "- Natural language alerting: alertas explicadas en lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dba29",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27d809",
   "metadata": {},
   "source": [
    "**Criterios**:\n",
    "\n",
    "- ✅ Parsing correcto de requisitos (15%)\n",
    "- ✅ Generación de código funcional (25%)\n",
    "- ✅ Validación y seguridad (15%)\n",
    "- ✅ Tests completos (15%)\n",
    "- ✅ Documentación clara (10%)\n",
    "- ✅ Workflow orquestado (10%)\n",
    "- ✅ Interfaz usable (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d050fa4",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0905e",
   "metadata": {},
   "source": [
    "Has construido una plataforma completa de self-service que:\n",
    "\n",
    "✅ Democratiza el acceso a la ingeniería de datos\n",
    "✅ Reduce tiempo de desarrollo de días a minutos\n",
    "✅ Mantiene estándares de calidad y seguridad\n",
    "✅ Genera documentación automáticamente\n",
    "✅ Facilita deployment y monitoreo\n",
    "\n",
    "**¡Felicitaciones por completar el módulo de GenAI!** 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d22e7",
   "metadata": {},
   "source": [
    "### 🎓 **Conclusión del Curso: De Junior a GenAI Expert**\n",
    "\n",
    "**El Viaje Completo: 40 Notebooks, 4 Niveles**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│           DATA ENGINEER COURSE: LEARNING PATH                │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  📚 NIVEL JUNIOR (10 notebooks)                              │\n",
    "│  ├─ Fundamentos: Python, Pandas, SQL                        │\n",
    "│  ├─ Data Manipulation: ETL básico                           │\n",
    "│  ├─ Visualización: Matplotlib, Seaborn                      │\n",
    "│  └─ Git: Control de versiones                               │\n",
    "│      ↓                                                       │\n",
    "│  🔧 NIVEL MID (10 notebooks)                                 │\n",
    "│  ├─ Airflow: Orquestación de pipelines                      │\n",
    "│  ├─ Streaming: Kafka, real-time processing                  │\n",
    "│  ├─ Cloud: AWS (S3, Redshift, Lambda)                       │\n",
    "│  ├─ Bases de Datos: PostgreSQL, MongoDB                     │\n",
    "│  └─ FastAPI: Servicios de datos                             │\n",
    "│      ↓                                                       │\n",
    "│  ⚡ NIVEL SENIOR (10 notebooks)                              │\n",
    "│  ├─ Data Governance: Calidad, linaje                        │\n",
    "│  ├─ Lakehouse: Delta, Iceberg                               │\n",
    "│  ├─ Spark Streaming: Processing distribuido                 │\n",
    "│  ├─ Arquitecturas: Medallion, Lambda, Kappa                 │\n",
    "│  ├─ ML Pipelines: Feature stores, MLOps                     │\n",
    "│  └─ FinOps: Optimización de costos                          │\n",
    "│      ↓                                                       │\n",
    "│  🤖 NIVEL GENAI (10 notebooks)                               │\n",
    "│  ├─ 01: Fundamentos LLMs & Prompting                        │\n",
    "│  ├─ 02: NL2SQL (Natural Language → SQL)                     │\n",
    "│  ├─ 03: Generación de Código ETL                            │\n",
    "│  ├─ 04: RAG para Documentación de Datos                     │\n",
    "│  ├─ 05: Embeddings & Similitud de Datos                     │\n",
    "│  ├─ 06: Agentes de Automatización                           │\n",
    "│  ├─ 07: Calidad y Validación con LLMs                       │\n",
    "│  ├─ 08: Síntesis y Aumento de Datos                         │\n",
    "│  ├─ 09: Proyecto Integrador 1 (RAG+NL2SQL Chatbot)          │\n",
    "│  └─ 10: Proyecto Integrador 2 (Plataforma Self-Service) ✅  │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Habilidades Adquiridas por Nivel**\n",
    "\n",
    "| Nivel | Skills Core | Tools Dominadas | Capacidad Laboral |\n",
    "|-------|-------------|-----------------|-------------------|\n",
    "| **Junior** | • Python (Pandas, NumPy)<br>• SQL básico<br>• ETL manual<br>• Git | Jupyter, Pandas, PostgreSQL, Git | Entry-level Data Engineer<br>Data Analyst |\n",
    "| **Mid** | • Airflow DAGs<br>• Streaming (Kafka)<br>• Cloud (AWS)<br>• APIs (FastAPI) | Airflow, Kafka, AWS, Docker, FastAPI, MongoDB | Mid-level Data Engineer<br>Data Platform Engineer |\n",
    "| **Senior** | • Data Architecture<br>• Distributed Systems (Spark)<br>• Lakehouse (Delta/Iceberg)<br>• MLOps | Spark, Delta Lake, Databricks, Kubernetes, Terraform | Senior Data Engineer<br>Data Architect<br>Staff Engineer |\n",
    "| **GenAI** | • LLM Integration<br>• RAG Systems<br>• Agentic Workflows<br>• Code Generation | OpenAI API, LangChain, ChromaDB, LangGraph, Streamlit | GenAI Data Engineer<br>ML Platform Engineer<br>AI Automation Specialist |\n",
    "\n",
    "**Comparación: Skills Tradicionales vs GenAI-Enhanced**\n",
    "\n",
    "```\n",
    "TRADITIONAL DATA ENGINEER              GENAI-ENHANCED DATA ENGINEER\n",
    "┌──────────────────────────┐          ┌──────────────────────────────┐\n",
    "│                          │          │                              │\n",
    "│ • Manual coding          │   →      │ • AI-assisted coding         │\n",
    "│ • Static pipelines       │   →      │ • Self-adapting pipelines    │\n",
    "│ • Manual documentation   │   →      │ • Auto-generated docs        │\n",
    "│ • Fixed schemas          │   →      │ • Schema inference           │\n",
    "│ • SQL only               │   →      │ • NL→SQL translation         │\n",
    "│ • Manual data quality    │   →      │ • LLM-powered validation     │\n",
    "│ • Code reviews (human)   │   →      │ • Multi-agent reviews        │\n",
    "│ • Static dashboards      │   →      │ • Conversational analytics   │\n",
    "│                          │          │                              │\n",
    "└──────────────────────────┘          └──────────────────────────────┘\n",
    "\n",
    "   Productivity: 1x                       Productivity: 3-5x\n",
    "   Time to Market: weeks                  Time to Market: hours\n",
    "   Error Rate: 5-10%                      Error Rate: 1-2%\n",
    "```\n",
    "\n",
    "**Proyecto Final: Impacto Real**\n",
    "\n",
    "Este proyecto integrador 2 (Plataforma Self-Service) representa el **máximo nivel de automatización** en Data Engineering:\n",
    "\n",
    "**Sin GenAI (Tradicional)**:\n",
    "```\n",
    "Usuario → escribe ticket → DE recibe → DE desarrolla (2-4 semanas) \n",
    "→ QA testing (1 semana) → Code review → Deploy → Usuario recibe\n",
    "```\n",
    "**Con GenAI (Plataforma)**:\n",
    "```\n",
    "Usuario → describe en NL → Plataforma genera (10 min) → Aprobación \n",
    "→ Deploy automático → Usuario recibe\n",
    "```\n",
    "\n",
    "**Métricas de Transformación**:\n",
    "- ⏱️ **Time to Production**: 3-4 semanas → 2 horas (96% reducción)\n",
    "- 💰 **Costo por Pipeline**: $5,100 → $163 (97% reducción)\n",
    "- 👥 **Democratización**: Solo DEs → Cualquier usuario\n",
    "- 📊 **Calidad**: Variable → Estandarizada (templates + validación)\n",
    "- 🧪 **Test Coverage**: ~40% → >80%\n",
    "\n",
    "**Próximos Pasos: Continuar Aprendiendo**\n",
    "\n",
    "**1. Profundizar en GenAI**\n",
    "```python\n",
    "# Áreas avanzadas\n",
    "topics = [\n",
    "    'Fine-tuning de LLMs para dominio específico',\n",
    "    'Prompt engineering avanzado (Chain-of-Thought, ReAct)',\n",
    "    'Multi-modal AI (texto + imágenes + tablas)',\n",
    "    'Autonomous agents (Auto-GPT, BabyAGI)',\n",
    "    'LLM evaluation & benchmarking',\n",
    "    'Cost optimization (caching, model selection)'\n",
    "]\n",
    "```\n",
    "\n",
    "**2. Especializaciones**\n",
    "- **ML Engineering**: Feature stores (Feast, Tecton), model serving (MLflow)\n",
    "- **Real-time Analytics**: Flink, Kafka Streams, ksqlDB\n",
    "- **Data Mesh**: Domain-oriented data platforms\n",
    "- **Observability**: OpenTelemetry, distributed tracing\n",
    "\n",
    "**3. Certificaciones Recomendadas**\n",
    "```\n",
    "Cloud:\n",
    "├─ AWS Certified Data Analytics - Specialty\n",
    "├─ Google Professional Data Engineer\n",
    "└─ Azure Data Engineer Associate\n",
    "\n",
    "Data:\n",
    "├─ Databricks Certified Data Engineer\n",
    "├─ Snowflake SnowPro Core\n",
    "└─ dbt Analytics Engineering\n",
    "\n",
    "AI/ML:\n",
    "├─ TensorFlow Developer Certificate\n",
    "├─ MLOps Specialization (Coursera)\n",
    "└─ Prompt Engineering for Developers (DeepLearning.AI)\n",
    "```\n",
    "\n",
    "**4. Construir Portfolio**\n",
    "```\n",
    "GitHub Portfolio Projects:\n",
    "├─ 1. End-to-end ETL pipeline (batch + streaming)\n",
    "│     Tech: Airflow, Spark, Delta Lake\n",
    "│\n",
    "├─ 2. RAG system para dominio específico\n",
    "│     Tech: OpenAI, ChromaDB, LangChain, FastAPI\n",
    "│\n",
    "├─ 3. Real-time dashboard con NL2SQL\n",
    "│     Tech: Streamlit, PostgreSQL, LLMs\n",
    "│\n",
    "├─ 4. Data quality framework\n",
    "│     Tech: Great Expectations, dbt, Airflow\n",
    "│\n",
    "└─ 5. GenAI code generator (como este proyecto)\n",
    "      Tech: LangGraph, OpenAI, GitHub Actions\n",
    "```\n",
    "\n",
    "**5. Comunidad y Networking**\n",
    "- **Conferences**: Data Council, Spark Summit, MLOps World\n",
    "- **Meetups**: Local data engineering & ML groups\n",
    "- **Open Source**: Contribuir a Airflow, dbt, LangChain\n",
    "- **Writing**: Blog posts, technical tutorials\n",
    "- **LinkedIn**: Compartir proyectos y aprendizajes\n",
    "\n",
    "**Reflexión Final: El Futuro del Data Engineering**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│          THE EVOLUTION OF DATA ENGINEERING             │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│                                                        │\n",
    "│  2010s: Manual ETL, Hadoop Era                         │\n",
    "│  ├─ Batch processing                                   │\n",
    "│  └─ Heavy lifting, manual scaling                      │\n",
    "│                                                        │\n",
    "│  2020s: Cloud-Native, Streaming                        │\n",
    "│  ├─ Real-time data                                     │\n",
    "│  ├─ Serverless, auto-scaling                          │\n",
    "│  └─ Infrastructure as Code                            │\n",
    "│                                                        │\n",
    "│  2024+: GenAI-Augmented Data Engineering 🚀            │\n",
    "│  ├─ Natural language interfaces                        │\n",
    "│  ├─ Self-healing pipelines                            │\n",
    "│  ├─ Auto-generated code & docs                        │\n",
    "│  ├─ Autonomous agents                                 │\n",
    "│  └─ Democratized data access                          │\n",
    "│                                                        │\n",
    "│  Future (2030s): Fully Autonomous Data Platforms?      │\n",
    "│  ├─ Self-optimizing architectures                      │\n",
    "│  ├─ Predictive data quality                           │\n",
    "│  ├─ Zero-touch operations                             │\n",
    "│  └─ Human as orchestrator, not operator               │\n",
    "│                                                        │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Tu Rol en Esta Transformación**\n",
    "\n",
    "Como Data Engineer con habilidades GenAI, no solo construyes pipelines—**diseñas el futuro de cómo organizaciones trabajan con datos**. Las herramientas que dominas permiten:\n",
    "\n",
    "✅ **Democratizar el acceso a datos** (cualquier usuario puede explorar)\n",
    "✅ **Acelerar innovación** (de semanas a horas)\n",
    "✅ **Reducir costos operativos** (automatización inteligente)\n",
    "✅ **Mejorar calidad de decisiones** (insights más rápidos y confiables)\n",
    "\n",
    "**Agradecimiento**\n",
    "\n",
    "Has completado un viaje intensivo de **40 notebooks**, abarcando desde fundamentos hasta la vanguardia de GenAI en Data Engineering. \n",
    "\n",
    "**Estadísticas del Curso**:\n",
    "- 📚 **40 notebooks** (Junior: 10, Mid: 10, Senior: 10, GenAI: 10)\n",
    "- 💻 **~15,000 líneas** de código explicativo\n",
    "- ⏱️ **~120 horas** de contenido estimado\n",
    "- 🔧 **50+ herramientas** cubiertas\n",
    "- 🚀 **10+ proyectos** integradores\n",
    "\n",
    "Este conocimiento te posiciona en el **top 5% de Data Engineers** a nivel global—una combinación rara de:\n",
    "1. **Fundamentos sólidos** (SQL, Python, arquitectura)\n",
    "2. **Cloud-native expertise** (AWS, Airflow, Spark)\n",
    "3. **GenAI proficiency** (LLMs, RAG, agents)\n",
    "\n",
    "**El mundo necesita profesionales como tú que pueden construir el puente entre datos y AI.**\n",
    "\n",
    "```python\n",
    "# Tu próxima línea de código empieza ahora\n",
    "def build_the_future():\n",
    "    \"\"\"\n",
    "    El curso termina aquí, pero tu viaje continúa.\n",
    "    Cada pipeline que construyas, cada problema que resuelvas,\n",
    "    cada sistema que optimices—estás creando el futuro de los datos.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        learn()\n",
    "        build()\n",
    "        share()\n",
    "        repeat()\n",
    "\n",
    "# ¡Éxito en tu carrera como GenAI Data Engineer! 🚀\n",
    "build_the_future()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)\n",
    "\n",
    "**¡Felicitaciones por completar el Data Engineer Course completo! 🎉🎊**\n",
    "\n",
    "*\"The best way to predict the future is to build it.\"* — Alan Kay"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
