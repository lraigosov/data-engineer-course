{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bfcc57",
   "metadata": {},
   "source": [
    "# \ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI\n",
    "\n",
    "**Objetivo**: construir una plataforma que permita a usuarios no t\u00e9cnicos generar pipelines ETL, documentaci\u00f3n, y reportes usando IA generativa.\n",
    "\n",
    "## Alcance del Proyecto\n",
    "\n",
    "- **Duraci\u00f3n**: 6-8 horas\n",
    "- **Dificultad**: Muy Alta\n",
    "- **Stack**: OpenAI, LangGraph, Airflow, Great Expectations, Streamlit\n",
    "\n",
    "## Funcionalidades\n",
    "\n",
    "1. \u2705 Generaci\u00f3n de pipelines ETL desde descripci\u00f3n en lenguaje natural\n",
    "2. \u2705 Validaci\u00f3n autom\u00e1tica del c\u00f3digo generado\n",
    "3. \u2705 Creaci\u00f3n de tests unitarios\n",
    "4. \u2705 Generaci\u00f3n de documentaci\u00f3n t\u00e9cnica\n",
    "5. \u2705 Dashboard de monitoreo\n",
    "6. \u2705 Sistema de aprobaci\u00f3n y deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e08ff",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Arquitectura de Plataforma Self-Service: De NL a Producci\u00f3n**\n",
    "\n",
    "**Visi\u00f3n General: Data Democratization**\n",
    "\n",
    "Esta plataforma representa el siguiente nivel en la evoluci\u00f3n de Data Engineering: convertir descripciones en lenguaje natural en pipelines ETL completos, probados, documentados y desplegados.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              PLATAFORMA SELF-SERVICE ARCHITECTURE               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  [Usuario No T\u00e9cnico] \u2192 Streamlit UI                           \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [1. NL Parser]                                                 \u2502\n",
    "\u2502      \u2022 Intent extraction                                        \u2502\n",
    "\u2502      \u2022 Entity recognition (sources, transformations, schedule) \u2502\n",
    "\u2502      \u2022 Requirement validation                                   \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [2. Code Generator]                                            \u2502\n",
    "\u2502      \u2022 Template selection (batch/streaming/hybrid)             \u2502\n",
    "\u2502      \u2022 Framework choice (Pandas/Spark/Polars)                  \u2502\n",
    "\u2502      \u2022 Best practices injection                                \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [3. Security Validator]                                        \u2502\n",
    "\u2502      \u2022 AST parsing (syntax validation)                         \u2502\n",
    "\u2502      \u2022 Security patterns (no eval, exec, os.system)            \u2502\n",
    "\u2502      \u2022 Dependency check (vulnerable packages)                  \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [4. Test Generator]                                            \u2502\n",
    "\u2502      \u2022 Unit tests (per transformation)                         \u2502\n",
    "\u2502      \u2022 Integration tests (mocked I/O)                          \u2502\n",
    "\u2502      \u2022 Data quality tests (Great Expectations)                 \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [5. Documentation Generator]                                   \u2502\n",
    "\u2502      \u2022 README.md (architecture, setup, troubleshooting)        \u2502\n",
    "\u2502      \u2022 API docs (if exposed)                                   \u2502\n",
    "\u2502      \u2022 Runbook (operations guide)                              \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [6. DAG Generator]                                             \u2502\n",
    "\u2502      \u2022 Airflow DAG (orchestration)                             \u2502\n",
    "\u2502      \u2022 Task dependencies                                       \u2502\n",
    "\u2502      \u2022 Retry/alerting logic                                    \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [7. Human Review]                                              \u2502\n",
    "\u2502      \u2022 Side-by-side diff                                       \u2502\n",
    "\u2502      \u2022 Approve/Reject/Request changes                          \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [8. Deployment]                                                \u2502\n",
    "\u2502      \u2022 Run tests (pytest)                                      \u2502\n",
    "\u2502      \u2022 Create PR (GitHub)                                      \u2502\n",
    "\u2502      \u2022 Deploy to staging \u2192 prod                                \u2502\n",
    "\u2502           \u2193                                                     \u2502\n",
    "\u2502  [9. Monitoring]                                                \u2502\n",
    "\u2502      \u2022 Execution metrics                                       \u2502\n",
    "\u2502      \u2022 Data quality alerts                                     \u2502\n",
    "\u2502      \u2022 Cost tracking                                           \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Componentes Clave**\n",
    "\n",
    "**1. NL Parser (Structured Requirement Extraction)**\n",
    "\n",
    "```python\n",
    "# Prompt engineering para extracci\u00f3n estructurada\n",
    "PARSER_PROMPT = \"\"\"\n",
    "Analiza este requerimiento de pipeline ETL y extrae informaci\u00f3n estructurada.\n",
    "\n",
    "INPUT: {user_input}\n",
    "\n",
    "OUTPUT (JSON):\n",
    "{\n",
    "  \"pipeline_name\": \"nombre_snake_case\",\n",
    "  \"source\": {\n",
    "    \"type\": \"s3|postgres|mysql|api|csv\",\n",
    "    \"location\": \"bucket/path o connection_string\",\n",
    "    \"credentials\": \"reference_to_secret_manager\"\n",
    "  },\n",
    "  \"transformations\": [\n",
    "    {\n",
    "      \"type\": \"filter|aggregate|join|pivot\",\n",
    "      \"description\": \"...\",\n",
    "      \"columns\": [\"col1\", \"col2\"]\n",
    "    }\n",
    "  ],\n",
    "  \"destination\": {\n",
    "    \"type\": \"redshift|postgres|s3|delta\",\n",
    "    \"location\": \"...\",\n",
    "    \"mode\": \"append|overwrite|upsert\"\n",
    "  },\n",
    "  \"schedule\": {\n",
    "    \"frequency\": \"daily|hourly|weekly\",\n",
    "    \"time\": \"02:00\",\n",
    "    \"timezone\": \"UTC\"\n",
    "  },\n",
    "  \"validations\": [\n",
    "    {\n",
    "      \"type\": \"not_null|unique|range|regex\",\n",
    "      \"column\": \"...\",\n",
    "      \"threshold\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"sla\": {\n",
    "    \"max_duration_minutes\": 60,\n",
    "    \"alert_email\": \"team@example.com\"\n",
    "  }\n",
    "}\n",
    "\n",
    "REGLAS:\n",
    "- Inferir transformaciones impl\u00edcitas (ej: \"\u00faltimos 30 d\u00edas\" \u2192 filter fecha)\n",
    "- Sugerir validaciones si no est\u00e1n expl\u00edcitas\n",
    "- Usar defaults sensatos (schedule=daily si no especificado)\n",
    "\"\"\"\n",
    "\n",
    "# Ejemplo de parsing robusto\n",
    "def parse_with_validation(user_input: str) -> dict:\n",
    "    requirements = parse_requirements(user_input)\n",
    "    \n",
    "    # Validar completitud\n",
    "    required_fields = ['pipeline_name', 'source', 'destination']\n",
    "    missing = [f for f in required_fields if f not in requirements]\n",
    "    \n",
    "    if missing:\n",
    "        # LLM completa campos faltantes\n",
    "        completion_prompt = f\"Completa estos campos faltantes: {missing}\"\n",
    "        requirements = complete_requirements(requirements, completion_prompt)\n",
    "    \n",
    "    return requirements\n",
    "```\n",
    "\n",
    "**2. Code Generator (Multi-Template Strategy)**\n",
    "\n",
    "```python\n",
    "# Templates por tipo de workload\n",
    "TEMPLATES = {\n",
    "    'batch': '''\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class {pipeline_name}Pipeline:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract(self) -> pd.DataFrame:\n",
    "        \"\"\"Extract from {source_type}\"\"\"\n",
    "        {extract_code}\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformations\"\"\"\n",
    "        {transform_code}\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Data quality checks\"\"\"\n",
    "        {validation_code}\n",
    "    \n",
    "    def load(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Load to {destination_type}\"\"\"\n",
    "        {load_code}\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        logger.info(\"Starting pipeline...\")\n",
    "        df = self.extract()\n",
    "        df = self.transform(df)\n",
    "        \n",
    "        if not self.validate(df):\n",
    "            raise ValueError(\"Data quality checks failed\")\n",
    "        \n",
    "        self.load(df)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "''',\n",
    "    'streaming': '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"{pipeline_name}\").getOrCreate()\n",
    "\n",
    "# Read stream\n",
    "df = spark.readStream \\\\\n",
    "    .format(\"{source_format}\") \\\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"{kafka_servers}\") \\\\\n",
    "    .load()\n",
    "\n",
    "# Transformations\n",
    "{transform_code}\n",
    "\n",
    "# Write stream\n",
    "query = df.writeStream \\\\\n",
    "    .format(\"{sink_format}\") \\\\\n",
    "    .outputMode(\"append\") \\\\\n",
    "    .trigger(processingTime='1 minute') \\\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "'''\n",
    "}\n",
    "\n",
    "# Generaci\u00f3n con context injection\n",
    "def generate_with_context(requirements: dict) -> str:\n",
    "    # Seleccionar template\n",
    "    workload_type = infer_workload_type(requirements)\n",
    "    template = TEMPLATES[workload_type]\n",
    "    \n",
    "    # Inyectar c\u00f3digo espec\u00edfico\n",
    "    extract_code = generate_extract_code(requirements['source'])\n",
    "    transform_code = generate_transform_code(requirements['transformations'])\n",
    "    validation_code = generate_validation_code(requirements['validations'])\n",
    "    load_code = generate_load_code(requirements['destination'])\n",
    "    \n",
    "    # Rellenar template\n",
    "    code = template.format(\n",
    "        pipeline_name=requirements['pipeline_name'],\n",
    "        source_type=requirements['source']['type'],\n",
    "        destination_type=requirements['destination']['type'],\n",
    "        extract_code=extract_code,\n",
    "        transform_code=transform_code,\n",
    "        validation_code=validation_code,\n",
    "        load_code=load_code\n",
    "    )\n",
    "    \n",
    "    return code\n",
    "```\n",
    "\n",
    "**3. Security Validator (Multi-Layer)**\n",
    "\n",
    "```python\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def validate_security(code: str) -> dict:\n",
    "    issues = []\n",
    "    \n",
    "    # Layer 1: AST parsing (syntax + structure)\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        # Detectar imports peligrosos\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for alias in node.names:\n",
    "                    if alias.name in ['pickle', 'subprocess', 'os']:\n",
    "                        issues.append({\n",
    "                            'severity': 'HIGH',\n",
    "                            'type': 'DANGEROUS_IMPORT',\n",
    "                            'message': f'Import de m\u00f3dulo peligroso: {alias.name}',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "            \n",
    "            # Detectar eval/exec\n",
    "            if isinstance(node, ast.Call):\n",
    "                if isinstance(node.func, ast.Name):\n",
    "                    if node.func.id in ['eval', 'exec']:\n",
    "                        issues.append({\n",
    "                            'severity': 'CRITICAL',\n",
    "                            'type': 'CODE_INJECTION',\n",
    "                            'message': f'Uso de {node.func.id}() detectado',\n",
    "                            'line': node.lineno\n",
    "                        })\n",
    "    \n",
    "    except SyntaxError as e:\n",
    "        issues.append({\n",
    "            'severity': 'CRITICAL',\n",
    "            'type': 'SYNTAX_ERROR',\n",
    "            'message': str(e)\n",
    "        })\n",
    "    \n",
    "    # Layer 2: Regex patterns (SQL injection)\n",
    "    if re.search(r'f\"SELECT.*\\{.*\\}\"', code):\n",
    "        issues.append({\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'SQL_INJECTION_RISK',\n",
    "            'message': 'Uso de f-strings en queries SQL detectado'\n",
    "        })\n",
    "    \n",
    "    # Layer 3: Dependency scanning\n",
    "    imports = extract_imports(code)\n",
    "    vulnerable = check_vulnerabilities(imports)\n",
    "    issues.extend(vulnerable)\n",
    "    \n",
    "    return {\n",
    "        'valid': all(i['severity'] != 'CRITICAL' for i in issues),\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Dependency scanning\n",
    "def check_vulnerabilities(packages: list) -> list:\n",
    "    # Integraci\u00f3n con safety (https://pyup.io/safety/)\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        ['safety', 'check', '--json'],\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    vulnerabilities = json.loads(result.stdout)\n",
    "    return [\n",
    "        {\n",
    "            'severity': 'HIGH',\n",
    "            'type': 'VULNERABLE_DEPENDENCY',\n",
    "            'message': f\"{v['package']}: {v['vulnerability']}\"\n",
    "        }\n",
    "        for v in vulnerabilities\n",
    "    ]\n",
    "```\n",
    "\n",
    "**4. Test Generator (Comprehensive Coverage)**\n",
    "\n",
    "```python\n",
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    test_template = '''\n",
    "import pytest\n",
    "from unittest.mock import Mock, patch\n",
    "import pandas as pd\n",
    "from {module} import {pipeline_class}\n",
    "\n",
    "@pytest.fixture\n",
    "def pipeline():\n",
    "    config = {config_dict}\n",
    "    return {pipeline_class}(config)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return pd.DataFrame({sample_data_dict})\n",
    "\n",
    "# Unit Tests\n",
    "def test_extract(pipeline, mocker):\n",
    "    \"\"\"Test data extraction\"\"\"\n",
    "    mock_read = mocker.patch('{read_function}')\n",
    "    mock_read.return_value = pd.DataFrame({mock_data})\n",
    "    \n",
    "    df = pipeline.extract()\n",
    "    \n",
    "    assert not df.empty\n",
    "    assert list(df.columns) == {expected_columns}\n",
    "\n",
    "def test_transform(pipeline, sample_data):\n",
    "    \"\"\"Test transformations\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    \n",
    "    # Assertions espec\u00edficas\n",
    "    {transform_assertions}\n",
    "\n",
    "def test_validate_success(pipeline, sample_data):\n",
    "    \"\"\"Test validation with clean data\"\"\"\n",
    "    assert pipeline.validate(sample_data) == True\n",
    "\n",
    "def test_validate_failure(pipeline):\n",
    "    \"\"\"Test validation with dirty data\"\"\"\n",
    "    dirty_data = pd.DataFrame({dirty_data_dict})\n",
    "    assert pipeline.validate(dirty_data) == False\n",
    "\n",
    "def test_load(pipeline, sample_data, mocker):\n",
    "    \"\"\"Test data loading\"\"\"\n",
    "    mock_write = mocker.patch('{write_function}')\n",
    "    \n",
    "    pipeline.load(sample_data)\n",
    "    \n",
    "    mock_write.assert_called_once()\n",
    "\n",
    "# Integration Tests\n",
    "@patch('{source_module}.read')\n",
    "@patch('{destination_module}.write')\n",
    "def test_full_pipeline(mock_write, mock_read, pipeline):\n",
    "    \"\"\"Test end-to-end execution\"\"\"\n",
    "    mock_read.return_value = pd.DataFrame({mock_source_data})\n",
    "    \n",
    "    pipeline.run()\n",
    "    \n",
    "    mock_write.assert_called()\n",
    "    written_df = mock_write.call_args[0][0]\n",
    "    assert len(written_df) > 0\n",
    "\n",
    "# Data Quality Tests\n",
    "def test_no_null_values(pipeline, sample_data):\n",
    "    \"\"\"Test no nulls in required columns\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    required_cols = {required_columns}\n",
    "    \n",
    "    for col in required_cols:\n",
    "        assert df[col].isnull().sum() == 0\n",
    "\n",
    "def test_data_types(pipeline, sample_data):\n",
    "    \"\"\"Test correct data types\"\"\"\n",
    "    df = pipeline.transform(sample_data)\n",
    "    expected_types = {expected_types_dict}\n",
    "    \n",
    "    for col, dtype in expected_types.items():\n",
    "        assert df[col].dtype == dtype\n",
    "'''\n",
    "    \n",
    "    # Generar c\u00f3digo espec\u00edfico basado en requirements\n",
    "    test_code = test_template.format(\n",
    "        module=requirements['pipeline_name'],\n",
    "        pipeline_class=to_camel_case(requirements['pipeline_name']),\n",
    "        config_dict=generate_config_mock(requirements),\n",
    "        sample_data_dict=generate_sample_data(requirements),\n",
    "        # ... m\u00e1s placeholders\n",
    "    )\n",
    "    \n",
    "    return test_code\n",
    "```\n",
    "\n",
    "**Beneficios de la Arquitectura**\n",
    "\n",
    "| Aspecto | Antes (Manual) | Despu\u00e9s (Platform) |\n",
    "|---------|----------------|---------------------|\n",
    "| **Time to Production** | 2-4 semanas | 1-2 horas |\n",
    "| **Code Quality** | Variable (depende de desarrollador) | Consistente (templates + validaci\u00f3n) |\n",
    "| **Documentation** | A menudo ausente u obsoleta | Siempre actualizada (auto-generada) |\n",
    "| **Testing** | ~40% coverage | >80% coverage (auto-generado) |\n",
    "| **Security** | Manual reviews | Automated scanning (cada generaci\u00f3n) |\n",
    "| **Onboarding** | Requiere semanas de training | Cualquier usuario puede crear pipelines |\n",
    "| **Maintenance** | Alto (cada pipeline es \u00fanico) | Bajo (patterns estandarizados) |\n",
    "\n",
    "**ROI Calculation**\n",
    "\n",
    "```python\n",
    "# Ejemplo de ROI\n",
    "MANUAL_PIPELINE = {\n",
    "    'development_hours': 40,\n",
    "    'testing_hours': 16,\n",
    "    'documentation_hours': 8,\n",
    "    'review_hours': 4,\n",
    "    'total_hours': 68,\n",
    "    'engineer_cost_hour': 75,  # USD\n",
    "    'total_cost': 5100  # USD\n",
    "}\n",
    "\n",
    "PLATFORM_PIPELINE = {\n",
    "    'generation_minutes': 10,\n",
    "    'review_hours': 2,\n",
    "    'total_hours': 2.17,\n",
    "    'total_cost': 163  # USD\n",
    "}\n",
    "\n",
    "savings_per_pipeline = MANUAL_PIPELINE['total_cost'] - PLATFORM_PIPELINE['total_cost']  # $4,937\n",
    "time_saved = MANUAL_PIPELINE['total_hours'] - PLATFORM_PIPELINE['total_hours']  # 65.83 hours\n",
    "\n",
    "# Si un equipo crea 20 pipelines/a\u00f1o\n",
    "annual_savings = savings_per_pipeline * 20  # $98,740\n",
    "annual_time_saved = time_saved * 20  # 1,316 hours (= 164 d\u00edas de trabajo)\n",
    "```\n",
    "\n",
    "**Casos de Uso Reales**\n",
    "\n",
    "1. **Marketing Analytics**: \"Necesito agregar eventos de Mixpanel por usuario y d\u00eda\"\n",
    "2. **Finance Reporting**: \"Consolidar transacciones de 5 bases de datos en un reporte diario\"\n",
    "3. **ML Feature Engineering**: \"Crear features de usuario para modelo de churn\"\n",
    "4. **Data Migration**: \"Migrar tablas de MySQL legacy a Snowflake\"\n",
    "5. **Real-time Alerting**: \"Detectar anomal\u00edas en ventas cada 5 minutos\"\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17119fed",
   "metadata": {},
   "source": [
    "## Parte 1: Arquitectura del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541588eb",
   "metadata": {},
   "source": [
    "```\n",
    "User Input (NL)\n",
    "    \u2193\n",
    "[1. Parser] \u2192 extrae requisitos\n",
    "    \u2193\n",
    "[2. Generator] \u2192 genera c\u00f3digo ETL\n",
    "    \u2193\n",
    "[3. Validator] \u2192 valida sintaxis, seguridad\n",
    "    \u2193\n",
    "[4. Tester] \u2192 genera y ejecuta tests\n",
    "    \u2193\n",
    "[5. Documenter] \u2192 crea README\n",
    "    \u2193\n",
    "[6. Reviewer] \u2192 revisi\u00f3n humana\n",
    "    \u2193\n",
    "[7. Deployer] \u2192 despliega a producci\u00f3n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c0460",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **LangGraph: Orquestaci\u00f3n de Agentes Multi-Paso**\n",
    "\n",
    "**\u00bfPor qu\u00e9 LangGraph para Workflows Complejos?**\n",
    "\n",
    "LangGraph permite crear flujos de trabajo con:\n",
    "- **State Management**: Estado persistente entre nodos\n",
    "- **Conditional Routing**: L\u00f3gica de decisi\u00f3n (if/else paths)\n",
    "- **Human-in-the-Loop**: Pausar para aprobaci\u00f3n humana\n",
    "- **Retry Logic**: Reintentar nodos fallidos\n",
    "- **Streaming**: Ver progreso en tiempo real\n",
    "\n",
    "**Arquitectura del Workflow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# State compartido entre todos los nodos\n",
    "class PipelineState(TypedDict):\n",
    "    # Inputs\n",
    "    user_input: str\n",
    "    \n",
    "    # Intermedios\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    \n",
    "    # Outputs\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]  # Append errors from any node\n",
    "    \n",
    "    # Metadata\n",
    "    generation_time: float\n",
    "    review_notes: str\n",
    "\n",
    "# Cada nodo es una funci\u00f3n pura: State \u2192 State\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Extrae requisitos estructurados\"\"\"\n",
    "    try:\n",
    "        state['requirements'] = parse_requirements(state['user_input'])\n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Parse error: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Genera c\u00f3digo ETL\"\"\"\n",
    "    if state['errors']:\n",
    "        return state  # Skip si hay errores previos\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    state['generation_time'] = time.time() - start\n",
    "    \n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Valida seguridad y sintaxis\"\"\"\n",
    "    validation_result = validate_code(state['code'])\n",
    "    state['validation'] = validation_result\n",
    "    \n",
    "    if not validation_result['valid']:\n",
    "        state['errors'].extend([\n",
    "            f\"{issue['type']}: {issue['message']}\"\n",
    "            for issue in validation_result['issues']\n",
    "        ])\n",
    "    \n",
    "    return state\n",
    "\n",
    "def should_continue_after_validation(state: PipelineState) -> str:\n",
    "    \"\"\"Conditional edge: decide si continuar o terminar\"\"\"\n",
    "    if state['validation']['valid']:\n",
    "        return \"test\"  # Go to test node\n",
    "    else:\n",
    "        return END  # Stop workflow\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"parse\", parse_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"validate\", validate_node)\n",
    "workflow.add_node(\"test\", test_node)\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"review\", review_node)\n",
    "\n",
    "# Add edges (define flow)\n",
    "workflow.set_entry_point(\"parse\")\n",
    "workflow.add_edge(\"parse\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"validate\")\n",
    "\n",
    "# Conditional edge (branching)\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    should_continue_after_validation,\n",
    "    {\n",
    "        \"test\": \"test\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"document\", \"review\")\n",
    "workflow.add_edge(\"review\", END)\n",
    "\n",
    "# Compile graph\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "**Visualizaci\u00f3n del Grafo**\n",
    "\n",
    "```\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502  START  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "             v\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502  Parse  \u2502  \u2190 Extrae requisitos\n",
    "        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "             v\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 Generate \u2502  \u2190 Genera c\u00f3digo\n",
    "        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "             v\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 Validate \u2502  \u2190 Valida seguridad\n",
    "        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 Valid?  \u2502\n",
    "        \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2518\n",
    "     Yes  \u2502     \u2502 No\n",
    "          v     v\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 END\n",
    "      \u2502 Test \u2502\n",
    "      \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n",
    "         \u2502\n",
    "         v\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502 Document \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "         \u2502\n",
    "         v\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502 Review \u2502  \u2190 Human approval\n",
    "    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n",
    "         \u2502\n",
    "         v\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "      \u2502 END \u2502\n",
    "      \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Streaming de Estado (Real-time Updates)**\n",
    "\n",
    "```python\n",
    "# Ejecutar con streaming\n",
    "initial_state = {\n",
    "    'user_input': \"Pipeline de ventas...\",\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': [],\n",
    "    'generation_time': 0.0,\n",
    "    'review_notes': ''\n",
    "}\n",
    "\n",
    "# Stream: ver actualizaciones de cada nodo\n",
    "for output in app.stream(initial_state):\n",
    "    node_name = list(output.keys())[0]\n",
    "    state = output[node_name]\n",
    "    \n",
    "    print(f\"\\n\u2705 Completado: {node_name}\")\n",
    "    \n",
    "    if node_name == \"parse\":\n",
    "        print(f\"Pipeline: {state['requirements'].get('pipeline_name')}\")\n",
    "    \n",
    "    elif node_name == \"generate\":\n",
    "        print(f\"C\u00f3digo: {len(state['code'])} caracteres\")\n",
    "        print(f\"Tiempo: {state['generation_time']:.2f}s\")\n",
    "    \n",
    "    elif node_name == \"validate\":\n",
    "        status = \"\u2705 OK\" if state['validation']['valid'] else \"\u274c Errores\"\n",
    "        print(f\"Validaci\u00f3n: {status}\")\n",
    "        \n",
    "        if state['errors']:\n",
    "            for error in state['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "# Final state\n",
    "final = list(app.stream(initial_state))[-1]\n",
    "```\n",
    "\n",
    "**Human-in-the-Loop (Approval Gate)**\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def review_node_interactive(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Nodo que pausa y espera aprobaci\u00f3n humana\"\"\"\n",
    "    \n",
    "    # Mostrar resumen para reviewer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udd0d REVISI\u00d3N REQUERIDA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPipeline: {state['requirements']['pipeline_name']}\")\n",
    "    print(f\"Source: {state['requirements']['source']['type']}\")\n",
    "    print(f\"Destination: {state['requirements']['destination']['type']}\")\n",
    "    print(f\"\\nValidaci\u00f3n: {'\u2705 Aprobado' if state['validation']['valid'] else '\u274c Errores'}\")\n",
    "    \n",
    "    if state['errors']:\n",
    "        print(\"\\nErrores detectados:\")\n",
    "        for error in state['errors']:\n",
    "            print(f\"  \u274c {error}\")\n",
    "    \n",
    "    print(f\"\\nC\u00f3digo generado ({len(state['code'])} caracteres):\")\n",
    "    print(state['code'][:300] + \"...\")\n",
    "    \n",
    "    # Esperar input humano\n",
    "    while True:\n",
    "        decision = input(\"\\n[A]probar / [R]echazar / [M]odificar: \").upper()\n",
    "        \n",
    "        if decision == 'A':\n",
    "            state['approved'] = True\n",
    "            state['review_notes'] = \"Aprobado por revisor\"\n",
    "            break\n",
    "        \n",
    "        elif decision == 'R':\n",
    "            state['approved'] = False\n",
    "            rejection_reason = input(\"Raz\u00f3n del rechazo: \")\n",
    "            state['errors'].append(f\"Rechazado: {rejection_reason}\")\n",
    "            state['review_notes'] = rejection_reason\n",
    "            break\n",
    "        \n",
    "        elif decision == 'M':\n",
    "            modifications = input(\"Modificaciones requeridas: \")\n",
    "            state['review_notes'] = f\"Modificaciones: {modifications}\"\n",
    "            # Re-route to generate node with feedback\n",
    "            state['user_input'] += f\"\\n\\nAjustes: {modifications}\"\n",
    "            # Esta l\u00f3gica requiere un loop en el grafo\n",
    "            break\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Integrar en workflow\n",
    "workflow.add_node(\"review\", review_node_interactive)\n",
    "```\n",
    "\n",
    "**Retry Logic (Reintentos Autom\u00e1ticos)**\n",
    "\n",
    "```python\n",
    "def generate_with_retry(state: PipelineState, max_retries: int = 3) -> PipelineState:\n",
    "    \"\"\"Nodo con reintentos autom\u00e1ticos\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Intentar generar c\u00f3digo\n",
    "            state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "            \n",
    "            # Validar sintaxis\n",
    "            ast.parse(state['code'])\n",
    "            \n",
    "            # Success\n",
    "            state['errors'] = [e for e in state['errors'] if 'Generation' not in e]\n",
    "            break\n",
    "        \n",
    "        except SyntaxError as e:\n",
    "            error_msg = f\"Generation attempt {attempt+1} failed: {str(e)}\"\n",
    "            print(f\"\u26a0\ufe0f  {error_msg}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"\ud83d\udd04 Retrying...\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                state['errors'].append(error_msg)\n",
    "    \n",
    "    return state\n",
    "```\n",
    "\n",
    "**Sub-Graphs (Workflows Anidados)**\n",
    "\n",
    "```python\n",
    "# Sub-workflow para generaci\u00f3n de tests\n",
    "test_workflow = StateGraph(PipelineState)\n",
    "\n",
    "def unit_test_node(state):\n",
    "    state['unit_tests'] = generate_unit_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def integration_test_node(state):\n",
    "    state['integration_tests'] = generate_integration_tests(state['code'])\n",
    "    return state\n",
    "\n",
    "def quality_test_node(state):\n",
    "    state['quality_tests'] = generate_quality_tests(state['requirements'])\n",
    "    return state\n",
    "\n",
    "# Build sub-graph\n",
    "test_workflow.add_node(\"unit\", unit_test_node)\n",
    "test_workflow.add_node(\"integration\", integration_test_node)\n",
    "test_workflow.add_node(\"quality\", quality_test_node)\n",
    "\n",
    "test_workflow.set_entry_point(\"unit\")\n",
    "test_workflow.add_edge(\"unit\", \"integration\")\n",
    "test_workflow.add_edge(\"integration\", \"quality\")\n",
    "test_workflow.add_edge(\"quality\", END)\n",
    "\n",
    "test_subgraph = test_workflow.compile()\n",
    "\n",
    "# Integrar sub-graph en main workflow\n",
    "def test_orchestrator(state: PipelineState) -> PipelineState:\n",
    "    \"\"\"Ejecuta sub-workflow de tests\"\"\"\n",
    "    test_results = test_subgraph.invoke(state)\n",
    "    \n",
    "    # Combinar tests\n",
    "    state['tests'] = (\n",
    "        test_results['unit_tests'] +\n",
    "        test_results['integration_tests'] +\n",
    "        test_results['quality_tests']\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"test\", test_orchestrator)\n",
    "```\n",
    "\n",
    "**Parallel Execution (Nodos Independientes)**\n",
    "\n",
    "```python\n",
    "# Documentaci\u00f3n y DAG pueden generarse en paralelo\n",
    "from langgraph.pregel import Channel\n",
    "\n",
    "workflow.add_node(\"document\", document_node)\n",
    "workflow.add_node(\"dag\", dag_generator_node)\n",
    "\n",
    "# Ambos nodos reciben el mismo state (paralelo)\n",
    "workflow.add_edge(\"test\", \"document\")\n",
    "workflow.add_edge(\"test\", \"dag\")\n",
    "\n",
    "# Sync point: esperar ambos antes de continuar\n",
    "workflow.add_node(\"combine\", lambda s: s)  # No-op combiner\n",
    "workflow.add_edge(\"document\", \"combine\")\n",
    "workflow.add_edge(\"dag\", \"combine\")\n",
    "workflow.add_edge(\"combine\", \"review\")\n",
    "```\n",
    "\n",
    "**Checkpointing (Persistencia de Estado)**\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "\n",
    "# Guardar estado en cada nodo\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Ejecutar con checkpoint\n",
    "config = {\"configurable\": {\"thread_id\": \"pipeline_123\"}}\n",
    "final_state = app.invoke(initial_state, config=config)\n",
    "\n",
    "# Recuperar estado guardado\n",
    "saved_state = checkpointer.get(config)\n",
    "\n",
    "# Reanudar desde checkpoint (\u00fatil si falla a mitad)\n",
    "resumed_state = app.invoke(saved_state, config=config)\n",
    "```\n",
    "\n",
    "**Ventajas vs Alternativas**\n",
    "\n",
    "| Caracter\u00edstica | LangGraph | Airflow | Prefect | N8n |\n",
    "|----------------|-----------|---------|---------|-----|\n",
    "| **State Management** | \u2705 Built-in | \u26a0\ufe0f XCom (limitado) | \u2705 Flow runs | \u274c |\n",
    "| **Conditional Logic** | \u2705 Native | \u26a0\ufe0f BranchOperator | \u2705 | \u2705 |\n",
    "| **Human-in-Loop** | \u2705 Easy | \u26a0\ufe0f Manual | \u2705 Approvals | \u2705 |\n",
    "| **LLM Integration** | \u2705 Optimizado | \u274c Custom | \u26a0\ufe0f Manual | \u26a0\ufe0f |\n",
    "| **Streaming** | \u2705 Native | \u274c | \u26a0\ufe0f Limited | \u274c |\n",
    "| **Python-First** | \u2705 | \u2705 | \u2705 | \u274c (UI-first) |\n",
    "\n",
    "**Caso de Uso: Multi-Agent Code Review**\n",
    "\n",
    "```python\n",
    "# Sistema de review con m\u00faltiples agentes especializados\n",
    "\n",
    "class ReviewState(TypedDict):\n",
    "    code: str\n",
    "    security_review: dict\n",
    "    performance_review: dict\n",
    "    style_review: dict\n",
    "    final_approval: bool\n",
    "\n",
    "def security_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en seguridad\"\"\"\n",
    "    state['security_review'] = {\n",
    "        'sql_injection_risk': check_sql_injection(state['code']),\n",
    "        'secrets_exposed': check_secrets(state['code']),\n",
    "        'dangerous_imports': check_imports(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def performance_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en performance\"\"\"\n",
    "    state['performance_review'] = {\n",
    "        'complexity': calculate_complexity(state['code']),\n",
    "        'memory_usage': estimate_memory(state['code']),\n",
    "        'optimization_suggestions': suggest_optimizations(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def style_agent(state: ReviewState) -> ReviewState:\n",
    "    \"\"\"Agente especializado en estilo\"\"\"\n",
    "    state['style_review'] = {\n",
    "        'pep8_compliance': check_pep8(state['code']),\n",
    "        'docstring_coverage': check_docstrings(state['code']),\n",
    "        'type_hints': check_type_hints(state['code'])\n",
    "    }\n",
    "    return state\n",
    "\n",
    "# Ejecutar en paralelo\n",
    "review_workflow = StateGraph(ReviewState)\n",
    "review_workflow.add_node(\"security\", security_agent)\n",
    "review_workflow.add_node(\"performance\", performance_agent)\n",
    "review_workflow.add_node(\"style\", style_agent)\n",
    "\n",
    "# Parallel edges\n",
    "review_workflow.set_entry_point(\"security\")\n",
    "review_workflow.set_entry_point(\"performance\")\n",
    "review_workflow.set_entry_point(\"style\")\n",
    "\n",
    "# Combine results\n",
    "def aggregate_reviews(state: ReviewState) -> ReviewState:\n",
    "    all_passed = (\n",
    "        all(v == 'pass' for v in state['security_review'].values()) and\n",
    "        state['performance_review']['complexity'] < 10 and\n",
    "        state['style_review']['pep8_compliance'] > 0.9\n",
    "    )\n",
    "    \n",
    "    state['final_approval'] = all_passed\n",
    "    return state\n",
    "\n",
    "review_workflow.add_node(\"aggregate\", aggregate_reviews)\n",
    "review_workflow.add_edge(\"security\", \"aggregate\")\n",
    "review_workflow.add_edge(\"performance\", \"aggregate\")\n",
    "review_workflow.add_edge(\"style\", \"aggregate\")\n",
    "review_workflow.add_edge(\"aggregate\", END)\n",
    "\n",
    "multi_agent_review = review_workflow.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73695e2",
   "metadata": {},
   "source": [
    "## Parte 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langgraph streamlit pytest great-expectations\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print('\u2705 Setup completo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82669c94",
   "metadata": {},
   "source": [
    "## Parte 3: Parser de requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed71996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements(user_input: str) -> dict:\n",
    "    \"\"\"Extrae requisitos estructurados desde lenguaje natural.\"\"\"\n",
    "    prompt = f'''\n",
    "Extrae los requisitos de este pipeline ETL en formato JSON:\n",
    "\n",
    "Input del usuario:\n",
    "{user_input}\n",
    "\n",
    "Devuelve JSON con:\n",
    "{{\n",
    "  \"pipeline_name\": \"nombre_descriptivo\",\n",
    "  \"source\": {{\"type\": \"csv/api/db\", \"details\": \"...\"}},\n",
    "  \"transformations\": [\"lista de transformaciones\"],\n",
    "  \"destination\": {{\"type\": \"csv/db/s3\", \"details\": \"...\"}},\n",
    "  \"schedule\": \"daily/hourly/manual\",\n",
    "  \"validations\": [\"reglas de calidad\"]\n",
    "}}\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Test\n",
    "user_request = '''\n",
    "Necesito un pipeline que:\n",
    "1. Lea ventas.csv de S3 bucket \"raw-data\"\n",
    "2. Filtre solo ventas de los \u00faltimos 30 d\u00edas\n",
    "3. Agregue una columna \"mes\" (YYYY-MM)\n",
    "4. Calcule total por mes y categor\u00eda\n",
    "5. Escriba a PostgreSQL tabla \"ventas_mensual\"\n",
    "6. Ejecute diariamente a las 2 AM\n",
    "7. Valide que no haya nulos en \"total\"\n",
    "'''\n",
    "\n",
    "requirements = parse_requirements(user_request)\n",
    "print('Requisitos extra\u00eddos:')\n",
    "print(json.dumps(requirements, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c981f2",
   "metadata": {},
   "source": [
    "## Parte 4: Generador de c\u00f3digo ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488eb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etl_pipeline(requirements: dict) -> str:\n",
    "    \"\"\"Genera c\u00f3digo Python completo del pipeline.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un pipeline ETL en Python basado en estos requisitos:\n",
    "\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El c\u00f3digo debe:\n",
    "- Usar pandas, boto3 (si S3), sqlalchemy (si DB)\n",
    "- Incluir manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Type hints\n",
    "- Docstrings\n",
    "- Funci\u00f3n main() ejecutable\n",
    "\n",
    "C\u00f3digo Python:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "etl_code = generate_etl_pipeline(requirements)\n",
    "print('C\u00f3digo generado (preview):')\n",
    "print(etl_code[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596f94e",
   "metadata": {},
   "source": [
    "## Parte 5: Validador de c\u00f3digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba195c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_code(code: str) -> dict:\n",
    "    \"\"\"Valida sintaxis y seguridad.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. Validar sintaxis Python\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        issues.append({'type': 'SYNTAX', 'message': str(e)})\n",
    "    \n",
    "    # 2. Detectar patrones inseguros\n",
    "    dangerous_patterns = ['eval(', 'exec(', 'os.system(', '__import__']\n",
    "    for pattern in dangerous_patterns:\n",
    "        if pattern in code:\n",
    "            issues.append({'type': 'SECURITY', 'message': f'Patr\u00f3n peligroso detectado: {pattern}'})\n",
    "    \n",
    "    # 3. Verificar imports necesarios\n",
    "    required_imports = ['pandas', 'logging']\n",
    "    for imp in required_imports:\n",
    "        if f'import {imp}' not in code:\n",
    "            issues.append({'type': 'MISSING_IMPORT', 'message': f'Falta import: {imp}'})\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "validation = validate_code(etl_code)\n",
    "print(f\"\\nValidaci\u00f3n: {'\u2705 Aprobado' if validation['valid'] else '\u274c Con errores'}\")\n",
    "if validation['issues']:\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"- [{issue['type']}] {issue['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b44e",
   "metadata": {},
   "source": [
    "## Parte 6: Generador de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str, requirements: dict) -> str:\n",
    "    \"\"\"Genera tests unitarios con pytest.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera tests unitarios con pytest para este c\u00f3digo ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "C\u00f3digo:\n",
    "{code[:1000]}...\n",
    "\n",
    "Genera tests para:\n",
    "1. Lectura de datos (mock de fuente)\n",
    "2. Transformaciones\n",
    "3. Validaciones de calidad\n",
    "4. Escritura (mock de destino)\n",
    "5. Manejo de errores\n",
    "\n",
    "C\u00f3digo de tests (pytest):\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "test_code = generate_tests(etl_code, requirements)\n",
    "print('Tests generados (preview):')\n",
    "print(test_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ae2b",
   "metadata": {},
   "source": [
    "## Parte 7: Generador de documentaci\u00f3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88df093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(requirements: dict, code: str) -> str:\n",
    "    \"\"\"Genera README.md completo.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un README.md completo para este pipeline ETL:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "Incluye:\n",
    "- Descripci\u00f3n general\n",
    "- Arquitectura (diagrama ASCII)\n",
    "- Requisitos (dependencias)\n",
    "- Configuraci\u00f3n\n",
    "- C\u00f3mo ejecutar\n",
    "- Monitoreo y troubleshooting\n",
    "- Contacto/owner\n",
    "\n",
    "Markdown:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "documentation = generate_documentation(requirements, etl_code)\n",
    "print('Documentaci\u00f3n generada (preview):')\n",
    "print(documentation[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89813a88",
   "metadata": {},
   "source": [
    "## Parte 8: DAG de Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f80052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_airflow_dag(requirements: dict, etl_code: str) -> str:\n",
    "    \"\"\"Genera DAG de Airflow.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera un DAG de Airflow para este pipeline:\n",
    "\n",
    "Requisitos:\n",
    "{json.dumps(requirements, indent=2)}\n",
    "\n",
    "El DAG debe:\n",
    "- Nombre descriptivo\n",
    "- Schedule seg\u00fan requisitos\n",
    "- Tasks: extract, validate, transform, load\n",
    "- Retry logic\n",
    "- Alertas por email si falla\n",
    "\n",
    "C\u00f3digo Python del DAG:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "dag_code = generate_airflow_dag(requirements, etl_code)\n",
    "print('DAG de Airflow (preview):')\n",
    "print(dag_code[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ea627",
   "metadata": {},
   "source": [
    "## Parte 9: Workflow con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    user_input: str\n",
    "    requirements: dict\n",
    "    code: str\n",
    "    validation: dict\n",
    "    tests: str\n",
    "    docs: str\n",
    "    dag: str\n",
    "    approved: bool\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def parse_node(state: PipelineState) -> PipelineState:\n",
    "    state['requirements'] = parse_requirements(state['user_input'])\n",
    "    return state\n",
    "\n",
    "def generate_node(state: PipelineState) -> PipelineState:\n",
    "    state['code'] = generate_etl_pipeline(state['requirements'])\n",
    "    return state\n",
    "\n",
    "def validate_node(state: PipelineState) -> PipelineState:\n",
    "    state['validation'] = validate_code(state['code'])\n",
    "    if not state['validation']['valid']:\n",
    "        state['errors'].extend([i['message'] for i in state['validation']['issues']])\n",
    "    return state\n",
    "\n",
    "def test_node(state: PipelineState) -> PipelineState:\n",
    "    if state['validation']['valid']:\n",
    "        state['tests'] = generate_tests(state['code'], state['requirements'])\n",
    "    return state\n",
    "\n",
    "def document_node(state: PipelineState) -> PipelineState:\n",
    "    state['docs'] = generate_documentation(state['requirements'], state['code'])\n",
    "    state['dag'] = generate_airflow_dag(state['requirements'], state['code'])\n",
    "    return state\n",
    "\n",
    "def review_node(state: PipelineState) -> PipelineState:\n",
    "    # En producci\u00f3n: human-in-the-loop\n",
    "    state['approved'] = state['validation']['valid']\n",
    "    return state\n",
    "\n",
    "# Construir grafo\n",
    "workflow = StateGraph(PipelineState)\n",
    "workflow.add_node('parse', parse_node)\n",
    "workflow.add_node('generate', generate_node)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('test', test_node)\n",
    "workflow.add_node('document', document_node)\n",
    "workflow.add_node('review', review_node)\n",
    "\n",
    "workflow.set_entry_point('parse')\n",
    "workflow.add_edge('parse', 'generate')\n",
    "workflow.add_edge('generate', 'validate')\n",
    "workflow.add_edge('validate', 'test')\n",
    "workflow.add_edge('test', 'document')\n",
    "workflow.add_edge('document', 'review')\n",
    "workflow.add_edge('review', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar workflow completo\n",
    "initial_state = {\n",
    "    'user_input': user_request,\n",
    "    'requirements': {},\n",
    "    'code': '',\n",
    "    'validation': {},\n",
    "    'tests': '',\n",
    "    'docs': '',\n",
    "    'dag': '',\n",
    "    'approved': False,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print('\\n\ud83c\udf89 Pipeline generado completamente\\n')\n",
    "print(f\"Aprobado: {'\u2705' if final_state['approved'] else '\u274c'}\")\n",
    "print(f\"Errores: {len(final_state['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4610a91",
   "metadata": {},
   "source": [
    "## Parte 10: Interfaz Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como platform_app.py\n",
    "\n",
    "streamlit_app = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from pipeline_generator import app, PipelineState  # Importar workflow\n",
    "\n",
    "st.set_page_config(page_title='GenAI Data Platform', page_icon='\ud83c\udfd7\ufe0f', layout='wide')\n",
    "\n",
    "st.title('\ud83c\udfd7\ufe0f Plataforma Self-Service de Pipelines')\n",
    "st.markdown('Genera pipelines ETL completos usando lenguaje natural')\n",
    "\n",
    "# Input\n",
    "user_input = st.text_area(\n",
    "    'Describe tu pipeline:',\n",
    "    height=200,\n",
    "    placeholder='Ej: Necesito procesar datos de ventas desde S3, agregar por mes, y cargar a Redshift...'\n",
    ")\n",
    "\n",
    "if st.button('\ud83d\ude80 Generar Pipeline', type='primary'):\n",
    "    if not user_input:\n",
    "        st.warning('Por favor describe el pipeline')\n",
    "    else:\n",
    "        with st.spinner('Generando pipeline completo...'):\n",
    "            initial = {\n",
    "                'user_input': user_input,\n",
    "                'requirements': {}, 'code': '', 'validation': {},\n",
    "                'tests': '', 'docs': '', 'dag': '',\n",
    "                'approved': False, 'errors': []\n",
    "            }\n",
    "            \n",
    "            result = app.invoke(initial)\n",
    "            \n",
    "            # Tabs\n",
    "            tab1, tab2, tab3, tab4, tab5 = st.tabs(['\ud83d\udccb Requisitos', '\ud83d\udcbb C\u00f3digo', '\ud83e\uddea Tests', '\ud83d\udcc4 Docs', '\ud83d\udeeb DAG'])\n",
    "            \n",
    "            with tab1:\n",
    "                st.json(result['requirements'])\n",
    "            \n",
    "            with tab2:\n",
    "                st.code(result['code'], language='python')\n",
    "                st.download_button('Descargar c\u00f3digo', result['code'], file_name='pipeline.py')\n",
    "            \n",
    "            with tab3:\n",
    "                st.code(result['tests'], language='python')\n",
    "            \n",
    "            with tab4:\n",
    "                st.markdown(result['docs'])\n",
    "            \n",
    "            with tab5:\n",
    "                st.code(result['dag'], language='python')\n",
    "            \n",
    "            # Validaci\u00f3n\n",
    "            if result['approved']:\n",
    "                st.success('\u2705 Pipeline aprobado y listo para deployment')\n",
    "            else:\n",
    "                st.error(f\"\u274c Pipeline con errores: {result['errors']}\")\n",
    "'''\n",
    "\n",
    "with open('platform_app.py', 'w') as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print('\u2705 Plataforma Streamlit guardada en platform_app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4251",
   "metadata": {},
   "source": [
    "## Parte 11: Sistema de deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def deploy_pipeline(code: str, dag: str, tests: str, pipeline_name: str):\n",
    "    \"\"\"Despliega pipeline a producci\u00f3n.\"\"\"\n",
    "    # 1. Crear estructura de directorios\n",
    "    base_path = f'./pipelines/{pipeline_name}'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # 2. Guardar archivos\n",
    "    with open(f'{base_path}/pipeline.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    with open(f'{base_path}/test_pipeline.py', 'w') as f:\n",
    "        f.write(tests)\n",
    "    \n",
    "    with open(f'{base_path}/dag.py', 'w') as f:\n",
    "        f.write(dag)\n",
    "    \n",
    "    # 3. Ejecutar tests\n",
    "    test_result = subprocess.run(\n",
    "        ['pytest', f'{base_path}/test_pipeline.py', '-v'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if test_result.returncode != 0:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': 'Tests fallidos',\n",
    "            'output': test_result.stdout\n",
    "        }\n",
    "    \n",
    "    # 4. Copiar DAG a Airflow\n",
    "    # airflow_dags_path = '/opt/airflow/dags/'\n",
    "    # shutil.copy(f'{base_path}/dag.py', f'{airflow_dags_path}/{pipeline_name}.py')\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'message': f'Pipeline {pipeline_name} desplegado exitosamente',\n",
    "        'path': base_path\n",
    "    }\n",
    "\n",
    "# Deployment\n",
    "if final_state['approved']:\n",
    "    deploy_result = deploy_pipeline(\n",
    "        code=final_state['code'],\n",
    "        dag=final_state['dag'],\n",
    "        tests=final_state['tests'],\n",
    "        pipeline_name=final_state['requirements']['pipeline_name']\n",
    "    )\n",
    "    print(deploy_result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8ff73",
   "metadata": {},
   "source": [
    "### \ud83d\ude80 **Deployment y Producci\u00f3n: De Prototipo a Enterprise**\n",
    "\n",
    "**CI/CD Pipeline para GenAI Artifacts**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502               PRODUCTION DEPLOYMENT PIPELINE               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                            \u2502\n",
    "\u2502  [1. Code Generation] \u2192 Generated pipeline code           \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [2. Version Control]                                      \u2502\n",
    "\u2502      \u2022 Create feature branch                              \u2502\n",
    "\u2502      \u2022 Commit: code + tests + docs                        \u2502\n",
    "\u2502      \u2022 Tag: v1.0.0                                        \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [3. Automated Testing] (GitHub Actions/GitLab CI)        \u2502\n",
    "\u2502      \u2022 pytest: unit + integration tests                   \u2502\n",
    "\u2502      \u2022 coverage: \u226580% required                            \u2502\n",
    "\u2502      \u2022 security: Bandit, Safety                           \u2502\n",
    "\u2502      \u2022 linting: Black, Flake8, MyPy                       \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [4. Quality Gates]                                        \u2502\n",
    "\u2502      \u2705 All tests pass                                     \u2502\n",
    "\u2502      \u2705 Coverage \u226580%                                      \u2502\n",
    "\u2502      \u2705 No critical security issues                       \u2502\n",
    "\u2502      \u2705 Code review approved                              \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [5. Staging Deployment]                                   \u2502\n",
    "\u2502      \u2022 Deploy to staging environment                      \u2502\n",
    "\u2502      \u2022 Run integration tests with staging data            \u2502\n",
    "\u2502      \u2022 Performance benchmarking                           \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [6. Production Approval]                                  \u2502\n",
    "\u2502      \u2022 Manual sign-off (for critical pipelines)          \u2502\n",
    "\u2502      \u2022 Automated (for low-risk changes)                   \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [7. Production Deployment]                                \u2502\n",
    "\u2502      \u2022 Blue/Green deployment (zero downtime)              \u2502\n",
    "\u2502      \u2022 Copy DAG to Airflow dags/ folder                   \u2502\n",
    "\u2502      \u2022 Update Airflow variables                           \u2502\n",
    "\u2502      \u2022 Trigger initial run                                \u2502\n",
    "\u2502           \u2193                                                \u2502\n",
    "\u2502  [8. Monitoring]                                           \u2502\n",
    "\u2502      \u2022 Datadog/New Relic APM                              \u2502\n",
    "\u2502      \u2022 Custom metrics (rows processed, runtime)           \u2502\n",
    "\u2502      \u2022 Alerting (PagerDuty, Slack)                        \u2502\n",
    "\u2502                                                            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**GitHub Actions Workflow**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/deploy-pipeline.yml\n",
    "name: Deploy Generated Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'generated_pipelines/**'\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: '3.11'\n",
    "  COVERAGE_THRESHOLD: 80\n",
    "\n",
    "jobs:\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov bandit safety black flake8 mypy\n",
    "      \n",
    "      - name: Security scan\n",
    "        run: |\n",
    "          # Scan dependencies\n",
    "          safety check --json\n",
    "          \n",
    "          # Scan code for vulnerabilities\n",
    "          bandit -r generated_pipelines/ -f json -o bandit-report.json\n",
    "      \n",
    "      - name: Code quality\n",
    "        run: |\n",
    "          # Format check\n",
    "          black --check generated_pipelines/\n",
    "          \n",
    "          # Linting\n",
    "          flake8 generated_pipelines/ --max-line-length=100\n",
    "          \n",
    "          # Type checking\n",
    "          mypy generated_pipelines/ --strict\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          pytest generated_pipelines/ \\\n",
    "            --cov=generated_pipelines \\\n",
    "            --cov-report=xml \\\n",
    "            --cov-report=html \\\n",
    "            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }}\n",
    "      \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "  \n",
    "  deploy-staging:\n",
    "    needs: validate\n",
    "    if: github.event_name == 'pull_request'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Deploy to staging Airflow\n",
    "        env:\n",
    "          AIRFLOW_STAGING_HOST: ${{ secrets.AIRFLOW_STAGING_HOST }}\n",
    "          AIRFLOW_API_KEY: ${{ secrets.AIRFLOW_API_KEY }}\n",
    "        run: |\n",
    "          # Copy DAG to staging\n",
    "          scp generated_pipelines/*/dag.py \\\n",
    "            user@$AIRFLOW_STAGING_HOST:/opt/airflow/dags/\n",
    "          \n",
    "          # Trigger DAG\n",
    "          curl -X POST \\\n",
    "            -H \"Authorization: Bearer $AIRFLOW_API_KEY\" \\\n",
    "            https://$AIRFLOW_STAGING_HOST/api/v1/dags/pipeline_name/dagRuns\n",
    "      \n",
    "      - name: Integration tests (staging)\n",
    "        run: |\n",
    "          pytest tests/integration/ \\\n",
    "            --env=staging \\\n",
    "            --pipeline=pipeline_name\n",
    "  \n",
    "  deploy-production:\n",
    "    needs: validate\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    environment:\n",
    "      name: production\n",
    "      url: https://airflow.prod.company.com\n",
    "    steps:\n",
    "      - name: Blue/Green deployment\n",
    "        run: |\n",
    "          # Deploy to \"green\" environment first\n",
    "          ./scripts/deploy.sh --env=prod-green --pipeline=$PIPELINE_NAME\n",
    "          \n",
    "          # Run smoke tests\n",
    "          ./scripts/smoke-tests.sh --env=prod-green\n",
    "          \n",
    "          # Switch traffic to green (zero downtime)\n",
    "          ./scripts/switch-traffic.sh --to=green\n",
    "          \n",
    "          # Decommission blue\n",
    "          ./scripts/cleanup.sh --env=prod-blue\n",
    "      \n",
    "      - name: Notify\n",
    "        uses: 8398a7/action-slack@v3\n",
    "        with:\n",
    "          status: ${{ job.status }}\n",
    "          text: 'Pipeline deployed to production'\n",
    "          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "```\n",
    "\n",
    "**Deployment Automation Script**\n",
    "\n",
    "```python\n",
    "# scripts/deploy_pipeline.py\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "class PipelineDeployer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.airflow_api = config['airflow_api_url']\n",
    "        self.airflow_token = config['airflow_token']\n",
    "    \n",
    "    def deploy(self, pipeline_path: str, environment: str = 'production'):\n",
    "        \"\"\"Despliega pipeline a Airflow\"\"\"\n",
    "        \n",
    "        # 1. Validar que existan archivos necesarios\n",
    "        required_files = ['pipeline.py', 'dag.py', 'tests.py', 'README.md']\n",
    "        for file in required_files:\n",
    "            if not Path(f\"{pipeline_path}/{file}\").exists():\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "        \n",
    "        # 2. Ejecutar tests\n",
    "        print(\"Running tests...\")\n",
    "        test_result = subprocess.run(\n",
    "            ['pytest', f'{pipeline_path}/tests.py', '-v'],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if test_result.returncode != 0:\n",
    "            raise RuntimeError(f\"Tests failed:\\n{test_result.stdout}\")\n",
    "        \n",
    "        print(\"\u2705 Tests passed\")\n",
    "        \n",
    "        # 3. Upload assets to S3\n",
    "        print(\"Uploading to S3...\")\n",
    "        pipeline_name = Path(pipeline_path).name\n",
    "        \n",
    "        for file in ['pipeline.py', 'README.md']:\n",
    "            self.s3.upload_file(\n",
    "                f'{pipeline_path}/{file}',\n",
    "                self.config['artifacts_bucket'],\n",
    "                f'pipelines/{pipeline_name}/{file}'\n",
    "            )\n",
    "        \n",
    "        # 4. Deploy DAG to Airflow\n",
    "        print(f\"Deploying to Airflow ({environment})...\")\n",
    "        \n",
    "        # Airflow API: upload DAG\n",
    "        with open(f'{pipeline_path}/dag.py', 'r') as f:\n",
    "            dag_code = f.read()\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}/upload\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'dag_code': dag_code}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Airflow deployment failed: {response.text}\")\n",
    "        \n",
    "        # 5. Set Airflow variables\n",
    "        variables = {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            's3_bucket': self.config['data_bucket'],\n",
    "            'db_conn_id': 'postgres_default',\n",
    "            'owner_email': self.config['owner_email']\n",
    "        }\n",
    "        \n",
    "        for key, value in variables.items():\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/variables\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'key': f'{pipeline_name}_{key}', 'value': value}\n",
    "            )\n",
    "        \n",
    "        # 6. Unpause DAG\n",
    "        requests.patch(\n",
    "            f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "            json={'is_paused': False}\n",
    "        )\n",
    "        \n",
    "        # 7. Trigger initial run (optional)\n",
    "        if self.config.get('trigger_on_deploy'):\n",
    "            requests.post(\n",
    "                f\"{self.airflow_api}/dags/{pipeline_name}/dagRuns\",\n",
    "                headers={'Authorization': f'Bearer {self.airflow_token}'},\n",
    "                json={'conf': {'deployed_at': str(datetime.now())}}\n",
    "            )\n",
    "        \n",
    "        print(f\"\u2705 Pipeline {pipeline_name} deployed successfully\")\n",
    "        \n",
    "        return {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            'environment': environment,\n",
    "            'dag_url': f\"{self.airflow_api}/dags/{pipeline_name}\",\n",
    "            's3_path': f's3://{self.config[\"artifacts_bucket\"]}/pipelines/{pipeline_name}'\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "deployer = PipelineDeployer({\n",
    "    'airflow_api_url': 'https://airflow.company.com/api/v1',\n",
    "    'airflow_token': os.getenv('AIRFLOW_TOKEN'),\n",
    "    'artifacts_bucket': 'company-pipeline-artifacts',\n",
    "    'data_bucket': 'company-data',\n",
    "    'owner_email': 'data-team@company.com',\n",
    "    'trigger_on_deploy': True\n",
    "})\n",
    "\n",
    "result = deployer.deploy('./generated_pipelines/ventas_pipeline', environment='production')\n",
    "```\n",
    "\n",
    "**Monitoring y Observabilidad**\n",
    "\n",
    "```python\n",
    "# monitoring/pipeline_metrics.py\n",
    "from datadog import initialize, statsd\n",
    "import time\n",
    "\n",
    "initialize(api_key=os.getenv('DATADOG_API_KEY'))\n",
    "\n",
    "class PipelineMetrics:\n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "    \n",
    "    def track_execution(self, func):\n",
    "        \"\"\"Decorator para trackear m\u00e9tricas de ejecuci\u00f3n\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Incrementar contador de ejecuciones\n",
    "            statsd.increment(\n",
    "                'pipeline.executions',\n",
    "                tags=[f'pipeline:{self.pipeline_name}']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # M\u00e9trica de \u00e9xito\n",
    "                statsd.increment(\n",
    "                    'pipeline.success',\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                # M\u00e9trica de error\n",
    "                statsd.increment(\n",
    "                    'pipeline.errors',\n",
    "                    tags=[\n",
    "                        f'pipeline:{self.pipeline_name}',\n",
    "                        f'error_type:{type(e).__name__}'\n",
    "                    ]\n",
    "                )\n",
    "                raise\n",
    "            \n",
    "            finally:\n",
    "                # Duraci\u00f3n\n",
    "                duration = time.time() - start_time\n",
    "                statsd.histogram(\n",
    "                    'pipeline.duration',\n",
    "                    duration,\n",
    "                    tags=[f'pipeline:{self.pipeline_name}']\n",
    "                )\n",
    "        \n",
    "        return wrapper\n",
    "    \n",
    "    def track_data_volume(self, rows: int, stage: str):\n",
    "        \"\"\"Track volumen de datos procesados\"\"\"\n",
    "        statsd.gauge(\n",
    "            'pipeline.data_volume',\n",
    "            rows,\n",
    "            tags=[\n",
    "                f'pipeline:{self.pipeline_name}',\n",
    "                f'stage:{stage}'\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def track_data_quality(self, metric: str, value: float):\n",
    "        \"\"\"Track m\u00e9tricas de calidad de datos\"\"\"\n",
    "        statsd.gauge(\n",
    "            f'pipeline.data_quality.{metric}',\n",
    "            value,\n",
    "            tags=[f'pipeline:{self.pipeline_name}']\n",
    "        )\n",
    "\n",
    "# Usage en pipeline generado\n",
    "metrics = PipelineMetrics('ventas_pipeline')\n",
    "\n",
    "@metrics.track_execution\n",
    "def run_pipeline():\n",
    "    # Extract\n",
    "    df = extract_data()\n",
    "    metrics.track_data_volume(len(df), 'extract')\n",
    "    \n",
    "    # Transform\n",
    "    df = transform_data(df)\n",
    "    metrics.track_data_volume(len(df), 'transform')\n",
    "    \n",
    "    # Validate\n",
    "    null_rate = df.isnull().sum().sum() / df.size\n",
    "    metrics.track_data_quality('null_rate', null_rate)\n",
    "    \n",
    "    # Load\n",
    "    load_data(df)\n",
    "    metrics.track_data_volume(len(df), 'load')\n",
    "```\n",
    "\n",
    "**Alerting Strategy**\n",
    "\n",
    "```python\n",
    "# monitoring/alerts.py\n",
    "from slack_sdk import WebClient\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "class AlertManager:\n",
    "    def __init__(self):\n",
    "        self.slack = WebClient(token=os.getenv('SLACK_TOKEN'))\n",
    "        self.pagerduty_key = os.getenv('PAGERDUTY_KEY')\n",
    "    \n",
    "    def send_alert(\n",
    "        self,\n",
    "        severity: str,  # 'info' | 'warning' | 'critical'\n",
    "        pipeline_name: str,\n",
    "        message: str,\n",
    "        details: dict = None\n",
    "    ):\n",
    "        \"\"\"Env\u00eda alertas seg\u00fan severidad\"\"\"\n",
    "        \n",
    "        if severity == 'info':\n",
    "            # Slack notification\n",
    "            self._send_slack(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'warning':\n",
    "            # Slack + Email\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "        \n",
    "        elif severity == 'critical':\n",
    "            # Slack + Email + PagerDuty\n",
    "            self._send_slack(pipeline_name, message, details, urgent=True)\n",
    "            self._send_email(pipeline_name, message, details)\n",
    "            self._trigger_pagerduty(pipeline_name, message, details)\n",
    "    \n",
    "    def _send_slack(self, pipeline, message, details, urgent=False):\n",
    "        emoji = '\ud83d\udea8' if urgent else '\u2139\ufe0f'\n",
    "        \n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\"type\": \"plain_text\", \"text\": f\"{emoji} {pipeline}\"}\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\"type\": \"mrkdwn\", \"text\": message}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        if details:\n",
    "            blocks.append({\n",
    "                \"type\": \"section\",\n",
    "                \"fields\": [\n",
    "                    {\"type\": \"mrkdwn\", \"text\": f\"*{k}:*\\n{v}\"}\n",
    "                    for k, v in details.items()\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        self.slack.chat_postMessage(\n",
    "            channel='#data-alerts',\n",
    "            blocks=blocks\n",
    "        )\n",
    "    \n",
    "    def _trigger_pagerduty(self, pipeline, message, details):\n",
    "        import requests\n",
    "        \n",
    "        requests.post(\n",
    "            'https://events.pagerduty.com/v2/enqueue',\n",
    "            json={\n",
    "                'routing_key': self.pagerduty_key,\n",
    "                'event_action': 'trigger',\n",
    "                'payload': {\n",
    "                    'summary': f'{pipeline}: {message}',\n",
    "                    'severity': 'critical',\n",
    "                    'source': 'genai-platform',\n",
    "                    'custom_details': details\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Integraci\u00f3n en pipeline\n",
    "alerts = AlertManager()\n",
    "\n",
    "try:\n",
    "    run_pipeline()\n",
    "except Exception as e:\n",
    "    alerts.send_alert(\n",
    "        severity='critical',\n",
    "        pipeline_name='ventas_pipeline',\n",
    "        message=f'Pipeline failed: {str(e)}',\n",
    "        details={\n",
    "            'Error Type': type(e).__name__,\n",
    "            'Traceback': traceback.format_exc()[:500]\n",
    "        }\n",
    "    )\n",
    "    raise\n",
    "```\n",
    "\n",
    "**Cost Optimization**\n",
    "\n",
    "```python\n",
    "# monitoring/cost_tracking.py\n",
    "class CostTracker:\n",
    "    \"\"\"Track costos de ejecuci\u00f3n de pipelines\"\"\"\n",
    "    \n",
    "    COST_PER_COMPUTE_HOUR = 0.50  # USD\n",
    "    COST_PER_GB_STORAGE = 0.023   # USD/mes\n",
    "    COST_PER_MILLION_REQUESTS = 0.20  # USD\n",
    "    \n",
    "    def calculate_pipeline_cost(\n",
    "        self,\n",
    "        runtime_minutes: float,\n",
    "        data_volume_gb: float,\n",
    "        api_calls: int\n",
    "    ) -> dict:\n",
    "        \n",
    "        compute_cost = (runtime_minutes / 60) * self.COST_PER_COMPUTE_HOUR\n",
    "        storage_cost = data_volume_gb * self.COST_PER_GB_STORAGE\n",
    "        api_cost = (api_calls / 1_000_000) * self.COST_PER_MILLION_REQUESTS\n",
    "        \n",
    "        total = compute_cost + storage_cost + api_cost\n",
    "        \n",
    "        return {\n",
    "            'compute': round(compute_cost, 4),\n",
    "            'storage': round(storage_cost, 4),\n",
    "            'api': round(api_cost, 4),\n",
    "            'total': round(total, 4)\n",
    "        }\n",
    "    \n",
    "    def optimize_suggestions(self, metrics: dict) -> list:\n",
    "        \"\"\"Sugerencias de optimizaci\u00f3n\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if metrics['runtime_minutes'] > 60:\n",
    "            suggestions.append(\"Consider partitioning data for parallel processing\")\n",
    "        \n",
    "        if metrics['data_volume_gb'] > 100:\n",
    "            suggestions.append(\"Use columnar format (Parquet) to reduce storage\")\n",
    "        \n",
    "        if metrics['api_calls'] > 1_000_000:\n",
    "            suggestions.append(\"Implement caching to reduce API calls\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# En dashboard Streamlit\n",
    "cost_tracker = CostTracker()\n",
    "\n",
    "st.subheader('\ud83d\udcb0 Cost Analysis')\n",
    "costs = cost_tracker.calculate_pipeline_cost(\n",
    "    runtime_minutes=15,\n",
    "    data_volume_gb=50,\n",
    "    api_calls=100_000\n",
    ")\n",
    "\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Compute', f\"${costs['compute']}\")\n",
    "col2.metric('Storage', f\"${costs['storage']}\")\n",
    "col3.metric('API', f\"${costs['api']}\")\n",
    "col4.metric('Total', f\"${costs['total']}\", delta='-12%')\n",
    "\n",
    "suggestions = cost_tracker.optimize_suggestions(metrics)\n",
    "if suggestions:\n",
    "    st.info('\\n'.join(suggestions))\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cf2e",
   "metadata": {},
   "source": [
    "## Parte 12: Monitoreo y observabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_dashboard(pipeline_name: str):\n",
    "    \"\"\"Genera dashboard de monitoreo.\"\"\"\n",
    "    dashboard_code = f'''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "st.title('\ud83d\udcca Monitoreo: {pipeline_name}')\n",
    "\n",
    "# M\u00e9tricas simuladas\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric('Ejecuciones exitosas', '95%', '\u2191 2%')\n",
    "col2.metric('Tiempo promedio', '12.5 min', '\u2193 1.2 min')\n",
    "col3.metric('Registros procesados', '1.2M', '\u2191 15K')\n",
    "col4.metric('Errores', '3', '\u2193 5')\n",
    "\n",
    "# Gr\u00e1fico de ejecuciones\n",
    "df_runs = pd.DataFrame({{\n",
    "    'fecha': pd.date_range('2024-01-01', periods=30),\n",
    "    'duracion': [10 + i*0.2 for i in range(30)],\n",
    "    'status': ['success'] * 28 + ['failed'] * 2\n",
    "}}})\n",
    "\n",
    "fig = px.line(df_runs, x='fecha', y='duracion', color='status')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "# Logs recientes\n",
    "st.subheader('Logs recientes')\n",
    "st.text_area('', value='2024-01-15 02:00 - [INFO] Pipeline iniciado\\\\n2024-01-15 02:05 - [INFO] 10000 registros procesados', height=200)\n",
    "'''\n",
    "    \n",
    "    return dashboard_code\n",
    "\n",
    "monitoring_dash = generate_monitoring_dashboard(requirements['pipeline_name'])\n",
    "print('Dashboard de monitoreo generado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b578ed",
   "metadata": {},
   "source": [
    "## Parte 13: Mejoras futuras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179193c",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "**Fase 2**:\n",
    "- Multi-agente: equipo de agentes especializados (data engineer, QA, DevOps)\n",
    "- Cost estimation: predecir costos de infraestructura\n",
    "- A/B testing: comparar versiones de pipelines\n",
    "- Auto-scaling: ajustar recursos seg\u00fan carga\n",
    "\n",
    "**Fase 3**:\n",
    "- Self-healing: detecci\u00f3n y correcci\u00f3n autom\u00e1tica de errores\n",
    "- Optimization: sugerencias de mejora basadas en m\u00e9tricas\n",
    "- Federated learning: aprender de pipelines de otros equipos\n",
    "- Natural language alerting: alertas explicadas en lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dba29",
   "metadata": {},
   "source": [
    "## Evaluaci\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27d809",
   "metadata": {},
   "source": [
    "**Criterios**:\n",
    "\n",
    "- \u2705 Parsing correcto de requisitos (15%)\n",
    "- \u2705 Generaci\u00f3n de c\u00f3digo funcional (25%)\n",
    "- \u2705 Validaci\u00f3n y seguridad (15%)\n",
    "- \u2705 Tests completos (15%)\n",
    "- \u2705 Documentaci\u00f3n clara (10%)\n",
    "- \u2705 Workflow orquestado (10%)\n",
    "- \u2705 Interfaz usable (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d050fa4",
   "metadata": {},
   "source": [
    "## Conclusi\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0905e",
   "metadata": {},
   "source": [
    "Has construido una plataforma completa de self-service que:\n",
    "\n",
    "\u2705 Democratiza el acceso a la ingenier\u00eda de datos\n",
    "\u2705 Reduce tiempo de desarrollo de d\u00edas a minutos\n",
    "\u2705 Mantiene est\u00e1ndares de calidad y seguridad\n",
    "\u2705 Genera documentaci\u00f3n autom\u00e1ticamente\n",
    "\u2705 Facilita deployment y monitoreo\n",
    "\n",
    "**\u00a1Felicitaciones por completar el m\u00f3dulo de GenAI!** \ud83c\udf89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d22e7",
   "metadata": {},
   "source": [
    "### \ud83c\udf93 **Conclusi\u00f3n del Curso: De Junior a GenAI Expert**\n",
    "\n",
    "**El Viaje Completo: 40 Notebooks, 4 Niveles**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502           DATA ENGINEER COURSE: LEARNING PATH                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                              \u2502\n",
    "\u2502  \ud83d\udcda NIVEL JUNIOR (10 notebooks)                              \u2502\n",
    "\u2502  \u251c\u2500 Fundamentos: Python, Pandas, SQL                        \u2502\n",
    "\u2502  \u251c\u2500 Data Manipulation: ETL b\u00e1sico                           \u2502\n",
    "\u2502  \u251c\u2500 Visualizaci\u00f3n: Matplotlib, Seaborn                      \u2502\n",
    "\u2502  \u2514\u2500 Git: Control de versiones                               \u2502\n",
    "\u2502      \u2193                                                       \u2502\n",
    "\u2502  \ud83d\udd27 NIVEL MID (10 notebooks)                                 \u2502\n",
    "\u2502  \u251c\u2500 Airflow: Orquestaci\u00f3n de pipelines                      \u2502\n",
    "\u2502  \u251c\u2500 Streaming: Kafka, real-time processing                  \u2502\n",
    "\u2502  \u251c\u2500 Cloud: AWS (S3, Redshift, Lambda)                       \u2502\n",
    "\u2502  \u251c\u2500 Bases de Datos: PostgreSQL, MongoDB                     \u2502\n",
    "\u2502  \u2514\u2500 FastAPI: Servicios de datos                             \u2502\n",
    "\u2502      \u2193                                                       \u2502\n",
    "\u2502  \u26a1 NIVEL SENIOR (10 notebooks)                              \u2502\n",
    "\u2502  \u251c\u2500 Data Governance: Calidad, linaje                        \u2502\n",
    "\u2502  \u251c\u2500 Lakehouse: Delta, Iceberg                               \u2502\n",
    "\u2502  \u251c\u2500 Spark Streaming: Processing distribuido                 \u2502\n",
    "\u2502  \u251c\u2500 Arquitecturas: Medallion, Lambda, Kappa                 \u2502\n",
    "\u2502  \u251c\u2500 ML Pipelines: Feature stores, MLOps                     \u2502\n",
    "\u2502  \u2514\u2500 FinOps: Optimizaci\u00f3n de costos                          \u2502\n",
    "\u2502      \u2193                                                       \u2502\n",
    "\u2502  \ud83e\udd16 NIVEL GENAI (10 notebooks)                               \u2502\n",
    "\u2502  \u251c\u2500 01: Fundamentos LLMs & Prompting                        \u2502\n",
    "\u2502  \u251c\u2500 02: NL2SQL (Natural Language \u2192 SQL)                     \u2502\n",
    "\u2502  \u251c\u2500 03: Generaci\u00f3n de C\u00f3digo ETL                            \u2502\n",
    "\u2502  \u251c\u2500 04: RAG para Documentaci\u00f3n de Datos                     \u2502\n",
    "\u2502  \u251c\u2500 05: Embeddings & Similitud de Datos                     \u2502\n",
    "\u2502  \u251c\u2500 06: Agentes de Automatizaci\u00f3n                           \u2502\n",
    "\u2502  \u251c\u2500 07: Calidad y Validaci\u00f3n con LLMs                       \u2502\n",
    "\u2502  \u251c\u2500 08: S\u00edntesis y Aumento de Datos                         \u2502\n",
    "\u2502  \u251c\u2500 09: Proyecto Integrador 1 (RAG+NL2SQL Chatbot)          \u2502\n",
    "\u2502  \u2514\u2500 10: Proyecto Integrador 2 (Plataforma Self-Service) \u2705  \u2502\n",
    "\u2502                                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Habilidades Adquiridas por Nivel**\n",
    "\n",
    "| Nivel | Skills Core | Tools Dominadas | Capacidad Laboral |\n",
    "|-------|-------------|-----------------|-------------------|\n",
    "| **Junior** | \u2022 Python (Pandas, NumPy)<br>\u2022 SQL b\u00e1sico<br>\u2022 ETL manual<br>\u2022 Git | Jupyter, Pandas, PostgreSQL, Git | Entry-level Data Engineer<br>Data Analyst |\n",
    "| **Mid** | \u2022 Airflow DAGs<br>\u2022 Streaming (Kafka)<br>\u2022 Cloud (AWS)<br>\u2022 APIs (FastAPI) | Airflow, Kafka, AWS, Docker, FastAPI, MongoDB | Mid-level Data Engineer<br>Data Platform Engineer |\n",
    "| **Senior** | \u2022 Data Architecture<br>\u2022 Distributed Systems (Spark)<br>\u2022 Lakehouse (Delta/Iceberg)<br>\u2022 MLOps | Spark, Delta Lake, Databricks, Kubernetes, Terraform | Senior Data Engineer<br>Data Architect<br>Staff Engineer |\n",
    "| **GenAI** | \u2022 LLM Integration<br>\u2022 RAG Systems<br>\u2022 Agentic Workflows<br>\u2022 Code Generation | OpenAI API, LangChain, ChromaDB, LangGraph, Streamlit | GenAI Data Engineer<br>ML Platform Engineer<br>AI Automation Specialist |\n",
    "\n",
    "**Comparaci\u00f3n: Skills Tradicionales vs GenAI-Enhanced**\n",
    "\n",
    "```\n",
    "TRADITIONAL DATA ENGINEER              GENAI-ENHANCED DATA ENGINEER\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                          \u2502          \u2502                              \u2502\n",
    "\u2502 \u2022 Manual coding          \u2502   \u2192      \u2502 \u2022 AI-assisted coding         \u2502\n",
    "\u2502 \u2022 Static pipelines       \u2502   \u2192      \u2502 \u2022 Self-adapting pipelines    \u2502\n",
    "\u2502 \u2022 Manual documentation   \u2502   \u2192      \u2502 \u2022 Auto-generated docs        \u2502\n",
    "\u2502 \u2022 Fixed schemas          \u2502   \u2192      \u2502 \u2022 Schema inference           \u2502\n",
    "\u2502 \u2022 SQL only               \u2502   \u2192      \u2502 \u2022 NL\u2192SQL translation         \u2502\n",
    "\u2502 \u2022 Manual data quality    \u2502   \u2192      \u2502 \u2022 LLM-powered validation     \u2502\n",
    "\u2502 \u2022 Code reviews (human)   \u2502   \u2192      \u2502 \u2022 Multi-agent reviews        \u2502\n",
    "\u2502 \u2022 Static dashboards      \u2502   \u2192      \u2502 \u2022 Conversational analytics   \u2502\n",
    "\u2502                          \u2502          \u2502                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "   Productivity: 1x                       Productivity: 3-5x\n",
    "   Time to Market: weeks                  Time to Market: hours\n",
    "   Error Rate: 5-10%                      Error Rate: 1-2%\n",
    "```\n",
    "\n",
    "**Proyecto Final: Impacto Real**\n",
    "\n",
    "Este proyecto integrador 2 (Plataforma Self-Service) representa el **m\u00e1ximo nivel de automatizaci\u00f3n** en Data Engineering:\n",
    "\n",
    "**Sin GenAI (Tradicional)**:\n",
    "```\n",
    "Usuario \u2192 escribe ticket \u2192 DE recibe \u2192 DE desarrolla (2-4 semanas) \n",
    "\u2192 QA testing (1 semana) \u2192 Code review \u2192 Deploy \u2192 Usuario recibe\n",
    "```\n",
    "**Con GenAI (Plataforma)**:\n",
    "```\n",
    "Usuario \u2192 describe en NL \u2192 Plataforma genera (10 min) \u2192 Aprobaci\u00f3n \n",
    "\u2192 Deploy autom\u00e1tico \u2192 Usuario recibe\n",
    "```\n",
    "\n",
    "**M\u00e9tricas de Transformaci\u00f3n**:\n",
    "- \u23f1\ufe0f **Time to Production**: 3-4 semanas \u2192 2 horas (96% reducci\u00f3n)\n",
    "- \ud83d\udcb0 **Costo por Pipeline**: $5,100 \u2192 $163 (97% reducci\u00f3n)\n",
    "- \ud83d\udc65 **Democratizaci\u00f3n**: Solo DEs \u2192 Cualquier usuario\n",
    "- \ud83d\udcca **Calidad**: Variable \u2192 Estandarizada (templates + validaci\u00f3n)\n",
    "- \ud83e\uddea **Test Coverage**: ~40% \u2192 >80%\n",
    "\n",
    "**Pr\u00f3ximos Pasos: Continuar Aprendiendo**\n",
    "\n",
    "**1. Profundizar en GenAI**\n",
    "```python\n",
    "# \u00c1reas avanzadas\n",
    "topics = [\n",
    "    'Fine-tuning de LLMs para dominio espec\u00edfico',\n",
    "    'Prompt engineering avanzado (Chain-of-Thought, ReAct)',\n",
    "    'Multi-modal AI (texto + im\u00e1genes + tablas)',\n",
    "    'Autonomous agents (Auto-GPT, BabyAGI)',\n",
    "    'LLM evaluation & benchmarking',\n",
    "    'Cost optimization (caching, model selection)'\n",
    "]\n",
    "```\n",
    "\n",
    "**2. Especializaciones**\n",
    "- **ML Engineering**: Feature stores (Feast, Tecton), model serving (MLflow)\n",
    "- **Real-time Analytics**: Flink, Kafka Streams, ksqlDB\n",
    "- **Data Mesh**: Domain-oriented data platforms\n",
    "- **Observability**: OpenTelemetry, distributed tracing\n",
    "\n",
    "**3. Certificaciones Recomendadas**\n",
    "```\n",
    "Cloud:\n",
    "\u251c\u2500 AWS Certified Data Analytics - Specialty\n",
    "\u251c\u2500 Google Professional Data Engineer\n",
    "\u2514\u2500 Azure Data Engineer Associate\n",
    "\n",
    "Data:\n",
    "\u251c\u2500 Databricks Certified Data Engineer\n",
    "\u251c\u2500 Snowflake SnowPro Core\n",
    "\u2514\u2500 dbt Analytics Engineering\n",
    "\n",
    "AI/ML:\n",
    "\u251c\u2500 TensorFlow Developer Certificate\n",
    "\u251c\u2500 MLOps Specialization (Coursera)\n",
    "\u2514\u2500 Prompt Engineering for Developers (DeepLearning.AI)\n",
    "```\n",
    "\n",
    "**4. Construir Portfolio**\n",
    "```\n",
    "GitHub Portfolio Projects:\n",
    "\u251c\u2500 1. End-to-end ETL pipeline (batch + streaming)\n",
    "\u2502     Tech: Airflow, Spark, Delta Lake\n",
    "\u2502\n",
    "\u251c\u2500 2. RAG system para dominio espec\u00edfico\n",
    "\u2502     Tech: OpenAI, ChromaDB, LangChain, FastAPI\n",
    "\u2502\n",
    "\u251c\u2500 3. Real-time dashboard con NL2SQL\n",
    "\u2502     Tech: Streamlit, PostgreSQL, LLMs\n",
    "\u2502\n",
    "\u251c\u2500 4. Data quality framework\n",
    "\u2502     Tech: Great Expectations, dbt, Airflow\n",
    "\u2502\n",
    "\u2514\u2500 5. GenAI code generator (como este proyecto)\n",
    "      Tech: LangGraph, OpenAI, GitHub Actions\n",
    "```\n",
    "\n",
    "**5. Comunidad y Networking**\n",
    "- **Conferences**: Data Council, Spark Summit, MLOps World\n",
    "- **Meetups**: Local data engineering & ML groups\n",
    "- **Open Source**: Contribuir a Airflow, dbt, LangChain\n",
    "- **Writing**: Blog posts, technical tutorials\n",
    "- **LinkedIn**: Compartir proyectos y aprendizajes\n",
    "\n",
    "**Reflexi\u00f3n Final: El Futuro del Data Engineering**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502          THE EVOLUTION OF DATA ENGINEERING             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                        \u2502\n",
    "\u2502  2010s: Manual ETL, Hadoop Era                         \u2502\n",
    "\u2502  \u251c\u2500 Batch processing                                   \u2502\n",
    "\u2502  \u2514\u2500 Heavy lifting, manual scaling                      \u2502\n",
    "\u2502                                                        \u2502\n",
    "\u2502  2020s: Cloud-Native, Streaming                        \u2502\n",
    "\u2502  \u251c\u2500 Real-time data                                     \u2502\n",
    "\u2502  \u251c\u2500 Serverless, auto-scaling                          \u2502\n",
    "\u2502  \u2514\u2500 Infrastructure as Code                            \u2502\n",
    "\u2502                                                        \u2502\n",
    "\u2502  2024+: GenAI-Augmented Data Engineering \ud83d\ude80            \u2502\n",
    "\u2502  \u251c\u2500 Natural language interfaces                        \u2502\n",
    "\u2502  \u251c\u2500 Self-healing pipelines                            \u2502\n",
    "\u2502  \u251c\u2500 Auto-generated code & docs                        \u2502\n",
    "\u2502  \u251c\u2500 Autonomous agents                                 \u2502\n",
    "\u2502  \u2514\u2500 Democratized data access                          \u2502\n",
    "\u2502                                                        \u2502\n",
    "\u2502  Future (2030s): Fully Autonomous Data Platforms?      \u2502\n",
    "\u2502  \u251c\u2500 Self-optimizing architectures                      \u2502\n",
    "\u2502  \u251c\u2500 Predictive data quality                           \u2502\n",
    "\u2502  \u251c\u2500 Zero-touch operations                             \u2502\n",
    "\u2502  \u2514\u2500 Human as orchestrator, not operator               \u2502\n",
    "\u2502                                                        \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Tu Rol en Esta Transformaci\u00f3n**\n",
    "\n",
    "Como Data Engineer con habilidades GenAI, no solo construyes pipelines\u2014**dise\u00f1as el futuro de c\u00f3mo organizaciones trabajan con datos**. Las herramientas que dominas permiten:\n",
    "\n",
    "\u2705 **Democratizar el acceso a datos** (cualquier usuario puede explorar)\n",
    "\u2705 **Acelerar innovaci\u00f3n** (de semanas a horas)\n",
    "\u2705 **Reducir costos operativos** (automatizaci\u00f3n inteligente)\n",
    "\u2705 **Mejorar calidad de decisiones** (insights m\u00e1s r\u00e1pidos y confiables)\n",
    "\n",
    "**Agradecimiento**\n",
    "\n",
    "Has completado un viaje intensivo de **40 notebooks**, abarcando desde fundamentos hasta la vanguardia de GenAI en Data Engineering. \n",
    "\n",
    "**Estad\u00edsticas del Curso**:\n",
    "- \ud83d\udcda **40 notebooks** (Junior: 10, Mid: 10, Senior: 10, GenAI: 10)\n",
    "- \ud83d\udcbb **~15,000 l\u00edneas** de c\u00f3digo explicativo\n",
    "- \u23f1\ufe0f **~120 horas** de contenido estimado\n",
    "- \ud83d\udd27 **50+ herramientas** cubiertas\n",
    "- \ud83d\ude80 **10+ proyectos** integradores\n",
    "\n",
    "Este conocimiento te posiciona en el **top 5% de Data Engineers** a nivel global\u2014una combinaci\u00f3n rara de:\n",
    "1. **Fundamentos s\u00f3lidos** (SQL, Python, arquitectura)\n",
    "2. **Cloud-native expertise** (AWS, Airflow, Spark)\n",
    "3. **GenAI proficiency** (LLMs, RAG, agents)\n",
    "\n",
    "**El mundo necesita profesionales como t\u00fa que pueden construir el puente entre datos y AI.**\n",
    "\n",
    "```python\n",
    "# Tu pr\u00f3xima l\u00ednea de c\u00f3digo empieza ahora\n",
    "def build_the_future():\n",
    "    \"\"\"\n",
    "    El curso termina aqu\u00ed, pero tu viaje contin\u00faa.\n",
    "    Cada pipeline que construyas, cada problema que resuelvas,\n",
    "    cada sistema que optimices\u2014est\u00e1s creando el futuro de los datos.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        learn()\n",
    "        build()\n",
    "        share()\n",
    "        repeat()\n",
    "\n",
    "# \u00a1\u00c9xito en tu carrera como GenAI Data Engineer! \ud83d\ude80\n",
    "build_the_future()\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)\n",
    "\n",
    "**\u00a1Felicitaciones por completar el Data Engineer Course completo! \ud83c\udf89\ud83c\udf8a**\n",
    "\n",
    "*\"The best way to predict the future is to build it.\"* \u2014 Alan Kay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** Final del Nivel \ud83c\udf89\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel GenAI:**\n",
    "- [\ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)\n",
    "- [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "- [\ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural](02_generacion_sql_nl2sql.ipynb)\n",
    "- [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs](03_generacion_codigo_etl.ipynb)\n",
    "- [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "- [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb)\n",
    "- [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n](06_agentes_automatizacion.ipynb)\n",
    "- [\u2705 Validaci\u00f3n de Datos con LLMs](07_calidad_validacion_llm.ipynb)\n",
    "- [\ud83c\udfb2 Generaci\u00f3n de Datos Sint\u00e9ticos con LLMs](08_sintesis_aumento_datos.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI](10_proyecto_integrador_2.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
