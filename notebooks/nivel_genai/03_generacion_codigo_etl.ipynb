{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d426fb",
   "metadata": {},
   "source": [
    "# ðŸ”§ GeneraciÃ³n AutomÃ¡tica de CÃ³digo ETL con LLMs\n",
    "\n",
    "Objetivo: automatizar la creaciÃ³n de pipelines ETL, transformaciones y scripts de datos usando IA generativa, con validaciÃ³n y best practices.\n",
    "\n",
    "- DuraciÃ³n: 90-120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: GenAI 01-02, experiencia con ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baafd8",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Arquitectura de GeneraciÃ³n de CÃ³digo ETL\n",
    "\n",
    "**EvoluciÃ³n histÃ³rica:**\n",
    "\n",
    "- **2015-2019:** Templates (Jinja2, Cookiecutter) - flexibilidad limitada\n",
    "- **2020-2022:** GPT-3/Codex - autocomplete a nivel funciÃ³n\n",
    "- **2023-2024:** GPT-4 - pipelines completos con autocorrecciÃ³n\n",
    "- **2024+:** LLM + MCP + RAG - cÃ³digo production-ready con contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b9b67",
   "metadata": {},
   "source": [
    "**Pipeline de 6 capas:**\n",
    "\n",
    "1. **Specification** - Parsear requerimientos en lenguaje natural\n",
    "2. **Context Retrieval (RAG)** - Buscar patrones similares en codebase\n",
    "3. **Code Generation** - Generar pipeline + config + tests\n",
    "4. **Validation** - Sintaxis, linting, tipos, seguridad\n",
    "5. **Self-Correction** - Iterar hasta 3 veces corrigiendo errores\n",
    "6. **Testing** - Unit + integration tests con 80%+ cobertura\n",
    "\n",
    "**Output final:** pipeline.py, config.yaml, tests, Dockerfile, CI/CD, README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aedc300",
   "metadata": {},
   "source": [
    "### SelecciÃ³n de Modelos LLM\n",
    "\n",
    "| Modelo | Caso de Uso | Fortalezas | Costo/1M tokens |\n",
    "|--------|-------------|------------|-----------------|\n",
    "| **GPT-4o** | Pipelines complejos | 95% accuracy, razonamiento excelente | $2.50 in / $10 out |\n",
    "| **Claude 3.5 Sonnet** | Transformaciones SQL/Pandas | Calidad cÃ³digo, refactoring | $3 in / $15 out |\n",
    "| **GPT-3.5-turbo** | Scripts simples, boilerplate | RÃ¡pido (1-2s), econÃ³mico | $0.50 in / $1.50 out |\n",
    "| **Codestral** | On-prem, OSS | Privacidad, customizable | Self-hosted |\n",
    "| **Gemini 1.5 Pro** | Contexto grande (2M tokens) | Ventana masiva, multimodal | $1.25 in / $5 out |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd38458",
   "metadata": {},
   "source": [
    "### FunciÃ³n: Construir Prompt para GeneraciÃ³n de CÃ³digo\n",
    "\n",
    "Esta funciÃ³n construye un prompt optimizado siguiendo best practices de prompt engineering para ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd020d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "def build_code_generation_prompt(\n",
    "    requirements: str,\n",
    "    tech_stack: List[str],\n",
    "    context: Optional[str] = None,\n",
    "    constraints: Optional[Dict] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Construye prompt optimizado para generaciÃ³n de cÃ³digo ETL.\n",
    "    \n",
    "    Best Practices:\n",
    "    - Especificar tech stack explÃ­citamente\n",
    "    - Incluir ejemplos del codebase (few-shot)\n",
    "    - Definir estÃ¡ndares de calidad\n",
    "    - Solicitar explicaciones (razonamiento)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert Data Engineer specializing in production ETL pipelines.\n",
    "\n",
    "**Requirements:**\n",
    "{requirements}\n",
    "\n",
    "**Tech Stack:**\n",
    "{', '.join(tech_stack)}\n",
    "\n",
    "**Coding Standards:**\n",
    "- Python 3.11+ with type hints (PEP 484)\n",
    "- Error handling with try-except and logging\n",
    "- Docstrings (Google style)\n",
    "- Modular functions (max 50 lines)\n",
    "- Configuration externalized (YAML/env vars)\n",
    "- Idempotent operations (safe to re-run)\n",
    "\n",
    "**Quality Checklist:**\n",
    "âœ… No hardcoded credentials\n",
    "âœ… Parameterized queries (prevent SQL injection)\n",
    "âœ… Retry logic with exponential backoff\n",
    "âœ… Metrics instrumentation (Prometheus)\n",
    "âœ… Comprehensive logging (structured JSON)\n",
    "âœ… Unit tests (pytest, 80%+ coverage)\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if context:\n",
    "        prompt += f\"\\n**Codebase Context (existing patterns):**\\n{context}\\n\"\n",
    "    \n",
    "    if constraints:\n",
    "        prompt += f\"\\n**Constraints:**\\n\"\n",
    "        for key, value in constraints.items():\n",
    "            prompt += f\"- {key}: {value}\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "**Output Format:**\n",
    "1. Main pipeline code (complete and executable)\n",
    "2. Configuration file (YAML)\n",
    "3. Unit tests (pytest)\n",
    "4. Brief explanation of design decisions\n",
    "\n",
    "Generate the code:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"âœ… FunciÃ³n build_code_generation_prompt() definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de4565",
   "metadata": {},
   "source": [
    "### Ejemplo: Pipeline Completo con OpenAI GPT-4o\n",
    "\n",
    "GeneraciÃ³n end-to-end usando el modelo mÃ¡s avanzado con mejor comprensiÃ³n de contexto y razonamiento sobre cÃ³digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf92d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Definir requisitos del pipeline\n",
    "requirements = \"\"\"\n",
    "Create an ETL pipeline that:\n",
    "1. Extracts data from PostgreSQL table 'sales_raw' (incremental load by timestamp)\n",
    "2. Transforms: clean nulls, deduplicate by transaction_id, calculate revenue\n",
    "3. Loads to S3 parquet partitioned by date (YYYY/MM/DD)\n",
    "4. Updates metadata table with execution stats\n",
    "\"\"\"\n",
    "\n",
    "tech_stack = [\n",
    "    \"Python 3.11\",\n",
    "    \"pandas 2.x\",\n",
    "    \"psycopg2\",\n",
    "    \"boto3\",\n",
    "    \"pyarrow\",\n",
    "    \"Apache Airflow 2.8\"\n",
    "]\n",
    "\n",
    "constraints = {\n",
    "    \"max_memory\": \"2GB\",\n",
    "    \"execution_time\": \"<10 minutes\",\n",
    "    \"idempotency\": \"required (safe re-runs)\",\n",
    "    \"logging\": \"structured JSON to CloudWatch\"\n",
    "}\n",
    "\n",
    "# Construir prompt con contexto\n",
    "prompt = build_code_generation_prompt(\n",
    "    requirements=requirements,\n",
    "    tech_stack=tech_stack,\n",
    "    constraints=constraints,\n",
    "    context=\"Existing pattern uses @task decorators, config from YAML, logger.info() for metrics\"\n",
    ")\n",
    "\n",
    "# Generar cÃ³digo con GPT-4o\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Mejor para cÃ³digo complejo\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Data Engineer with 10+ years in production ETL systems.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3,  # Baja creatividad = cÃ³digo mÃ¡s determinista\n",
    "    max_tokens=3000\n",
    ")\n",
    "\n",
    "generated_code = response.choices[0].message.content\n",
    "\n",
    "print(\"ðŸ”¥ CÃ³digo generado por GPT-4o:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_code)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ’° Tokens: {response.usage.total_tokens} | Costo aprox: ${response.usage.total_tokens * 0.00003:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4c707",
   "metadata": {},
   "source": [
    "### ðŸ§  RAG-Enhanced Generation: Usar DocumentaciÃ³n del Codebase\n",
    "\n",
    "Para proyectos grandes, es crÃ­tico que el LLM tenga acceso a patrones existentes, arquitectura y convenciones. **RAG (Retrieval-Augmented Generation)** recupera fragmentos relevantes del codebase antes de generar:\n",
    "\n",
    "**Beneficios:**\n",
    "- âœ… Consistencia con estilos existentes\n",
    "- âœ… ReutilizaciÃ³n de utilidades ya implementadas\n",
    "- âœ… Evita reinventar la rueda\n",
    "- âœ… Respeta decisiones arquitectÃ³nicas previas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Indexar codebase existente\n",
    "def index_codebase(repo_path: str, collection_name: str = \"etl_code\"):\n",
    "    \"\"\"Indexa archivos Python del proyecto con embeddings.\"\"\"\n",
    "    \n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    python_files = Path(repo_path).rglob(\"*.py\")\n",
    "    \n",
    "    for idx, file_path in enumerate(python_files):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            code_content = f.read()\n",
    "        \n",
    "        # Generar embedding del cÃ³digo\n",
    "        embedding = model.encode(code_content).tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[code_content],\n",
    "            metadatas=[{\"file\": str(file_path), \"type\": \"python\"}],\n",
    "            ids=[f\"file_{idx}\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ… Indexados {idx+1} archivos Python\")\n",
    "    return collection\n",
    "\n",
    "# 2. Buscar cÃ³digo relevante para la tarea\n",
    "def retrieve_relevant_code(query: str, collection, top_k: int = 3) -> str:\n",
    "    \"\"\"Recupera los k archivos mÃ¡s similares semÃ¡nticamente a la query.\"\"\"\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\\n--- Ejemplo del codebase ---\\n\\n\".join(\n",
    "        [f\"# {meta['file']}\\n{doc}\" for doc, meta in zip(results['documents'][0], results['metadatas'][0])]\n",
    "    )\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 3. Generar con contexto RAG\n",
    "collection = index_codebase(\"./scripts/etl\")\n",
    "\n",
    "relevant_context = retrieve_relevant_code(\n",
    "    query=\"PostgreSQL incremental load with Airflow\",\n",
    "    collection=collection,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "# Usar el contexto en el prompt\n",
    "rag_prompt = build_code_generation_prompt(\n",
    "    requirements=requirements,\n",
    "    tech_stack=tech_stack,\n",
    "    context=relevant_context,  # â† AquÃ­ inyectamos cÃ³digo similar existente\n",
    "    constraints=constraints\n",
    ")\n",
    "\n",
    "response_rag = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a Data Engineer. Reuse patterns from the codebase context.\"},\n",
    "        {\"role\": \"user\", \"content\": rag_prompt}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  CÃ³digo generado con RAG (contexto del codebase):\")\n",
    "print(response_rag.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52720e7c",
   "metadata": {},
   "source": [
    "### âš¡ OptimizaciÃ³n de Tokens y Costos\n",
    "\n",
    "Para prompts largos (con contexto RAG), es crÃ­tico optimizar el uso de tokens para reducir costos y latencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414aef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def optimize_prompt(prompt: str, max_tokens: int = 1500, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Recorta el prompt si excede max_tokens, preservando secciones crÃ­ticas.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(prompt)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return prompt\n",
    "    \n",
    "    # Estrategia: mantener inicio (requirements) y final (instrucciones)\n",
    "    # Truncar el medio (contexto RAG)\n",
    "    keep_start = max_tokens // 2\n",
    "    keep_end = max_tokens // 2\n",
    "    \n",
    "    truncated_tokens = tokens[:keep_start] + tokens[-keep_end:]\n",
    "    optimized_prompt = encoding.decode(truncated_tokens)\n",
    "    \n",
    "    print(f\"âš ï¸ Prompt truncado: {len(tokens)} â†’ {len(truncated_tokens)} tokens\")\n",
    "    return optimized_prompt\n",
    "\n",
    "# Ejemplo: prompt original muy largo\n",
    "long_prompt = rag_prompt  # Asumiendo que rag_prompt tiene 3000 tokens\n",
    "\n",
    "optimized = optimize_prompt(long_prompt, max_tokens=1500, model=\"gpt-4o\")\n",
    "\n",
    "print(f\"Tokens originales: {len(tiktoken.encoding_for_model('gpt-4o').encode(long_prompt))}\")\n",
    "print(f\"Tokens optimizados: {len(tiktoken.encoding_for_model('gpt-4o').encode(optimized))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81963ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComparaciÃ³n de costos por modelo (precios 2024)\n",
    "models_pricing = {\n",
    "    \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},  # por 1K tokens\n",
    "    \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "    \"claude-3.5-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    \"codestral\": {\"input\": 0.001, \"output\": 0.003}\n",
    "}\n",
    "\n",
    "prompt_tokens = 1500\n",
    "completion_tokens = 2000\n",
    "\n",
    "print(\"ðŸ’° ComparaciÃ³n de costos para 1 generaciÃ³n (1.5K input + 2K output):\\n\")\n",
    "for model, prices in models_pricing.items():\n",
    "    cost = (prompt_tokens * prices[\"input\"] / 1000) + (completion_tokens * prices[\"output\"] / 1000)\n",
    "    print(f\"{model:20s} ${cost:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Para 1000 generaciones por mes:\")\n",
    "for model, prices in models_pricing.items():\n",
    "    cost = ((prompt_tokens * prices[\"input\"] / 1000) + (completion_tokens * prices[\"output\"] / 1000)) * 1000\n",
    "    print(f\"{model:20s} ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ba1a1",
   "metadata": {},
   "source": [
    "## ðŸ›¡ï¸ Code Validation & Security: Multi-Layer Quality Assurance\n",
    "\n",
    "El cÃ³digo generado por LLMs **no es confiable por defecto**. Necesita pasar por un pipeline de validaciÃ³n riguroso antes de ejecutarse en producciÃ³n:\n",
    "\n",
    "### Arquitectura del Pipeline de ValidaciÃ³n (5 Capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6e97a",
   "metadata": {},
   "source": [
    "```\n",
    "Generated Code\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAYER 1: SYNTAX VALIDATION         â”‚\n",
    "â”‚  â”œâ”€ AST Parsing (ast.parse)         â”‚\n",
    "â”‚  â”œâ”€ Compile Check (compile())       â”‚\n",
    "â”‚  â””â”€ Python Version Compatibility    â”‚\n",
    "â”‚  â†’ Result: Syntactically valid code â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚ PASS\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAYER 2: STATIC ANALYSIS           â”‚\n",
    "â”‚  â”œâ”€ Linting (ruff/flake8/pylint)    â”‚\n",
    "â”‚  â”œâ”€ Type Checking (mypy)            â”‚\n",
    "â”‚  â”œâ”€ Complexity (radon: CC < 10)     â”‚\n",
    "â”‚  â”œâ”€ Code Smells (pylint)            â”‚\n",
    "â”‚  â””â”€ Formatting (black/autopep8)     â”‚\n",
    "â”‚  â†’ Result: Clean, readable code     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚ PASS\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAYER 3: SECURITY SCAN             â”‚\n",
    "â”‚  â”œâ”€ Bandit (vulnerability detection)â”‚\n",
    "â”‚  â”œâ”€ Safety (dependency vulnerabilities)â”‚\n",
    "â”‚  â”œâ”€ Secrets Detection (detect-secrets)â”‚\n",
    "â”‚  â”œâ”€ SQL Injection Check (sqlparse)  â”‚\n",
    "â”‚  â””â”€ Path Traversal Check            â”‚\n",
    "â”‚  â†’ Result: Secure code              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚ PASS\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAYER 4: FUNCTIONAL TESTING        â”‚\n",
    "â”‚  â”œâ”€ Unit Tests (pytest)             â”‚\n",
    "â”‚  â”œâ”€ Integration Tests (testcontainers)â”‚\n",
    "â”‚  â”œâ”€ Property-Based (hypothesis)     â”‚\n",
    "â”‚  â”œâ”€ Coverage (pytest-cov > 80%)     â”‚\n",
    "â”‚  â””â”€ Performance (locust benchmarks) â”‚\n",
    "â”‚  â†’ Result: Functionally correct     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚ PASS\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LAYER 5: RUNTIME VALIDATION        â”‚\n",
    "â”‚  â”œâ”€ Dry-Run with Sample Data        â”‚\n",
    "â”‚  â”œâ”€ Memory Profiling (memory_profiler)â”‚\n",
    "â”‚  â”œâ”€ Execution Time Check            â”‚\n",
    "â”‚  â””â”€ Resource Limits (cgroups)       â”‚\n",
    "â”‚  â†’ Result: Production-ready         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95ea8d",
   "metadata": {},
   "source": [
    "### Clase CodeValidator: ImplementaciÃ³n del Sistema de ValidaciÃ³n\n",
    "\n",
    "Validador multi-capa que verifica sintaxis, linting, tipos, seguridad y complejidad del cÃ³digo generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "class CodeValidator:\n",
    "    \"\"\"Multi-layer validation system for LLM-generated code.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def validate_syntax(self, code: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Layer 1: AST parsing and compilation.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            compile(code, '<generated>', 'exec')\n",
    "            \n",
    "            issues = []\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Expr) and isinstance(node.value, ast.Call):\n",
    "                    if hasattr(node.value.func, 'id') and node.value.func.id == 'exec':\n",
    "                        issues.append(\"âš ï¸ exec() detected (security risk)\")\n",
    "                if isinstance(node, ast.ExceptHandler) and node.type is None:\n",
    "                    issues.append(\"âš ï¸ Bare except: clause (catch specific exceptions)\")\n",
    "            \n",
    "            if issues:\n",
    "                return True, \"Valid with warnings:\\\\n\" + \"\\\\n\".join(issues)\n",
    "            return True, \"âœ… Syntax valid\"\n",
    "        except SyntaxError as e:\n",
    "            return False, f\"âŒ Syntax Error at line {e.lineno}: {e.msg}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"âŒ Compilation Error: {str(e)}\"\n",
    "    \n",
    "    def validate_linting(self, code: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Layer 2: Static analysis with ruff/flake8.\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_path = f.name\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ruff', 'check', temp_path, '--output-format', 'json'],\n",
    "                capture_output=True, text=True, timeout=10\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                return True, \"âœ… No linting issues\"\n",
    "            \n",
    "            issues = json.loads(result.stdout)\n",
    "            errors = [f\"Line {i['location']['row']}: {i['code']} - {i['message']}\" \n",
    "                     for i in issues[:5]]\n",
    "            return False, \"âŒ Linting issues:\\\\n\" + \"\\\\n\".join(errors)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            result = subprocess.run(\n",
    "                ['flake8', temp_path, '--max-line-length', '100'],\n",
    "                capture_output=True, text=True, timeout=10\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                return True, \"âœ… No linting issues (flake8)\"\n",
    "            return False, f\"âŒ Flake8 issues:\\\\n{result.stdout[:500]}\"\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return False, \"âŒ Linting timeout\"\n",
    "        finally:\n",
    "            Path(temp_path).unlink()\n",
    "    \n",
    "    def validate_types(self, code: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Layer 2.5: Type checking with mypy.\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_path = f.name\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['mypy', temp_path, '--no-error-summary'],\n",
    "                capture_output=True, text=True, timeout=15\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                return True, \"âœ… Type checking passed\"\n",
    "            \n",
    "            errors = [line for line in result.stdout.split('\\\\n') \n",
    "                     if 'error:' in line and 'import' not in line.lower()]\n",
    "            \n",
    "            if not errors:\n",
    "                return True, \"âœ… Type checking passed (ignoring imports)\"\n",
    "            return False, \"âŒ Type errors:\\\\n\" + \"\\\\n\".join(errors[:3])\n",
    "        except FileNotFoundError:\n",
    "            return True, \"âš ï¸ mypy not installed\"\n",
    "        finally:\n",
    "            Path(temp_path).unlink()\n",
    "    \n",
    "    def validate_security(self, code: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Layer 3: Security vulnerability scanning with bandit.\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_path = f.name\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['bandit', '-r', temp_path, '-f', 'json'],\n",
    "                capture_output=True, text=True, timeout=10\n",
    "            )\n",
    "            \n",
    "            report = json.loads(result.stdout)\n",
    "            critical = [i for i in report.get('results', [])\n",
    "                       if i['issue_severity'] in ['HIGH', 'MEDIUM']]\n",
    "            \n",
    "            if not critical:\n",
    "                return True, \"âœ… No security vulnerabilities\"\n",
    "            \n",
    "            errors = [f\"Line {i['line_number']}: [{i['issue_severity']}] {i['issue_text']}\"\n",
    "                     for i in critical[:3]]\n",
    "            return False, \"âŒ Security issues:\\\\n\" + \"\\\\n\".join(errors)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            # Manual checks\n",
    "            dangerous = [\n",
    "                ('eval(', 'Code execution vulnerability'),\n",
    "                ('exec(', 'Code execution vulnerability'),\n",
    "                ('pickle.loads', 'Insecure deserialization'),\n",
    "                ('password = \"', 'Hardcoded password'),\n",
    "                ('api_key = \"', 'Hardcoded API key'),\n",
    "            ]\n",
    "            \n",
    "            found = [f\"âš ï¸ {desc}: '{pat}'\" for pat, desc in dangerous if pat in code]\n",
    "            if found:\n",
    "                return False, \"âŒ Security issues:\\\\n\" + \"\\\\n\".join(found)\n",
    "            return True, \"âœ… Basic security check passed\"\n",
    "        finally:\n",
    "            Path(temp_path).unlink()\n",
    "    \n",
    "    def validate_complexity(self, code: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Layer 2: Cyclomatic complexity with radon.\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_path = f.name\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['radon', 'cc', temp_path, '-j'],\n",
    "                capture_output=True, text=True, timeout=10\n",
    "            )\n",
    "            \n",
    "            data = json.loads(result.stdout)\n",
    "            complex_funcs = []\n",
    "            for file_data in data.values():\n",
    "                for item in file_data:\n",
    "                    if item['complexity'] > 10:\n",
    "                        complex_funcs.append(\n",
    "                            f\"{item['name']}: CC={item['complexity']} (line {item['lineno']})\"\n",
    "                        )\n",
    "            \n",
    "            if not complex_funcs:\n",
    "                return True, \"âœ… Complexity OK\"\n",
    "            return False, \"âš ï¸ High complexity:\\\\n\" + \"\\\\n\".join(complex_funcs)\n",
    "        except FileNotFoundError:\n",
    "            return True, \"âš ï¸ radon not installed\"\n",
    "        finally:\n",
    "            Path(temp_path).unlink()\n",
    "    \n",
    "    def validate_all(self, code: str) -> Dict:\n",
    "        \"\"\"Execute full validation pipeline.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Layer 1: Syntax (critical)\n",
    "        syntax_pass, syntax_msg = self.validate_syntax(code)\n",
    "        results['syntax'] = {'passed': syntax_pass, 'message': syntax_msg}\n",
    "        \n",
    "        if not syntax_pass:\n",
    "            results['is_valid'] = False\n",
    "            results['overall_score'] = 0\n",
    "            return results\n",
    "        \n",
    "        # Layer 2: Linting\n",
    "        lint_pass, lint_msg = self.validate_linting(code)\n",
    "        results['linting'] = {'passed': lint_pass, 'message': lint_msg}\n",
    "        \n",
    "        # Layer 2.5: Types\n",
    "        types_pass, types_msg = self.validate_types(code)\n",
    "        results['types'] = {'passed': types_pass, 'message': types_msg}\n",
    "        \n",
    "        # Layer 3: Security (critical)\n",
    "        security_pass, security_msg = self.validate_security(code)\n",
    "        results['security'] = {'passed': security_pass, 'message': security_msg}\n",
    "        \n",
    "        # Layer 2: Complexity\n",
    "        complexity_pass, complexity_msg = self.validate_complexity(code)\n",
    "        results['complexity'] = {'passed': complexity_pass, 'message': complexity_msg}\n",
    "        \n",
    "        # Calculate score\n",
    "        weights = {\n",
    "            'syntax': 30, 'security': 30, 'linting': 15, 'types': 15, 'complexity': 10\n",
    "        }\n",
    "        \n",
    "        score = sum(weights[k] for k, v in results.items()\n",
    "                   if k in weights and v.get('passed', False))\n",
    "        \n",
    "        results['overall_score'] = score\n",
    "        results['is_valid'] = score >= 70  # 70% threshold\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… Clase CodeValidator completa cargada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88665166",
   "metadata": {},
   "source": [
    "### Ejemplo de Uso: Validar CÃ³digo Generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64821c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = CodeValidator()\n",
    "\n",
    "generated_code = '''\n",
    "def process_sales(data):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data)\n",
    "    df['total'] = df['quantity'] * df['price']\n",
    "    return df.to_dict('records')\n",
    "'''\n",
    "\n",
    "results = validator.validate_all(generated_code)\n",
    "\n",
    "print(f\"Overall Score: {results['overall_score']}/100\")\n",
    "print(f\"Production Ready: {results['is_valid']}\")\n",
    "print()\n",
    "\n",
    "for check, result in results.items():\n",
    "    if isinstance(result, dict) and 'message' in result:\n",
    "        status = \"âœ…\" if result.get('passed', False) else \"âŒ\"\n",
    "        print(f\"{status} {check.upper()}: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6d841",
   "metadata": {},
   "source": [
    "### ðŸ”„ Self-Correction Loop: Iterative Fixing con LLM\n",
    "\n",
    "Si el cÃ³digo falla la validaciÃ³n, podemos usar el LLM para **autocorregirse** basÃ¡ndose en los errores reportados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_correct_code(\n",
    "    code: str,\n",
    "    validation_results: Dict,\n",
    "    max_attempts: int = 3\n",
    ") -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Iteratively fix code based on validation errors.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Extract error messages from validation\n",
    "    2. Send to LLM with context: original code + errors\n",
    "    3. Re-validate corrected code\n",
    "    4. Repeat until valid or max_attempts reached\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        if validation_results['is_valid']:\n",
    "            return code, True\n",
    "        \n",
    "        # Collect errors\n",
    "        errors = []\n",
    "        for check, result in validation_results.items():\n",
    "            if isinstance(result, dict) and not result.get('passed', True):\n",
    "                errors.append(f\"{check}: {result['message']}\")\n",
    "        \n",
    "        # Correction prompt\n",
    "        correction_prompt = f\"\"\"\n",
    "Fix the following Python code based on validation errors:\n",
    "\n",
    "**Original Code:**\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "**Validation Errors:**\n",
    "{chr(10).join(errors)}\n",
    "\n",
    "**Instructions:**\n",
    "- Fix all syntax, linting, type, and security issues\n",
    "- Maintain the original functionality\n",
    "- Return only the corrected code (no explanations)\n",
    "\n",
    "**Corrected Code:**\n",
    "\"\"\"\n",
    "        \n",
    "        # Call LLM for correction\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": correction_prompt}],\n",
    "            temperature=0.1  # Deterministic fixes\n",
    "        )\n",
    "        \n",
    "        corrected_code = response.choices[0].message.content.strip()\n",
    "        corrected_code = corrected_code.replace('```python', '').replace('```', '').strip()\n",
    "        \n",
    "        # Re-validate\n",
    "        validation_results = validator.validate_all(corrected_code)\n",
    "        code = corrected_code\n",
    "        \n",
    "        print(f\"Attempt {attempt + 1}: Score = {validation_results['overall_score']}/100\")\n",
    "        \n",
    "        if validation_results['is_valid']:\n",
    "            return code, True\n",
    "    \n",
    "    return code, False\n",
    "\n",
    "# Example: Self-correction workflow\n",
    "initial_code = '''\n",
    "def risky_function(user_input):\n",
    "    result = eval(user_input)  # Security issue!\n",
    "    return result\n",
    "'''\n",
    "\n",
    "validation = validator.validate_all(initial_code)\n",
    "print(f\"Initial Score: {validation['overall_score']}/100\")\n",
    "\n",
    "corrected_code, success = self_correct_code(initial_code, validation)\n",
    "if success:\n",
    "    print(\"\\\\nâœ… Code successfully corrected and validated!\")\n",
    "    print(corrected_code)\n",
    "else:\n",
    "    print(\"\\\\nâŒ Failed to correct code after 3 attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4c1b7",
   "metadata": {},
   "source": [
    "### ðŸ” Secrets Detection & Environment Variable Enforcement\n",
    "\n",
    "Detector automÃ¡tico de secretos hardcodeados y reemplazo por variables de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def detect_secrets(code: str) -> List[str]:\n",
    "    \"\"\"Detect hardcoded secrets and suggest env var usage.\"\"\"\n",
    "    \n",
    "    patterns = [\n",
    "        (r'password\\s*=\\s*[\"\\'](.{3,})[\"\\']', 'Hardcoded password'),\n",
    "        (r'api_key\\s*=\\s*[\"\\'](.{10,})[\"\\']', 'Hardcoded API key'),\n",
    "        (r'secret\\s*=\\s*[\"\\'](.{10,})[\"\\']', 'Hardcoded secret'),\n",
    "        (r'token\\s*=\\s*[\"\\'](.{10,})[\"\\']', 'Hardcoded token'),\n",
    "        (r'AKIA[0-9A-Z]{16}', 'AWS Access Key'),\n",
    "        (r'postgres://.*:.*@', 'Database URL with credentials'),\n",
    "    ]\n",
    "    \n",
    "    findings = []\n",
    "    for pattern, description in patterns:\n",
    "        matches = re.findall(pattern, code, re.IGNORECASE)\n",
    "        if matches:\n",
    "            findings.append(f\"âŒ {description} detected\")\n",
    "            findings.append(f\"   â†’ Replace with: os.getenv('{description.upper().replace(' ', '_')}')\")\n",
    "    \n",
    "    return findings\n",
    "\n",
    "def fix_secrets(code: str) -> str:\n",
    "    \"\"\"Automatically replace hardcoded secrets with os.getenv().\"\"\"\n",
    "    \n",
    "    replacements = [\n",
    "        (r'password\\s*=\\s*[\"\\'](.{3,})[\"\\']', 'password = os.getenv(\"DB_PASSWORD\")'),\n",
    "        (r'api_key\\s*=\\s*[\"\\'](.{10,})[\"\\']', 'api_key = os.getenv(\"API_KEY\")'),\n",
    "    ]\n",
    "    \n",
    "    fixed_code = code\n",
    "    needs_import = False\n",
    "    \n",
    "    for pattern, replacement in replacements:\n",
    "        if re.search(pattern, fixed_code, re.IGNORECASE):\n",
    "            fixed_code = re.sub(pattern, replacement, fixed_code, flags=re.IGNORECASE)\n",
    "            needs_import = True\n",
    "    \n",
    "    # Add 'import os' if needed\n",
    "    if needs_import and 'import os' not in fixed_code:\n",
    "        fixed_code = 'import os\\\\n\\\\n' + fixed_code\n",
    "    \n",
    "    return fixed_code\n",
    "\n",
    "# Example\n",
    "bad_code = '''\n",
    "def connect_db():\n",
    "    password = \"super_secret_123\"\n",
    "    api_key = \"sk_live_abc123xyz\"\n",
    "    return create_connection(password, api_key)\n",
    "'''\n",
    "\n",
    "print(\"Secrets detected:\")\n",
    "secrets = detect_secrets(bad_code)\n",
    "print(\"\\\\n\".join(secrets))\n",
    "\n",
    "print(\"\\\\nâœ… Fixed code:\")\n",
    "fixed_code = fix_secrets(bad_code)\n",
    "print(fixed_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c6301",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test Generation: Automated Quality Assurance\n",
    "\n",
    "Los LLMs pueden generar suites completas de tests para cÃ³digo ETL, incluyendo:\n",
    "- âœ… Unit tests (funciones individuales)\n",
    "- âœ… Integration tests (componentes interconectados)\n",
    "- âœ… Property-based tests (invariantes con Hypothesis)\n",
    "- âœ… Performance tests (benchmarks de rendimiento)\n",
    "- âœ… Edge cases (valores lÃ­mite, errores esperados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af3753",
   "metadata": {},
   "source": [
    "### FunciÃ³n: Generar Tests AutomÃ¡ticamente con LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str, test_types: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate comprehensive test suite for given code.\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to generate tests for\n",
    "        test_types: List of test types ['unit', 'integration', 'property', 'performance']\n",
    "    \n",
    "    Returns:\n",
    "        Complete pytest test suite as string\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_types is None:\n",
    "        test_types = ['unit', 'integration', 'edge_cases']\n",
    "    \n",
    "    test_generation_prompt = f\"\"\"\n",
    "Generate a comprehensive pytest test suite for the following Python code.\n",
    "\n",
    "**Code to Test:**\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "**Required Test Types:**\n",
    "{', '.join(test_types)}\n",
    "\n",
    "**Test Requirements:**\n",
    "1. Use pytest framework with fixtures\n",
    "2. Cover happy path, edge cases, and error cases\n",
    "3. Include parametrized tests (@pytest.mark.parametrize)\n",
    "4. Mock external dependencies (files, databases, APIs)\n",
    "5. Use descriptive test names (test_function_behavior_scenario)\n",
    "6. Include docstrings explaining what each test verifies\n",
    "7. Aim for 80%+ code coverage\n",
    "8. Use tmp_path fixture for file operations\n",
    "\n",
    "**Output Format:**\n",
    "- Complete, executable test file\n",
    "- Import all necessary modules (pytest, unittest.mock, etc.)\n",
    "- Use proper assertions (assert, pytest.raises, etc.)\n",
    "\n",
    "Generate the test suite:\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a QA expert specializing in Python testing.\"},\n",
    "            {\"role\": \"user\", \"content\": test_generation_prompt}\n",
    "        ],\n",
    "        temperature=0.3  # Slightly higher for diverse test cases\n",
    "    )\n",
    "    \n",
    "    test_code = response.choices[0].message.content.strip()\n",
    "    test_code = test_code.replace('```python', '').replace('```', '').strip()\n",
    "    \n",
    "    return test_code\n",
    "\n",
    "print(\"âœ… FunciÃ³n generate_tests() definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf04815",
   "metadata": {},
   "source": [
    "### Ejemplo: Generar Tests para un Pipeline ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_code = '''\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def extract_sales(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract sales data from CSV file.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"CSV file is empty\")\n",
    "    return df\n",
    "\n",
    "def transform_sales(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform sales data: filter, clean, aggregate.\"\"\"\n",
    "    df_clean = df[df['amount'] > 0].copy()\n",
    "    df_clean['month'] = pd.to_datetime(df_clean['date']).dt.to_period('M')\n",
    "    df_agg = df_clean.groupby(['date', 'month']).agg({\n",
    "        'amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    return df_agg\n",
    "\n",
    "def load_sales(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Load transformed data to Parquet.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"No data to load\")\n",
    "    df.to_parquet(output_path, index=False, compression='snappy')\n",
    "\n",
    "def run_etl_pipeline(input_path: str, output_path: str) -> Dict:\n",
    "    \"\"\"Full ETL pipeline orchestration.\"\"\"\n",
    "    df_raw = extract_sales(input_path)\n",
    "    df_transformed = transform_sales(df_raw)\n",
    "    load_sales(df_transformed, output_path)\n",
    "    return {\n",
    "        'rows_extracted': len(df_raw),\n",
    "        'rows_loaded': len(df_transformed),\n",
    "        'success': True\n",
    "    }\n",
    "'''\n",
    "\n",
    "generated_tests = generate_tests(etl_code)\n",
    "print(\"Generated Test Suite:\")\n",
    "print(generated_tests[:1000])  # Show first 1000 chars\n",
    "print(\"\\\\n[... truncated for display ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385e7d7",
   "metadata": {},
   "source": [
    "### ðŸ“Š Advanced Testing Patterns\n",
    "\n",
    "Patrones avanzados para testing de pipelines ETL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Property-Based Testing with Hypothesis\n",
    "from hypothesis import given, strategies as st\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@given(st.lists(st.integers(min_value=1, max_value=1000)))\n",
    "def test_transform_preserves_positive_amounts(amounts):\n",
    "    \"\"\"Property: transformation only keeps positive amounts.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'amount': amounts,\n",
    "        'date': ['2024-01-01'] * len(amounts),\n",
    "        'transaction_id': list(range(len(amounts)))\n",
    "    })\n",
    "    result = transform_sales(df)\n",
    "    assert len(result) > 0  # At least one aggregate row\n",
    "\n",
    "# 2. Parametrized Tests for Edge Cases\n",
    "@pytest.mark.parametrize(\"test_input,expected_error\", [\n",
    "    (pd.DataFrame(), AttributeError),  # Empty dataframe\n",
    "    (pd.DataFrame({'amount': [-1, -2], 'date': ['2024-01-01', '2024-01-02'], \n",
    "                   'transaction_id': [1, 2]}), None),  # All negative\n",
    "])\n",
    "def test_transform_edge_cases(test_input, expected_error):\n",
    "    if expected_error:\n",
    "        with pytest.raises(expected_error):\n",
    "            transform_sales(test_input)\n",
    "    else:\n",
    "        result = transform_sales(test_input)\n",
    "        assert isinstance(result, pd.DataFrame)\n",
    "\n",
    "# 3. Performance Test\n",
    "import time\n",
    "\n",
    "def test_pipeline_performance(tmp_path):\n",
    "    \"\"\"Performance test: 100K rows should complete in <2s.\"\"\"\n",
    "    large_df = pd.DataFrame({\n",
    "        'transaction_id': range(100_000),\n",
    "        'date': ['2024-01-01'] * 100_000,\n",
    "        'amount': [100] * 100_000\n",
    "    })\n",
    "    \n",
    "    input_file = tmp_path / \"large.csv\"\n",
    "    large_df.to_csv(input_file, index=False)\n",
    "    \n",
    "    start = time.time()\n",
    "    output_file = tmp_path / \"output.parquet\"\n",
    "    run_etl_pipeline(str(input_file), str(output_file))\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    assert duration < 2, f\"Too slow: {duration:.2f}s\"\n",
    "\n",
    "print(\"âœ… Advanced test patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574e32d",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Test Coverage Analysis & Improvement\n",
    "\n",
    "AnÃ¡lisis automÃ¡tico de cobertura y generaciÃ³n de tests adicionales para cubrir lÃ­neas faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def run_tests_with_coverage(test_file: str, code_file: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Execute tests and generate coverage report.\n",
    "    \n",
    "    Returns dict with coverage metrics and test results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run pytest with coverage\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            'pytest', test_file,\n",
    "            f'--cov={code_file}',\n",
    "            '--cov-report=json:coverage.json',\n",
    "            '-v'\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Parse coverage JSON\n",
    "    try:\n",
    "        with open('coverage.json') as f:\n",
    "            cov_data = json.load(f)\n",
    "        \n",
    "        file_cov = cov_data['files'].get(code_file, {})\n",
    "        summary = file_cov.get('summary', {})\n",
    "        \n",
    "        return {\n",
    "            'coverage_percent': summary.get('percent_covered', 0),\n",
    "            'lines_covered': summary.get('covered_lines', 0),\n",
    "            'lines_total': summary.get('num_statements', 0),\n",
    "            'missing_lines': file_cov.get('missing_lines', []),\n",
    "            'test_output': result.stdout\n",
    "        }\n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        return {\n",
    "            'coverage_percent': 0,\n",
    "            'error': str(e),\n",
    "            'test_output': result.stdout\n",
    "        }\n",
    "\n",
    "def improve_test_coverage(code: str, existing_tests: str, coverage_report: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate additional tests to improve coverage.\n",
    "    \n",
    "    Focuses on uncovered lines from coverage report.\n",
    "    \"\"\"\n",
    "    \n",
    "    improvement_prompt = f\"\"\"\n",
    "The following code has {coverage_report.get('coverage_percent', 0):.1f}% test coverage.\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "**Existing Tests:**\n",
    "```python\n",
    "{existing_tests}\n",
    "```\n",
    "\n",
    "**Uncovered Lines:** {coverage_report.get('missing_lines', [])}\n",
    "\n",
    "**Task:** Generate additional pytest tests to cover the missing lines.\n",
    "Focus on:\n",
    "- Error handling branches\n",
    "- Edge cases not yet tested\n",
    "- Conditional logic branches\n",
    "\n",
    "Generate only the NEW tests (don't repeat existing ones):\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": improvement_prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"âœ… Coverage analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40269607",
   "metadata": {},
   "source": [
    "## ðŸš€ Production Deployment: End-to-End Code Generation\n",
    "\n",
    "GeneraciÃ³n completa de proyectos ETL listos para producciÃ³n, incluyendo:\n",
    "- âœ… CÃ³digo del pipeline ETL\n",
    "- âœ… Tests completos (unit + integration)\n",
    "- âœ… Dockerfile optimizado\n",
    "- âœ… CI/CD pipeline (GitHub Actions)\n",
    "- âœ… Airflow DAG\n",
    "- âœ… DocumentaciÃ³n (README.md)\n",
    "- âœ… ConfiguraciÃ³n (YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bf344",
   "metadata": {},
   "source": [
    "### Clase ETLProjectGenerator: Generador Completo de Proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class PipelineSpec:\n",
    "    \"\"\"Specification for ETL pipeline project.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    source: Dict\n",
    "    target: Dict\n",
    "    transformations: List[str]\n",
    "    owner: str\n",
    "    sla_minutes: int\n",
    "    compliance: List[str]\n",
    "\n",
    "class ETLProjectGenerator:\n",
    "    \"\"\"End-to-end project generator for production ETL pipelines.\"\"\"\n",
    "    \n",
    "    def __init__(self, spec: PipelineSpec, output_dir: str = \"./generated_pipeline\"):\n",
    "        self.spec = spec\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.validator = CodeValidator()\n",
    "    \n",
    "    def _save_file(self, filename: str, content: str):\n",
    "        \"\"\"Save file to output directory.\"\"\"\n",
    "        file_path = self.output_dir / filename\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        file_path.write_text(content)\n",
    "    \n",
    "    def generate_full_project(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate complete ETL project structure.\n",
    "        \n",
    "        Returns:\n",
    "            Path to generated project directory\n",
    "        \"\"\"\n",
    "        print(f\"ðŸš€ Generating project: {self.spec.name}\")\n",
    "        \n",
    "        # 1. Generate main pipeline code\n",
    "        pipeline_code = self._generate_pipeline_code()\n",
    "        self._save_file(\"pipeline.py\", pipeline_code)\n",
    "        print(\"âœ… Generated pipeline.py\")\n",
    "        \n",
    "        # 2. Generate configuration\n",
    "        config = self._generate_config()\n",
    "        self._save_file(\"config.yaml\", yaml.dump(config))\n",
    "        print(\"âœ… Generated config.yaml\")\n",
    "        \n",
    "        # 3. Generate tests\n",
    "        tests = self._generate_tests(pipeline_code)\n",
    "        self._save_file(\"test_pipeline.py\", tests)\n",
    "        print(\"âœ… Generated test_pipeline.py\")\n",
    "        \n",
    "        # 4. Generate Dockerfile\n",
    "        dockerfile = self._generate_dockerfile()\n",
    "        self._save_file(\"Dockerfile\", dockerfile)\n",
    "        print(\"âœ… Generated Dockerfile\")\n",
    "        \n",
    "        # 5. Generate CI/CD\n",
    "        ci_yaml = self._generate_ci_cd()\n",
    "        self._save_file(\".github/workflows/ci.yml\", ci_yaml)\n",
    "        print(\"âœ… Generated CI/CD pipeline\")\n",
    "        \n",
    "        # 6. Generate documentation\n",
    "        readme = self._generate_readme()\n",
    "        self._save_file(\"README.md\", readme)\n",
    "        print(\"âœ… Generated README.md\")\n",
    "        \n",
    "        # 7. Generate Airflow DAG\n",
    "        dag_code = self._generate_airflow_dag()\n",
    "        self._save_file(\"airflow_dag.py\", dag_code)\n",
    "        print(\"âœ… Generated Airflow DAG\")\n",
    "        \n",
    "        # 8. Validate all\n",
    "        validation_results = self._validate_generated_code()\n",
    "        print(f\"\\\\nðŸ“Š Validation Score: {validation_results['overall_score']}/100\")\n",
    "        \n",
    "        return str(self.output_dir)\n",
    "    \n",
    "    def _generate_pipeline_code(self) -> str:\n",
    "        \"\"\"Generate main ETL pipeline code using LLM.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate production-ready ETL pipeline with:\n",
    "\n",
    "**Name:** {self.spec.name}\n",
    "**Source:** {self.spec.source['type']} â†’ **Target:** {self.spec.target['type']}\n",
    "**Transformations:** {', '.join(self.spec.transformations)}\n",
    "**SLA:** {self.spec.sla_minutes} min | **Compliance:** {', '.join(self.spec.compliance)}\n",
    "\n",
    "Requirements:\n",
    "- Python 3.11+ with type hints\n",
    "- Retry logic (exponential backoff, 3 max)\n",
    "- Structured logging (JSON)\n",
    "- Prometheus metrics\n",
    "- YAML config\n",
    "- Idempotent\n",
    "\n",
    "Generate complete code:\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        code = response.choices[0].message.content.strip()\n",
    "        return code.replace('```python', '').replace('```', '').strip()\n",
    "    \n",
    "    def _generate_config(self) -> Dict:\n",
    "        \"\"\"Generate YAML configuration.\"\"\"\n",
    "        return {\n",
    "            'pipeline': {\n",
    "                'name': self.spec.name,\n",
    "                'version': '1.0.0',\n",
    "                'owner': self.spec.owner,\n",
    "                'sla_minutes': self.spec.sla_minutes\n",
    "            },\n",
    "            'source': self.spec.source,\n",
    "            'target': self.spec.target,\n",
    "            'retry': {'max_attempts': 3, 'backoff_factor': 2},\n",
    "            'logging': {'level': 'INFO', 'format': 'json'}\n",
    "        }\n",
    "    \n",
    "    def _generate_tests(self, pipeline_code: str) -> str:\n",
    "        \"\"\"Generate test suite.\"\"\"\n",
    "        return generate_tests(pipeline_code)\n",
    "    \n",
    "    def _generate_dockerfile(self) -> str:\n",
    "        \"\"\"Generate optimized Dockerfile.\"\"\"\n",
    "        return f'''FROM python:3.11-slim as builder\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir --user -r requirements.txt\n",
    "\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "COPY pipeline.py config.yaml ./\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "RUN useradd -m -u 1000 pipeline && chown -R pipeline:pipeline /app\n",
    "USER pipeline\n",
    "CMD [\"python\", \"pipeline.py\"]\n",
    "'''\n",
    "    \n",
    "    def _generate_ci_cd(self) -> str:\n",
    "        \"\"\"Generate GitHub Actions CI/CD.\"\"\"\n",
    "        return f'''name: CI/CD - {self.spec.name}\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "  pull_request:\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      - run: pip install -r requirements.txt pytest bandit ruff\n",
    "      - run: ruff check .\n",
    "      - run: bandit -r pipeline.py\n",
    "      - run: pytest test_pipeline.py --cov=pipeline\n",
    "'''\n",
    "    \n",
    "    def _generate_readme(self) -> str:\n",
    "        \"\"\"Generate README documentation.\"\"\"\n",
    "        return f'''# {self.spec.name}\n",
    "\n",
    "{self.spec.description}\n",
    "\n",
    "## Architecture\n",
    "- **Source:** {self.spec.source['type']}\n",
    "- **Target:** {self.spec.target['type']}\n",
    "- **SLA:** {self.spec.sla_minutes} minutes\n",
    "\n",
    "## Quickstart\n",
    "```bash\n",
    "docker build -t {self.spec.name} .\n",
    "docker run {self.spec.name}\n",
    "```\n",
    "'''\n",
    "    \n",
    "    def _generate_airflow_dag(self) -> str:\n",
    "        \"\"\"Generate Airflow DAG.\"\"\"\n",
    "        return f'''from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {{\n",
    "    'owner': '{self.spec.owner}',\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}}\n",
    "\n",
    "with DAG(\n",
    "    '{self.spec.name}',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    start_date=datetime(2024, 1, 1)\n",
    ") as dag:\n",
    "    \n",
    "    def run_pipeline():\n",
    "        from pipeline import run_pipeline\n",
    "        return run_pipeline('config.yaml')\n",
    "    \n",
    "    task = PythonOperator(\n",
    "        task_id='run_etl',\n",
    "        python_callable=run_pipeline\n",
    "    )\n",
    "'''\n",
    "    \n",
    "    def _validate_generated_code(self) -> Dict:\n",
    "        \"\"\"Validate all generated code.\"\"\"\n",
    "        pipeline_path = self.output_dir / \"pipeline.py\"\n",
    "        if pipeline_path.exists():\n",
    "            code = pipeline_path.read_text()\n",
    "            return self.validator.validate_all(code)\n",
    "        return {'overall_score': 0}\n",
    "\n",
    "print(\"âœ… Clase ETLProjectGenerator definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc56a",
   "metadata": {},
   "source": [
    "### Ejemplo: Generar Proyecto ETL Completo\n",
    "\n",
    "Generar proyecto end-to-end desde una especificaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0abe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project specification\n",
    "spec = PipelineSpec(\n",
    "    name=\"sales_analytics_etl\",\n",
    "    description=\"Daily ETL pipeline to process sales data from PostgreSQL to S3 Data Lake\",\n",
    "    source={\n",
    "        'type': 'postgresql',\n",
    "        'host': 'db.example.com',\n",
    "        'database': 'sales_db',\n",
    "        'table': 'transactions'\n",
    "    },\n",
    "    target={\n",
    "        'type': 's3',\n",
    "        'bucket': 'datalake-prod',\n",
    "        'prefix': 'sales/processed/',\n",
    "        'format': 'parquet'\n",
    "    },\n",
    "    transformations=[\n",
    "        'Filter out test transactions',\n",
    "        'Deduplicate by transaction_id',\n",
    "        'Calculate revenue (quantity * price)',\n",
    "        'Aggregate by product and date',\n",
    "        'Add data quality metrics'\n",
    "    ],\n",
    "    owner='data-engineering-team',\n",
    "    sla_minutes=30,\n",
    "    compliance=['GDPR', 'SOC2', 'PII-masking']\n",
    ")\n",
    "\n",
    "# Generate complete project\n",
    "generator = ETLProjectGenerator(spec, output_dir=\"./sales_analytics_pipeline\")\n",
    "project_path = generator.generate_full_project()\n",
    "\n",
    "print(f\"\\\\nâœ… Project generated successfully at: {project_path}\")\n",
    "print(\"\\\\nGenerated files:\")\n",
    "for file in Path(project_path).rglob(\"*\"):\n",
    "    if file.is_file():\n",
    "        print(f\"  - {file.relative_to(project_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c3632",
   "metadata": {},
   "source": [
    "## 1. GeneraciÃ³n de funciÃ³n ETL bÃ¡sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165223b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_etl_code(description: str) -> str:\n",
    "    prompt = f'''\n",
    "Eres un experto en Python y pipelines ETL. Genera cÃ³digo Python completo y ejecutable.\n",
    "\n",
    "Requerimientos:\n",
    "{description}\n",
    "\n",
    "Incluye:\n",
    "- Imports necesarios\n",
    "- Manejo de errores\n",
    "- Logging bÃ¡sico\n",
    "- Docstrings\n",
    "- Type hints\n",
    "\n",
    "CÃ³digo:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "desc = '''\n",
    "FunciÃ³n que:\n",
    "1. Lee CSV de ventas desde S3 (boto3)\n",
    "2. Filtra filas con total > 0\n",
    "3. Agrega columna \"mes\" (YYYY-MM) desde \"fecha\"\n",
    "4. Escribe a Parquet particionado por mes\n",
    "'''\n",
    "\n",
    "code = generate_etl_code(desc)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dda91b",
   "metadata": {},
   "source": [
    "## 2. GeneraciÃ³n de transformaciÃ³n Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_spec = '''\n",
    "Dataset: transacciones bancarias (CSV)\n",
    "Columnas: trans_id, fecha, monto, tipo (debito/credito), cuenta_id\n",
    "\n",
    "Transformaciones:\n",
    "1. Convertir fecha a datetime\n",
    "2. Crear columna \"anio_mes\" (formato YYYY-MM)\n",
    "3. Calcular saldo acumulado por cuenta_id ordenado por fecha\n",
    "4. Agregar flag is_anomaly si monto > 3 desviaciones estÃ¡ndar de la media de esa cuenta\n",
    "5. Exportar a CSV con encoding UTF-8\n",
    "'''\n",
    "\n",
    "transform_code = generate_etl_code(transformation_spec)\n",
    "print(transform_code[:500] + '...')  # Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2f97d",
   "metadata": {},
   "source": [
    "## 3. ValidaciÃ³n de cÃ³digo generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import subprocess\n",
    "\n",
    "def validate_python_syntax(code: str) -> tuple[bool, str]:\n",
    "    \"\"\"Valida sintaxis Python sin ejecutar.\"\"\"\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True, 'Sintaxis vÃ¡lida'\n",
    "    except SyntaxError as e:\n",
    "        return False, f'Error de sintaxis: {e}'\n",
    "\n",
    "def lint_code(code: str, tool='flake8') -> str:\n",
    "    \"\"\"Ejecuta linter (requiere instalado).\"\"\"\n",
    "    try:\n",
    "        with open('/tmp/temp_code.py', 'w') as f:\n",
    "            f.write(code)\n",
    "        result = subprocess.run([tool, '/tmp/temp_code.py'], capture_output=True, text=True)\n",
    "        return result.stdout if result.returncode == 0 else result.stderr\n",
    "    except Exception as e:\n",
    "        return f'Error linting: {e}'\n",
    "\n",
    "valid, msg = validate_python_syntax(code)\n",
    "print(f'{\"âœ…\" if valid else \"âŒ\"} {msg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78453daa",
   "metadata": {},
   "source": [
    "## 4. GeneraciÃ³n de DAG de Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_spec = '''\n",
    "DAG de Airflow para:\n",
    "- Nombre: ventas_daily_etl\n",
    "- Schedule: diario a las 2 AM\n",
    "- Tareas:\n",
    "  1. extract_s3: descarga ventas.csv de S3\n",
    "  2. validate: Great Expectations (columnas no nulas, total > 0)\n",
    "  3. transform: agrega mes, calcula mÃ©tricas\n",
    "  4. load_db: inserta en PostgreSQL (tabla ventas_daily)\n",
    "  5. send_alert: email si falla cualquier paso\n",
    "- Retries: 2 con delay de 5 min\n",
    "'''\n",
    "\n",
    "dag_code = generate_etl_code(dag_spec)\n",
    "print(dag_code[:600] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c217a9",
   "metadata": {},
   "source": [
    "## 5. IteraciÃ³n y mejora con feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_code(original_code: str, feedback: str) -> str:\n",
    "    prompt = f'''\n",
    "Mejora este cÃ³digo Python segÃºn el feedback:\n",
    "\n",
    "CÃ³digo original:\n",
    "{original_code}\n",
    "\n",
    "Feedback:\n",
    "{feedback}\n",
    "\n",
    "CÃ³digo mejorado:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "feedback_example = '''\n",
    "- AÃ±adir retry con exponential backoff en la descarga de S3\n",
    "- Usar context manager para conexiÃ³n a DB\n",
    "- Loggear nÃºmero de filas procesadas\n",
    "'''\n",
    "\n",
    "improved = refine_code(code, feedback_example)\n",
    "print(improved[:400] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455aad6",
   "metadata": {},
   "source": [
    "## 6. GeneraciÃ³n de tests unitarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7956fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(code: str) -> str:\n",
    "    prompt = f'''\n",
    "Genera tests unitarios con pytest para este cÃ³digo:\n",
    "\n",
    "{code}\n",
    "\n",
    "Incluye:\n",
    "- Test de caso normal (happy path)\n",
    "- Test con datos vacÃ­os\n",
    "- Test con errores (valores nulos, tipos incorrectos)\n",
    "- Mocks para I/O externo (S3, DB)\n",
    "\n",
    "Tests:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```python','').replace('```','')\n",
    "\n",
    "test_code = generate_tests(code)\n",
    "print(test_code[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35a34c",
   "metadata": {},
   "source": [
    "## 7. Buenas prÃ¡cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1e9a8",
   "metadata": {},
   "source": [
    "- **Revisar siempre**: nunca ejecutes cÃ³digo generado sin inspecciÃ³n humana.\n",
    "- **ValidaciÃ³n automÃ¡tica**: sintaxis, linting, tests.\n",
    "- **Versionado**: guarda cÃ³digo generado en Git con mensaje descriptivo.\n",
    "- **Plantillas**: usa templates para estructura consistente.\n",
    "- **IteraciÃ³n**: refina con feedback humano y re-generaciÃ³n.\n",
    "- **DocumentaciÃ³n**: genera README y comentarios junto con cÃ³digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fae6a5",
   "metadata": {},
   "source": [
    "## 8. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89411c7c",
   "metadata": {},
   "source": [
    "1. Genera un script que migre datos de MongoDB a PostgreSQL.\n",
    "2. Crea un pipeline Spark (PySpark) que lea Parquet y escriba Delta Lake.\n",
    "3. Automatiza generaciÃ³n de un data quality report con Great Expectations.\n",
    "4. Construye un CLI (Click/Typer) generado por LLM para ejecutar ETLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## ðŸ§­ NavegaciÃ³n",
    "",
    "**â† Anterior:** [â† GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "",
    "**Siguiente â†’:** [RAG: DocumentaciÃ³n de Datos â†’](04_rag_documentacion_datos.ipynb)",
    "",
    "**ðŸ“š Ãndice de Nivel GenAI:**",
    "- [ComparaciÃ³n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [GeneraciÃ³n AutomÃ¡tica de CÃ³digo ETL](03_generacion_codigo_etl.ipynb) â† ðŸ”µ EstÃ¡s aquÃ­",
    "- [RAG: DocumentaciÃ³n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y AutomatizaciÃ³n](06_agentes_automatizacion.ipynb)",
    "- [ValidaciÃ³n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [SÃ­ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**ðŸŽ“ Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## ðŸ§­ NavegaciÃ³n",
    "",
    "**â† Anterior:** [â† GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "",
    "**Siguiente â†’:** [RAG: DocumentaciÃ³n de Datos â†’](04_rag_documentacion_datos.ipynb)",
    "",
    "**ðŸ“š Ãndice de Nivel GenAI:**",
    "- [ComparaciÃ³n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [GeneraciÃ³n AutomÃ¡tica de CÃ³digo ETL](03_generacion_codigo_etl.ipynb) â† ðŸ”µ EstÃ¡s aquÃ­",
    "- [RAG: DocumentaciÃ³n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y AutomatizaciÃ³n](06_agentes_automatizacion.ipynb)",
    "- [ValidaciÃ³n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [SÃ­ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**ðŸŽ“ Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## ðŸ§­ NavegaciÃ³n",
    "",
    "**â† Anterior:** [â† GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "",
    "**Siguiente â†’:** [RAG: DocumentaciÃ³n de Datos â†’](04_rag_documentacion_datos.ipynb)",
    "",
    "**ðŸ“š Ãndice de Nivel GenAI:**",
    "- [ComparaciÃ³n OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)",
    "- [Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)",
    "- [GeneraciÃ³n SQL: NL2SQL](02_generacion_sql_nl2sql.ipynb)",
    "- [GeneraciÃ³n AutomÃ¡tica de CÃ³digo ETL](03_generacion_codigo_etl.ipynb) â† ðŸ”µ EstÃ¡s aquÃ­",
    "- [RAG: DocumentaciÃ³n de Datos](04_rag_documentacion_datos.ipynb)",
    "- [Embeddings y Similitud de Datos](05_embeddings_similitud_datos.ipynb)",
    "- [Agentes y AutomatizaciÃ³n](06_agentes_automatizacion.ipynb)",
    "- [ValidaciÃ³n y Calidad con LLMs](07_calidad_validacion_llm.ipynb)",
    "- [SÃ­ntesis y Aumento de Datos](08_sintesis_aumento_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**ðŸŽ“ Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}