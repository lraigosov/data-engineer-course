{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d526b91",
   "metadata": {},
   "source": [
    "# 📊 Text-to-SQL: Generación de Consultas SQL desde Lenguaje Natural\n",
    "\n",
    "Objetivo: implementar sistemas NL2SQL (Natural Language to SQL) robustos usando LLMs, con validación, seguridad y optimización.\n",
    "\n",
    "- Duración: 90-120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: GenAI 01, SQL intermedio, schemas de base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1491d26",
   "metadata": {},
   "source": [
    "## 1. Caso base: prompt simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca14444",
   "metadata": {},
   "source": [
    "### 🏗️ **NL2SQL Architecture: From Research to Production**\n",
    "\n",
    "**¿Qué es NL2SQL?**\n",
    "\n",
    "Natural Language to SQL (NL2SQL) es un sistema que traduce preguntas en lenguaje natural a consultas SQL ejecutables. Es uno de los use cases más valiosos de LLMs para democratizar acceso a datos.\n",
    "\n",
    "**Evolution of NL2SQL:**\n",
    "\n",
    "```\n",
    "2015-2018: Rule-Based Systems\n",
    "├── Regex patterns + templates\n",
    "├── Limited vocabulary\n",
    "└── Accuracy: ~30-40%\n",
    "\n",
    "2018-2020: Seq2Seq Models\n",
    "├── LSTM/GRU encoders\n",
    "├── Better generalization\n",
    "└── Accuracy: ~50-60%\n",
    "\n",
    "2020-2023: Pre-trained LLMs\n",
    "├── BERT → T5 → GPT-3\n",
    "├── Spider benchmark: 70%+\n",
    "└── Fine-tuned models (SQLCoder)\n",
    "\n",
    "2023-Present: GPT-4 + RAG\n",
    "├── Zero-shot: 80-85%\n",
    "├── Few-shot: 85-90%\n",
    "└── Production-ready with validation\n",
    "```\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    NL2SQL System                            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. Schema Retrieval (RAG)                                  │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ Question → Embed → Search Vector DB   │              │\n",
    "│     │ Return: Relevant tables + columns     │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "│                        ↓                                     │\n",
    "│  2. Prompt Construction                                     │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ • Schema context (DDL + samples)      │              │\n",
    "│     │ • Few-shot examples                   │              │\n",
    "│     │ • Dialect-specific instructions       │              │\n",
    "│     │ • Query constraints (LIMIT, timeout)  │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "│                        ↓                                     │\n",
    "│  3. LLM Generation                                          │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ GPT-4o / Claude 3.5 / SQLCoder        │              │\n",
    "│     │ Temperature: 0 (deterministic)        │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "│                        ↓                                     │\n",
    "│  4. Validation & Safety                                     │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ • SQL parsing (sqlparse)              │              │\n",
    "│     │ • Read-only check                     │              │\n",
    "│     │ • Injection prevention                │              │\n",
    "│     │ • EXPLAIN plan analysis               │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "│                        ↓                                     │\n",
    "│  5. Execution Engine                                        │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ • Dry run mode (EXPLAIN only)         │              │\n",
    "│     │ • Timeout enforcement                 │              │\n",
    "│     │ • Result caching                      │              │\n",
    "│     │ • Error handling                      │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "│                        ↓                                     │\n",
    "│  6. Post-Processing                                         │\n",
    "│     ┌───────────────────────────────────────┐              │\n",
    "│     │ • Result formatting                   │              │\n",
    "│     │ • Aggregation                         │              │\n",
    "│     │ • Visualization hints                 │              │\n",
    "│     └───────────────────────────────────────┘              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Schema Representation Formats:**\n",
    "\n",
    "**1. DDL (Data Definition Language):**\n",
    "```sql\n",
    "CREATE TABLE sales (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    customer_id INT NOT NULL REFERENCES customers(customer_id),\n",
    "    product_id INT NOT NULL REFERENCES products(product_id),\n",
    "    sale_date DATE NOT NULL,\n",
    "    quantity INT CHECK (quantity > 0),\n",
    "    total_amount DECIMAL(10,2),\n",
    "    status VARCHAR(20) DEFAULT 'pending',\n",
    "    INDEX idx_customer (customer_id),\n",
    "    INDEX idx_date (sale_date)\n",
    ");\n",
    "```\n",
    "\n",
    "**2. JSON Schema:**\n",
    "```json\n",
    "{\n",
    "    \"tables\": {\n",
    "        \"sales\": {\n",
    "            \"columns\": {\n",
    "                \"sale_id\": {\"type\": \"INT\", \"primary_key\": true},\n",
    "                \"customer_id\": {\"type\": \"INT\", \"foreign_key\": \"customers.customer_id\"},\n",
    "                \"sale_date\": {\"type\": \"DATE\", \"nullable\": false},\n",
    "                \"total_amount\": {\"type\": \"DECIMAL(10,2)\"}\n",
    "            },\n",
    "            \"indexes\": [\"idx_customer\", \"idx_date\"],\n",
    "            \"row_count\": 1500000,\n",
    "            \"description\": \"Stores all sales transactions\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**3. Natural Language Description:**\n",
    "```python\n",
    "schema_description = \"\"\"\n",
    "Database: E-commerce Sales System\n",
    "\n",
    "Tables:\n",
    "1. sales (1.5M rows)\n",
    "   - sale_id: unique identifier for each sale\n",
    "   - customer_id: links to customers table\n",
    "   - product_id: links to products table\n",
    "   - sale_date: when the sale occurred\n",
    "   - quantity: number of items sold\n",
    "   - total_amount: total price in USD\n",
    "   - status: pending, completed, or cancelled\n",
    "\n",
    "2. customers (50K rows)\n",
    "   - customer_id: unique customer identifier\n",
    "   - name: customer full name\n",
    "   - email: contact email\n",
    "   - country: customer location\n",
    "   - signup_date: when they registered\n",
    "\n",
    "3. products (5K rows)\n",
    "   - product_id: unique product identifier\n",
    "   - name: product name\n",
    "   - category: electronics, clothing, books, etc.\n",
    "   - price: unit price in USD\n",
    "   - stock_quantity: current inventory\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Schema Retrieval with RAG:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Embed schema components\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ChromaDB for schema storage\n",
    "chroma_client = chromadb.Client()\n",
    "schema_collection = chroma_client.create_collection(\"database_schema\")\n",
    "\n",
    "# Index tables with descriptions\n",
    "tables_data = [\n",
    "    {\n",
    "        \"id\": \"sales\",\n",
    "        \"text\": \"sales table stores transactions with customer_id, product_id, date, quantity, amount\",\n",
    "        \"metadata\": {\"type\": \"table\", \"row_count\": 1500000}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"customers\",\n",
    "        \"text\": \"customers table has customer info: name, email, country, signup_date\",\n",
    "        \"metadata\": {\"type\": \"table\", \"row_count\": 50000}\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables_data:\n",
    "    schema_collection.add(\n",
    "        ids=[table[\"id\"]],\n",
    "        documents=[table[\"text\"]],\n",
    "        metadatas=[table[\"metadata\"]]\n",
    "    )\n",
    "\n",
    "# Query relevant schema\n",
    "def get_relevant_schema(question: str, top_k: int = 3):\n",
    "    \"\"\"Retrieve relevant tables for question\"\"\"\n",
    "    results = schema_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    relevant_tables = results['ids'][0]\n",
    "    return relevant_tables\n",
    "\n",
    "# Example\n",
    "question = \"What are the top selling products last month?\"\n",
    "relevant = get_relevant_schema(question)\n",
    "print(f\"Relevant tables: {relevant}\")\n",
    "# Output: ['sales', 'products']\n",
    "```\n",
    "\n",
    "**Model Comparison for NL2SQL:**\n",
    "\n",
    "| Model | Accuracy (Spider) | Context | Cost | Best For |\n",
    "|-------|------------------|---------|------|----------|\n",
    "| GPT-4o | 85-90% | 128K | $$ | Complex queries, multi-table joins |\n",
    "| GPT-4o-mini | 75-80% | 128K | $ | Simple queries, high volume |\n",
    "| Claude 3.5 Sonnet | 88-92% | 200K | $$$ | Best accuracy, long schemas |\n",
    "| Gemini 1.5 Flash | 80-85% | 1M | $ | Huge schemas, fast |\n",
    "| SQLCoder 70B | 82-85% | 8K | Free | Self-hosted, fine-tuned |\n",
    "| Llama 3 70B + LoRA | 75-80% | 8K | Free | Budget option, trainable |\n",
    "\n",
    "**SQLCoder (Fine-tuned Open Source):**\n",
    "\n",
    "```python\n",
    "# Defog SQLCoder - specialized for SQL generation\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"defog/sqlcoder-70b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"defog/sqlcoder-70b-alpha\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True  # Quantization para reducir VRAM\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"### Task\n",
    "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
    "\n",
    "### Database Schema\n",
    "{schema}\n",
    "\n",
    "### SQL\n",
    "SELECT\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "**Advantages of SQLCoder:**\n",
    "- ✅ No API costs (run locally)\n",
    "- ✅ Data privacy (nothing sent externally)\n",
    "- ✅ Fine-tunable on your schema\n",
    "- ❌ Requires GPU (70B model needs 48GB VRAM or quantization)\n",
    "- ❌ Slower inference than API calls\n",
    "\n",
    "**Prompt Engineering for NL2SQL:**\n",
    "\n",
    "```python\n",
    "from string import Template\n",
    "\n",
    "NL2SQL_PROMPT = Template(\"\"\"\n",
    "You are an expert $dialect SQL developer. Generate a query to answer the question.\n",
    "\n",
    "### Database Schema\n",
    "$schema\n",
    "\n",
    "### Instructions\n",
    "- Use table aliases for readability\n",
    "- Add appropriate JOINs based on foreign keys\n",
    "- Include LIMIT clause (max $max_rows rows)\n",
    "- Use aggregate functions when appropriate\n",
    "- Handle NULL values properly\n",
    "- Return ONLY the SQL query, no explanations\n",
    "\n",
    "### Examples\n",
    "$examples\n",
    "\n",
    "### Question\n",
    "$question\n",
    "\n",
    "### SQL Query\n",
    "\"\"\")\n",
    "\n",
    "# Usage\n",
    "prompt = NL2SQL_PROMPT.substitute(\n",
    "    dialect=\"PostgreSQL\",\n",
    "    schema=schema_ddl,\n",
    "    max_rows=1000,\n",
    "    examples=few_shot_examples,\n",
    "    question=\"Show revenue by product category for Q4 2024\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Handling Complex Queries:**\n",
    "\n",
    "```python\n",
    "# Multi-step query decomposition\n",
    "def complex_nl2sql(question: str, schema: str) -> str:\n",
    "    \"\"\"For complex questions, break into sub-queries\"\"\"\n",
    "    \n",
    "    decomposition_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    \n",
    "    Is this a complex question requiring multiple steps?\n",
    "    If yes, break it down into sub-questions.\n",
    "    If no, answer \"simple\".\n",
    "    \n",
    "    Format:\n",
    "    Type: simple | complex\n",
    "    Sub-questions (if complex):\n",
    "    1. ...\n",
    "    2. ...\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ask_llm(decomposition_prompt, temperature=0)\n",
    "    \n",
    "    if \"simple\" in response.lower():\n",
    "        return nl_to_sql(question, schema)\n",
    "    \n",
    "    # Generate SQL for each sub-question\n",
    "    sub_questions = extract_sub_questions(response)\n",
    "    sub_sqls = [nl_to_sql(sq, schema) for sq in sub_questions]\n",
    "    \n",
    "    # Combine with CTEs\n",
    "    combined_prompt = f\"\"\"\n",
    "    Combine these sub-queries into a single query using CTEs:\n",
    "    \n",
    "    {chr(10).join(f\"-- Step {i+1}: {sq}\" for i, sq in enumerate(sub_questions))}\n",
    "    {chr(10).join(sub_sqls)}\n",
    "    \n",
    "    Generate final query with CTEs:\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_llm(combined_prompt, temperature=0)\n",
    "\n",
    "# Example\n",
    "question = \"Compare average order value between new and returning customers for each product category\"\n",
    "# → Breaks into: 1) Identify new vs returning, 2) Calculate AOV, 3) Group by category\n",
    "sql = complex_nl2sql(question, schema)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929429b9",
   "metadata": {},
   "source": [
    "### 🛡️ **Security & Validation: Preventing SQL Injection**\n",
    "\n",
    "**Security Threats in NL2SQL:**\n",
    "\n",
    "1. **SQL Injection via Prompt**: Usuario manipula pregunta para inyectar código\n",
    "2. **Data Exfiltration**: Query extrae datos sensibles no autorizados\n",
    "3. **Denial of Service**: Queries costosas que consumen recursos\n",
    "4. **Schema Exposure**: Revelar estructura de base de datos\n",
    "\n",
    "**Defense Layers:**\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────┐\n",
    "│         Security Architecture              │\n",
    "├────────────────────────────────────────────┤\n",
    "│                                            │\n",
    "│  Layer 1: Input Sanitization              │\n",
    "│  ├─ Remove SQL keywords from question     │\n",
    "│  ├─ Escape special characters             │\n",
    "│  └─ Length limits                          │\n",
    "│                                            │\n",
    "│  Layer 2: LLM Guardrails                  │\n",
    "│  ├─ System prompt constraints             │\n",
    "│  ├─ Read-only emphasis                    │\n",
    "│  └─ Whitelist operations                   │\n",
    "│                                            │\n",
    "│  Layer 3: SQL Parsing & Analysis          │\n",
    "│  ├─ sqlparse validation                   │\n",
    "│  ├─ AST inspection                        │\n",
    "│  └─ Keyword blacklist                      │\n",
    "│                                            │\n",
    "│  Layer 4: Query Plan Analysis             │\n",
    "│  ├─ EXPLAIN cost estimation               │\n",
    "│  ├─ Timeout prediction                    │\n",
    "│  └─ Resource limits                        │\n",
    "│                                            │\n",
    "│  Layer 5: Execution Sandbox               │\n",
    "│  ├─ Read-only user                        │\n",
    "│  ├─ Row limit enforcement                 │\n",
    "│  ├─ Timeout killer                        │\n",
    "│  └─ Query logging                          │\n",
    "└────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "**1. Input Sanitization:**\n",
    "\n",
    "```python\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "def sanitize_question(question: str) -> Tuple[str, bool]:\n",
    "    \"\"\"Remove potentially dangerous patterns from user input\"\"\"\n",
    "    \n",
    "    # Check length\n",
    "    if len(question) > 500:\n",
    "        return \"\", False\n",
    "    \n",
    "    # Suspicious patterns\n",
    "    dangerous_patterns = [\n",
    "        r\";\\s*(drop|delete|update|insert|alter|create|truncate)\",  # Multiple statements\n",
    "        r\"--\",  # SQL comments\n",
    "        r\"/\\*.*\\*/\",  # Block comments\n",
    "        r\"union\\s+select\",  # Union injection\n",
    "        r\"exec\\s*\\(\",  # Stored proc execution\n",
    "        r\"xp_cmdshell\",  # System commands\n",
    "    ]\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    for pattern in dangerous_patterns:\n",
    "        if re.search(pattern, question_lower):\n",
    "            return \"\", False\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', question).strip()\n",
    "    \n",
    "    return cleaned, True\n",
    "\n",
    "# Example\n",
    "test_questions = [\n",
    "    \"Show top 10 sales\",  # Safe\n",
    "    \"Show sales; DROP TABLE customers; --\",  # SQL injection\n",
    "    \"SELECT * FROM users UNION SELECT password FROM admin\",  # Union injection\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    cleaned, safe = sanitize_question(q)\n",
    "    print(f\"{'✅' if safe else '❌'} {q[:50]}\")\n",
    "```\n",
    "\n",
    "**2. SQL Validation with sqlparse:**\n",
    "\n",
    "```python\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier, Where\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "def validate_sql_safety(sql: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Comprehensive SQL safety validation\"\"\"\n",
    "    \n",
    "    # Parse SQL\n",
    "    try:\n",
    "        parsed = sqlparse.parse(sql)\n",
    "        if not parsed:\n",
    "            return False, \"Invalid SQL syntax\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Parsing error: {e}\"\n",
    "    \n",
    "    statement = parsed[0]\n",
    "    \n",
    "    # 1. Check if read-only (SELECT only)\n",
    "    if statement.get_type() != 'SELECT':\n",
    "        return False, f\"Only SELECT queries allowed, got: {statement.get_type()}\"\n",
    "    \n",
    "    # 2. Check for nested dangerous operations\n",
    "    sql_lower = sql.lower()\n",
    "    write_operations = ['insert', 'update', 'delete', 'drop', 'create', 'alter', 'truncate', 'replace']\n",
    "    \n",
    "    for op in write_operations:\n",
    "        # Check for operation as separate word\n",
    "        if re.search(rf'\\b{op}\\b', sql_lower):\n",
    "            return False, f\"Forbidden operation: {op.upper()}\"\n",
    "    \n",
    "    # 3. Check for system functions/procedures\n",
    "    dangerous_functions = [\n",
    "        'exec', 'execute', 'xp_cmdshell', 'sp_executesql',\n",
    "        'dbms_output', 'utl_file', 'load_file', 'outfile'\n",
    "    ]\n",
    "    \n",
    "    for func in dangerous_functions:\n",
    "        if func in sql_lower:\n",
    "            return False, f\"Forbidden function: {func}\"\n",
    "    \n",
    "    # 4. Check for multiple statements (SQL injection)\n",
    "    if len(parsed) > 1:\n",
    "        return False, \"Multiple statements not allowed\"\n",
    "    \n",
    "    # 5. Verify LIMIT clause exists (prevent large result sets)\n",
    "    has_limit = 'limit' in sql_lower or 'fetch first' in sql_lower or 'top' in sql_lower\n",
    "    if not has_limit:\n",
    "        return False, \"Query must include LIMIT clause\"\n",
    "    \n",
    "    # 6. Check for UNION (common injection vector)\n",
    "    if 'union' in sql_lower:\n",
    "        # UNION is valid but requires manual review\n",
    "        return False, \"UNION queries require manual approval\"\n",
    "    \n",
    "    return True, \"Query validated\"\n",
    "\n",
    "# Test cases\n",
    "test_sqls = [\n",
    "    \"SELECT * FROM sales LIMIT 100\",  # Safe\n",
    "    \"SELECT * FROM sales; DROP TABLE customers;\",  # Multiple statements\n",
    "    \"SELECT * FROM sales WHERE id = (SELECT password FROM users)\",  # Subquery injection\n",
    "    \"DELETE FROM sales WHERE id = 1\",  # Write operation\n",
    "    \"SELECT * FROM sales\",  # No LIMIT\n",
    "]\n",
    "\n",
    "for sql in test_sqls:\n",
    "    safe, message = validate_sql_safety(sql)\n",
    "    print(f\"{'✅' if safe else '❌'} {message}\")\n",
    "    print(f\"   {sql[:60]}\\n\")\n",
    "```\n",
    "\n",
    "**3. Query Cost Estimation (EXPLAIN):**\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "\n",
    "def estimate_query_cost(sql: str, conn) -> Tuple[bool, dict]:\n",
    "    \"\"\"Use EXPLAIN to estimate query cost before execution\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get query plan\n",
    "        explain_sql = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {sql}\"\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(explain_sql)\n",
    "            plan = cur.fetchone()[0][0]\n",
    "        \n",
    "        # Extract cost metrics\n",
    "        total_cost = plan['Plan']['Total Cost']\n",
    "        estimated_rows = plan['Plan']['Plan Rows']\n",
    "        \n",
    "        # Define thresholds\n",
    "        MAX_COST = 10000  # Arbitrary units\n",
    "        MAX_ROWS = 100000\n",
    "        \n",
    "        if total_cost > MAX_COST:\n",
    "            return False, {\n",
    "                \"reason\": \"Query cost too high\",\n",
    "                \"cost\": total_cost,\n",
    "                \"threshold\": MAX_COST\n",
    "            }\n",
    "        \n",
    "        if estimated_rows > MAX_ROWS:\n",
    "            return False, {\n",
    "                \"reason\": \"Too many rows\",\n",
    "                \"rows\": estimated_rows,\n",
    "                \"threshold\": MAX_ROWS\n",
    "            }\n",
    "        \n",
    "        return True, {\n",
    "            \"cost\": total_cost,\n",
    "            \"rows\": estimated_rows,\n",
    "            \"approved\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "sql = \"SELECT * FROM large_table WHERE date > '2020-01-01' LIMIT 1000\"\n",
    "conn = psycopg2.connect(\"dbname=mydb\")\n",
    "\n",
    "approved, metrics = estimate_query_cost(sql, conn)\n",
    "if approved:\n",
    "    print(f\"✅ Query approved: {metrics}\")\n",
    "else:\n",
    "    print(f\"❌ Query rejected: {metrics}\")\n",
    "```\n",
    "\n",
    "**4. Read-Only Database User:**\n",
    "\n",
    "```sql\n",
    "-- Create read-only role for NL2SQL execution\n",
    "CREATE ROLE nl2sql_readonly;\n",
    "\n",
    "-- Grant SELECT only on specific schemas\n",
    "GRANT USAGE ON SCHEMA public TO nl2sql_readonly;\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA public TO nl2sql_readonly;\n",
    "\n",
    "-- Deny write operations explicitly\n",
    "REVOKE INSERT, UPDATE, DELETE, TRUNCATE ON ALL TABLES IN SCHEMA public FROM nl2sql_readonly;\n",
    "\n",
    "-- Create user\n",
    "CREATE USER nl2sql_user WITH PASSWORD 'secure_password';\n",
    "GRANT nl2sql_readonly TO nl2sql_user;\n",
    "\n",
    "-- Set session limits\n",
    "ALTER ROLE nl2sql_user SET statement_timeout = '10s';\n",
    "ALTER ROLE nl2sql_user SET lock_timeout = '5s';\n",
    "```\n",
    "\n",
    "**5. Execution with Timeout & Limits:**\n",
    "\n",
    "```python\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class QueryTimeout(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def query_timeout(seconds: int):\n",
    "    \"\"\"Context manager to enforce query timeout\"\"\"\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise QueryTimeout(f\"Query exceeded {seconds}s timeout\")\n",
    "    \n",
    "    # Set alarm\n",
    "    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(seconds)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "def execute_safe_query(sql: str, conn, max_rows: int = 1000, timeout_sec: int = 10):\n",
    "    \"\"\"Execute query with safety constraints\"\"\"\n",
    "    \n",
    "    # Validate SQL\n",
    "    safe, message = validate_sql_safety(sql)\n",
    "    if not safe:\n",
    "        raise ValueError(f\"Unsafe query: {message}\")\n",
    "    \n",
    "    # Force LIMIT if not present\n",
    "    sql_lower = sql.lower()\n",
    "    if 'limit' not in sql_lower:\n",
    "        sql += f\" LIMIT {max_rows}\"\n",
    "    \n",
    "    try:\n",
    "        with query_timeout(timeout_sec):\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "        \n",
    "        # Additional row limit check\n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "            print(f\"⚠️ Results truncated to {max_rows} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except QueryTimeout as e:\n",
    "        raise TimeoutError(f\"Query timeout: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Query execution failed: {e}\")\n",
    "\n",
    "# Example\n",
    "sql = \"SELECT * FROM sales WHERE date >= '2024-01-01'\"\n",
    "df = execute_safe_query(sql, conn, max_rows=500, timeout_sec=5)\n",
    "```\n",
    "\n",
    "**6. Audit Logging:**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nl2sql')\n",
    "\n",
    "def log_nl2sql_query(\n",
    "    user_id: str,\n",
    "    question: str,\n",
    "    sql_generated: str,\n",
    "    execution_status: str,\n",
    "    rows_returned: int,\n",
    "    execution_time_ms: float,\n",
    "    error: str = None\n",
    "):\n",
    "    \"\"\"Log all NL2SQL interactions for audit\"\"\"\n",
    "    \n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"user_id\": user_id,\n",
    "        \"question_hash\": hashlib.sha256(question.encode()).hexdigest()[:16],\n",
    "        \"question_length\": len(question),\n",
    "        \"sql_hash\": hashlib.sha256(sql_generated.encode()).hexdigest()[:16],\n",
    "        \"execution_status\": execution_status,  # success | validation_failed | timeout | error\n",
    "        \"rows_returned\": rows_returned,\n",
    "        \"execution_time_ms\": execution_time_ms,\n",
    "        \"error\": error\n",
    "    }\n",
    "    \n",
    "    # Only log full text in dev environment\n",
    "    if os.getenv(\"ENV\") == \"dev\":\n",
    "        log_entry[\"question\"] = question[:200]\n",
    "        log_entry[\"sql\"] = sql_generated[:500]\n",
    "    \n",
    "    logger.info(json.dumps(log_entry))\n",
    "\n",
    "# Usage\n",
    "log_nl2sql_query(\n",
    "    user_id=\"user_123\",\n",
    "    question=\"Show top 10 customers by revenue\",\n",
    "    sql_generated=\"SELECT customer_id, SUM(total) FROM sales GROUP BY customer_id ORDER BY 2 DESC LIMIT 10\",\n",
    "    execution_status=\"success\",\n",
    "    rows_returned=10,\n",
    "    execution_time_ms=45.2\n",
    ")\n",
    "```\n",
    "\n",
    "**7. Human-in-the-Loop for High-Risk Queries:**\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "\n",
    "class RiskLevel(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "def assess_query_risk(sql: str) -> RiskLevel:\n",
    "    \"\"\"Determine if query needs human approval\"\"\"\n",
    "    \n",
    "    sql_lower = sql.lower()\n",
    "    \n",
    "    # High risk indicators\n",
    "    high_risk = [\n",
    "        'union',  # Complex joins\n",
    "        'cross join',  # Cartesian products\n",
    "        'recursive',  # CTEs\n",
    "        'window function',  # Complex analytics\n",
    "    ]\n",
    "    \n",
    "    # Medium risk indicators\n",
    "    medium_risk = [\n",
    "        'subquery',\n",
    "        'having',\n",
    "        'case when',\n",
    "        'join' in sql_lower and sql_lower.count('join') > 2,  # Multiple joins\n",
    "    ]\n",
    "    \n",
    "    for indicator in high_risk:\n",
    "        if indicator in sql_lower:\n",
    "            return RiskLevel.HIGH\n",
    "    \n",
    "    for indicator in medium_risk:\n",
    "        if indicator in sql_lower:\n",
    "            return RiskLevel.MEDIUM\n",
    "    \n",
    "    return RiskLevel.LOW\n",
    "\n",
    "def execute_with_approval(question: str, sql: str, conn):\n",
    "    \"\"\"Execute query with risk-based approval\"\"\"\n",
    "    \n",
    "    risk = assess_query_risk(sql)\n",
    "    \n",
    "    if risk == RiskLevel.HIGH:\n",
    "        print(f\"⚠️ HIGH RISK query requires manual approval:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"SQL: {sql}\\n\")\n",
    "        \n",
    "        approval = input(\"Approve execution? (yes/no): \")\n",
    "        if approval.lower() != \"yes\":\n",
    "            return None\n",
    "    \n",
    "    elif risk == RiskLevel.MEDIUM:\n",
    "        print(f\"⚠️ MEDIUM RISK query - review recommended\")\n",
    "        print(f\"SQL: {sql[:100]}...\\n\")\n",
    "    \n",
    "    # Execute approved query\n",
    "    return execute_safe_query(sql, conn)\n",
    "```\n",
    "\n",
    "**Security Checklist:**\n",
    "\n",
    "- ✅ Input sanitization (length, dangerous patterns)\n",
    "- ✅ SQL parsing validation (sqlparse)\n",
    "- ✅ Read-only database user\n",
    "- ✅ LIMIT clause enforcement\n",
    "- ✅ Query timeout (10s default)\n",
    "- ✅ EXPLAIN cost estimation\n",
    "- ✅ Audit logging (all queries)\n",
    "- ✅ Human approval for high-risk queries\n",
    "- ✅ Rate limiting per user\n",
    "- ✅ Row count limits (1000 default)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3f477",
   "metadata": {},
   "source": [
    "### 🎯 **Advanced Techniques: Few-Shot, Chain-of-Thought & Self-Correction**\n",
    "\n",
    "**1. Few-Shot Learning for NL2SQL**\n",
    "\n",
    "Few-shot examples dramatically improve accuracy by showing the LLM the desired output format and handling of edge cases.\n",
    "\n",
    "**Example Selection Strategies:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class FewShotSelector:\n",
    "    \"\"\"Dynamically select most relevant examples for each question\"\"\"\n",
    "    \n",
    "    def __init__(self, example_pool: list):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.examples = example_pool\n",
    "        \n",
    "        # Pre-compute embeddings for all examples\n",
    "        self.embeddings = self.model.encode([ex['question'] for ex in self.examples])\n",
    "    \n",
    "    def select_examples(self, question: str, k: int = 3) -> list:\n",
    "        \"\"\"Select k most similar examples\"\"\"\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.model.encode([question])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [self.examples[i] for i in top_indices]\n",
    "\n",
    "# Example pool with diverse query types\n",
    "example_pool = [\n",
    "    {\n",
    "        \"question\": \"How many orders were placed yesterday?\",\n",
    "        \"sql\": \"SELECT COUNT(*) FROM orders WHERE DATE(created_at) = CURRENT_DATE - INTERVAL '1 day';\",\n",
    "        \"explanation\": \"Count aggregation with date filter\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Top 5 customers by total spend\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    SUM(o.total_amount) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 5;\n",
    "\"\"\",\n",
    "        \"explanation\": \"Aggregation with JOIN and ORDER BY\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Products without any sales this month\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT p.product_id, p.name\n",
    "FROM products p\n",
    "LEFT JOIN orders o ON p.product_id = o.product_id \n",
    "    AND DATE_TRUNC('month', o.created_at) = DATE_TRUNC('month', CURRENT_DATE)\n",
    "WHERE o.order_id IS NULL\n",
    "LIMIT 100;\n",
    "\"\"\",\n",
    "        \"explanation\": \"LEFT JOIN with NULL check for missing records\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Running total of sales by date\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    DATE(created_at) as sale_date,\n",
    "    SUM(total_amount) as daily_total,\n",
    "    SUM(SUM(total_amount)) OVER (ORDER BY DATE(created_at)) as running_total\n",
    "FROM orders\n",
    "GROUP BY DATE(created_at)\n",
    "ORDER BY sale_date\n",
    "LIMIT 365;\n",
    "\"\"\",\n",
    "        \"explanation\": \"Window function for running totals\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Customers who haven't ordered in 90 days\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    MAX(o.created_at) as last_order_date,\n",
    "    CURRENT_DATE - MAX(o.created_at)::date as days_since_order\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "HAVING MAX(o.created_at) < CURRENT_DATE - INTERVAL '90 days'\n",
    "    OR MAX(o.created_at) IS NULL\n",
    "ORDER BY last_order_date NULLS FIRST\n",
    "LIMIT 100;\n",
    "\"\"\",\n",
    "        \"explanation\": \"HAVING clause with date comparison\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Usage\n",
    "selector = FewShotSelector(example_pool)\n",
    "question = \"Show customers with no recent purchases\"\n",
    "relevant_examples = selector.select_examples(question, k=2)\n",
    "\n",
    "print(\"Selected examples:\")\n",
    "for ex in relevant_examples:\n",
    "    print(f\"\\nQ: {ex['question']}\")\n",
    "    print(f\"SQL: {ex['sql'][:100]}...\")\n",
    "```\n",
    "\n",
    "**Few-Shot Prompt Construction:**\n",
    "\n",
    "```python\n",
    "def build_fewshot_prompt(question: str, schema: str, examples: list, dialect: str = \"PostgreSQL\") -> str:\n",
    "    \"\"\"Construct optimized few-shot prompt\"\"\"\n",
    "    \n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"Example {i+1}:\\n\"\n",
    "        f\"Question: {ex['question']}\\n\"\n",
    "        f\"SQL:\\n{ex['sql'].strip()}\\n\"\n",
    "        f\"Why: {ex['explanation']}\"\n",
    "        for i, ex in enumerate(examples)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert {dialect} developer. Generate SQL to answer questions.\n",
    "\n",
    "Database Schema:\n",
    "{schema}\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "Now generate SQL for this question:\n",
    "Question: {question}\n",
    "\n",
    "Requirements:\n",
    "- Use table aliases (e.g., FROM orders o)\n",
    "- Include LIMIT clause (max 1000 rows)\n",
    "- Handle NULL values appropriately\n",
    "- Use appropriate indexes mentioned in schema\n",
    "- Return ONLY the SQL query\n",
    "\n",
    "SQL:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "question = \"Which product categories have declining sales this quarter vs last quarter?\"\n",
    "examples = selector.select_examples(question, k=2)\n",
    "prompt = build_fewshot_prompt(question, schema_ddl, examples)\n",
    "\n",
    "sql = ask_llm(prompt, temperature=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Chain-of-Thought for Complex Queries**\n",
    "\n",
    "Break complex questions into steps, generating SQL iteratively.\n",
    "\n",
    "```python\n",
    "def nl2sql_with_cot(question: str, schema: str) -> dict:\n",
    "    \"\"\"Generate SQL with chain-of-thought reasoning\"\"\"\n",
    "    \n",
    "    # Step 1: Decompose question\n",
    "    decomposition_prompt = f\"\"\"\n",
    "Analyze this database question step-by-step:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Schema: {schema}\n",
    "\n",
    "Break down the question into logical steps:\n",
    "1. What tables are needed?\n",
    "2. What joins are required?\n",
    "3. What filters should be applied?\n",
    "4. What aggregations are needed?\n",
    "5. What is the final output format?\n",
    "\n",
    "Provide step-by-step reasoning:\n",
    "\"\"\"\n",
    "    \n",
    "    reasoning = ask_llm(decomposition_prompt, temperature=0)\n",
    "    \n",
    "    # Step 2: Generate SQL with reasoning\n",
    "    sql_prompt = f\"\"\"\n",
    "Based on this reasoning:\n",
    "\n",
    "{reasoning}\n",
    "\n",
    "Generate the final SQL query for PostgreSQL:\n",
    "\n",
    "Requirements:\n",
    "- Include comments for each major section\n",
    "- Use CTEs for complex logic\n",
    "- Add LIMIT clause\n",
    "\n",
    "SQL:\n",
    "\"\"\"\n",
    "    \n",
    "    sql = ask_llm(sql_prompt, temperature=0)\n",
    "    \n",
    "    return {\n",
    "        \"sql\": sql,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"complexity\": \"high\" if \"cte\" in sql.lower() or \"window\" in sql.lower() else \"medium\"\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"\"\"\n",
    "Compare the average order value and customer retention rate \n",
    "between customers acquired via different marketing channels, \n",
    "but only for customers who have made at least 3 purchases\n",
    "\"\"\"\n",
    "\n",
    "result = nl2sql_with_cot(question, schema_ddl)\n",
    "print(\"Reasoning:\")\n",
    "print(result[\"reasoning\"])\n",
    "print(\"\\nGenerated SQL:\")\n",
    "print(result[\"sql\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Self-Correction with Execution Feedback**\n",
    "\n",
    "Let the LLM fix its own errors by providing execution feedback.\n",
    "\n",
    "```python\n",
    "def nl2sql_with_self_correction(question: str, schema: str, conn, max_attempts: int = 3) -> dict:\n",
    "    \"\"\"Generate SQL with self-correction loop\"\"\"\n",
    "    \n",
    "    attempts = []\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        # Generate SQL\n",
    "        if attempt == 0:\n",
    "            # First attempt: standard generation\n",
    "            sql = nl_to_sql(question, schema)\n",
    "        else:\n",
    "            # Subsequent attempts: include error feedback\n",
    "            correction_prompt = f\"\"\"\n",
    "Previous attempt failed with error:\n",
    "\n",
    "Question: {question}\n",
    "Schema: {schema}\n",
    "\n",
    "Previous SQL (FAILED):\n",
    "{attempts[-1]['sql']}\n",
    "\n",
    "Error:\n",
    "{attempts[-1]['error']}\n",
    "\n",
    "Generate a corrected SQL query that fixes this error.\n",
    "Analyze the error message and adjust the query accordingly.\n",
    "\n",
    "Corrected SQL:\n",
    "\"\"\"\n",
    "            sql = ask_llm(correction_prompt, temperature=0.1)\n",
    "        \n",
    "        # Validate and execute\n",
    "        safe, validation_msg = validate_sql_safety(sql)\n",
    "        \n",
    "        if not safe:\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"sql\": sql,\n",
    "                \"error\": f\"Validation failed: {validation_msg}\",\n",
    "                \"success\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Try execution\n",
    "        try:\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "            \n",
    "            # Success!\n",
    "            return {\n",
    "                \"sql\": sql,\n",
    "                \"result\": df,\n",
    "                \"attempts\": attempt + 1,\n",
    "                \"success\": True,\n",
    "                \"history\": attempts\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"sql\": sql,\n",
    "                \"error\": error_msg,\n",
    "                \"success\": False\n",
    "            })\n",
    "    \n",
    "    # All attempts failed\n",
    "    return {\n",
    "        \"sql\": None,\n",
    "        \"result\": None,\n",
    "        \"attempts\": max_attempts,\n",
    "        \"success\": False,\n",
    "        \"history\": attempts,\n",
    "        \"final_error\": \"Max retry attempts reached\"\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"Show monthly revenue trend for 2024\"\n",
    "result = nl2sql_with_self_correction(question, schema_ddl, conn)\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"✅ Success after {result['attempts']} attempt(s)\")\n",
    "    print(result[\"result\"].head())\n",
    "else:\n",
    "    print(f\"❌ Failed after {result['attempts']} attempts\")\n",
    "    for hist in result[\"history\"]:\n",
    "        print(f\"\\nAttempt {hist['attempt']}: {hist['error']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. SQL Optimization with LLM**\n",
    "\n",
    "```python\n",
    "def optimize_generated_sql(sql: str, schema: str, conn) -> dict:\n",
    "    \"\"\"Analyze and optimize generated SQL\"\"\"\n",
    "    \n",
    "    # Get EXPLAIN plan\n",
    "    try:\n",
    "        explain_query = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {sql}\"\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(explain_query)\n",
    "            plan = cur.fetchone()[0][0]\n",
    "        \n",
    "        cost = plan['Plan']['Total Cost']\n",
    "        rows = plan['Plan']['Plan Rows']\n",
    "        \n",
    "        # If cost is high, ask LLM to optimize\n",
    "        if cost > 1000:  # Threshold\n",
    "            optimization_prompt = f\"\"\"\n",
    "This SQL query has high cost ({cost:.0f}) and may be slow:\n",
    "\n",
    "{sql}\n",
    "\n",
    "Schema with indexes:\n",
    "{schema}\n",
    "\n",
    "Query execution plan shows:\n",
    "- Total Cost: {cost}\n",
    "- Estimated Rows: {rows}\n",
    "\n",
    "Optimize this query:\n",
    "1. Add appropriate indexes hints\n",
    "2. Rewrite inefficient subqueries\n",
    "3. Consider materialized CTEs\n",
    "4. Optimize JOIN order\n",
    "\n",
    "Provide optimized SQL:\n",
    "\"\"\"\n",
    "            \n",
    "            optimized_sql = ask_llm(optimization_prompt, temperature=0)\n",
    "            \n",
    "            # Compare costs\n",
    "            explain_optimized = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {optimized_sql}\"\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(explain_optimized)\n",
    "                plan_opt = cur.fetchone()[0][0]\n",
    "            \n",
    "            cost_opt = plan_opt['Plan']['Total Cost']\n",
    "            \n",
    "            return {\n",
    "                \"original_sql\": sql,\n",
    "                \"original_cost\": cost,\n",
    "                \"optimized_sql\": optimized_sql,\n",
    "                \"optimized_cost\": cost_opt,\n",
    "                \"improvement\": f\"{((cost - cost_opt) / cost * 100):.1f}%\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"sql\": sql,\n",
    "            \"cost\": cost,\n",
    "            \"optimization\": \"Not needed (cost acceptable)\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example\n",
    "sql = \"SELECT * FROM orders o JOIN customers c ON o.customer_id = c.customer_id WHERE o.date > '2024-01-01'\"\n",
    "result = optimize_generated_sql(sql, schema_ddl, conn)\n",
    "\n",
    "if \"improvement\" in result:\n",
    "    print(f\"Optimization improved cost by {result['improvement']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Multi-Turn Conversation (Refinement)**\n",
    "\n",
    "```python\n",
    "class NL2SQLConversation:\n",
    "    \"\"\"Multi-turn conversation for query refinement\"\"\"\n",
    "    \n",
    "    def __init__(self, schema: str, conn):\n",
    "        self.schema = schema\n",
    "        self.conn = conn\n",
    "        self.history = []\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Process question with conversation context\"\"\"\n",
    "        \n",
    "        # Build context from history\n",
    "        context = self._build_context()\n",
    "        \n",
    "        # Generate SQL\n",
    "        prompt = f\"\"\"\n",
    "{context}\n",
    "\n",
    "Schema: {self.schema}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Generate SQL considering previous conversation context.\n",
    "\n",
    "SQL:\n",
    "\"\"\"\n",
    "        \n",
    "        sql = ask_llm(prompt, temperature=0)\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            df = pd.read_sql_query(sql, self.conn)\n",
    "            \n",
    "            # Store in history\n",
    "            self.history.append({\n",
    "                \"question\": question,\n",
    "                \"sql\": sql,\n",
    "                \"result_preview\": df.head(3).to_dict(),\n",
    "                \"row_count\": len(df)\n",
    "            })\n",
    "            \n",
    "            return {\"sql\": sql, \"result\": df, \"success\": True}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"success\": False}\n",
    "    \n",
    "    def _build_context(self) -> str:\n",
    "        \"\"\"Build conversation context\"\"\"\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"Previous conversation:\\n\"\n",
    "        for i, turn in enumerate(self.history[-3:], 1):  # Last 3 turns\n",
    "            context += f\"\\n{i}. Q: {turn['question']}\"\n",
    "            context += f\"\\n   SQL: {turn['sql'][:100]}...\"\n",
    "            context += f\"\\n   Returned {turn['row_count']} rows\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Example conversation\n",
    "conv = NL2SQLConversation(schema_ddl, conn)\n",
    "\n",
    "# Turn 1\n",
    "result1 = conv.query(\"Show top 10 products by sales\")\n",
    "print(result1[\"result\"])\n",
    "\n",
    "# Turn 2 (refinement with context)\n",
    "result2 = conv.query(\"Now filter to only electronics category\")\n",
    "print(result2[\"result\"])\n",
    "\n",
    "# Turn 3 (further refinement)\n",
    "result3 = conv.query(\"And sort by profit margin instead\")\n",
    "print(result3[\"result\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Ensemble Methods (Multiple LLMs)**\n",
    "\n",
    "```python\n",
    "def nl2sql_ensemble(question: str, schema: str, models: list = [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\"]) -> dict:\n",
    "    \"\"\"Generate SQL with multiple models and vote\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            sql = nl_to_sql(question, schema, model=model)\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"sql\": sql,\n",
    "                \"sql_hash\": hashlib.sha256(sql.encode()).hexdigest()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Model {model} failed: {e}\")\n",
    "    \n",
    "    # Count identical queries (consensus)\n",
    "    from collections import Counter\n",
    "    sql_hashes = [r[\"sql_hash\"] for r in results]\n",
    "    consensus = Counter(sql_hashes).most_common(1)[0]\n",
    "    \n",
    "    consensus_count = consensus[1]\n",
    "    consensus_hash = consensus[0]\n",
    "    \n",
    "    # Get SQL for consensus\n",
    "    consensus_sql = next(r[\"sql\"] for r in results if r[\"sql_hash\"] == consensus_hash)\n",
    "    \n",
    "    return {\n",
    "        \"consensus_sql\": consensus_sql,\n",
    "        \"agreement\": f\"{consensus_count}/{len(models)}\",\n",
    "        \"all_results\": results\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"Monthly active users in Q4 2024\"\n",
    "result = nl2sql_ensemble(question, schema_ddl)\n",
    "\n",
    "print(f\"Consensus ({result['agreement']} models agree):\")\n",
    "print(result[\"consensus_sql\"])\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ad63b",
   "metadata": {},
   "source": [
    "### 📊 **Evaluation & Production Metrics: Measuring NL2SQL Quality**\n",
    "\n",
    "**Evaluation Challenges:**\n",
    "\n",
    "NL2SQL evaluation es complejo porque:\n",
    "1. **Múltiples SQL válidos**: Misma pregunta → diferentes queries correctas\n",
    "2. **Equivalencia semántica**: Queries sintácticamente diferentes pero semánticamente idénticas\n",
    "3. **Partial correctness**: Query puede ser parcialmente correcta\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│          NL2SQL Evaluation Metrics              │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│                                                 │\n",
    "│  1. Exact Match (EM)                            │\n",
    "│     SQL generado == SQL esperado                │\n",
    "│     ❌ Demasiado estricto                        │\n",
    "│                                                 │\n",
    "│  2. Execution Accuracy (EX)                     │\n",
    "│     Result(SQL_gen) == Result(SQL_gold)         │\n",
    "│     ✅ Mejor métrica práctica                    │\n",
    "│                                                 │\n",
    "│  3. Component Match                             │\n",
    "│     SELECT clause: 90%                          │\n",
    "│     FROM clause: 95%                            │\n",
    "│     WHERE clause: 85%                           │\n",
    "│     GROUP BY: 80%                               │\n",
    "│     ORDER BY: 75%                               │\n",
    "│                                                 │\n",
    "│  4. Test Suite Execution (TSE)                  │\n",
    "│     % of test cases passed                      │\n",
    "│     ✅ Robusto a variaciones sintácticas         │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**1. Execution Accuracy (Gold Standard):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def execution_accuracy(\n",
    "    generated_sql: str,\n",
    "    gold_sql: str,\n",
    "    conn,\n",
    "    timeout: int = 10\n",
    ") -> Tuple[bool, dict]:\n",
    "    \"\"\"Compare results of generated vs gold SQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Execute both queries\n",
    "        df_gen = pd.read_sql_query(generated_sql, conn)\n",
    "        df_gold = pd.read_sql_query(gold_sql, conn)\n",
    "        \n",
    "        # Sort both dataframes for comparison\n",
    "        df_gen_sorted = df_gen.sort_values(by=list(df_gen.columns)).reset_index(drop=True)\n",
    "        df_gold_sorted = df_gold.sort_values(by=list(df_gold.columns)).reset_index(drop=True)\n",
    "        \n",
    "        # Compare\n",
    "        if df_gen_sorted.equals(df_gold_sorted):\n",
    "            return True, {\"match\": \"exact\", \"rows\": len(df_gen)}\n",
    "        \n",
    "        # Check if row counts match\n",
    "        if len(df_gen) != len(df_gold):\n",
    "            return False, {\n",
    "                \"match\": \"row_count_mismatch\",\n",
    "                \"generated_rows\": len(df_gen),\n",
    "                \"gold_rows\": len(df_gold)\n",
    "            }\n",
    "        \n",
    "        # Check column differences\n",
    "        if set(df_gen.columns) != set(df_gold.columns):\n",
    "            return False, {\n",
    "                \"match\": \"column_mismatch\",\n",
    "                \"generated_cols\": list(df_gen.columns),\n",
    "                \"gold_cols\": list(df_gold.columns)\n",
    "            }\n",
    "        \n",
    "        # Partial match (values differ)\n",
    "        return False, {\n",
    "            \"match\": \"value_mismatch\",\n",
    "            \"diff_rows\": (df_gen_sorted != df_gold_sorted).sum().sum()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Example\n",
    "generated = \"SELECT customer_id, SUM(total) as revenue FROM sales GROUP BY customer_id ORDER BY revenue DESC LIMIT 10\"\n",
    "gold = \"SELECT customer_id, SUM(total_amount) as revenue FROM sales GROUP BY customer_id ORDER BY 2 DESC LIMIT 10\"\n",
    "\n",
    "match, details = execution_accuracy(generated, gold, conn)\n",
    "print(f\"{'✅' if match else '❌'} {details}\")\n",
    "```\n",
    "\n",
    "**2. Component-Level Evaluation:**\n",
    "\n",
    "```python\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier, Where\n",
    "\n",
    "def parse_sql_components(sql: str) -> dict:\n",
    "    \"\"\"Extract components from SQL query\"\"\"\n",
    "    \n",
    "    parsed = sqlparse.parse(sql)[0]\n",
    "    components = {\n",
    "        \"select\": [],\n",
    "        \"from\": [],\n",
    "        \"where\": [],\n",
    "        \"group_by\": [],\n",
    "        \"order_by\": [],\n",
    "        \"limit\": None\n",
    "    }\n",
    "    \n",
    "    # Extract SELECT columns\n",
    "    for token in parsed.tokens:\n",
    "        if token.ttype is sqlparse.tokens.DML and token.value.upper() == 'SELECT':\n",
    "            # Next token is column list\n",
    "            continue\n",
    "    \n",
    "    # Simplified extraction (full implementation uses AST traversal)\n",
    "    sql_lower = sql.lower()\n",
    "    \n",
    "    # Extract table names\n",
    "    if 'from' in sql_lower:\n",
    "        from_clause = sql_lower.split('from')[1].split('where')[0] if 'where' in sql_lower else sql_lower.split('from')[1]\n",
    "        components[\"from\"] = [t.strip() for t in from_clause.split('join')]\n",
    "    \n",
    "    return components\n",
    "\n",
    "def component_accuracy(generated_sql: str, gold_sql: str) -> dict:\n",
    "    \"\"\"Compare SQL components\"\"\"\n",
    "    \n",
    "    gen_comp = parse_sql_components(generated_sql)\n",
    "    gold_comp = parse_sql_components(gold_sql)\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for component in [\"select\", \"from\", \"where\", \"group_by\", \"order_by\"]:\n",
    "        gen_set = set(gen_comp.get(component, []))\n",
    "        gold_set = set(gold_comp.get(component, []))\n",
    "        \n",
    "        if not gold_set:\n",
    "            scores[component] = 1.0 if not gen_set else 0.0\n",
    "        else:\n",
    "            # Jaccard similarity\n",
    "            intersection = len(gen_set & gold_set)\n",
    "            union = len(gen_set | gold_set)\n",
    "            scores[component] = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # Overall score (weighted average)\n",
    "    weights = {\"select\": 0.3, \"from\": 0.25, \"where\": 0.25, \"group_by\": 0.1, \"order_by\": 0.1}\n",
    "    overall = sum(scores[k] * weights[k] for k in weights)\n",
    "    \n",
    "    return {\n",
    "        \"component_scores\": scores,\n",
    "        \"overall_score\": overall\n",
    "    }\n",
    "\n",
    "# Example\n",
    "scores = component_accuracy(generated, gold)\n",
    "print(f\"Overall Score: {scores['overall_score']:.2%}\")\n",
    "for comp, score in scores['component_scores'].items():\n",
    "    print(f\"  {comp}: {score:.2%}\")\n",
    "```\n",
    "\n",
    "**3. Spider Benchmark Evaluation:**\n",
    "\n",
    "```python\n",
    "# Spider dataset: https://yale-lily.github.io/spider\n",
    "# 10,181 questions + SQL pairs across 200 databases\n",
    "\n",
    "def evaluate_on_spider_subset(model_fn, test_cases: list) -> dict:\n",
    "    \"\"\"Evaluate model on Spider benchmark subset\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"total\": len(test_cases),\n",
    "        \"exact_match\": 0,\n",
    "        \"execution_match\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    for i, case in enumerate(test_cases):\n",
    "        question = case[\"question\"]\n",
    "        gold_sql = case[\"sql\"]\n",
    "        db_id = case[\"db_id\"]\n",
    "        \n",
    "        try:\n",
    "            # Generate SQL\n",
    "            generated_sql = model_fn(question, case[\"schema\"])\n",
    "            \n",
    "            # Check exact match\n",
    "            if generated_sql.strip().lower() == gold_sql.strip().lower():\n",
    "                results[\"exact_match\"] += 1\n",
    "            \n",
    "            # Check execution match\n",
    "            # (requires database instances from Spider)\n",
    "            conn = get_spider_db_connection(db_id)\n",
    "            match, _ = execution_accuracy(generated_sql, gold_sql, conn)\n",
    "            \n",
    "            if match:\n",
    "                results[\"execution_match\"] += 1\n",
    "            \n",
    "            results[\"details\"].append({\n",
    "                \"case_id\": i,\n",
    "                \"exact_match\": generated_sql.strip().lower() == gold_sql.strip().lower(),\n",
    "                \"execution_match\": match\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"details\"].append({\n",
    "                \"case_id\": i,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate percentages\n",
    "    results[\"exact_match_pct\"] = results[\"exact_match\"] / results[\"total\"] * 100\n",
    "    results[\"execution_match_pct\"] = results[\"execution_match\"] / results[\"total\"] * 100\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "test_cases = load_spider_subset(100)  # 100 test cases\n",
    "results = evaluate_on_spider_subset(nl_to_sql, test_cases)\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match_pct']:.1f}%\")\n",
    "print(f\"Execution Accuracy: {results['execution_match_pct']:.1f}%\")\n",
    "```\n",
    "\n",
    "**4. Production Monitoring Metrics:**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class NL2SQLMetrics:\n",
    "    \"\"\"Production metrics for NL2SQL system\"\"\"\n",
    "    \n",
    "    timestamp: datetime\n",
    "    \n",
    "    # Volume metrics\n",
    "    total_queries: int\n",
    "    queries_per_minute: float\n",
    "    \n",
    "    # Success metrics\n",
    "    validation_pass_rate: float  # % passed SQL validation\n",
    "    execution_success_rate: float  # % executed without error\n",
    "    result_quality_score: float  # % with non-empty results\n",
    "    \n",
    "    # Performance metrics\n",
    "    avg_generation_time_ms: float\n",
    "    p95_generation_time_ms: float\n",
    "    avg_execution_time_ms: float\n",
    "    p95_execution_time_ms: float\n",
    "    \n",
    "    # Cost metrics\n",
    "    total_tokens_consumed: int\n",
    "    total_cost_usd: float\n",
    "    cost_per_query: float\n",
    "    \n",
    "    # User satisfaction\n",
    "    thumbs_up_rate: float  # % of queries with positive feedback\n",
    "    retry_rate: float  # % of users who retry same question\n",
    "    \n",
    "    # Error breakdown\n",
    "    validation_errors: int\n",
    "    execution_errors: int\n",
    "    timeout_errors: int\n",
    "\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and aggregate NL2SQL metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "    \n",
    "    def log_query(\n",
    "        self,\n",
    "        question: str,\n",
    "        sql: str,\n",
    "        generation_time_ms: float,\n",
    "        execution_time_ms: float,\n",
    "        validation_passed: bool,\n",
    "        execution_success: bool,\n",
    "        result_rows: int,\n",
    "        tokens_used: int,\n",
    "        cost_usd: float,\n",
    "        user_feedback: str = None  # 'thumbs_up', 'thumbs_down', None\n",
    "    ):\n",
    "        self.queries.append({\n",
    "            \"timestamp\": datetime.utcnow(),\n",
    "            \"question\": question,\n",
    "            \"sql\": sql,\n",
    "            \"generation_time_ms\": generation_time_ms,\n",
    "            \"execution_time_ms\": execution_time_ms,\n",
    "            \"validation_passed\": validation_passed,\n",
    "            \"execution_success\": execution_success,\n",
    "            \"result_rows\": result_rows,\n",
    "            \"tokens_used\": tokens_used,\n",
    "            \"cost_usd\": cost_usd,\n",
    "            \"user_feedback\": user_feedback\n",
    "        })\n",
    "    \n",
    "    def get_metrics(self, time_window_minutes: int = 60) -> NL2SQLMetrics:\n",
    "        \"\"\"Calculate metrics for recent time window\"\"\"\n",
    "        \n",
    "        cutoff = datetime.utcnow() - timedelta(minutes=time_window_minutes)\n",
    "        recent = [q for q in self.queries if q[\"timestamp\"] >= cutoff]\n",
    "        \n",
    "        if not recent:\n",
    "            return None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total = len(recent)\n",
    "        \n",
    "        validation_passed = sum(1 for q in recent if q[\"validation_passed\"])\n",
    "        execution_success = sum(1 for q in recent if q[\"execution_success\"])\n",
    "        non_empty_results = sum(1 for q in recent if q[\"result_rows\"] > 0)\n",
    "        \n",
    "        generation_times = [q[\"generation_time_ms\"] for q in recent]\n",
    "        execution_times = [q[\"execution_time_ms\"] for q in recent if q[\"execution_time_ms\"]]\n",
    "        \n",
    "        thumbs_up = sum(1 for q in recent if q[\"user_feedback\"] == \"thumbs_up\")\n",
    "        total_feedback = sum(1 for q in recent if q[\"user_feedback\"])\n",
    "        \n",
    "        return NL2SQLMetrics(\n",
    "            timestamp=datetime.utcnow(),\n",
    "            total_queries=total,\n",
    "            queries_per_minute=total / time_window_minutes,\n",
    "            validation_pass_rate=validation_passed / total,\n",
    "            execution_success_rate=execution_success / total,\n",
    "            result_quality_score=non_empty_results / total,\n",
    "            avg_generation_time_ms=np.mean(generation_times),\n",
    "            p95_generation_time_ms=np.percentile(generation_times, 95),\n",
    "            avg_execution_time_ms=np.mean(execution_times) if execution_times else 0,\n",
    "            p95_execution_time_ms=np.percentile(execution_times, 95) if execution_times else 0,\n",
    "            total_tokens_consumed=sum(q[\"tokens_used\"] for q in recent),\n",
    "            total_cost_usd=sum(q[\"cost_usd\"] for q in recent),\n",
    "            cost_per_query=sum(q[\"cost_usd\"] for q in recent) / total,\n",
    "            thumbs_up_rate=thumbs_up / total_feedback if total_feedback > 0 else 0,\n",
    "            retry_rate=0,  # Calculate based on user session analysis\n",
    "            validation_errors=total - validation_passed,\n",
    "            execution_errors=validation_passed - execution_success,\n",
    "            timeout_errors=0  # Track separately\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "collector = MetricsCollector()\n",
    "\n",
    "# Log queries as they happen\n",
    "collector.log_query(\n",
    "    question=\"Top 10 products\",\n",
    "    sql=\"SELECT * FROM products ORDER BY sales DESC LIMIT 10\",\n",
    "    generation_time_ms=450,\n",
    "    execution_time_ms=120,\n",
    "    validation_passed=True,\n",
    "    execution_success=True,\n",
    "    result_rows=10,\n",
    "    tokens_used=250,\n",
    "    cost_usd=0.001,\n",
    "    user_feedback=\"thumbs_up\"\n",
    ")\n",
    "\n",
    "# Get hourly metrics\n",
    "metrics = collector.get_metrics(time_window_minutes=60)\n",
    "print(f\"Queries/min: {metrics.queries_per_minute:.1f}\")\n",
    "print(f\"Success Rate: {metrics.execution_success_rate:.1%}\")\n",
    "print(f\"Avg Latency: {metrics.avg_generation_time_ms:.0f}ms\")\n",
    "print(f\"Cost/Query: ${metrics.cost_per_query:.4f}\")\n",
    "```\n",
    "\n",
    "**5. Grafana Dashboard Metrics:**\n",
    "\n",
    "```yaml\n",
    "# Prometheus metrics for NL2SQL\n",
    "nl2sql_queries_total{status=\"success|validation_failed|execution_failed\"}\n",
    "nl2sql_generation_duration_seconds{model=\"gpt-4o\"}\n",
    "nl2sql_execution_duration_seconds\n",
    "nl2sql_tokens_consumed_total{model=\"gpt-4o\",type=\"input|output\"}\n",
    "nl2sql_cost_usd_total{model=\"gpt-4o\"}\n",
    "nl2sql_user_satisfaction{feedback=\"thumbs_up|thumbs_down\"}\n",
    "nl2sql_result_rows{quantile=\"0.5|0.95|0.99\"}\n",
    "```\n",
    "\n",
    "**Grafana Panels:**\n",
    "1. **Request Rate**: `rate(nl2sql_queries_total[5m])`\n",
    "2. **Success Rate**: `rate(nl2sql_queries_total{status=\"success\"}[5m]) / rate(nl2sql_queries_total[5m])`\n",
    "3. **Latency p95**: `histogram_quantile(0.95, nl2sql_generation_duration_seconds)`\n",
    "4. **Cost Burn**: `rate(nl2sql_cost_usd_total[1h]) * 24 * 30` ($/month)\n",
    "5. **User Satisfaction**: `rate(nl2sql_user_satisfaction{feedback=\"thumbs_up\"}[1h]) / rate(nl2sql_user_satisfaction[1h])`\n",
    "\n",
    "**6. A/B Testing Framework:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"A/B test different NL2SQL strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, variants: dict):\n",
    "        self.variants = variants  # {variant_name: generator_function}\n",
    "        self.results = {name: [] for name in variants.keys()}\n",
    "    \n",
    "    def generate(self, question: str, schema: str, user_id: str) -> dict:\n",
    "        \"\"\"Route user to variant and track results\"\"\"\n",
    "        \n",
    "        # Consistent variant assignment per user\n",
    "        variant_name = self._assign_variant(user_id)\n",
    "        generator = self.variants[variant_name]\n",
    "        \n",
    "        # Generate SQL\n",
    "        start = time.time()\n",
    "        sql = generator(question, schema)\n",
    "        generation_time = (time.time() - start) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"sql\": sql,\n",
    "            \"generation_time_ms\": generation_time\n",
    "        }\n",
    "    \n",
    "    def log_result(\n",
    "        self,\n",
    "        variant: str,\n",
    "        success: bool,\n",
    "        generation_time_ms: float,\n",
    "        user_feedback: str = None\n",
    "    ):\n",
    "        \"\"\"Log result for variant\"\"\"\n",
    "        self.results[variant].append({\n",
    "            \"success\": success,\n",
    "            \"generation_time_ms\": generation_time_ms,\n",
    "            \"user_feedback\": user_feedback\n",
    "        })\n",
    "    \n",
    "    def analyze(self) -> dict:\n",
    "        \"\"\"Statistical analysis of variants\"\"\"\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        for variant, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            analysis[variant] = {\n",
    "                \"sample_size\": len(results),\n",
    "                \"success_rate\": sum(1 for r in results if r[\"success\"]) / len(results),\n",
    "                \"avg_latency_ms\": np.mean([r[\"generation_time_ms\"] for r in results]),\n",
    "                \"thumbs_up_rate\": sum(1 for r in results if r[\"user_feedback\"] == \"thumbs_up\") / len(results)\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _assign_variant(self, user_id: str) -> str:\n",
    "        \"\"\"Consistent hash-based assignment\"\"\"\n",
    "        hash_val = hash(user_id) % 100\n",
    "        \n",
    "        # 50/50 split (adjust as needed)\n",
    "        threshold = 50\n",
    "        variants_list = list(self.variants.keys())\n",
    "        \n",
    "        return variants_list[0] if hash_val < threshold else variants_list[1]\n",
    "\n",
    "# Example\n",
    "variants = {\n",
    "    \"baseline\": lambda q, s: nl_to_sql(q, s, model=\"gpt-4o-mini\"),\n",
    "    \"optimized\": lambda q, s: nl_to_sql_fewshot(q, s, model=\"gpt-4o\")\n",
    "}\n",
    "\n",
    "ab_test = ABTestFramework(variants)\n",
    "\n",
    "# Run experiment\n",
    "for user_id in users:\n",
    "    result = ab_test.generate(question, schema, user_id)\n",
    "    \n",
    "    # Execute and collect feedback\n",
    "    success = execute_and_validate(result[\"sql\"])\n",
    "    feedback = get_user_feedback(user_id)\n",
    "    \n",
    "    ab_test.log_result(\n",
    "        variant=result[\"variant\"],\n",
    "        success=success,\n",
    "        generation_time_ms=result[\"generation_time_ms\"],\n",
    "        user_feedback=feedback\n",
    "    )\n",
    "\n",
    "# Analyze results\n",
    "analysis = ab_test.analyze()\n",
    "print(\"A/B Test Results:\")\n",
    "for variant, metrics in analysis.items():\n",
    "    print(f\"\\n{variant}:\")\n",
    "    print(f\"  Success Rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Latency: {metrics['avg_latency_ms']:.0f}ms\")\n",
    "    print(f\"  Satisfaction: {metrics['thumbs_up_rate']:.1%}\")\n",
    "```\n",
    "\n",
    "**Production Readiness Checklist:**\n",
    "\n",
    "- ✅ Evaluation metrics tracked (Execution Accuracy, Component Match)\n",
    "- ✅ Continuous monitoring (Prometheus + Grafana)\n",
    "- ✅ A/B testing framework (compare strategies)\n",
    "- ✅ User feedback loop (thumbs up/down)\n",
    "- ✅ Error tracking & categorization\n",
    "- ✅ Cost monitoring & alerts\n",
    "- ✅ Latency SLOs (p95 < 2s)\n",
    "- ✅ Quality SLOs (success rate > 90%)\n",
    "- ✅ Regression testing suite (100+ test cases)\n",
    "- ✅ Model version tracking (experiments)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def nl_to_sql_simple(question: str) -> str:\n",
    "    prompt = f'''\n",
    "Convierte esta pregunta en SQL:\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "SQL:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "print(nl_to_sql_simple('Muestra las 10 ventas más recientes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6235d4",
   "metadata": {},
   "source": [
    "## 2. Mejorado: con contexto de schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08223e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_context = '''\n",
    "Tablas disponibles:\n",
    "\n",
    "ventas:\n",
    "  - venta_id (int, PK)\n",
    "  - fecha (date)\n",
    "  - cliente_id (int, FK)\n",
    "  - producto_id (int, FK)\n",
    "  - cantidad (int)\n",
    "  - total (decimal)\n",
    "\n",
    "clientes:\n",
    "  - cliente_id (int, PK)\n",
    "  - nombre (varchar)\n",
    "  - email (varchar)\n",
    "  - pais (varchar)\n",
    "\n",
    "productos:\n",
    "  - producto_id (int, PK)\n",
    "  - nombre (varchar)\n",
    "  - categoria (varchar)\n",
    "  - precio (decimal)\n",
    "'''\n",
    "\n",
    "def nl_to_sql(question: str, schema: str = schema_context, dialect='postgresql') -> str:\n",
    "    prompt = f'''\n",
    "Eres un experto en SQL. Genera una consulta {dialect} válida para responder la pregunta.\n",
    "\n",
    "Schema de base de datos:\n",
    "{schema}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Genera SOLO la consulta SQL, sin explicaciones adicionales.\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```sql','').replace('```','')\n",
    "\n",
    "query = nl_to_sql('¿Cuál es el top 5 de productos más vendidos por categoría?')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8de7f6",
   "metadata": {},
   "source": [
    "## 3. Validación y seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f232fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlparse\n",
    "\n",
    "def is_safe_query(sql: str) -> tuple[bool, str]:\n",
    "    \"\"\"Valida que la consulta sea de solo lectura y segura.\"\"\"\n",
    "    sql_lower = sql.lower()\n",
    "    # Bloquear operaciones de escritura\n",
    "    dangerous = ['insert', 'update', 'delete', 'drop', 'create', 'alter', 'truncate', 'exec', 'execute']\n",
    "    for kw in dangerous:\n",
    "        if re.search(rf'\\b{kw}\\b', sql_lower):\n",
    "            return False, f'Operación bloqueada: {kw}'\n",
    "    \n",
    "    # Solo permitir SELECT\n",
    "    if not sql_lower.strip().startswith('select'):\n",
    "        return False, 'Solo se permiten consultas SELECT'\n",
    "    \n",
    "    return True, 'OK'\n",
    "\n",
    "test_queries = [\n",
    "    'SELECT * FROM ventas LIMIT 10',\n",
    "    'DELETE FROM ventas WHERE fecha < \\'2020-01-01\\'',\n",
    "    'SELECT COUNT(*) FROM clientes'\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    safe, msg = is_safe_query(q)\n",
    "    print(f'{\"✅\" if safe else \"❌\"} {msg}: {q[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35420c",
   "metadata": {},
   "source": [
    "## 4. Ejecución segura con SQLite demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b91c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Setup demo DB\n",
    "conn = sqlite3.connect(':memory:')\n",
    "conn.executescript('''\n",
    "CREATE TABLE ventas (venta_id INT, fecha TEXT, cliente_id INT, producto_id INT, cantidad INT, total REAL);\n",
    "INSERT INTO ventas VALUES (1,'2025-10-01',10,101,2,200.0),(2,'2025-10-02',11,102,1,50.0),(3,'2025-10-03',10,101,1,100.0);\n",
    "CREATE TABLE productos (producto_id INT, nombre TEXT, categoria TEXT);\n",
    "INSERT INTO productos VALUES (101,'Laptop','Electronics'),(102,'Mouse','Electronics');\n",
    "''')\n",
    "\n",
    "def execute_nl_query(question: str):\n",
    "    sql = nl_to_sql(question)\n",
    "    print(f'SQL generado: {sql}\\n')\n",
    "    safe, msg = is_safe_query(sql)\n",
    "    if not safe:\n",
    "        return f'❌ Consulta bloqueada: {msg}'\n",
    "    try:\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return f'Error ejecutando SQL: {e}'\n",
    "\n",
    "result = execute_nl_query('Muestra el total de ventas por producto')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959e1fb",
   "metadata": {},
   "source": [
    "## 5. Few-shot con ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = '''\n",
    "Ejemplos:\n",
    "\n",
    "Q: ¿Cuántas ventas hubo ayer?\n",
    "SQL: SELECT COUNT(*) FROM ventas WHERE fecha = CURRENT_DATE - INTERVAL '1 day';\n",
    "\n",
    "Q: Top 3 clientes por gasto total\n",
    "SQL: SELECT c.nombre, SUM(v.total) as gasto FROM ventas v JOIN clientes c ON v.cliente_id=c.cliente_id GROUP BY c.nombre ORDER BY gasto DESC LIMIT 3;\n",
    "\n",
    "Q: Productos sin ventas en octubre\n",
    "SQL: SELECT p.nombre FROM productos p LEFT JOIN ventas v ON p.producto_id=v.producto_id AND v.fecha >= '2025-10-01' AND v.fecha < '2025-11-01' WHERE v.venta_id IS NULL;\n",
    "'''\n",
    "\n",
    "def nl_to_sql_fewshot(question: str) -> str:\n",
    "    prompt = f'''\n",
    "Genera SQL para responder preguntas sobre ventas.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Ahora genera SQL para:\n",
    "Q: {question}\n",
    "SQL:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```sql','').replace('```','')\n",
    "\n",
    "query = nl_to_sql_fewshot('Ventas por categoría en los últimos 7 días')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18767f23",
   "metadata": {},
   "source": [
    "## 6. Optimización y caché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5773ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_nl_to_sql(question: str) -> str:\n",
    "    return nl_to_sql(question)\n",
    "\n",
    "# Llamadas repetidas usan caché\n",
    "q = '¿Cuál es el total de ventas del mes actual?'\n",
    "print('Primera llamada:')\n",
    "print(cached_nl_to_sql(q))\n",
    "print('\\nSegunda llamada (desde caché):')\n",
    "print(cached_nl_to_sql(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd71a3",
   "metadata": {},
   "source": [
    "## 7. Buenas prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567494ca",
   "metadata": {},
   "source": [
    "- **Schema completo**: incluye tipos, PKs, FKs, índices.\n",
    "- **Ejemplos representativos**: few-shot con casos edge.\n",
    "- **Validación estricta**: whitelist de operaciones permitidas.\n",
    "- **Timeout y límites**: evita consultas costosas.\n",
    "- **Logging**: registra pregunta, SQL generado, resultado.\n",
    "- **Feedback loop**: almacena correcciones humanas para fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b842d21",
   "metadata": {},
   "source": [
    "## 8. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef2b94",
   "metadata": {},
   "source": [
    "1. Agrega soporte multi-idioma (inglés/español).\n",
    "2. Implementa un sistema de aprobación humana para SQL complejos.\n",
    "3. Crea un dashboard Streamlit donde usuarios escriban preguntas y vean resultados.\n",
    "4. Añade explicación del SQL generado en lenguaje natural."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
