{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d526b91",
   "metadata": {},
   "source": [
    "# \ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural\n",
    "\n",
    "Objetivo: implementar sistemas NL2SQL (Natural Language to SQL) robustos usando LLMs, con validaci\u00f3n, seguridad y optimizaci\u00f3n.\n",
    "\n",
    "- Duraci\u00f3n: 90-120 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: GenAI 01, SQL intermedio, schemas de base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1491d26",
   "metadata": {},
   "source": [
    "## 1. Caso base: prompt simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca14444",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **NL2SQL Architecture: From Research to Production**\n",
    "\n",
    "**\u00bfQu\u00e9 es NL2SQL?**\n",
    "\n",
    "Natural Language to SQL (NL2SQL) es un sistema que traduce preguntas en lenguaje natural a consultas SQL ejecutables. Es uno de los use cases m\u00e1s valiosos de LLMs para democratizar acceso a datos.\n",
    "\n",
    "**Evolution of NL2SQL:**\n",
    "\n",
    "```\n",
    "2015-2018: Rule-Based Systems\n",
    "\u251c\u2500\u2500 Regex patterns + templates\n",
    "\u251c\u2500\u2500 Limited vocabulary\n",
    "\u2514\u2500\u2500 Accuracy: ~30-40%\n",
    "\n",
    "2018-2020: Seq2Seq Models\n",
    "\u251c\u2500\u2500 LSTM/GRU encoders\n",
    "\u251c\u2500\u2500 Better generalization\n",
    "\u2514\u2500\u2500 Accuracy: ~50-60%\n",
    "\n",
    "2020-2023: Pre-trained LLMs\n",
    "\u251c\u2500\u2500 BERT \u2192 T5 \u2192 GPT-3\n",
    "\u251c\u2500\u2500 Spider benchmark: 70%+\n",
    "\u2514\u2500\u2500 Fine-tuned models (SQLCoder)\n",
    "\n",
    "2023-Present: GPT-4 + RAG\n",
    "\u251c\u2500\u2500 Zero-shot: 80-85%\n",
    "\u251c\u2500\u2500 Few-shot: 85-90%\n",
    "\u2514\u2500\u2500 Production-ready with validation\n",
    "```\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    NL2SQL System                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  1. Schema Retrieval (RAG)                                  \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 Question \u2192 Embed \u2192 Search Vector DB   \u2502              \u2502\n",
    "\u2502     \u2502 Return: Relevant tables + columns     \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502                        \u2193                                     \u2502\n",
    "\u2502  2. Prompt Construction                                     \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 \u2022 Schema context (DDL + samples)      \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Few-shot examples                   \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Dialect-specific instructions       \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Query constraints (LIMIT, timeout)  \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502                        \u2193                                     \u2502\n",
    "\u2502  3. LLM Generation                                          \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 GPT-4o / Claude 3.5 / SQLCoder        \u2502              \u2502\n",
    "\u2502     \u2502 Temperature: 0 (deterministic)        \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502                        \u2193                                     \u2502\n",
    "\u2502  4. Validation & Safety                                     \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 \u2022 SQL parsing (sqlparse)              \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Read-only check                     \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Injection prevention                \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 EXPLAIN plan analysis               \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502                        \u2193                                     \u2502\n",
    "\u2502  5. Execution Engine                                        \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 \u2022 Dry run mode (EXPLAIN only)         \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Timeout enforcement                 \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Result caching                      \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Error handling                      \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2502                        \u2193                                     \u2502\n",
    "\u2502  6. Post-Processing                                         \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
    "\u2502     \u2502 \u2022 Result formatting                   \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Aggregation                         \u2502              \u2502\n",
    "\u2502     \u2502 \u2022 Visualization hints                 \u2502              \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Schema Representation Formats:**\n",
    "\n",
    "**1. DDL (Data Definition Language):**\n",
    "```sql\n",
    "CREATE TABLE sales (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    customer_id INT NOT NULL REFERENCES customers(customer_id),\n",
    "    product_id INT NOT NULL REFERENCES products(product_id),\n",
    "    sale_date DATE NOT NULL,\n",
    "    quantity INT CHECK (quantity > 0),\n",
    "    total_amount DECIMAL(10,2),\n",
    "    status VARCHAR(20) DEFAULT 'pending',\n",
    "    INDEX idx_customer (customer_id),\n",
    "    INDEX idx_date (sale_date)\n",
    ");\n",
    "```\n",
    "\n",
    "**2. JSON Schema:**\n",
    "```json\n",
    "{\n",
    "    \"tables\": {\n",
    "        \"sales\": {\n",
    "            \"columns\": {\n",
    "                \"sale_id\": {\"type\": \"INT\", \"primary_key\": true},\n",
    "                \"customer_id\": {\"type\": \"INT\", \"foreign_key\": \"customers.customer_id\"},\n",
    "                \"sale_date\": {\"type\": \"DATE\", \"nullable\": false},\n",
    "                \"total_amount\": {\"type\": \"DECIMAL(10,2)\"}\n",
    "            },\n",
    "            \"indexes\": [\"idx_customer\", \"idx_date\"],\n",
    "            \"row_count\": 1500000,\n",
    "            \"description\": \"Stores all sales transactions\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**3. Natural Language Description:**\n",
    "```python\n",
    "schema_description = \"\"\"\n",
    "Database: E-commerce Sales System\n",
    "\n",
    "Tables:\n",
    "1. sales (1.5M rows)\n",
    "   - sale_id: unique identifier for each sale\n",
    "   - customer_id: links to customers table\n",
    "   - product_id: links to products table\n",
    "   - sale_date: when the sale occurred\n",
    "   - quantity: number of items sold\n",
    "   - total_amount: total price in USD\n",
    "   - status: pending, completed, or cancelled\n",
    "\n",
    "2. customers (50K rows)\n",
    "   - customer_id: unique customer identifier\n",
    "   - name: customer full name\n",
    "   - email: contact email\n",
    "   - country: customer location\n",
    "   - signup_date: when they registered\n",
    "\n",
    "3. products (5K rows)\n",
    "   - product_id: unique product identifier\n",
    "   - name: product name\n",
    "   - category: electronics, clothing, books, etc.\n",
    "   - price: unit price in USD\n",
    "   - stock_quantity: current inventory\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Schema Retrieval with RAG:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Embed schema components\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ChromaDB for schema storage\n",
    "chroma_client = chromadb.Client()\n",
    "schema_collection = chroma_client.create_collection(\"database_schema\")\n",
    "\n",
    "# Index tables with descriptions\n",
    "tables_data = [\n",
    "    {\n",
    "        \"id\": \"sales\",\n",
    "        \"text\": \"sales table stores transactions with customer_id, product_id, date, quantity, amount\",\n",
    "        \"metadata\": {\"type\": \"table\", \"row_count\": 1500000}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"customers\",\n",
    "        \"text\": \"customers table has customer info: name, email, country, signup_date\",\n",
    "        \"metadata\": {\"type\": \"table\", \"row_count\": 50000}\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables_data:\n",
    "    schema_collection.add(\n",
    "        ids=[table[\"id\"]],\n",
    "        documents=[table[\"text\"]],\n",
    "        metadatas=[table[\"metadata\"]]\n",
    "    )\n",
    "\n",
    "# Query relevant schema\n",
    "def get_relevant_schema(question: str, top_k: int = 3):\n",
    "    \"\"\"Retrieve relevant tables for question\"\"\"\n",
    "    results = schema_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    relevant_tables = results['ids'][0]\n",
    "    return relevant_tables\n",
    "\n",
    "# Example\n",
    "question = \"What are the top selling products last month?\"\n",
    "relevant = get_relevant_schema(question)\n",
    "print(f\"Relevant tables: {relevant}\")\n",
    "# Output: ['sales', 'products']\n",
    "```\n",
    "\n",
    "**Model Comparison for NL2SQL:**\n",
    "\n",
    "| Model | Accuracy (Spider) | Context | Cost | Best For |\n",
    "|-------|------------------|---------|------|----------|\n",
    "| GPT-4o | 85-90% | 128K | $$ | Complex queries, multi-table joins |\n",
    "| GPT-4o-mini | 75-80% | 128K | $ | Simple queries, high volume |\n",
    "| Claude 3.5 Sonnet | 88-92% | 200K | $$$ | Best accuracy, long schemas |\n",
    "| Gemini 1.5 Flash | 80-85% | 1M | $ | Huge schemas, fast |\n",
    "| SQLCoder 70B | 82-85% | 8K | Free | Self-hosted, fine-tuned |\n",
    "| Llama 3 70B + LoRA | 75-80% | 8K | Free | Budget option, trainable |\n",
    "\n",
    "**SQLCoder (Fine-tuned Open Source):**\n",
    "\n",
    "```python\n",
    "# Defog SQLCoder - specialized for SQL generation\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"defog/sqlcoder-70b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"defog/sqlcoder-70b-alpha\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True  # Quantization para reducir VRAM\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"### Task\n",
    "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
    "\n",
    "### Database Schema\n",
    "{schema}\n",
    "\n",
    "### SQL\n",
    "SELECT\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "**Advantages of SQLCoder:**\n",
    "- \u2705 No API costs (run locally)\n",
    "- \u2705 Data privacy (nothing sent externally)\n",
    "- \u2705 Fine-tunable on your schema\n",
    "- \u274c Requires GPU (70B model needs 48GB VRAM or quantization)\n",
    "- \u274c Slower inference than API calls\n",
    "\n",
    "**Prompt Engineering for NL2SQL:**\n",
    "\n",
    "```python\n",
    "from string import Template\n",
    "\n",
    "NL2SQL_PROMPT = Template(\"\"\"\n",
    "You are an expert $dialect SQL developer. Generate a query to answer the question.\n",
    "\n",
    "### Database Schema\n",
    "$schema\n",
    "\n",
    "### Instructions\n",
    "- Use table aliases for readability\n",
    "- Add appropriate JOINs based on foreign keys\n",
    "- Include LIMIT clause (max $max_rows rows)\n",
    "- Use aggregate functions when appropriate\n",
    "- Handle NULL values properly\n",
    "- Return ONLY the SQL query, no explanations\n",
    "\n",
    "### Examples\n",
    "$examples\n",
    "\n",
    "### Question\n",
    "$question\n",
    "\n",
    "### SQL Query\n",
    "\"\"\")\n",
    "\n",
    "# Usage\n",
    "prompt = NL2SQL_PROMPT.substitute(\n",
    "    dialect=\"PostgreSQL\",\n",
    "    schema=schema_ddl,\n",
    "    max_rows=1000,\n",
    "    examples=few_shot_examples,\n",
    "    question=\"Show revenue by product category for Q4 2024\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Handling Complex Queries:**\n",
    "\n",
    "```python\n",
    "# Multi-step query decomposition\n",
    "def complex_nl2sql(question: str, schema: str) -> str:\n",
    "    \"\"\"For complex questions, break into sub-queries\"\"\"\n",
    "    \n",
    "    decomposition_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    \n",
    "    Is this a complex question requiring multiple steps?\n",
    "    If yes, break it down into sub-questions.\n",
    "    If no, answer \"simple\".\n",
    "    \n",
    "    Format:\n",
    "    Type: simple | complex\n",
    "    Sub-questions (if complex):\n",
    "    1. ...\n",
    "    2. ...\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ask_llm(decomposition_prompt, temperature=0)\n",
    "    \n",
    "    if \"simple\" in response.lower():\n",
    "        return nl_to_sql(question, schema)\n",
    "    \n",
    "    # Generate SQL for each sub-question\n",
    "    sub_questions = extract_sub_questions(response)\n",
    "    sub_sqls = [nl_to_sql(sq, schema) for sq in sub_questions]\n",
    "    \n",
    "    # Combine with CTEs\n",
    "    combined_prompt = f\"\"\"\n",
    "    Combine these sub-queries into a single query using CTEs:\n",
    "    \n",
    "    {chr(10).join(f\"-- Step {i+1}: {sq}\" for i, sq in enumerate(sub_questions))}\n",
    "    {chr(10).join(sub_sqls)}\n",
    "    \n",
    "    Generate final query with CTEs:\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_llm(combined_prompt, temperature=0)\n",
    "\n",
    "# Example\n",
    "question = \"Compare average order value between new and returning customers for each product category\"\n",
    "# \u2192 Breaks into: 1) Identify new vs returning, 2) Calculate AOV, 3) Group by category\n",
    "sql = complex_nl2sql(question, schema)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929429b9",
   "metadata": {},
   "source": [
    "### \ud83d\udee1\ufe0f **Security & Validation: Preventing SQL Injection**\n",
    "\n",
    "**Security Threats in NL2SQL:**\n",
    "\n",
    "1. **SQL Injection via Prompt**: Usuario manipula pregunta para inyectar c\u00f3digo\n",
    "2. **Data Exfiltration**: Query extrae datos sensibles no autorizados\n",
    "3. **Denial of Service**: Queries costosas que consumen recursos\n",
    "4. **Schema Exposure**: Revelar estructura de base de datos\n",
    "\n",
    "**Defense Layers:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502         Security Architecture              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                            \u2502\n",
    "\u2502  Layer 1: Input Sanitization              \u2502\n",
    "\u2502  \u251c\u2500 Remove SQL keywords from question     \u2502\n",
    "\u2502  \u251c\u2500 Escape special characters             \u2502\n",
    "\u2502  \u2514\u2500 Length limits                          \u2502\n",
    "\u2502                                            \u2502\n",
    "\u2502  Layer 2: LLM Guardrails                  \u2502\n",
    "\u2502  \u251c\u2500 System prompt constraints             \u2502\n",
    "\u2502  \u251c\u2500 Read-only emphasis                    \u2502\n",
    "\u2502  \u2514\u2500 Whitelist operations                   \u2502\n",
    "\u2502                                            \u2502\n",
    "\u2502  Layer 3: SQL Parsing & Analysis          \u2502\n",
    "\u2502  \u251c\u2500 sqlparse validation                   \u2502\n",
    "\u2502  \u251c\u2500 AST inspection                        \u2502\n",
    "\u2502  \u2514\u2500 Keyword blacklist                      \u2502\n",
    "\u2502                                            \u2502\n",
    "\u2502  Layer 4: Query Plan Analysis             \u2502\n",
    "\u2502  \u251c\u2500 EXPLAIN cost estimation               \u2502\n",
    "\u2502  \u251c\u2500 Timeout prediction                    \u2502\n",
    "\u2502  \u2514\u2500 Resource limits                        \u2502\n",
    "\u2502                                            \u2502\n",
    "\u2502  Layer 5: Execution Sandbox               \u2502\n",
    "\u2502  \u251c\u2500 Read-only user                        \u2502\n",
    "\u2502  \u251c\u2500 Row limit enforcement                 \u2502\n",
    "\u2502  \u251c\u2500 Timeout killer                        \u2502\n",
    "\u2502  \u2514\u2500 Query logging                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "**1. Input Sanitization:**\n",
    "\n",
    "```python\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "def sanitize_question(question: str) -> Tuple[str, bool]:\n",
    "    \"\"\"Remove potentially dangerous patterns from user input\"\"\"\n",
    "    \n",
    "    # Check length\n",
    "    if len(question) > 500:\n",
    "        return \"\", False\n",
    "    \n",
    "    # Suspicious patterns\n",
    "    dangerous_patterns = [\n",
    "        r\";\\s*(drop|delete|update|insert|alter|create|truncate)\",  # Multiple statements\n",
    "        r\"--\",  # SQL comments\n",
    "        r\"/\\*.*\\*/\",  # Block comments\n",
    "        r\"union\\s+select\",  # Union injection\n",
    "        r\"exec\\s*\\(\",  # Stored proc execution\n",
    "        r\"xp_cmdshell\",  # System commands\n",
    "    ]\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    for pattern in dangerous_patterns:\n",
    "        if re.search(pattern, question_lower):\n",
    "            return \"\", False\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', question).strip()\n",
    "    \n",
    "    return cleaned, True\n",
    "\n",
    "# Example\n",
    "test_questions = [\n",
    "    \"Show top 10 sales\",  # Safe\n",
    "    \"Show sales; DROP TABLE customers; --\",  # SQL injection\n",
    "    \"SELECT * FROM users UNION SELECT password FROM admin\",  # Union injection\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    cleaned, safe = sanitize_question(q)\n",
    "    print(f\"{'\u2705' if safe else '\u274c'} {q[:50]}\")\n",
    "```\n",
    "\n",
    "**2. SQL Validation with sqlparse:**\n",
    "\n",
    "```python\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier, Where\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "def validate_sql_safety(sql: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Comprehensive SQL safety validation\"\"\"\n",
    "    \n",
    "    # Parse SQL\n",
    "    try:\n",
    "        parsed = sqlparse.parse(sql)\n",
    "        if not parsed:\n",
    "            return False, \"Invalid SQL syntax\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Parsing error: {e}\"\n",
    "    \n",
    "    statement = parsed[0]\n",
    "    \n",
    "    # 1. Check if read-only (SELECT only)\n",
    "    if statement.get_type() != 'SELECT':\n",
    "        return False, f\"Only SELECT queries allowed, got: {statement.get_type()}\"\n",
    "    \n",
    "    # 2. Check for nested dangerous operations\n",
    "    sql_lower = sql.lower()\n",
    "    write_operations = ['insert', 'update', 'delete', 'drop', 'create', 'alter', 'truncate', 'replace']\n",
    "    \n",
    "    for op in write_operations:\n",
    "        # Check for operation as separate word\n",
    "        if re.search(rf'\\b{op}\\b', sql_lower):\n",
    "            return False, f\"Forbidden operation: {op.upper()}\"\n",
    "    \n",
    "    # 3. Check for system functions/procedures\n",
    "    dangerous_functions = [\n",
    "        'exec', 'execute', 'xp_cmdshell', 'sp_executesql',\n",
    "        'dbms_output', 'utl_file', 'load_file', 'outfile'\n",
    "    ]\n",
    "    \n",
    "    for func in dangerous_functions:\n",
    "        if func in sql_lower:\n",
    "            return False, f\"Forbidden function: {func}\"\n",
    "    \n",
    "    # 4. Check for multiple statements (SQL injection)\n",
    "    if len(parsed) > 1:\n",
    "        return False, \"Multiple statements not allowed\"\n",
    "    \n",
    "    # 5. Verify LIMIT clause exists (prevent large result sets)\n",
    "    has_limit = 'limit' in sql_lower or 'fetch first' in sql_lower or 'top' in sql_lower\n",
    "    if not has_limit:\n",
    "        return False, \"Query must include LIMIT clause\"\n",
    "    \n",
    "    # 6. Check for UNION (common injection vector)\n",
    "    if 'union' in sql_lower:\n",
    "        # UNION is valid but requires manual review\n",
    "        return False, \"UNION queries require manual approval\"\n",
    "    \n",
    "    return True, \"Query validated\"\n",
    "\n",
    "# Test cases\n",
    "test_sqls = [\n",
    "    \"SELECT * FROM sales LIMIT 100\",  # Safe\n",
    "    \"SELECT * FROM sales; DROP TABLE customers;\",  # Multiple statements\n",
    "    \"SELECT * FROM sales WHERE id = (SELECT password FROM users)\",  # Subquery injection\n",
    "    \"DELETE FROM sales WHERE id = 1\",  # Write operation\n",
    "    \"SELECT * FROM sales\",  # No LIMIT\n",
    "]\n",
    "\n",
    "for sql in test_sqls:\n",
    "    safe, message = validate_sql_safety(sql)\n",
    "    print(f\"{'\u2705' if safe else '\u274c'} {message}\")\n",
    "    print(f\"   {sql[:60]}\\n\")\n",
    "```\n",
    "\n",
    "**3. Query Cost Estimation (EXPLAIN):**\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "\n",
    "def estimate_query_cost(sql: str, conn) -> Tuple[bool, dict]:\n",
    "    \"\"\"Use EXPLAIN to estimate query cost before execution\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get query plan\n",
    "        explain_sql = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {sql}\"\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(explain_sql)\n",
    "            plan = cur.fetchone()[0][0]\n",
    "        \n",
    "        # Extract cost metrics\n",
    "        total_cost = plan['Plan']['Total Cost']\n",
    "        estimated_rows = plan['Plan']['Plan Rows']\n",
    "        \n",
    "        # Define thresholds\n",
    "        MAX_COST = 10000  # Arbitrary units\n",
    "        MAX_ROWS = 100000\n",
    "        \n",
    "        if total_cost > MAX_COST:\n",
    "            return False, {\n",
    "                \"reason\": \"Query cost too high\",\n",
    "                \"cost\": total_cost,\n",
    "                \"threshold\": MAX_COST\n",
    "            }\n",
    "        \n",
    "        if estimated_rows > MAX_ROWS:\n",
    "            return False, {\n",
    "                \"reason\": \"Too many rows\",\n",
    "                \"rows\": estimated_rows,\n",
    "                \"threshold\": MAX_ROWS\n",
    "            }\n",
    "        \n",
    "        return True, {\n",
    "            \"cost\": total_cost,\n",
    "            \"rows\": estimated_rows,\n",
    "            \"approved\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "sql = \"SELECT * FROM large_table WHERE date > '2020-01-01' LIMIT 1000\"\n",
    "conn = psycopg2.connect(\"dbname=mydb\")\n",
    "\n",
    "approved, metrics = estimate_query_cost(sql, conn)\n",
    "if approved:\n",
    "    print(f\"\u2705 Query approved: {metrics}\")\n",
    "else:\n",
    "    print(f\"\u274c Query rejected: {metrics}\")\n",
    "```\n",
    "\n",
    "**4. Read-Only Database User:**\n",
    "\n",
    "```sql\n",
    "-- Create read-only role for NL2SQL execution\n",
    "CREATE ROLE nl2sql_readonly;\n",
    "\n",
    "-- Grant SELECT only on specific schemas\n",
    "GRANT USAGE ON SCHEMA public TO nl2sql_readonly;\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA public TO nl2sql_readonly;\n",
    "\n",
    "-- Deny write operations explicitly\n",
    "REVOKE INSERT, UPDATE, DELETE, TRUNCATE ON ALL TABLES IN SCHEMA public FROM nl2sql_readonly;\n",
    "\n",
    "-- Create user\n",
    "CREATE USER nl2sql_user WITH PASSWORD 'secure_password';\n",
    "GRANT nl2sql_readonly TO nl2sql_user;\n",
    "\n",
    "-- Set session limits\n",
    "ALTER ROLE nl2sql_user SET statement_timeout = '10s';\n",
    "ALTER ROLE nl2sql_user SET lock_timeout = '5s';\n",
    "```\n",
    "\n",
    "**5. Execution with Timeout & Limits:**\n",
    "\n",
    "```python\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class QueryTimeout(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def query_timeout(seconds: int):\n",
    "    \"\"\"Context manager to enforce query timeout\"\"\"\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise QueryTimeout(f\"Query exceeded {seconds}s timeout\")\n",
    "    \n",
    "    # Set alarm\n",
    "    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(seconds)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "def execute_safe_query(sql: str, conn, max_rows: int = 1000, timeout_sec: int = 10):\n",
    "    \"\"\"Execute query with safety constraints\"\"\"\n",
    "    \n",
    "    # Validate SQL\n",
    "    safe, message = validate_sql_safety(sql)\n",
    "    if not safe:\n",
    "        raise ValueError(f\"Unsafe query: {message}\")\n",
    "    \n",
    "    # Force LIMIT if not present\n",
    "    sql_lower = sql.lower()\n",
    "    if 'limit' not in sql_lower:\n",
    "        sql += f\" LIMIT {max_rows}\"\n",
    "    \n",
    "    try:\n",
    "        with query_timeout(timeout_sec):\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "        \n",
    "        # Additional row limit check\n",
    "        if len(df) > max_rows:\n",
    "            df = df.head(max_rows)\n",
    "            print(f\"\u26a0\ufe0f Results truncated to {max_rows} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except QueryTimeout as e:\n",
    "        raise TimeoutError(f\"Query timeout: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Query execution failed: {e}\")\n",
    "\n",
    "# Example\n",
    "sql = \"SELECT * FROM sales WHERE date >= '2024-01-01'\"\n",
    "df = execute_safe_query(sql, conn, max_rows=500, timeout_sec=5)\n",
    "```\n",
    "\n",
    "**6. Audit Logging:**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nl2sql')\n",
    "\n",
    "def log_nl2sql_query(\n",
    "    user_id: str,\n",
    "    question: str,\n",
    "    sql_generated: str,\n",
    "    execution_status: str,\n",
    "    rows_returned: int,\n",
    "    execution_time_ms: float,\n",
    "    error: str = None\n",
    "):\n",
    "    \"\"\"Log all NL2SQL interactions for audit\"\"\"\n",
    "    \n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"user_id\": user_id,\n",
    "        \"question_hash\": hashlib.sha256(question.encode()).hexdigest()[:16],\n",
    "        \"question_length\": len(question),\n",
    "        \"sql_hash\": hashlib.sha256(sql_generated.encode()).hexdigest()[:16],\n",
    "        \"execution_status\": execution_status,  # success | validation_failed | timeout | error\n",
    "        \"rows_returned\": rows_returned,\n",
    "        \"execution_time_ms\": execution_time_ms,\n",
    "        \"error\": error\n",
    "    }\n",
    "    \n",
    "    # Only log full text in dev environment\n",
    "    if os.getenv(\"ENV\") == \"dev\":\n",
    "        log_entry[\"question\"] = question[:200]\n",
    "        log_entry[\"sql\"] = sql_generated[:500]\n",
    "    \n",
    "    logger.info(json.dumps(log_entry))\n",
    "\n",
    "# Usage\n",
    "log_nl2sql_query(\n",
    "    user_id=\"user_123\",\n",
    "    question=\"Show top 10 customers by revenue\",\n",
    "    sql_generated=\"SELECT customer_id, SUM(total) FROM sales GROUP BY customer_id ORDER BY 2 DESC LIMIT 10\",\n",
    "    execution_status=\"success\",\n",
    "    rows_returned=10,\n",
    "    execution_time_ms=45.2\n",
    ")\n",
    "```\n",
    "\n",
    "**7. Human-in-the-Loop for High-Risk Queries:**\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "\n",
    "class RiskLevel(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "def assess_query_risk(sql: str) -> RiskLevel:\n",
    "    \"\"\"Determine if query needs human approval\"\"\"\n",
    "    \n",
    "    sql_lower = sql.lower()\n",
    "    \n",
    "    # High risk indicators\n",
    "    high_risk = [\n",
    "        'union',  # Complex joins\n",
    "        'cross join',  # Cartesian products\n",
    "        'recursive',  # CTEs\n",
    "        'window function',  # Complex analytics\n",
    "    ]\n",
    "    \n",
    "    # Medium risk indicators\n",
    "    medium_risk = [\n",
    "        'subquery',\n",
    "        'having',\n",
    "        'case when',\n",
    "        'join' in sql_lower and sql_lower.count('join') > 2,  # Multiple joins\n",
    "    ]\n",
    "    \n",
    "    for indicator in high_risk:\n",
    "        if indicator in sql_lower:\n",
    "            return RiskLevel.HIGH\n",
    "    \n",
    "    for indicator in medium_risk:\n",
    "        if indicator in sql_lower:\n",
    "            return RiskLevel.MEDIUM\n",
    "    \n",
    "    return RiskLevel.LOW\n",
    "\n",
    "def execute_with_approval(question: str, sql: str, conn):\n",
    "    \"\"\"Execute query with risk-based approval\"\"\"\n",
    "    \n",
    "    risk = assess_query_risk(sql)\n",
    "    \n",
    "    if risk == RiskLevel.HIGH:\n",
    "        print(f\"\u26a0\ufe0f HIGH RISK query requires manual approval:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"SQL: {sql}\\n\")\n",
    "        \n",
    "        approval = input(\"Approve execution? (yes/no): \")\n",
    "        if approval.lower() != \"yes\":\n",
    "            return None\n",
    "    \n",
    "    elif risk == RiskLevel.MEDIUM:\n",
    "        print(f\"\u26a0\ufe0f MEDIUM RISK query - review recommended\")\n",
    "        print(f\"SQL: {sql[:100]}...\\n\")\n",
    "    \n",
    "    # Execute approved query\n",
    "    return execute_safe_query(sql, conn)\n",
    "```\n",
    "\n",
    "**Security Checklist:**\n",
    "\n",
    "- \u2705 Input sanitization (length, dangerous patterns)\n",
    "- \u2705 SQL parsing validation (sqlparse)\n",
    "- \u2705 Read-only database user\n",
    "- \u2705 LIMIT clause enforcement\n",
    "- \u2705 Query timeout (10s default)\n",
    "- \u2705 EXPLAIN cost estimation\n",
    "- \u2705 Audit logging (all queries)\n",
    "- \u2705 Human approval for high-risk queries\n",
    "- \u2705 Rate limiting per user\n",
    "- \u2705 Row count limits (1000 default)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3f477",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf **Advanced Techniques: Few-Shot, Chain-of-Thought & Self-Correction**\n",
    "\n",
    "**1. Few-Shot Learning for NL2SQL**\n",
    "\n",
    "Few-shot examples dramatically improve accuracy by showing the LLM the desired output format and handling of edge cases.\n",
    "\n",
    "**Example Selection Strategies:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class FewShotSelector:\n",
    "    \"\"\"Dynamically select most relevant examples for each question\"\"\"\n",
    "    \n",
    "    def __init__(self, example_pool: list):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.examples = example_pool\n",
    "        \n",
    "        # Pre-compute embeddings for all examples\n",
    "        self.embeddings = self.model.encode([ex['question'] for ex in self.examples])\n",
    "    \n",
    "    def select_examples(self, question: str, k: int = 3) -> list:\n",
    "        \"\"\"Select k most similar examples\"\"\"\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.model.encode([question])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [self.examples[i] for i in top_indices]\n",
    "\n",
    "# Example pool with diverse query types\n",
    "example_pool = [\n",
    "    {\n",
    "        \"question\": \"How many orders were placed yesterday?\",\n",
    "        \"sql\": \"SELECT COUNT(*) FROM orders WHERE DATE(created_at) = CURRENT_DATE - INTERVAL '1 day';\",\n",
    "        \"explanation\": \"Count aggregation with date filter\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Top 5 customers by total spend\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    SUM(o.total_amount) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 5;\n",
    "\"\"\",\n",
    "        \"explanation\": \"Aggregation with JOIN and ORDER BY\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Products without any sales this month\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT p.product_id, p.name\n",
    "FROM products p\n",
    "LEFT JOIN orders o ON p.product_id = o.product_id \n",
    "    AND DATE_TRUNC('month', o.created_at) = DATE_TRUNC('month', CURRENT_DATE)\n",
    "WHERE o.order_id IS NULL\n",
    "LIMIT 100;\n",
    "\"\"\",\n",
    "        \"explanation\": \"LEFT JOIN with NULL check for missing records\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Running total of sales by date\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    DATE(created_at) as sale_date,\n",
    "    SUM(total_amount) as daily_total,\n",
    "    SUM(SUM(total_amount)) OVER (ORDER BY DATE(created_at)) as running_total\n",
    "FROM orders\n",
    "GROUP BY DATE(created_at)\n",
    "ORDER BY sale_date\n",
    "LIMIT 365;\n",
    "\"\"\",\n",
    "        \"explanation\": \"Window function for running totals\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Customers who haven't ordered in 90 days\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    MAX(o.created_at) as last_order_date,\n",
    "    CURRENT_DATE - MAX(o.created_at)::date as days_since_order\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name\n",
    "HAVING MAX(o.created_at) < CURRENT_DATE - INTERVAL '90 days'\n",
    "    OR MAX(o.created_at) IS NULL\n",
    "ORDER BY last_order_date NULLS FIRST\n",
    "LIMIT 100;\n",
    "\"\"\",\n",
    "        \"explanation\": \"HAVING clause with date comparison\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Usage\n",
    "selector = FewShotSelector(example_pool)\n",
    "question = \"Show customers with no recent purchases\"\n",
    "relevant_examples = selector.select_examples(question, k=2)\n",
    "\n",
    "print(\"Selected examples:\")\n",
    "for ex in relevant_examples:\n",
    "    print(f\"\\nQ: {ex['question']}\")\n",
    "    print(f\"SQL: {ex['sql'][:100]}...\")\n",
    "```\n",
    "\n",
    "**Few-Shot Prompt Construction:**\n",
    "\n",
    "```python\n",
    "def build_fewshot_prompt(question: str, schema: str, examples: list, dialect: str = \"PostgreSQL\") -> str:\n",
    "    \"\"\"Construct optimized few-shot prompt\"\"\"\n",
    "    \n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"Example {i+1}:\\n\"\n",
    "        f\"Question: {ex['question']}\\n\"\n",
    "        f\"SQL:\\n{ex['sql'].strip()}\\n\"\n",
    "        f\"Why: {ex['explanation']}\"\n",
    "        for i, ex in enumerate(examples)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert {dialect} developer. Generate SQL to answer questions.\n",
    "\n",
    "Database Schema:\n",
    "{schema}\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "Now generate SQL for this question:\n",
    "Question: {question}\n",
    "\n",
    "Requirements:\n",
    "- Use table aliases (e.g., FROM orders o)\n",
    "- Include LIMIT clause (max 1000 rows)\n",
    "- Handle NULL values appropriately\n",
    "- Use appropriate indexes mentioned in schema\n",
    "- Return ONLY the SQL query\n",
    "\n",
    "SQL:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "question = \"Which product categories have declining sales this quarter vs last quarter?\"\n",
    "examples = selector.select_examples(question, k=2)\n",
    "prompt = build_fewshot_prompt(question, schema_ddl, examples)\n",
    "\n",
    "sql = ask_llm(prompt, temperature=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Chain-of-Thought for Complex Queries**\n",
    "\n",
    "Break complex questions into steps, generating SQL iteratively.\n",
    "\n",
    "```python\n",
    "def nl2sql_with_cot(question: str, schema: str) -> dict:\n",
    "    \"\"\"Generate SQL with chain-of-thought reasoning\"\"\"\n",
    "    \n",
    "    # Step 1: Decompose question\n",
    "    decomposition_prompt = f\"\"\"\n",
    "Analyze this database question step-by-step:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Schema: {schema}\n",
    "\n",
    "Break down the question into logical steps:\n",
    "1. What tables are needed?\n",
    "2. What joins are required?\n",
    "3. What filters should be applied?\n",
    "4. What aggregations are needed?\n",
    "5. What is the final output format?\n",
    "\n",
    "Provide step-by-step reasoning:\n",
    "\"\"\"\n",
    "    \n",
    "    reasoning = ask_llm(decomposition_prompt, temperature=0)\n",
    "    \n",
    "    # Step 2: Generate SQL with reasoning\n",
    "    sql_prompt = f\"\"\"\n",
    "Based on this reasoning:\n",
    "\n",
    "{reasoning}\n",
    "\n",
    "Generate the final SQL query for PostgreSQL:\n",
    "\n",
    "Requirements:\n",
    "- Include comments for each major section\n",
    "- Use CTEs for complex logic\n",
    "- Add LIMIT clause\n",
    "\n",
    "SQL:\n",
    "\"\"\"\n",
    "    \n",
    "    sql = ask_llm(sql_prompt, temperature=0)\n",
    "    \n",
    "    return {\n",
    "        \"sql\": sql,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"complexity\": \"high\" if \"cte\" in sql.lower() or \"window\" in sql.lower() else \"medium\"\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"\"\"\n",
    "Compare the average order value and customer retention rate \n",
    "between customers acquired via different marketing channels, \n",
    "but only for customers who have made at least 3 purchases\n",
    "\"\"\"\n",
    "\n",
    "result = nl2sql_with_cot(question, schema_ddl)\n",
    "print(\"Reasoning:\")\n",
    "print(result[\"reasoning\"])\n",
    "print(\"\\nGenerated SQL:\")\n",
    "print(result[\"sql\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Self-Correction with Execution Feedback**\n",
    "\n",
    "Let the LLM fix its own errors by providing execution feedback.\n",
    "\n",
    "```python\n",
    "def nl2sql_with_self_correction(question: str, schema: str, conn, max_attempts: int = 3) -> dict:\n",
    "    \"\"\"Generate SQL with self-correction loop\"\"\"\n",
    "    \n",
    "    attempts = []\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        # Generate SQL\n",
    "        if attempt == 0:\n",
    "            # First attempt: standard generation\n",
    "            sql = nl_to_sql(question, schema)\n",
    "        else:\n",
    "            # Subsequent attempts: include error feedback\n",
    "            correction_prompt = f\"\"\"\n",
    "Previous attempt failed with error:\n",
    "\n",
    "Question: {question}\n",
    "Schema: {schema}\n",
    "\n",
    "Previous SQL (FAILED):\n",
    "{attempts[-1]['sql']}\n",
    "\n",
    "Error:\n",
    "{attempts[-1]['error']}\n",
    "\n",
    "Generate a corrected SQL query that fixes this error.\n",
    "Analyze the error message and adjust the query accordingly.\n",
    "\n",
    "Corrected SQL:\n",
    "\"\"\"\n",
    "            sql = ask_llm(correction_prompt, temperature=0.1)\n",
    "        \n",
    "        # Validate and execute\n",
    "        safe, validation_msg = validate_sql_safety(sql)\n",
    "        \n",
    "        if not safe:\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"sql\": sql,\n",
    "                \"error\": f\"Validation failed: {validation_msg}\",\n",
    "                \"success\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Try execution\n",
    "        try:\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "            \n",
    "            # Success!\n",
    "            return {\n",
    "                \"sql\": sql,\n",
    "                \"result\": df,\n",
    "                \"attempts\": attempt + 1,\n",
    "                \"success\": True,\n",
    "                \"history\": attempts\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"sql\": sql,\n",
    "                \"error\": error_msg,\n",
    "                \"success\": False\n",
    "            })\n",
    "    \n",
    "    # All attempts failed\n",
    "    return {\n",
    "        \"sql\": None,\n",
    "        \"result\": None,\n",
    "        \"attempts\": max_attempts,\n",
    "        \"success\": False,\n",
    "        \"history\": attempts,\n",
    "        \"final_error\": \"Max retry attempts reached\"\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"Show monthly revenue trend for 2024\"\n",
    "result = nl2sql_with_self_correction(question, schema_ddl, conn)\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"\u2705 Success after {result['attempts']} attempt(s)\")\n",
    "    print(result[\"result\"].head())\n",
    "else:\n",
    "    print(f\"\u274c Failed after {result['attempts']} attempts\")\n",
    "    for hist in result[\"history\"]:\n",
    "        print(f\"\\nAttempt {hist['attempt']}: {hist['error']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. SQL Optimization with LLM**\n",
    "\n",
    "```python\n",
    "def optimize_generated_sql(sql: str, schema: str, conn) -> dict:\n",
    "    \"\"\"Analyze and optimize generated SQL\"\"\"\n",
    "    \n",
    "    # Get EXPLAIN plan\n",
    "    try:\n",
    "        explain_query = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {sql}\"\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(explain_query)\n",
    "            plan = cur.fetchone()[0][0]\n",
    "        \n",
    "        cost = plan['Plan']['Total Cost']\n",
    "        rows = plan['Plan']['Plan Rows']\n",
    "        \n",
    "        # If cost is high, ask LLM to optimize\n",
    "        if cost > 1000:  # Threshold\n",
    "            optimization_prompt = f\"\"\"\n",
    "This SQL query has high cost ({cost:.0f}) and may be slow:\n",
    "\n",
    "{sql}\n",
    "\n",
    "Schema with indexes:\n",
    "{schema}\n",
    "\n",
    "Query execution plan shows:\n",
    "- Total Cost: {cost}\n",
    "- Estimated Rows: {rows}\n",
    "\n",
    "Optimize this query:\n",
    "1. Add appropriate indexes hints\n",
    "2. Rewrite inefficient subqueries\n",
    "3. Consider materialized CTEs\n",
    "4. Optimize JOIN order\n",
    "\n",
    "Provide optimized SQL:\n",
    "\"\"\"\n",
    "            \n",
    "            optimized_sql = ask_llm(optimization_prompt, temperature=0)\n",
    "            \n",
    "            # Compare costs\n",
    "            explain_optimized = f\"EXPLAIN (FORMAT JSON, ANALYZE false) {optimized_sql}\"\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(explain_optimized)\n",
    "                plan_opt = cur.fetchone()[0][0]\n",
    "            \n",
    "            cost_opt = plan_opt['Plan']['Total Cost']\n",
    "            \n",
    "            return {\n",
    "                \"original_sql\": sql,\n",
    "                \"original_cost\": cost,\n",
    "                \"optimized_sql\": optimized_sql,\n",
    "                \"optimized_cost\": cost_opt,\n",
    "                \"improvement\": f\"{((cost - cost_opt) / cost * 100):.1f}%\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"sql\": sql,\n",
    "            \"cost\": cost,\n",
    "            \"optimization\": \"Not needed (cost acceptable)\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example\n",
    "sql = \"SELECT * FROM orders o JOIN customers c ON o.customer_id = c.customer_id WHERE o.date > '2024-01-01'\"\n",
    "result = optimize_generated_sql(sql, schema_ddl, conn)\n",
    "\n",
    "if \"improvement\" in result:\n",
    "    print(f\"Optimization improved cost by {result['improvement']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**5. Multi-Turn Conversation (Refinement)**\n",
    "\n",
    "```python\n",
    "class NL2SQLConversation:\n",
    "    \"\"\"Multi-turn conversation for query refinement\"\"\"\n",
    "    \n",
    "    def __init__(self, schema: str, conn):\n",
    "        self.schema = schema\n",
    "        self.conn = conn\n",
    "        self.history = []\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Process question with conversation context\"\"\"\n",
    "        \n",
    "        # Build context from history\n",
    "        context = self._build_context()\n",
    "        \n",
    "        # Generate SQL\n",
    "        prompt = f\"\"\"\n",
    "{context}\n",
    "\n",
    "Schema: {self.schema}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Generate SQL considering previous conversation context.\n",
    "\n",
    "SQL:\n",
    "\"\"\"\n",
    "        \n",
    "        sql = ask_llm(prompt, temperature=0)\n",
    "        \n",
    "        # Execute\n",
    "        try:\n",
    "            df = pd.read_sql_query(sql, self.conn)\n",
    "            \n",
    "            # Store in history\n",
    "            self.history.append({\n",
    "                \"question\": question,\n",
    "                \"sql\": sql,\n",
    "                \"result_preview\": df.head(3).to_dict(),\n",
    "                \"row_count\": len(df)\n",
    "            })\n",
    "            \n",
    "            return {\"sql\": sql, \"result\": df, \"success\": True}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"success\": False}\n",
    "    \n",
    "    def _build_context(self) -> str:\n",
    "        \"\"\"Build conversation context\"\"\"\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"Previous conversation:\\n\"\n",
    "        for i, turn in enumerate(self.history[-3:], 1):  # Last 3 turns\n",
    "            context += f\"\\n{i}. Q: {turn['question']}\"\n",
    "            context += f\"\\n   SQL: {turn['sql'][:100]}...\"\n",
    "            context += f\"\\n   Returned {turn['row_count']} rows\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Example conversation\n",
    "conv = NL2SQLConversation(schema_ddl, conn)\n",
    "\n",
    "# Turn 1\n",
    "result1 = conv.query(\"Show top 10 products by sales\")\n",
    "print(result1[\"result\"])\n",
    "\n",
    "# Turn 2 (refinement with context)\n",
    "result2 = conv.query(\"Now filter to only electronics category\")\n",
    "print(result2[\"result\"])\n",
    "\n",
    "# Turn 3 (further refinement)\n",
    "result3 = conv.query(\"And sort by profit margin instead\")\n",
    "print(result3[\"result\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Ensemble Methods (Multiple LLMs)**\n",
    "\n",
    "```python\n",
    "def nl2sql_ensemble(question: str, schema: str, models: list = [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\"]) -> dict:\n",
    "    \"\"\"Generate SQL with multiple models and vote\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            sql = nl_to_sql(question, schema, model=model)\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"sql\": sql,\n",
    "                \"sql_hash\": hashlib.sha256(sql.encode()).hexdigest()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Model {model} failed: {e}\")\n",
    "    \n",
    "    # Count identical queries (consensus)\n",
    "    from collections import Counter\n",
    "    sql_hashes = [r[\"sql_hash\"] for r in results]\n",
    "    consensus = Counter(sql_hashes).most_common(1)[0]\n",
    "    \n",
    "    consensus_count = consensus[1]\n",
    "    consensus_hash = consensus[0]\n",
    "    \n",
    "    # Get SQL for consensus\n",
    "    consensus_sql = next(r[\"sql\"] for r in results if r[\"sql_hash\"] == consensus_hash)\n",
    "    \n",
    "    return {\n",
    "        \"consensus_sql\": consensus_sql,\n",
    "        \"agreement\": f\"{consensus_count}/{len(models)}\",\n",
    "        \"all_results\": results\n",
    "    }\n",
    "\n",
    "# Example\n",
    "question = \"Monthly active users in Q4 2024\"\n",
    "result = nl2sql_ensemble(question, schema_ddl)\n",
    "\n",
    "print(f\"Consensus ({result['agreement']} models agree):\")\n",
    "print(result[\"consensus_sql\"])\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ad63b",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **Evaluation & Production Metrics: Measuring NL2SQL Quality**\n",
    "\n",
    "**Evaluation Challenges:**\n",
    "\n",
    "NL2SQL evaluation es complejo porque:\n",
    "1. **M\u00faltiples SQL v\u00e1lidos**: Misma pregunta \u2192 diferentes queries correctas\n",
    "2. **Equivalencia sem\u00e1ntica**: Queries sint\u00e1cticamente diferentes pero sem\u00e1nticamente id\u00e9nticas\n",
    "3. **Partial correctness**: Query puede ser parcialmente correcta\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502          NL2SQL Evaluation Metrics              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                 \u2502\n",
    "\u2502  1. Exact Match (EM)                            \u2502\n",
    "\u2502     SQL generado == SQL esperado                \u2502\n",
    "\u2502     \u274c Demasiado estricto                        \u2502\n",
    "\u2502                                                 \u2502\n",
    "\u2502  2. Execution Accuracy (EX)                     \u2502\n",
    "\u2502     Result(SQL_gen) == Result(SQL_gold)         \u2502\n",
    "\u2502     \u2705 Mejor m\u00e9trica pr\u00e1ctica                    \u2502\n",
    "\u2502                                                 \u2502\n",
    "\u2502  3. Component Match                             \u2502\n",
    "\u2502     SELECT clause: 90%                          \u2502\n",
    "\u2502     FROM clause: 95%                            \u2502\n",
    "\u2502     WHERE clause: 85%                           \u2502\n",
    "\u2502     GROUP BY: 80%                               \u2502\n",
    "\u2502     ORDER BY: 75%                               \u2502\n",
    "\u2502                                                 \u2502\n",
    "\u2502  4. Test Suite Execution (TSE)                  \u2502\n",
    "\u2502     % of test cases passed                      \u2502\n",
    "\u2502     \u2705 Robusto a variaciones sint\u00e1cticas         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**1. Execution Accuracy (Gold Standard):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def execution_accuracy(\n",
    "    generated_sql: str,\n",
    "    gold_sql: str,\n",
    "    conn,\n",
    "    timeout: int = 10\n",
    ") -> Tuple[bool, dict]:\n",
    "    \"\"\"Compare results of generated vs gold SQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Execute both queries\n",
    "        df_gen = pd.read_sql_query(generated_sql, conn)\n",
    "        df_gold = pd.read_sql_query(gold_sql, conn)\n",
    "        \n",
    "        # Sort both dataframes for comparison\n",
    "        df_gen_sorted = df_gen.sort_values(by=list(df_gen.columns)).reset_index(drop=True)\n",
    "        df_gold_sorted = df_gold.sort_values(by=list(df_gold.columns)).reset_index(drop=True)\n",
    "        \n",
    "        # Compare\n",
    "        if df_gen_sorted.equals(df_gold_sorted):\n",
    "            return True, {\"match\": \"exact\", \"rows\": len(df_gen)}\n",
    "        \n",
    "        # Check if row counts match\n",
    "        if len(df_gen) != len(df_gold):\n",
    "            return False, {\n",
    "                \"match\": \"row_count_mismatch\",\n",
    "                \"generated_rows\": len(df_gen),\n",
    "                \"gold_rows\": len(df_gold)\n",
    "            }\n",
    "        \n",
    "        # Check column differences\n",
    "        if set(df_gen.columns) != set(df_gold.columns):\n",
    "            return False, {\n",
    "                \"match\": \"column_mismatch\",\n",
    "                \"generated_cols\": list(df_gen.columns),\n",
    "                \"gold_cols\": list(df_gold.columns)\n",
    "            }\n",
    "        \n",
    "        # Partial match (values differ)\n",
    "        return False, {\n",
    "            \"match\": \"value_mismatch\",\n",
    "            \"diff_rows\": (df_gen_sorted != df_gold_sorted).sum().sum()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Example\n",
    "generated = \"SELECT customer_id, SUM(total) as revenue FROM sales GROUP BY customer_id ORDER BY revenue DESC LIMIT 10\"\n",
    "gold = \"SELECT customer_id, SUM(total_amount) as revenue FROM sales GROUP BY customer_id ORDER BY 2 DESC LIMIT 10\"\n",
    "\n",
    "match, details = execution_accuracy(generated, gold, conn)\n",
    "print(f\"{'\u2705' if match else '\u274c'} {details}\")\n",
    "```\n",
    "\n",
    "**2. Component-Level Evaluation:**\n",
    "\n",
    "```python\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier, Where\n",
    "\n",
    "def parse_sql_components(sql: str) -> dict:\n",
    "    \"\"\"Extract components from SQL query\"\"\"\n",
    "    \n",
    "    parsed = sqlparse.parse(sql)[0]\n",
    "    components = {\n",
    "        \"select\": [],\n",
    "        \"from\": [],\n",
    "        \"where\": [],\n",
    "        \"group_by\": [],\n",
    "        \"order_by\": [],\n",
    "        \"limit\": None\n",
    "    }\n",
    "    \n",
    "    # Extract SELECT columns\n",
    "    for token in parsed.tokens:\n",
    "        if token.ttype is sqlparse.tokens.DML and token.value.upper() == 'SELECT':\n",
    "            # Next token is column list\n",
    "            continue\n",
    "    \n",
    "    # Simplified extraction (full implementation uses AST traversal)\n",
    "    sql_lower = sql.lower()\n",
    "    \n",
    "    # Extract table names\n",
    "    if 'from' in sql_lower:\n",
    "        from_clause = sql_lower.split('from')[1].split('where')[0] if 'where' in sql_lower else sql_lower.split('from')[1]\n",
    "        components[\"from\"] = [t.strip() for t in from_clause.split('join')]\n",
    "    \n",
    "    return components\n",
    "\n",
    "def component_accuracy(generated_sql: str, gold_sql: str) -> dict:\n",
    "    \"\"\"Compare SQL components\"\"\"\n",
    "    \n",
    "    gen_comp = parse_sql_components(generated_sql)\n",
    "    gold_comp = parse_sql_components(gold_sql)\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for component in [\"select\", \"from\", \"where\", \"group_by\", \"order_by\"]:\n",
    "        gen_set = set(gen_comp.get(component, []))\n",
    "        gold_set = set(gold_comp.get(component, []))\n",
    "        \n",
    "        if not gold_set:\n",
    "            scores[component] = 1.0 if not gen_set else 0.0\n",
    "        else:\n",
    "            # Jaccard similarity\n",
    "            intersection = len(gen_set & gold_set)\n",
    "            union = len(gen_set | gold_set)\n",
    "            scores[component] = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # Overall score (weighted average)\n",
    "    weights = {\"select\": 0.3, \"from\": 0.25, \"where\": 0.25, \"group_by\": 0.1, \"order_by\": 0.1}\n",
    "    overall = sum(scores[k] * weights[k] for k in weights)\n",
    "    \n",
    "    return {\n",
    "        \"component_scores\": scores,\n",
    "        \"overall_score\": overall\n",
    "    }\n",
    "\n",
    "# Example\n",
    "scores = component_accuracy(generated, gold)\n",
    "print(f\"Overall Score: {scores['overall_score']:.2%}\")\n",
    "for comp, score in scores['component_scores'].items():\n",
    "    print(f\"  {comp}: {score:.2%}\")\n",
    "```\n",
    "\n",
    "**3. Spider Benchmark Evaluation:**\n",
    "\n",
    "```python\n",
    "# Spider dataset: https://yale-lily.github.io/spider\n",
    "# 10,181 questions + SQL pairs across 200 databases\n",
    "\n",
    "def evaluate_on_spider_subset(model_fn, test_cases: list) -> dict:\n",
    "    \"\"\"Evaluate model on Spider benchmark subset\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"total\": len(test_cases),\n",
    "        \"exact_match\": 0,\n",
    "        \"execution_match\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    for i, case in enumerate(test_cases):\n",
    "        question = case[\"question\"]\n",
    "        gold_sql = case[\"sql\"]\n",
    "        db_id = case[\"db_id\"]\n",
    "        \n",
    "        try:\n",
    "            # Generate SQL\n",
    "            generated_sql = model_fn(question, case[\"schema\"])\n",
    "            \n",
    "            # Check exact match\n",
    "            if generated_sql.strip().lower() == gold_sql.strip().lower():\n",
    "                results[\"exact_match\"] += 1\n",
    "            \n",
    "            # Check execution match\n",
    "            # (requires database instances from Spider)\n",
    "            conn = get_spider_db_connection(db_id)\n",
    "            match, _ = execution_accuracy(generated_sql, gold_sql, conn)\n",
    "            \n",
    "            if match:\n",
    "                results[\"execution_match\"] += 1\n",
    "            \n",
    "            results[\"details\"].append({\n",
    "                \"case_id\": i,\n",
    "                \"exact_match\": generated_sql.strip().lower() == gold_sql.strip().lower(),\n",
    "                \"execution_match\": match\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"details\"].append({\n",
    "                \"case_id\": i,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate percentages\n",
    "    results[\"exact_match_pct\"] = results[\"exact_match\"] / results[\"total\"] * 100\n",
    "    results[\"execution_match_pct\"] = results[\"execution_match\"] / results[\"total\"] * 100\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "test_cases = load_spider_subset(100)  # 100 test cases\n",
    "results = evaluate_on_spider_subset(nl_to_sql, test_cases)\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match_pct']:.1f}%\")\n",
    "print(f\"Execution Accuracy: {results['execution_match_pct']:.1f}%\")\n",
    "```\n",
    "\n",
    "**4. Production Monitoring Metrics:**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class NL2SQLMetrics:\n",
    "    \"\"\"Production metrics for NL2SQL system\"\"\"\n",
    "    \n",
    "    timestamp: datetime\n",
    "    \n",
    "    # Volume metrics\n",
    "    total_queries: int\n",
    "    queries_per_minute: float\n",
    "    \n",
    "    # Success metrics\n",
    "    validation_pass_rate: float  # % passed SQL validation\n",
    "    execution_success_rate: float  # % executed without error\n",
    "    result_quality_score: float  # % with non-empty results\n",
    "    \n",
    "    # Performance metrics\n",
    "    avg_generation_time_ms: float\n",
    "    p95_generation_time_ms: float\n",
    "    avg_execution_time_ms: float\n",
    "    p95_execution_time_ms: float\n",
    "    \n",
    "    # Cost metrics\n",
    "    total_tokens_consumed: int\n",
    "    total_cost_usd: float\n",
    "    cost_per_query: float\n",
    "    \n",
    "    # User satisfaction\n",
    "    thumbs_up_rate: float  # % of queries with positive feedback\n",
    "    retry_rate: float  # % of users who retry same question\n",
    "    \n",
    "    # Error breakdown\n",
    "    validation_errors: int\n",
    "    execution_errors: int\n",
    "    timeout_errors: int\n",
    "\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and aggregate NL2SQL metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "    \n",
    "    def log_query(\n",
    "        self,\n",
    "        question: str,\n",
    "        sql: str,\n",
    "        generation_time_ms: float,\n",
    "        execution_time_ms: float,\n",
    "        validation_passed: bool,\n",
    "        execution_success: bool,\n",
    "        result_rows: int,\n",
    "        tokens_used: int,\n",
    "        cost_usd: float,\n",
    "        user_feedback: str = None  # 'thumbs_up', 'thumbs_down', None\n",
    "    ):\n",
    "        self.queries.append({\n",
    "            \"timestamp\": datetime.utcnow(),\n",
    "            \"question\": question,\n",
    "            \"sql\": sql,\n",
    "            \"generation_time_ms\": generation_time_ms,\n",
    "            \"execution_time_ms\": execution_time_ms,\n",
    "            \"validation_passed\": validation_passed,\n",
    "            \"execution_success\": execution_success,\n",
    "            \"result_rows\": result_rows,\n",
    "            \"tokens_used\": tokens_used,\n",
    "            \"cost_usd\": cost_usd,\n",
    "            \"user_feedback\": user_feedback\n",
    "        })\n",
    "    \n",
    "    def get_metrics(self, time_window_minutes: int = 60) -> NL2SQLMetrics:\n",
    "        \"\"\"Calculate metrics for recent time window\"\"\"\n",
    "        \n",
    "        cutoff = datetime.utcnow() - timedelta(minutes=time_window_minutes)\n",
    "        recent = [q for q in self.queries if q[\"timestamp\"] >= cutoff]\n",
    "        \n",
    "        if not recent:\n",
    "            return None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total = len(recent)\n",
    "        \n",
    "        validation_passed = sum(1 for q in recent if q[\"validation_passed\"])\n",
    "        execution_success = sum(1 for q in recent if q[\"execution_success\"])\n",
    "        non_empty_results = sum(1 for q in recent if q[\"result_rows\"] > 0)\n",
    "        \n",
    "        generation_times = [q[\"generation_time_ms\"] for q in recent]\n",
    "        execution_times = [q[\"execution_time_ms\"] for q in recent if q[\"execution_time_ms\"]]\n",
    "        \n",
    "        thumbs_up = sum(1 for q in recent if q[\"user_feedback\"] == \"thumbs_up\")\n",
    "        total_feedback = sum(1 for q in recent if q[\"user_feedback\"])\n",
    "        \n",
    "        return NL2SQLMetrics(\n",
    "            timestamp=datetime.utcnow(),\n",
    "            total_queries=total,\n",
    "            queries_per_minute=total / time_window_minutes,\n",
    "            validation_pass_rate=validation_passed / total,\n",
    "            execution_success_rate=execution_success / total,\n",
    "            result_quality_score=non_empty_results / total,\n",
    "            avg_generation_time_ms=np.mean(generation_times),\n",
    "            p95_generation_time_ms=np.percentile(generation_times, 95),\n",
    "            avg_execution_time_ms=np.mean(execution_times) if execution_times else 0,\n",
    "            p95_execution_time_ms=np.percentile(execution_times, 95) if execution_times else 0,\n",
    "            total_tokens_consumed=sum(q[\"tokens_used\"] for q in recent),\n",
    "            total_cost_usd=sum(q[\"cost_usd\"] for q in recent),\n",
    "            cost_per_query=sum(q[\"cost_usd\"] for q in recent) / total,\n",
    "            thumbs_up_rate=thumbs_up / total_feedback if total_feedback > 0 else 0,\n",
    "            retry_rate=0,  # Calculate based on user session analysis\n",
    "            validation_errors=total - validation_passed,\n",
    "            execution_errors=validation_passed - execution_success,\n",
    "            timeout_errors=0  # Track separately\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "collector = MetricsCollector()\n",
    "\n",
    "# Log queries as they happen\n",
    "collector.log_query(\n",
    "    question=\"Top 10 products\",\n",
    "    sql=\"SELECT * FROM products ORDER BY sales DESC LIMIT 10\",\n",
    "    generation_time_ms=450,\n",
    "    execution_time_ms=120,\n",
    "    validation_passed=True,\n",
    "    execution_success=True,\n",
    "    result_rows=10,\n",
    "    tokens_used=250,\n",
    "    cost_usd=0.001,\n",
    "    user_feedback=\"thumbs_up\"\n",
    ")\n",
    "\n",
    "# Get hourly metrics\n",
    "metrics = collector.get_metrics(time_window_minutes=60)\n",
    "print(f\"Queries/min: {metrics.queries_per_minute:.1f}\")\n",
    "print(f\"Success Rate: {metrics.execution_success_rate:.1%}\")\n",
    "print(f\"Avg Latency: {metrics.avg_generation_time_ms:.0f}ms\")\n",
    "print(f\"Cost/Query: ${metrics.cost_per_query:.4f}\")\n",
    "```\n",
    "\n",
    "**5. Grafana Dashboard Metrics:**\n",
    "\n",
    "```yaml\n",
    "# Prometheus metrics for NL2SQL\n",
    "nl2sql_queries_total{status=\"success|validation_failed|execution_failed\"}\n",
    "nl2sql_generation_duration_seconds{model=\"gpt-4o\"}\n",
    "nl2sql_execution_duration_seconds\n",
    "nl2sql_tokens_consumed_total{model=\"gpt-4o\",type=\"input|output\"}\n",
    "nl2sql_cost_usd_total{model=\"gpt-4o\"}\n",
    "nl2sql_user_satisfaction{feedback=\"thumbs_up|thumbs_down\"}\n",
    "nl2sql_result_rows{quantile=\"0.5|0.95|0.99\"}\n",
    "```\n",
    "\n",
    "**Grafana Panels:**\n",
    "1. **Request Rate**: `rate(nl2sql_queries_total[5m])`\n",
    "2. **Success Rate**: `rate(nl2sql_queries_total{status=\"success\"}[5m]) / rate(nl2sql_queries_total[5m])`\n",
    "3. **Latency p95**: `histogram_quantile(0.95, nl2sql_generation_duration_seconds)`\n",
    "4. **Cost Burn**: `rate(nl2sql_cost_usd_total[1h]) * 24 * 30` ($/month)\n",
    "5. **User Satisfaction**: `rate(nl2sql_user_satisfaction{feedback=\"thumbs_up\"}[1h]) / rate(nl2sql_user_satisfaction[1h])`\n",
    "\n",
    "**6. A/B Testing Framework:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"A/B test different NL2SQL strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, variants: dict):\n",
    "        self.variants = variants  # {variant_name: generator_function}\n",
    "        self.results = {name: [] for name in variants.keys()}\n",
    "    \n",
    "    def generate(self, question: str, schema: str, user_id: str) -> dict:\n",
    "        \"\"\"Route user to variant and track results\"\"\"\n",
    "        \n",
    "        # Consistent variant assignment per user\n",
    "        variant_name = self._assign_variant(user_id)\n",
    "        generator = self.variants[variant_name]\n",
    "        \n",
    "        # Generate SQL\n",
    "        start = time.time()\n",
    "        sql = generator(question, schema)\n",
    "        generation_time = (time.time() - start) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"sql\": sql,\n",
    "            \"generation_time_ms\": generation_time\n",
    "        }\n",
    "    \n",
    "    def log_result(\n",
    "        self,\n",
    "        variant: str,\n",
    "        success: bool,\n",
    "        generation_time_ms: float,\n",
    "        user_feedback: str = None\n",
    "    ):\n",
    "        \"\"\"Log result for variant\"\"\"\n",
    "        self.results[variant].append({\n",
    "            \"success\": success,\n",
    "            \"generation_time_ms\": generation_time_ms,\n",
    "            \"user_feedback\": user_feedback\n",
    "        })\n",
    "    \n",
    "    def analyze(self) -> dict:\n",
    "        \"\"\"Statistical analysis of variants\"\"\"\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        for variant, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            analysis[variant] = {\n",
    "                \"sample_size\": len(results),\n",
    "                \"success_rate\": sum(1 for r in results if r[\"success\"]) / len(results),\n",
    "                \"avg_latency_ms\": np.mean([r[\"generation_time_ms\"] for r in results]),\n",
    "                \"thumbs_up_rate\": sum(1 for r in results if r[\"user_feedback\"] == \"thumbs_up\") / len(results)\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _assign_variant(self, user_id: str) -> str:\n",
    "        \"\"\"Consistent hash-based assignment\"\"\"\n",
    "        hash_val = hash(user_id) % 100\n",
    "        \n",
    "        # 50/50 split (adjust as needed)\n",
    "        threshold = 50\n",
    "        variants_list = list(self.variants.keys())\n",
    "        \n",
    "        return variants_list[0] if hash_val < threshold else variants_list[1]\n",
    "\n",
    "# Example\n",
    "variants = {\n",
    "    \"baseline\": lambda q, s: nl_to_sql(q, s, model=\"gpt-4o-mini\"),\n",
    "    \"optimized\": lambda q, s: nl_to_sql_fewshot(q, s, model=\"gpt-4o\")\n",
    "}\n",
    "\n",
    "ab_test = ABTestFramework(variants)\n",
    "\n",
    "# Run experiment\n",
    "for user_id in users:\n",
    "    result = ab_test.generate(question, schema, user_id)\n",
    "    \n",
    "    # Execute and collect feedback\n",
    "    success = execute_and_validate(result[\"sql\"])\n",
    "    feedback = get_user_feedback(user_id)\n",
    "    \n",
    "    ab_test.log_result(\n",
    "        variant=result[\"variant\"],\n",
    "        success=success,\n",
    "        generation_time_ms=result[\"generation_time_ms\"],\n",
    "        user_feedback=feedback\n",
    "    )\n",
    "\n",
    "# Analyze results\n",
    "analysis = ab_test.analyze()\n",
    "print(\"A/B Test Results:\")\n",
    "for variant, metrics in analysis.items():\n",
    "    print(f\"\\n{variant}:\")\n",
    "    print(f\"  Success Rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Latency: {metrics['avg_latency_ms']:.0f}ms\")\n",
    "    print(f\"  Satisfaction: {metrics['thumbs_up_rate']:.1%}\")\n",
    "```\n",
    "\n",
    "**Production Readiness Checklist:**\n",
    "\n",
    "- \u2705 Evaluation metrics tracked (Execution Accuracy, Component Match)\n",
    "- \u2705 Continuous monitoring (Prometheus + Grafana)\n",
    "- \u2705 A/B testing framework (compare strategies)\n",
    "- \u2705 User feedback loop (thumbs up/down)\n",
    "- \u2705 Error tracking & categorization\n",
    "- \u2705 Cost monitoring & alerts\n",
    "- \u2705 Latency SLOs (p95 < 2s)\n",
    "- \u2705 Quality SLOs (success rate > 90%)\n",
    "- \u2705 Regression testing suite (100+ test cases)\n",
    "- \u2705 Model version tracking (experiments)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def nl_to_sql_simple(question: str) -> str:\n",
    "    prompt = f'''\n",
    "Convierte esta pregunta en SQL:\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "SQL:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "print(nl_to_sql_simple('Muestra las 10 ventas m\u00e1s recientes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6235d4",
   "metadata": {},
   "source": [
    "## 2. Mejorado: con contexto de schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08223e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_context = '''\n",
    "Tablas disponibles:\n",
    "\n",
    "ventas:\n",
    "  - venta_id (int, PK)\n",
    "  - fecha (date)\n",
    "  - cliente_id (int, FK)\n",
    "  - producto_id (int, FK)\n",
    "  - cantidad (int)\n",
    "  - total (decimal)\n",
    "\n",
    "clientes:\n",
    "  - cliente_id (int, PK)\n",
    "  - nombre (varchar)\n",
    "  - email (varchar)\n",
    "  - pais (varchar)\n",
    "\n",
    "productos:\n",
    "  - producto_id (int, PK)\n",
    "  - nombre (varchar)\n",
    "  - categoria (varchar)\n",
    "  - precio (decimal)\n",
    "'''\n",
    "\n",
    "def nl_to_sql(question: str, schema: str = schema_context, dialect='postgresql') -> str:\n",
    "    prompt = f'''\n",
    "Eres un experto en SQL. Genera una consulta {dialect} v\u00e1lida para responder la pregunta.\n",
    "\n",
    "Schema de base de datos:\n",
    "{schema}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Genera SOLO la consulta SQL, sin explicaciones adicionales.\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```sql','').replace('```','')\n",
    "\n",
    "query = nl_to_sql('\u00bfCu\u00e1l es el top 5 de productos m\u00e1s vendidos por categor\u00eda?')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8de7f6",
   "metadata": {},
   "source": [
    "## 3. Validaci\u00f3n y seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f232fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlparse\n",
    "\n",
    "def is_safe_query(sql: str) -> tuple[bool, str]:\n",
    "    \"\"\"Valida que la consulta sea de solo lectura y segura.\"\"\"\n",
    "    sql_lower = sql.lower()\n",
    "    # Bloquear operaciones de escritura\n",
    "    dangerous = ['insert', 'update', 'delete', 'drop', 'create', 'alter', 'truncate', 'exec', 'execute']\n",
    "    for kw in dangerous:\n",
    "        if re.search(rf'\\b{kw}\\b', sql_lower):\n",
    "            return False, f'Operaci\u00f3n bloqueada: {kw}'\n",
    "    \n",
    "    # Solo permitir SELECT\n",
    "    if not sql_lower.strip().startswith('select'):\n",
    "        return False, 'Solo se permiten consultas SELECT'\n",
    "    \n",
    "    return True, 'OK'\n",
    "\n",
    "test_queries = [\n",
    "    'SELECT * FROM ventas LIMIT 10',\n",
    "    'DELETE FROM ventas WHERE fecha < \\'2020-01-01\\'',\n",
    "    'SELECT COUNT(*) FROM clientes'\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    safe, msg = is_safe_query(q)\n",
    "    print(f'{\"\u2705\" if safe else \"\u274c\"} {msg}: {q[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35420c",
   "metadata": {},
   "source": [
    "## 4. Ejecuci\u00f3n segura con SQLite demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b91c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Setup demo DB\n",
    "conn = sqlite3.connect(':memory:')\n",
    "conn.executescript('''\n",
    "CREATE TABLE ventas (venta_id INT, fecha TEXT, cliente_id INT, producto_id INT, cantidad INT, total REAL);\n",
    "INSERT INTO ventas VALUES (1,'2025-10-01',10,101,2,200.0),(2,'2025-10-02',11,102,1,50.0),(3,'2025-10-03',10,101,1,100.0);\n",
    "CREATE TABLE productos (producto_id INT, nombre TEXT, categoria TEXT);\n",
    "INSERT INTO productos VALUES (101,'Laptop','Electronics'),(102,'Mouse','Electronics');\n",
    "''')\n",
    "\n",
    "def execute_nl_query(question: str):\n",
    "    sql = nl_to_sql(question)\n",
    "    print(f'SQL generado: {sql}\\n')\n",
    "    safe, msg = is_safe_query(sql)\n",
    "    if not safe:\n",
    "        return f'\u274c Consulta bloqueada: {msg}'\n",
    "    try:\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return f'Error ejecutando SQL: {e}'\n",
    "\n",
    "result = execute_nl_query('Muestra el total de ventas por producto')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959e1fb",
   "metadata": {},
   "source": [
    "## 5. Few-shot con ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = '''\n",
    "Ejemplos:\n",
    "\n",
    "Q: \u00bfCu\u00e1ntas ventas hubo ayer?\n",
    "SQL: SELECT COUNT(*) FROM ventas WHERE fecha = CURRENT_DATE - INTERVAL '1 day';\n",
    "\n",
    "Q: Top 3 clientes por gasto total\n",
    "SQL: SELECT c.nombre, SUM(v.total) as gasto FROM ventas v JOIN clientes c ON v.cliente_id=c.cliente_id GROUP BY c.nombre ORDER BY gasto DESC LIMIT 3;\n",
    "\n",
    "Q: Productos sin ventas en octubre\n",
    "SQL: SELECT p.nombre FROM productos p LEFT JOIN ventas v ON p.producto_id=v.producto_id AND v.fecha >= '2025-10-01' AND v.fecha < '2025-11-01' WHERE v.venta_id IS NULL;\n",
    "'''\n",
    "\n",
    "def nl_to_sql_fewshot(question: str) -> str:\n",
    "    prompt = f'''\n",
    "Genera SQL para responder preguntas sobre ventas.\n",
    "\n",
    "{schema_context}\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Ahora genera SQL para:\n",
    "Q: {question}\n",
    "SQL:\n",
    "'''\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().replace('```sql','').replace('```','')\n",
    "\n",
    "query = nl_to_sql_fewshot('Ventas por categor\u00eda en los \u00faltimos 7 d\u00edas')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18767f23",
   "metadata": {},
   "source": [
    "## 6. Optimizaci\u00f3n y cach\u00e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5773ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_nl_to_sql(question: str) -> str:\n",
    "    return nl_to_sql(question)\n",
    "\n",
    "# Llamadas repetidas usan cach\u00e9\n",
    "q = '\u00bfCu\u00e1l es el total de ventas del mes actual?'\n",
    "print('Primera llamada:')\n",
    "print(cached_nl_to_sql(q))\n",
    "print('\\nSegunda llamada (desde cach\u00e9):')\n",
    "print(cached_nl_to_sql(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd71a3",
   "metadata": {},
   "source": [
    "## 7. Buenas pr\u00e1cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567494ca",
   "metadata": {},
   "source": [
    "- **Schema completo**: incluye tipos, PKs, FKs, \u00edndices.\n",
    "- **Ejemplos representativos**: few-shot con casos edge.\n",
    "- **Validaci\u00f3n estricta**: whitelist de operaciones permitidas.\n",
    "- **Timeout y l\u00edmites**: evita consultas costosas.\n",
    "- **Logging**: registra pregunta, SQL generado, resultado.\n",
    "- **Feedback loop**: almacena correcciones humanas para fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b842d21",
   "metadata": {},
   "source": [
    "## 8. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef2b94",
   "metadata": {},
   "source": [
    "1. Agrega soporte multi-idioma (ingl\u00e9s/espa\u00f1ol).\n",
    "2. Implementa un sistema de aprobaci\u00f3n humana para SQL complejos.\n",
    "3. Crea un dashboard Streamlit donde usuarios escriban preguntas y vean resultados.\n",
    "4. A\u00f1ade explicaci\u00f3n del SQL generado en lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs \u2192](03_generacion_codigo_etl.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel GenAI:**\n",
    "- [\ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)\n",
    "- [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "- [\ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural](02_generacion_sql_nl2sql.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs](03_generacion_codigo_etl.ipynb)\n",
    "- [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "- [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb)\n",
    "- [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n](06_agentes_automatizacion.ipynb)\n",
    "- [\u2705 Validaci\u00f3n de Datos con LLMs](07_calidad_validacion_llm.ipynb)\n",
    "- [\ud83c\udfb2 Generaci\u00f3n de Datos Sint\u00e9ticos con LLMs](08_sintesis_aumento_datos.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
