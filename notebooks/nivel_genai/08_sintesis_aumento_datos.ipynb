{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cc95f4",
   "metadata": {},
   "source": [
    "# üé≤ Generaci√≥n de Datos Sint√©ticos con LLMs\n",
    "\n",
    "Objetivo: usar IA generativa para crear datos realistas de prueba, aumentar datasets, anonimizar datos sensibles, y generar casos edge para testing.\n",
    "\n",
    "- Duraci√≥n: 90 min\n",
    "- Dificultad: Media\n",
    "- Stack: OpenAI, Faker, SDV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ca884",
   "metadata": {},
   "source": [
    "### üé≤ **Datos Sint√©ticos con LLMs: De Testing a Producci√≥n**\n",
    "\n",
    "**¬øPor qu√© Generar Datos Sint√©ticos?**\n",
    "\n",
    "```\n",
    "Problema Tradicional                  Soluci√≥n con LLMs\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚ùå Datos reales sensibles (PII)      ‚úÖ Anonimizaci√≥n inteligente\n",
    "‚ùå Datasets peque√±os para ML         ‚úÖ Data augmentation realista\n",
    "‚ùå Casos edge dif√≠ciles de crear     ‚úÖ Generaci√≥n guiada por prompts\n",
    "‚ùå Testing con datos de producci√≥n   ‚úÖ Synthetic data on-demand\n",
    "‚ùå Cold start (nuevos productos)     ‚úÖ Datos de arranque realistas\n",
    "```\n",
    "\n",
    "**Evoluci√≥n de T√©cnicas:**\n",
    "\n",
    "| Era | T√©cnica | Limitaciones | Ejemplo |\n",
    "|-----|---------|--------------|---------|\n",
    "| **2010s** | Random data | No realista, no correlacionado | `random.randint(1, 100)` |\n",
    "| **2015** | Faker/Mimesis | Patrones predefinidos, sin contexto | `faker.name()` ‚Üí \"John Doe\" |\n",
    "| **2018** | GANs/VAEs | Requiere entrenamiento, dif√≠cil interpretar | CTGAN para tabular data |\n",
    "| **2020** | SDV (Synthetic Data Vault) | Preserva correlaciones, lento con schemas complejos | Gaussian Copula |\n",
    "| **2023+** | **LLMs (GPT-4, Claude)** | **Context-aware, flexible, explainable** | Prompt ‚Üí datos coherentes |\n",
    "\n",
    "**Arquitectura de Generaci√≥n LLM:**\n",
    "\n",
    "```python\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   SYNTHETIC DATA PIPELINE                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£ SCHEMA DEFINITION                                        ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ\n",
    "‚îÇ     ‚îÇ JSON Schema / Pydantic Model    ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ + Business Rules                ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ + Data Relationships            ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îÇ                    ‚Üì                                          ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ PROMPT ENGINEERING                                       ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Schema ‚Üí Structured Prompt      ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ + Examples (few-shot)           ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ + Constraints (range, format)   ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ + Context (business domain)     ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îÇ                    ‚Üì                                          ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ LLM GENERATION                                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ\n",
    "‚îÇ     ‚îÇ GPT-4: High quality, expensive  ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ GPT-3.5: Fast, cheap, good      ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Batch API: 50% cheaper, 24h     ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Temperature: 0.7-0.9 diversity  ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îÇ                    ‚Üì                                          ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£ VALIDATION & POST-PROCESSING                             ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Pydantic validation             ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Great Expectations checks       ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Statistical distribution match  ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Uniqueness constraints          ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îÇ                    ‚Üì                                          ‚îÇ\n",
    "‚îÇ  5Ô∏è‚É£ OUTPUT STORAGE                                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ\n",
    "‚îÇ     ‚îÇ CSV/Parquet (batch)             ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ Database (structured)           ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îÇ S3 (data lake)                  ‚îÇ                     ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Generador de Datos Sint√©ticos (Producci√≥n):**\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from openai import OpenAI\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "class CustomerSchema(BaseModel):\n",
    "    \"\"\"Schema con validaci√≥n autom√°tica\"\"\"\n",
    "    customer_id: int = Field(..., gt=0, lt=1000000)\n",
    "    name: str = Field(..., min_length=2, max_length=100)\n",
    "    email: str = Field(..., regex=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "    age: int = Field(..., ge=18, le=100)\n",
    "    city: str\n",
    "    country: str = Field(default='USA')\n",
    "    signup_date: str = Field(..., regex=r'^\\d{4}-\\d{2}-\\d{2}$')\n",
    "    lifetime_value: float = Field(..., ge=0, le=100000)\n",
    "    \n",
    "    @validator('email')\n",
    "    def validate_email_domain(cls, v):\n",
    "        \"\"\"Rechaza emails de prueba\"\"\"\n",
    "        if any(d in v for d in ['test.com', 'example.com', 'fake.com']):\n",
    "            raise ValueError('Email domain not allowed')\n",
    "        return v\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    \"\"\"Generador robusto de datos sint√©ticos\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'gpt-4', temperature: float = 0.8):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        schema: BaseModel, \n",
    "        count: int,\n",
    "        context: Optional[str] = None,\n",
    "        few_shot_examples: Optional[List[dict]] = None\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Genera datos sint√©ticos con validaci√≥n.\n",
    "        \n",
    "        Args:\n",
    "            schema: Pydantic model que define estructura\n",
    "            count: N√∫mero de registros a generar\n",
    "            context: Contexto de negocio para realismo\n",
    "            few_shot_examples: Ejemplos para guiar generaci√≥n\n",
    "        \n",
    "        Returns:\n",
    "            Lista de diccionarios validados\n",
    "        \"\"\"\n",
    "        # Construir prompt estructurado\n",
    "        prompt_parts = [\n",
    "            f\"Generate {count} realistic synthetic records following this schema:\\n\",\n",
    "            self._schema_to_prompt(schema),\n",
    "        ]\n",
    "        \n",
    "        if context:\n",
    "            prompt_parts.append(f\"\\nBusiness context: {context}\")\n",
    "        \n",
    "        if few_shot_examples:\n",
    "            prompt_parts.append(\"\\nExamples of desired output:\")\n",
    "            prompt_parts.append(json.dumps(few_shot_examples, indent=2))\n",
    "        \n",
    "        prompt_parts.append(f\"\\n\\nGenerate exactly {count} records as a JSON array.\")\n",
    "        prompt_parts.append(\"Ensure diversity, realism, and consistency.\")\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        \n",
    "        # Llamada LLM\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data generation expert. Always return valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=self.temperature,\n",
    "            response_format={\"type\": \"json_object\"}  # Garantiza JSON v√°lido\n",
    "        )\n",
    "        \n",
    "        # Parse y validaci√≥n\n",
    "        raw_data = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Extraer array (maneja respuestas con wrapper)\n",
    "        if isinstance(raw_data, dict) and 'records' in raw_data:\n",
    "            raw_data = raw_data['records']\n",
    "        elif isinstance(raw_data, dict) and 'data' in raw_data:\n",
    "            raw_data = raw_data['data']\n",
    "        \n",
    "        # Validar cada registro con Pydantic\n",
    "        validated = []\n",
    "        errors = []\n",
    "        \n",
    "        for i, record in enumerate(raw_data):\n",
    "            try:\n",
    "                validated_record = schema(**record)\n",
    "                validated.append(validated_record.dict())\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Record {i}: {str(e)}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"‚ö†Ô∏è  Validation errors: {len(errors)}/{len(raw_data)}\")\n",
    "            for error in errors[:3]:  # Mostrar primeros 3\n",
    "                print(f\"   {error}\")\n",
    "        \n",
    "        return validated\n",
    "    \n",
    "    def _schema_to_prompt(self, schema: BaseModel) -> str:\n",
    "        \"\"\"Convierte Pydantic schema a descripci√≥n legible\"\"\"\n",
    "        lines = [\"Schema:\"]\n",
    "        for field_name, field in schema.__fields__.items():\n",
    "            field_type = field.outer_type_.__name__\n",
    "            constraints = []\n",
    "            \n",
    "            if hasattr(field, 'field_info'):\n",
    "                if field.field_info.ge is not None:\n",
    "                    constraints.append(f\">= {field.field_info.ge}\")\n",
    "                if field.field_info.le is not None:\n",
    "                    constraints.append(f\"<= {field.field_info.le}\")\n",
    "                if field.field_info.min_length:\n",
    "                    constraints.append(f\"min_length={field.field_info.min_length}\")\n",
    "                if field.field_info.max_length:\n",
    "                    constraints.append(f\"max_length={field.field_info.max_length}\")\n",
    "            \n",
    "            constraint_str = f\" ({', '.join(constraints)})\" if constraints else \"\"\n",
    "            lines.append(f\"  - {field_name}: {field_type}{constraint_str}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Uso\n",
    "generator = SyntheticDataGenerator(model='gpt-4', temperature=0.8)\n",
    "\n",
    "customers = generator.generate(\n",
    "    schema=CustomerSchema,\n",
    "    count=100,\n",
    "    context=\"B2B SaaS company targeting enterprise customers in tech industry\",\n",
    "    few_shot_examples=[\n",
    "        {\n",
    "            \"customer_id\": 1001,\n",
    "            \"name\": \"Acme Corp\",\n",
    "            \"email\": \"contact@acme-corp.com\",\n",
    "            \"age\": 45,  # Company age\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"country\": \"USA\",\n",
    "            \"signup_date\": \"2024-03-15\",\n",
    "            \"lifetime_value\": 15000.00\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(customers)} valid records\")\n",
    "```\n",
    "\n",
    "**Ventajas vs T√©cnicas Tradicionales:**\n",
    "\n",
    "| Aspecto | Faker | SDV (Copula) | LLMs (GPT-4) |\n",
    "|---------|-------|--------------|--------------|\n",
    "| **Realismo sem√°ntico** | ‚ùå Bajo (nombres random) | ‚ö†Ô∏è Medio (preserva correlaciones) | ‚úÖ Alto (contexto de negocio) |\n",
    "| **Coherencia cross-field** | ‚ùå No (email no match name) | ‚ö†Ô∏è Limitado | ‚úÖ S√≠ (email coherente con empresa) |\n",
    "| **Flexibilidad** | ‚ö†Ô∏è Providers predefinidos | ‚ùå Requiere training | ‚úÖ Prompt engineering |\n",
    "| **Casos edge** | ‚ùå Dif√≠cil especificar | ‚ùå No garantizado | ‚úÖ Expl√≠cito en prompt |\n",
    "| **Velocidad** | ‚úÖ Muy r√°pido (1M/min) | ‚ö†Ô∏è Medio (1K/min) | ‚ùå Lento (100/min) |\n",
    "| **Costo** | ‚úÖ Gratis | ‚úÖ Gratis | ‚ùå $0.03/1K registros (GPT-4) |\n",
    "| **Setup** | ‚úÖ Trivial | ‚ùå Complejo (fit model) | ‚úÖ Simple (API key) |\n",
    "\n",
    "**Cu√°ndo Usar Cada T√©cnica:**\n",
    "\n",
    "```python\n",
    "# 1. FAKER: Volumen alto, estructura simple\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "[fake.name() for _ in range(10000)]  # R√°pido y barato\n",
    "\n",
    "# 2. SDV: Preservar distribuciones estad√≠sticas complejas\n",
    "from sdv.tabular import GaussianCopula\n",
    "model = GaussianCopula()\n",
    "model.fit(real_data)  # Aprende correlaciones\n",
    "synthetic = model.sample(1000)  # Replica distribuciones\n",
    "\n",
    "# 3. LLMs: Realismo sem√°ntico, coherencia contextual\n",
    "generator.generate(\n",
    "    schema=ComplexBusinessSchema,\n",
    "    context=\"Healthcare provider with HIPAA compliance\",\n",
    "    count=100\n",
    ")\n",
    "```\n",
    "\n",
    "**Estrategia H√≠brida (Best Practice):**\n",
    "\n",
    "```python\n",
    "def generate_hybrid_dataset(count: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combina velocidad de Faker con inteligencia de LLMs.\n",
    "    \n",
    "    Faker: 95% de campos (estructura, IDs, timestamps)\n",
    "    LLM: 5% de campos (texto libre, relaciones complejas)\n",
    "    \"\"\"\n",
    "    # Paso 1: Estructura base con Faker (r√°pido)\n",
    "    base_data = []\n",
    "    fake = Faker()\n",
    "    \n",
    "    for i in range(count):\n",
    "        base_data.append({\n",
    "            'customer_id': i + 1000,\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'created_at': fake.date_time_between(start_date='-2y'),\n",
    "            'city': fake.city(),\n",
    "            'country': fake.country()\n",
    "        })\n",
    "    \n",
    "    # Paso 2: Batch enriquecimiento con LLM (econ√≥mico)\n",
    "    # Procesar en lotes de 50 para eficiencia\n",
    "    batch_size = 50\n",
    "    \n",
    "    for i in range(0, len(base_data), batch_size):\n",
    "        batch = base_data[i:i+batch_size]\n",
    "        \n",
    "        # Single LLM call para batch completo\n",
    "        prompt = f\"\"\"\n",
    "        Enrich these {len(batch)} customer records with:\n",
    "        - company_name (realistic, matching city/country)\n",
    "        - industry (choose from: tech, finance, healthcare, retail, manufacturing)\n",
    "        - company_size (employees: 10-50, 51-200, 201-1000, 1001+)\n",
    "        \n",
    "        Input records:\n",
    "        {json.dumps(batch, indent=2, default=str)}\n",
    "        \n",
    "        Return JSON array with original fields + new fields.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',  # M√°s barato para este task\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        enriched_batch = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Merge resultados\n",
    "        for j, enriched in enumerate(enriched_batch):\n",
    "            base_data[i+j].update(enriched)\n",
    "    \n",
    "    return pd.DataFrame(base_data)\n",
    "\n",
    "# Resultado: 10K registros en ~2 min, costo ~$0.50\n",
    "# vs 100% LLM: ~20 min, costo ~$5\n",
    "# vs 100% Faker: ~10s, pero sin coherencia sem√°ntica\n",
    "```\n",
    "\n",
    "**Cost Optimization (Producci√≥n):**\n",
    "\n",
    "```python\n",
    "# ‚ùå Costoso: 1 llamada por registro\n",
    "for i in range(1000):\n",
    "    generate_one_record()  # $30 total\n",
    "\n",
    "# ‚úÖ Eficiente: Batch de 50 registros por llamada\n",
    "for batch in chunked(range(1000), 50):\n",
    "    generate_batch(50)  # $1.50 total (20x m√°s barato)\n",
    "\n",
    "# ‚úÖ M√°s eficiente: Batch API (50% descuento, 24h latencia)\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_file = client.files.create(\n",
    "    file=open('batch_requests.jsonl', 'rb'),\n",
    "    purpose='batch'\n",
    ")\n",
    "\n",
    "batch_job = client.batches.create(\n",
    "    input_file_id=batch_file.id,\n",
    "    endpoint='/v1/chat/completions',\n",
    "    completion_window='24h'\n",
    ")\n",
    "# Mismo resultado: $0.75 (40x m√°s barato que individual)\n",
    "```\n",
    "\n",
    "**M√©tricas de Calidad:**\n",
    "\n",
    "```python\n",
    "def evaluate_synthetic_quality(real_df, synthetic_df):\n",
    "    \"\"\"Compara datos sint√©ticos vs reales\"\"\"\n",
    "    \n",
    "    # 1. Distribuciones estad√≠sticas (KS test)\n",
    "    from scipy.stats import ks_2samp\n",
    "    \n",
    "    for col in real_df.select_dtypes(include=[np.number]).columns:\n",
    "        statistic, pvalue = ks_2samp(real_df[col], synthetic_df[col])\n",
    "        print(f\"{col}: KS statistic={statistic:.3f}, p-value={pvalue:.3f}\")\n",
    "        # p-value > 0.05 ‚Üí distribuciones similares ‚úÖ\n",
    "    \n",
    "    # 2. Correlaciones preservadas\n",
    "    real_corr = real_df.corr()\n",
    "    synth_corr = synthetic_df.corr()\n",
    "    corr_diff = np.abs(real_corr - synth_corr).mean().mean()\n",
    "    print(f\"Correlation MAE: {corr_diff:.3f}\")  # Menor es mejor\n",
    "    \n",
    "    # 3. Privacy: no memorizaci√≥n\n",
    "    # Synthetic data NO debe contener registros id√©nticos a reales\n",
    "    merged = real_df.merge(synthetic_df, how='inner')\n",
    "    print(f\"Identical records: {len(merged)} (should be 0)\")\n",
    "    \n",
    "    # 4. Utility: ML performance\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    # Train on synthetic, test on real\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(synthetic_df.drop('target', axis=1), synthetic_df['target'])\n",
    "    pred = model.predict(real_df.drop('target', axis=1))\n",
    "    f1 = f1_score(real_df['target'], pred)\n",
    "    print(f\"F1 score (synth‚Üíreal): {f1:.3f}\")  # Close to 1.0 ‚úÖ\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f407f",
   "metadata": {},
   "source": [
    "## 1. Generaci√≥n simple de registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ceb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_synthetic_records(schema: dict, count: int) -> list:\n",
    "    \"\"\"Genera registros sint√©ticos seg√∫n esquema.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera {count} registros de datos realistas en formato JSON seg√∫n este esquema:\n",
    "\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Devuelve un array JSON con {count} objetos.\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# Esquema de clientes\n",
    "customer_schema = {\n",
    "    'customer_id': 'int',\n",
    "    'name': 'string (nombre completo realista)',\n",
    "    'email': 'string (email v√°lido)',\n",
    "    'age': 'int (18-75)',\n",
    "    'city': 'string (ciudad de USA)',\n",
    "    'signup_date': 'date (YYYY-MM-DD, √∫ltimos 2 a√±os)'\n",
    "}\n",
    "\n",
    "customers = generate_synthetic_records(customer_schema, 5)\n",
    "print(json.dumps(customers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2a228",
   "metadata": {},
   "source": [
    "### üîÑ **Data Augmentation: De Pocos a Muchos Datos**\n",
    "\n",
    "**Problema de Pocos Datos en ML:**\n",
    "\n",
    "```\n",
    "Escenario Com√∫n                       Impacto en Modelos\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Startup con 100 clientes              ‚Üí Overfitting severo\n",
    "Nueva categor√≠a producto (10 ventas)  ‚Üí No puede entrenar modelo\n",
    "Casos raros (fraude 0.1%)            ‚Üí Clase desbalanceada\n",
    "Migraci√≥n de sistema (hist√≥rico lost) ‚Üí Cold start\n",
    "```\n",
    "\n",
    "**T√©cnicas de Augmentation por Tipo de Dato:**\n",
    "\n",
    "```python\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              DATA AUGMENTATION STRATEGIES                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  üìä TABULAR DATA                                               ‚îÇ\n",
    "‚îÇ     Traditional:                                                ‚îÇ\n",
    "‚îÇ       ‚Ä¢ SMOTE (Synthetic Minority Over-sampling)               ‚îÇ\n",
    "‚îÇ       ‚Ä¢ ADASYN (Adaptive Synthetic Sampling)                   ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Random Over-sampling with noise                        ‚îÇ\n",
    "‚îÇ     LLM-based:                                                  ‚îÇ\n",
    "‚îÇ       ‚úÖ Context-aware variations                              ‚îÇ\n",
    "‚îÇ       ‚úÖ Relationship preservation                             ‚îÇ\n",
    "‚îÇ       ‚úÖ Domain knowledge injection                            ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  üìù TEXT DATA                                                  ‚îÇ\n",
    "‚îÇ     Traditional:                                                ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Synonym replacement (WordNet)                          ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Back-translation (EN‚ÜíES‚ÜíEN)                            ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Random insertion/deletion                              ‚îÇ\n",
    "‚îÇ     LLM-based:                                                  ‚îÇ\n",
    "‚îÇ       ‚úÖ Paraphrase generation (maintains meaning)             ‚îÇ\n",
    "‚îÇ       ‚úÖ Style transfer (formal ‚Üî casual)                      ‚îÇ\n",
    "‚îÇ       ‚úÖ Entity substitution (coherent)                        ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  üñºÔ∏è IMAGE DATA (menos com√∫n en data engineering)              ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Rotation, flip, crop                                     ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Color jittering                                          ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Mixup, CutMix                                            ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  ‚è±Ô∏è TIME SERIES                                                ‚îÇ\n",
    "‚îÇ     Traditional:                                                ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Jittering (add noise)                                  ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Scaling, magnitude warping                             ‚îÇ\n",
    "‚îÇ       ‚Ä¢ Time warping                                           ‚îÇ\n",
    "‚îÇ     LLM-based:                                                  ‚îÇ\n",
    "‚îÇ       ‚úÖ Seasonal pattern generation                           ‚îÇ\n",
    "‚îÇ       ‚úÖ Anomaly injection (realistic)                         ‚îÇ\n",
    "‚îÇ       ‚úÖ Multi-variate correlation                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Tabular Data Augmentation con LLMs:**\n",
    "\n",
    "```python\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "class LLMDataAugmenter:\n",
    "    \"\"\"Aumenta datasets tabulares con LLMs manteniendo coherencia\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'gpt-4'):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "    \n",
    "    def augment_records(\n",
    "        self,\n",
    "        original_records: List[Dict],\n",
    "        target_count: int,\n",
    "        augmentation_strategy: str = 'variation'\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Genera registros sint√©ticos basados en originales.\n",
    "        \n",
    "        Strategies:\n",
    "            - 'variation': Variaciones sutiles (mismo patr√≥n)\n",
    "            - 'interpolation': Interpolaci√≥n entre registros\n",
    "            - 'extrapolation': Extrapolaci√≥n (casos m√°s extremos)\n",
    "        \"\"\"\n",
    "        \n",
    "        if augmentation_strategy == 'variation':\n",
    "            return self._augment_by_variation(original_records, target_count)\n",
    "        elif augmentation_strategy == 'interpolation':\n",
    "            return self._augment_by_interpolation(original_records, target_count)\n",
    "        elif augmentation_strategy == 'extrapolation':\n",
    "            return self._augment_by_extrapolation(original_records, target_count)\n",
    "    \n",
    "    def _augment_by_variation(self, records: List[Dict], count: int) -> List[Dict]:\n",
    "        \"\"\"Genera variaciones realistas de registros existentes\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You have {len(records)} original data records. Generate {count} new synthetic records\n",
    "        that are VARIATIONS of the originals.\n",
    "        \n",
    "        Requirements:\n",
    "        - Maintain the same schema/fields\n",
    "        - Preserve statistical distributions (ranges, types)\n",
    "        - Ensure diversity (don't just copy)\n",
    "        - Keep business logic consistent (e.g., if price depends on category)\n",
    "        - Introduce realistic variations (similar but not identical)\n",
    "        \n",
    "        Original records:\n",
    "        {json.dumps(records, indent=2)}\n",
    "        \n",
    "        Generate {count} new records as JSON array.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data augmentation expert.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.8,  # Alta para diversidad\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result.get('records', result.get('data', []))\n",
    "    \n",
    "    def _augment_by_interpolation(self, records: List[Dict], count: int) -> List[Dict]:\n",
    "        \"\"\"Crea registros 'entre' dos existentes\"\"\"\n",
    "        \n",
    "        # Seleccionar pares de registros para interpolar\n",
    "        import random\n",
    "        pairs = [(records[i], records[j]) for i in range(len(records)) \n",
    "                 for j in range(i+1, len(records))]\n",
    "        selected_pairs = random.sample(pairs, min(count, len(pairs)))\n",
    "        \n",
    "        augmented = []\n",
    "        for record1, record2 in selected_pairs:\n",
    "            prompt = f\"\"\"\n",
    "            Create a synthetic record that is an \"interpolation\" between these two:\n",
    "            \n",
    "            Record A: {json.dumps(record1)}\n",
    "            Record B: {json.dumps(record2)}\n",
    "            \n",
    "            Generate a record that represents a middle point:\n",
    "            - Numeric fields: average or intermediate value\n",
    "            - Categorical fields: pick one or create logical intermediate\n",
    "            - Text fields: combine or blend characteristics\n",
    "            \n",
    "            Return single JSON object.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model='gpt-3.5-turbo',  # M√°s barato para task simple\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                temperature=0.5\n",
    "            )\n",
    "            \n",
    "            augmented.append(json.loads(response.choices[0].message.content))\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def _augment_by_extrapolation(self, records: List[Dict], count: int) -> List[Dict]:\n",
    "        \"\"\"Genera casos m√°s extremos/raros basados en patrones\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on these {len(records)} examples, generate {count} EXTRAPOLATED records.\n",
    "        \n",
    "        Extrapolation means:\n",
    "        - Extend beyond the observed range (but stay realistic)\n",
    "        - Create edge cases (high values, low values, unusual combinations)\n",
    "        - Maintain domain validity (no impossible values)\n",
    "        \n",
    "        Original records:\n",
    "        {json.dumps(records, indent=2)}\n",
    "        \n",
    "        Generate {count} extrapolated records (JSON array).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result.get('records', result.get('data', []))\n",
    "\n",
    "# Ejemplo: Augmentar dataset peque√±o\n",
    "original_customers = [\n",
    "    {\"name\": \"Tech Startup Inc\", \"industry\": \"SaaS\", \"employees\": 25, \"revenue\": 500000},\n",
    "    {\"name\": \"Finance Corp\", \"industry\": \"Banking\", \"employees\": 150, \"revenue\": 5000000},\n",
    "    {\"name\": \"Retail Shop\", \"industry\": \"E-commerce\", \"employees\": 10, \"revenue\": 200000}\n",
    "]\n",
    "\n",
    "augmenter = LLMDataAugmenter(model='gpt-4')\n",
    "\n",
    "# Estrategia 1: Variaciones (m√°s com√∫n)\n",
    "variations = augmenter.augment_records(\n",
    "    original_customers, \n",
    "    target_count=10,\n",
    "    augmentation_strategy='variation'\n",
    ")\n",
    "print(f\"‚úÖ Generated {len(variations)} variations\")\n",
    "\n",
    "# Estrategia 2: Interpolaciones (crear registros intermedios)\n",
    "interpolations = augmenter.augment_records(\n",
    "    original_customers,\n",
    "    target_count=5,\n",
    "    augmentation_strategy='interpolation'\n",
    ")\n",
    "print(f\"‚úÖ Generated {len(interpolations)} interpolations\")\n",
    "\n",
    "# Estrategia 3: Extrapolaciones (casos extremos)\n",
    "extrapolations = augmenter.augment_records(\n",
    "    original_customers,\n",
    "    target_count=5,\n",
    "    augmentation_strategy='extrapolation'\n",
    ")\n",
    "print(f\"‚úÖ Generated {len(extrapolations)} extrapolations\")\n",
    "\n",
    "# Combinar todo\n",
    "df_original = pd.DataFrame(original_customers)\n",
    "df_augmented = pd.DataFrame(variations + interpolations + extrapolations)\n",
    "\n",
    "print(f\"\\nüìä Dataset growth: {len(df_original)} ‚Üí {len(df_augmented)} records\")\n",
    "```\n",
    "\n",
    "**Text Augmentation (Paraphrasing):**\n",
    "\n",
    "```python\n",
    "def augment_text_data(texts: List[str], multiplier: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Augmenta dataset de texto con par√°frasis.\n",
    "    √ötil para NLP tasks (sentiment analysis, classification).\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    for text in texts:\n",
    "        prompt = f\"\"\"\n",
    "        Generate {multiplier} paraphrases of this text.\n",
    "        \n",
    "        Requirements:\n",
    "        - Maintain the same meaning\n",
    "        - Use different wording/structure\n",
    "        - Keep the same sentiment/tone\n",
    "        - Realistic variations (not just synonym swap)\n",
    "        \n",
    "        Original text: \"{text}\"\n",
    "        \n",
    "        Return JSON: {{\"paraphrases\": [\"version1\", \"version2\", ...]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        augmented.extend(result['paraphrases'])\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Ejemplo: Reviews de productos\n",
    "original_reviews = [\n",
    "    \"This product is amazing! Highly recommend.\",\n",
    "    \"Terrible quality, waste of money.\"\n",
    "]\n",
    "\n",
    "augmented_reviews = augment_text_data(original_reviews, multiplier=5)\n",
    "# Original: 2 reviews\n",
    "# Augmented: 10 reviews (5 variations each)\n",
    "```\n",
    "\n",
    "**Time Series Augmentation:**\n",
    "\n",
    "```python\n",
    "def augment_timeseries(\n",
    "    series: pd.DataFrame,\n",
    "    num_synthetic_series: int,\n",
    "    preserve_pattern: str\n",
    ") -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Genera series temporales sint√©ticas con patrones similares.\n",
    "    \n",
    "    Args:\n",
    "        series: DataFrame con columnas [date, value]\n",
    "        num_synthetic_series: Cu√°ntas series generar\n",
    "        preserve_pattern: 'trend', 'seasonality', 'both'\n",
    "    \"\"\"\n",
    "    \n",
    "    # An√°lisis estad√≠stico de serie original\n",
    "    stats = {\n",
    "        'mean': series['value'].mean(),\n",
    "        'std': series['value'].std(),\n",
    "        'min': series['value'].min(),\n",
    "        'max': series['value'].max(),\n",
    "        'trend': 'increasing' if series['value'].iloc[-1] > series['value'].iloc[0] else 'decreasing'\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Generate {num_synthetic_series} synthetic time series similar to this one.\n",
    "    \n",
    "    Original series statistics:\n",
    "    - Mean: {stats['mean']:.2f}\n",
    "    - Std Dev: {stats['std']:.2f}\n",
    "    - Range: [{stats['min']:.2f}, {stats['max']:.2f}]\n",
    "    - Trend: {stats['trend']}\n",
    "    - Length: {len(series)} data points\n",
    "    \n",
    "    Preserve pattern: {preserve_pattern}\n",
    "    \n",
    "    Sample of original series:\n",
    "    {series.head(10).to_dict('records')}\n",
    "    ...\n",
    "    {series.tail(10).to_dict('records')}\n",
    "    \n",
    "    Generate {num_synthetic_series} series with similar characteristics.\n",
    "    Each series should have {len(series)} points.\n",
    "    \n",
    "    Return JSON: {{\"series\": [[values1], [values2], ...]}}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # Convertir a DataFrames\n",
    "    synthetic_series = []\n",
    "    for values in result['series']:\n",
    "        df = pd.DataFrame({\n",
    "            'date': pd.date_range(start='2024-01-01', periods=len(values), freq='D'),\n",
    "            'value': values\n",
    "        })\n",
    "        synthetic_series.append(df)\n",
    "    \n",
    "    return synthetic_series\n",
    "\n",
    "# Ejemplo: Ventas diarias (solo 30 d√≠as reales)\n",
    "real_sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=30, freq='D'),\n",
    "    'value': [1000 + 50*i + np.random.randn()*100 for i in range(30)]\n",
    "})\n",
    "\n",
    "# Generar 10 series sint√©ticas similares\n",
    "synthetic_sales = augment_timeseries(real_sales, num_synthetic_series=10, preserve_pattern='both')\n",
    "\n",
    "# Ahora tenemos 11 series (1 real + 10 sint√©ticas) para entrenar modelo de forecasting\n",
    "```\n",
    "\n",
    "**Class Balancing (Minority Class Augmentation):**\n",
    "\n",
    "```python\n",
    "def balance_classes_with_llm(df: pd.DataFrame, target_col: str, minority_class: str):\n",
    "    \"\"\"\n",
    "    Aumenta clase minoritaria para balancear dataset.\n",
    "    \n",
    "    √ötil para:\n",
    "    - Fraud detection (fraude es <1% de transacciones)\n",
    "    - Churn prediction (churn rate ~5%)\n",
    "    - Anomaly detection (anomal√≠as raras)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular desbalance\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    majority_count = class_counts.max()\n",
    "    minority_count = class_counts[minority_class]\n",
    "    \n",
    "    needed = majority_count - minority_count\n",
    "    \n",
    "    print(f\"üìä Class distribution:\")\n",
    "    print(class_counts)\n",
    "    print(f\"\\nüéØ Need to generate {needed} {minority_class} samples\")\n",
    "    \n",
    "    # Obtener ejemplos de clase minoritaria\n",
    "    minority_samples = df[df[target_col] == minority_class].to_dict('records')\n",
    "    \n",
    "    # Generar sint√©ticos\n",
    "    prompt = f\"\"\"\n",
    "    Generate {needed} synthetic records for the minority class: {minority_class}\n",
    "    \n",
    "    These are examples of {minority_class} records:\n",
    "    {json.dumps(minority_samples[:10], indent=2, default=str)}  # Max 10 ejemplos\n",
    "    \n",
    "    Generate {needed} new similar records that:\n",
    "    - Maintain the characteristics of {minority_class} class\n",
    "    - Are diverse (not just copies)\n",
    "    - Are realistic and coherent\n",
    "    \n",
    "    Return JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    synthetic_records = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # Asegurar que tienen el target correcto\n",
    "    for record in synthetic_records:\n",
    "        record[target_col] = minority_class\n",
    "    \n",
    "    # Combinar con dataset original\n",
    "    df_synthetic = pd.DataFrame(synthetic_records)\n",
    "    df_balanced = pd.concat([df, df_synthetic], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Balanced dataset:\")\n",
    "    print(df_balanced[target_col].value_counts())\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Ejemplo: Dataset de transacciones (99% leg√≠timas, 1% fraude)\n",
    "transactions = pd.DataFrame({\n",
    "    'amount': np.random.lognormal(5, 1, 1000),\n",
    "    'merchant': np.random.choice(['Amazon', 'Walmart', 'Target'], 1000),\n",
    "    'is_fraud': ['No'] * 990 + ['Yes'] * 10  # Desbalanceado!\n",
    "})\n",
    "\n",
    "# Balancear generando m√°s casos de fraude\n",
    "balanced_transactions = balance_classes_with_llm(transactions, 'is_fraud', 'Yes')\n",
    "# Ahora: 990 leg√≠timas, 980 fraude sint√©ticas ‚Üí ~50/50 balance\n",
    "```\n",
    "\n",
    "**Validation de Augmented Data:**\n",
    "\n",
    "```python\n",
    "def validate_augmentation_quality(original: pd.DataFrame, augmented: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Verifica que datos augmentados son v√°lidos y √∫tiles.\n",
    "    \n",
    "    Checks:\n",
    "    1. Schema consistency\n",
    "    2. Statistical similarity\n",
    "    3. No memorization (no duplicates exactos)\n",
    "    4. Diversity (no todos iguales)\n",
    "    5. ML utility (mejora performance)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç AUGMENTATION QUALITY CHECKS\\n\")\n",
    "    \n",
    "    # 1. Schema consistency\n",
    "    assert list(original.columns) == list(augmented.columns), \"Schema mismatch!\"\n",
    "    print(\"‚úÖ Schema consistent\")\n",
    "    \n",
    "    # 2. Statistical similarity (numeric columns)\n",
    "    for col in original.select_dtypes(include=[np.number]).columns:\n",
    "        orig_mean = original[col].mean()\n",
    "        aug_mean = augmented[col].mean()\n",
    "        diff_pct = abs(orig_mean - aug_mean) / orig_mean * 100\n",
    "        \n",
    "        print(f\"üìä {col}:\")\n",
    "        print(f\"   Original mean: {orig_mean:.2f}\")\n",
    "        print(f\"   Augmented mean: {aug_mean:.2f}\")\n",
    "        print(f\"   Difference: {diff_pct:.1f}%\")\n",
    "        \n",
    "        if diff_pct > 20:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: Significant distribution shift\")\n",
    "    \n",
    "    # 3. No memorization\n",
    "    combined = pd.concat([original, augmented])\n",
    "    duplicates = combined.duplicated().sum()\n",
    "    print(f\"\\nüîí Duplicate check: {duplicates} duplicates\")\n",
    "    assert duplicates == 0, \"Augmented data contains exact copies!\"\n",
    "    \n",
    "    # 4. Diversity\n",
    "    unique_ratio = augmented.drop_duplicates().shape[0] / augmented.shape[0]\n",
    "    print(f\"üé® Diversity: {unique_ratio*100:.1f}% unique records\")\n",
    "    assert unique_ratio > 0.9, \"Low diversity in augmented data\"\n",
    "    \n",
    "    # 5. ML Utility (si hay target)\n",
    "    if 'target' in original.columns:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        # Baseline: Train y test en original\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            original.drop('target', axis=1), original['target'], test_size=0.2\n",
    "        )\n",
    "        \n",
    "        model_baseline = RandomForestClassifier(random_state=42)\n",
    "        model_baseline.fit(X_train, y_train)\n",
    "        f1_baseline = f1_score(y_test, model_baseline.predict(X_test), average='macro')\n",
    "        \n",
    "        # Con augmentation: Train en original+augmented, test en original\n",
    "        X_aug = pd.concat([X_train, augmented.drop('target', axis=1)])\n",
    "        y_aug = pd.concat([y_train, augmented['target']])\n",
    "        \n",
    "        model_aug = RandomForestClassifier(random_state=42)\n",
    "        model_aug.fit(X_aug, y_aug)\n",
    "        f1_aug = f1_score(y_test, model_aug.predict(X_test), average='macro')\n",
    "        \n",
    "        print(f\"\\nü§ñ ML Performance:\")\n",
    "        print(f\"   Baseline F1: {f1_baseline:.3f}\")\n",
    "        print(f\"   With augmentation F1: {f1_aug:.3f}\")\n",
    "        print(f\"   Improvement: {(f1_aug - f1_baseline)*100:.1f}%\")\n",
    "        \n",
    "        if f1_aug > f1_baseline:\n",
    "            print(\"   ‚úÖ Augmentation improves model!\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Augmentation does not help (or hurts)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DO:\n",
    "# 1. Start small, validate, scale\n",
    "augmented_sample = augment(original[:10], count=20)\n",
    "validate_augmentation_quality(original[:10], augmented_sample)\n",
    "# Si pasa validaci√≥n ‚Üí escalar a dataset completo\n",
    "\n",
    "# 2. Preserve distributions\n",
    "# Comparar histogramas antes/despu√©s\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "original['price'].hist(ax=axes[0], bins=30)\n",
    "augmented['price'].hist(ax=axes[1], bins=30)\n",
    "axes[0].set_title('Original')\n",
    "axes[1].set_title('Augmented')\n",
    "\n",
    "# 3. Use temperature wisely\n",
    "# Alta temp (0.8-1.0) ‚Üí m√°s diversidad (buenos para augmentation)\n",
    "# Baja temp (0.3-0.5) ‚Üí m√°s conservador (buenos para variaciones sutiles)\n",
    "\n",
    "# ‚ùå DON'T:\n",
    "# 1. No augmentar test set (solo train!)\n",
    "X_train_aug = augment(X_train)  # ‚úÖ\n",
    "X_test_aug = augment(X_test)    # ‚ùå Contamination!\n",
    "\n",
    "# 2. No augmentar sin validar\n",
    "# Siempre verificar que augmentation mejora (no da√±a) el modelo\n",
    "\n",
    "# 3. No sobre-augmentar\n",
    "# Ratio 1:5 (original:augmented) es m√°ximo recomendado\n",
    "# M√°s all√° de eso, el modelo puede aprender \"synthetic patterns\"\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a4935",
   "metadata": {},
   "source": [
    "## 2. Generaci√≥n con contexto de negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sales_data(num_records: int, context: str) -> list:\n",
    "    \"\"\"Genera datos de ventas con contexto.\"\"\"\n",
    "    prompt = f'''\n",
    "Contexto de negocio: {context}\n",
    "\n",
    "Genera {num_records} transacciones de venta realistas con:\n",
    "- transaction_id (UUID)\n",
    "- date (√∫ltimos 30 d√≠as)\n",
    "- product (nombre coherente con el contexto)\n",
    "- quantity (int)\n",
    "- unit_price (float)\n",
    "- total (quantity * unit_price)\n",
    "- payment_method (cash, card, online)\n",
    "\n",
    "JSON array:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "sales = generate_sales_data(\n",
    "    num_records=3,\n",
    "    context='Tienda de electr√≥nica especializada en laptops y accesorios'\n",
    ")\n",
    "\n",
    "for sale in sales:\n",
    "    print(sale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ed1df",
   "metadata": {},
   "source": [
    "## 3. Generaci√≥n de casos edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_cases(normal_schema: dict) -> list:\n",
    "    \"\"\"Genera casos extremos para testing.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera 10 casos edge/extremos para testing basados en este esquema:\n",
    "\n",
    "{json.dumps(normal_schema, indent=2)}\n",
    "\n",
    "Incluye casos como:\n",
    "- Valores nulos\n",
    "- Strings vac√≠os\n",
    "- N√∫meros negativos/cero\n",
    "- Fechas futuras/pasadas extremas\n",
    "- Caracteres especiales\n",
    "- Valores muy largos\n",
    "\n",
    "JSON array con campo adicional 'edge_case_type':\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "edge_cases = generate_edge_cases(customer_schema)\n",
    "print(f'Generados {len(edge_cases)} casos edge:\\n')\n",
    "for case in edge_cases[:3]:\n",
    "    print(f\"Tipo: {case.get('edge_case_type')}\")\n",
    "    print(case)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50869c",
   "metadata": {},
   "source": [
    "## 4. Anonimizaci√≥n inteligente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc1a1d",
   "metadata": {},
   "source": [
    "### üîí **Anonimizaci√≥n Inteligente: Privacy-Preserving Synthetic Data**\n",
    "\n",
    "**¬øPor qu√© Anonimizaci√≥n con LLMs?**\n",
    "\n",
    "```\n",
    "M√©todos Tradicionales              Problemas\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "üîπ Masking (XXX-XX-1234)          ‚Üí Pierde utilidad, patrones obvios\n",
    "üîπ Hashing (SHA256)               ‚Üí Irreversible pero no realista\n",
    "üîπ Randomization                  ‚Üí Rompe correlaciones\n",
    "üîπ K-anonymity                    ‚Üí Dif√≠cil con high-dimensional data\n",
    "üîπ Differential Privacy           ‚Üí Requiere expertise matem√°tico\n",
    "\n",
    "LLMs                              Ventajas\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚úÖ Context-aware replacement      ‚Üí Mantiene realismo sem√°ntico\n",
    "‚úÖ Relationship preservation      ‚Üí Coherencia cross-field\n",
    "‚úÖ Flexible privacy levels        ‚Üí Trade-off utilidad/privacidad\n",
    "‚úÖ Explainable transformations    ‚Üí Auditable\n",
    "```\n",
    "\n",
    "**Niveles de Anonimizaci√≥n:**\n",
    "\n",
    "```python\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  ANONYMIZATION SPECTRUM                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                ‚îÇ\n",
    "‚îÇ  Level 1: REDACTION (Lowest Utility)                          ‚îÇ\n",
    "‚îÇ     Original: \"John Smith, john@acme.com, 555-1234\"          ‚îÇ\n",
    "‚îÇ     Result:   \"‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà, ‚ñà‚ñà‚ñà‚ñà@‚ñà‚ñà‚ñà‚ñà.com, ‚ñà‚ñà‚ñà-‚ñà‚ñà‚ñà‚ñà\"         ‚îÇ\n",
    "‚îÇ     Use case: Public display, low risk                        ‚îÇ\n",
    "‚îÇ                                                                ‚îÇ\n",
    "‚îÇ  Level 2: DETERMINISTIC MASKING                               ‚îÇ\n",
    "‚îÇ     Original: \"John Smith, john@acme.com, 555-1234\"          ‚îÇ\n",
    "‚îÇ     Result:   \"User_12345, user12345@example.com, ***-****\"  ‚îÇ\n",
    "‚îÇ     Use case: Internal analytics, need consistency            ‚îÇ\n",
    "‚îÇ                                                                ‚îÇ\n",
    "‚îÇ  Level 3: PSEUDONYMIZATION (Reversible)                       ‚îÇ\n",
    "‚îÇ     Original: \"John Smith, john@acme.com, 555-1234\"          ‚îÇ\n",
    "‚îÇ     Result:   \"Jane Doe, jane@techcorp.com, 555-5678\"        ‚îÇ\n",
    "‚îÇ              + Mapping table (encrypted)                      ‚îÇ\n",
    "‚îÇ     Use case: Development, testing, need to reverse           ‚îÇ\n",
    "‚îÇ                                                                ‚îÇ\n",
    "‚îÇ  Level 4: SYNTHETIC REPLACEMENT (Highest Utility)             ‚îÇ\n",
    "‚îÇ     Original: \"John Smith, 35, NYC, Software Engineer, $120K\" ‚îÇ\n",
    "‚îÇ     Result:   \"Emma Wilson, 33, Boston, Data Scientist, $115K\"‚îÇ\n",
    "‚îÇ     Properties: Similar distribution, no reverse mapping      ‚îÇ\n",
    "‚îÇ     Use case: ML training, external sharing, research         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**LLM-Powered Anonymizer (Production-Ready):**\n",
    "\n",
    "```python\n",
    "from typing import Dict, List, Optional, Literal\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class AnonymizationConfig(BaseModel):\n",
    "    \"\"\"Configuraci√≥n de anonimizaci√≥n por campo\"\"\"\n",
    "    field_name: str\n",
    "    pii_type: Literal['name', 'email', 'phone', 'ssn', 'address', 'date_of_birth', 'free_text']\n",
    "    anonymization_level: Literal['redact', 'mask', 'pseudonymize', 'synthetic']\n",
    "    preserve_format: bool = True  # Mantener formato (ej: email sigue siendo email@domain.com)\n",
    "    preserve_domain: bool = False  # Para emails: mantener dominio (@acme.com)\n",
    "    preserve_distribution: bool = True  # Para num√©ricos: mantener distribuci√≥n estad√≠stica\n",
    "\n",
    "class SmartAnonymizer:\n",
    "    \"\"\"Anonimizador inteligente con LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'gpt-4'):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.pseudonym_cache = {}  # Para consistency (mismo input ‚Üí mismo output)\n",
    "    \n",
    "    def anonymize_dataset(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        config: List[AnonymizationConfig],\n",
    "        consistency: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Anonimiza dataset completo preservando coherencia.\n",
    "        \n",
    "        Args:\n",
    "            data: Lista de diccionarios a anonimizar\n",
    "            config: Configuraci√≥n por campo\n",
    "            consistency: Si True, mismo valor original ‚Üí mismo anonimizado\n",
    "        \n",
    "        Returns:\n",
    "            Dataset anonimizado\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build field-level instructions\n",
    "        field_configs = {cfg.field_name: cfg for cfg in config}\n",
    "        \n",
    "        # Sample para entender contexto\n",
    "        sample = data[:5] if len(data) > 5 else data\n",
    "        \n",
    "        prompt = self._build_anonymization_prompt(sample, field_configs)\n",
    "        \n",
    "        # Procesar en batches para eficiencia\n",
    "        batch_size = 50\n",
    "        anonymized_data = []\n",
    "        \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            \n",
    "            batch_prompt = f\"\"\"\n",
    "            Anonymize this batch of {len(batch)} records following these rules:\n",
    "            \n",
    "            {prompt}\n",
    "            \n",
    "            Records to anonymize:\n",
    "            {json.dumps(batch, indent=2, default=str)}\n",
    "            \n",
    "            Return JSON array with anonymized records (same schema).\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a privacy expert. Anonymize data while preserving utility.\"},\n",
    "                    {\"role\": \"user\", \"content\": batch_prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            anonymized_batch = result.get('records', result.get('data', []))\n",
    "            \n",
    "            # Apply consistency if needed\n",
    "            if consistency:\n",
    "                anonymized_batch = self._ensure_consistency(batch, anonymized_batch, field_configs)\n",
    "            \n",
    "            anonymized_data.extend(anonymized_batch)\n",
    "        \n",
    "        return anonymized_data\n",
    "    \n",
    "    def _build_anonymization_prompt(\n",
    "        self, \n",
    "        sample: List[Dict], \n",
    "        configs: Dict[str, AnonymizationConfig]\n",
    "    ) -> str:\n",
    "        \"\"\"Construye instrucciones claras para LLM\"\"\"\n",
    "        \n",
    "        rules = [\"ANONYMIZATION RULES:\"]\n",
    "        \n",
    "        for field_name, config in configs.items():\n",
    "            rule = f\"\\n{field_name} ({config.pii_type}):\"\n",
    "            \n",
    "            if config.anonymization_level == 'redact':\n",
    "                rule += \" ‚Üí REDACT completely (use '‚ñà‚ñà‚ñà‚ñà' or '[REDACTED]')\"\n",
    "            \n",
    "            elif config.anonymization_level == 'mask':\n",
    "                rule += \" ‚Üí MASK with generic placeholder\"\n",
    "                if config.pii_type == 'email':\n",
    "                    rule += \" (use 'user{id}@example.com')\"\n",
    "                elif config.pii_type == 'phone':\n",
    "                    rule += \" (use 'XXX-XXX-{last4}')\"\n",
    "            \n",
    "            elif config.anonymization_level == 'pseudonymize':\n",
    "                rule += \" ‚Üí REPLACE with realistic alternative\"\n",
    "                if config.preserve_format:\n",
    "                    rule += \", MAINTAIN format\"\n",
    "                if config.preserve_domain and config.pii_type == 'email':\n",
    "                    rule += \", KEEP domain\"\n",
    "            \n",
    "            elif config.anonymization_level == 'synthetic':\n",
    "                rule += \" ‚Üí GENERATE synthetic data\"\n",
    "                if config.preserve_distribution:\n",
    "                    rule += \", PRESERVE statistical distribution\"\n",
    "            \n",
    "            rules.append(rule)\n",
    "        \n",
    "        rules.append(\"\\n\\nGENERAL PRINCIPLES:\")\n",
    "        rules.append(\"- Maintain coherence (if name is 'John', email shouldn't be 'maria@...')\")\n",
    "        rules.append(\"- Preserve relationships between fields\")\n",
    "        rules.append(\"- Keep data realistic and useful for analytics\")\n",
    "        rules.append(\"- Same original value ‚Üí same anonymized value (consistency)\")\n",
    "        \n",
    "        return \"\\n\".join(rules)\n",
    "    \n",
    "    def _ensure_consistency(\n",
    "        self,\n",
    "        original_batch: List[Dict],\n",
    "        anonymized_batch: List[Dict],\n",
    "        configs: Dict[str, AnonymizationConfig]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Garantiza que valores repetidos se anonimicen consistentemente.\n",
    "        \n",
    "        Ejemplo: Si \"john@acme.com\" aparece 3 veces, las 3 deben mapearse\n",
    "                 al mismo valor anonimizado \"jane@techcorp.com\".\n",
    "        \"\"\"\n",
    "        \n",
    "        for field_name in configs.keys():\n",
    "            # Build mapping: original ‚Üí anonymized\n",
    "            mapping = {}\n",
    "            \n",
    "            for orig, anon in zip(original_batch, anonymized_batch):\n",
    "                orig_value = orig.get(field_name)\n",
    "                anon_value = anon.get(field_name)\n",
    "                \n",
    "                if orig_value is not None:\n",
    "                    # Hash original value para lookup\n",
    "                    key = self._hash_value(orig_value)\n",
    "                    \n",
    "                    if key not in self.pseudonym_cache:\n",
    "                        self.pseudonym_cache[key] = anon_value\n",
    "                    \n",
    "                    # Apply consistent mapping\n",
    "                    anon[field_name] = self.pseudonym_cache[key]\n",
    "        \n",
    "        return anonymized_batch\n",
    "    \n",
    "    def _hash_value(self, value) -> str:\n",
    "        \"\"\"Hash determin√≠stico para cache key\"\"\"\n",
    "        return hashlib.sha256(str(value).encode()).hexdigest()\n",
    "\n",
    "# Ejemplo: Anonimizar base de datos de empleados\n",
    "employees = [\n",
    "    {\n",
    "        \"employee_id\": 1001,\n",
    "        \"name\": \"John Smith\",\n",
    "        \"email\": \"john.smith@acme.com\",\n",
    "        \"phone\": \"555-123-4567\",\n",
    "        \"salary\": 120000,\n",
    "        \"department\": \"Engineering\",\n",
    "        \"performance_review\": \"Excellent developer, strong communication skills\"\n",
    "    },\n",
    "    {\n",
    "        \"employee_id\": 1002,\n",
    "        \"name\": \"Maria Garcia\",\n",
    "        \"email\": \"maria.garcia@acme.com\",\n",
    "        \"phone\": \"555-234-5678\",\n",
    "        \"salary\": 95000,\n",
    "        \"department\": \"Marketing\",\n",
    "        \"performance_review\": \"Great team player, needs to improve presentation skills\"\n",
    "    },\n",
    "    # ... m√°s empleados\n",
    "]\n",
    "\n",
    "# Configurar anonimizaci√≥n\n",
    "config = [\n",
    "    AnonymizationConfig(\n",
    "        field_name='name',\n",
    "        pii_type='name',\n",
    "        anonymization_level='synthetic',\n",
    "        preserve_format=True\n",
    "    ),\n",
    "    AnonymizationConfig(\n",
    "        field_name='email',\n",
    "        pii_type='email',\n",
    "        anonymization_level='pseudonymize',\n",
    "        preserve_format=True,\n",
    "        preserve_domain=True  # Mantener @acme.com\n",
    "    ),\n",
    "    AnonymizationConfig(\n",
    "        field_name='phone',\n",
    "        pii_type='phone',\n",
    "        anonymization_level='mask',\n",
    "        preserve_format=True\n",
    "    ),\n",
    "    AnonymizationConfig(\n",
    "        field_name='performance_review',\n",
    "        pii_type='free_text',\n",
    "        anonymization_level='synthetic',  # Reescribir sin nombres/detalles espec√≠ficos\n",
    "        preserve_format=False\n",
    "    )\n",
    "]\n",
    "\n",
    "anonymizer = SmartAnonymizer(model='gpt-4')\n",
    "anonymized_employees = anonymizer.anonymize_dataset(employees, config, consistency=True)\n",
    "\n",
    "print(\"Original:\", employees[0])\n",
    "print(\"\\nAnonymized:\", anonymized_employees[0])\n",
    "```\n",
    "\n",
    "**Ejemplo Output:**\n",
    "\n",
    "```python\n",
    "Original: {\n",
    "    \"employee_id\": 1001,\n",
    "    \"name\": \"John Smith\",\n",
    "    \"email\": \"john.smith@acme.com\",\n",
    "    \"phone\": \"555-123-4567\",\n",
    "    \"salary\": 120000,\n",
    "    \"department\": \"Engineering\",\n",
    "    \"performance_review\": \"Excellent developer, strong communication skills\"\n",
    "}\n",
    "\n",
    "Anonymized: {\n",
    "    \"employee_id\": 1001,  # No PII, se mantiene\n",
    "    \"name\": \"Alex Thompson\",  # Synthetic name\n",
    "    \"email\": \"alex.thompson@acme.com\",  # Preserved domain\n",
    "    \"phone\": \"XXX-XXX-4567\",  # Masked (last 4 preserved)\n",
    "    \"salary\": 118000,  # Slightly perturbed (preserve distribution)\n",
    "    \"department\": \"Engineering\",  # No PII, se mantiene\n",
    "    \"performance_review\": \"Strong technical contributor with good team collaboration\"  # Rewritten\n",
    "}\n",
    "```\n",
    "\n",
    "**Privacy Metrics:**\n",
    "\n",
    "```python\n",
    "def calculate_privacy_metrics(\n",
    "    original: List[Dict],\n",
    "    anonymized: List[Dict],\n",
    "    pii_fields: List[str]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Mide qu√© tan bien se preserva privacidad.\n",
    "    \n",
    "    Metrics:\n",
    "    - PII Removal Rate: % de PII eliminado\n",
    "    - Uniqueness: Si combinaciones √∫nicas se reducen (k-anonymity)\n",
    "    - Re-identification Risk: Probabilidad de linkear a original\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. PII Removal Rate\n",
    "    pii_removed = 0\n",
    "    pii_total = 0\n",
    "    \n",
    "    for orig, anon in zip(original, anonymized):\n",
    "        for field in pii_fields:\n",
    "            pii_total += 1\n",
    "            if orig[field] != anon[field]:\n",
    "                pii_removed += 1\n",
    "    \n",
    "    metrics['pii_removal_rate'] = pii_removed / pii_total * 100\n",
    "    \n",
    "    # 2. K-anonymity\n",
    "    # Combinaciones √∫nicas de quasi-identifiers\n",
    "    orig_combinations = set()\n",
    "    anon_combinations = set()\n",
    "    \n",
    "    quasi_ids = ['age', 'zip_code', 'gender']  # Ejemplo\n",
    "    \n",
    "    for record in original:\n",
    "        combo = tuple(record.get(q) for q in quasi_ids if q in record)\n",
    "        orig_combinations.add(combo)\n",
    "    \n",
    "    for record in anonymized:\n",
    "        combo = tuple(record.get(q) for q in quasi_ids if q in record)\n",
    "        anon_combinations.add(combo)\n",
    "    \n",
    "    metrics['original_unique_combinations'] = len(orig_combinations)\n",
    "    metrics['anonymized_unique_combinations'] = len(anon_combinations)\n",
    "    metrics['k_anonymity_improvement'] = (\n",
    "        (len(orig_combinations) - len(anon_combinations)) / len(orig_combinations) * 100\n",
    "    )\n",
    "    \n",
    "    # 3. Re-identification Risk (simplified)\n",
    "    # Si alguien puede match 1-1 original‚Üíanonymized\n",
    "    perfect_matches = 0\n",
    "    for orig, anon in zip(original, anonymized):\n",
    "        # Check si non-PII fields son id√©nticos (facilita re-id)\n",
    "        non_pii_match = all(\n",
    "            orig.get(k) == anon.get(k) \n",
    "            for k in orig.keys() \n",
    "            if k not in pii_fields\n",
    "        )\n",
    "        if non_pii_match:\n",
    "            perfect_matches += 1\n",
    "    \n",
    "    metrics['re_identification_risk'] = perfect_matches / len(original) * 100\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluar\n",
    "privacy_metrics = calculate_privacy_metrics(\n",
    "    original=employees,\n",
    "    anonymized=anonymized_employees,\n",
    "    pii_fields=['name', 'email', 'phone', 'performance_review']\n",
    ")\n",
    "\n",
    "print(\"üìä PRIVACY METRICS:\")\n",
    "print(f\"PII Removal Rate: {privacy_metrics['pii_removal_rate']:.1f}%\")\n",
    "print(f\"K-anonymity Improvement: {privacy_metrics['k_anonymity_improvement']:.1f}%\")\n",
    "print(f\"Re-identification Risk: {privacy_metrics['re_identification_risk']:.1f}%\")\n",
    "```\n",
    "\n",
    "**Utility Preservation:**\n",
    "\n",
    "```python\n",
    "def calculate_utility_metrics(\n",
    "    original: pd.DataFrame,\n",
    "    anonymized: pd.DataFrame,\n",
    "    numeric_cols: List[str],\n",
    "    categorical_cols: List[str]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Mide qu√© tan √∫til sigue siendo el dataset anonimizado.\n",
    "    \n",
    "    Metrics:\n",
    "    - Statistical similarity (numeric)\n",
    "    - Categorical distribution preservation\n",
    "    - Correlation preservation\n",
    "    - ML model performance\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Statistical Similarity (numeric)\n",
    "    for col in numeric_cols:\n",
    "        orig_mean = original[col].mean()\n",
    "        anon_mean = anonymized[col].mean()\n",
    "        \n",
    "        orig_std = original[col].std()\n",
    "        anon_std = anonymized[col].std()\n",
    "        \n",
    "        mean_diff = abs(orig_mean - anon_mean) / orig_mean * 100\n",
    "        std_diff = abs(orig_std - anon_std) / orig_std * 100\n",
    "        \n",
    "        metrics[f'{col}_mean_diff_pct'] = mean_diff\n",
    "        metrics[f'{col}_std_diff_pct'] = std_diff\n",
    "    \n",
    "    # 2. Categorical Distribution\n",
    "    for col in categorical_cols:\n",
    "        orig_dist = original[col].value_counts(normalize=True)\n",
    "        anon_dist = anonymized[col].value_counts(normalize=True)\n",
    "        \n",
    "        # Jensen-Shannon divergence\n",
    "        from scipy.spatial.distance import jensenshannon\n",
    "        js_div = jensenshannon(orig_dist, anon_dist)\n",
    "        metrics[f'{col}_distribution_similarity'] = 1 - js_div  # 1 = identical\n",
    "    \n",
    "    # 3. Correlation Preservation\n",
    "    orig_corr = original[numeric_cols].corr()\n",
    "    anon_corr = anonymized[numeric_cols].corr()\n",
    "    \n",
    "    corr_mae = np.abs(orig_corr - anon_corr).mean().mean()\n",
    "    metrics['correlation_mae'] = corr_mae\n",
    "    \n",
    "    # 4. ML Performance (if target exists)\n",
    "    if 'target' in original.columns:\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import f1_score\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Train on original\n",
    "        X_orig = original.drop('target', axis=1)\n",
    "        y_orig = original['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.2)\n",
    "        \n",
    "        model_orig = RandomForestClassifier()\n",
    "        model_orig.fit(X_train, y_train)\n",
    "        f1_orig = f1_score(y_test, model_orig.predict(X_test), average='macro')\n",
    "        \n",
    "        # Train on anonymized\n",
    "        X_anon = anonymized.drop('target', axis=1)\n",
    "        y_anon = anonymized['target']\n",
    "        \n",
    "        model_anon = RandomForestClassifier()\n",
    "        model_anon.fit(X_anon, y_anon)\n",
    "        f1_anon = f1_score(y_test, model_anon.predict(X_test), average='macro')\n",
    "        \n",
    "        metrics['ml_f1_original'] = f1_orig\n",
    "        metrics['ml_f1_anonymized'] = f1_anon\n",
    "        metrics['ml_utility_retention'] = (f1_anon / f1_orig) * 100\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluar utilidad\n",
    "utility_metrics = calculate_utility_metrics(\n",
    "    original=pd.DataFrame(employees),\n",
    "    anonymized=pd.DataFrame(anonymized_employees),\n",
    "    numeric_cols=['salary'],\n",
    "    categorical_cols=['department']\n",
    ")\n",
    "\n",
    "print(\"\\nüìà UTILITY METRICS:\")\n",
    "print(f\"Salary mean diff: {utility_metrics.get('salary_mean_diff_pct', 0):.1f}%\")\n",
    "print(f\"Department distribution similarity: {utility_metrics.get('department_distribution_similarity', 0):.3f}\")\n",
    "print(f\"ML utility retention: {utility_metrics.get('ml_utility_retention', 0):.1f}%\")\n",
    "```\n",
    "\n",
    "**Compliance & Auditing:**\n",
    "\n",
    "```python\n",
    "class AnonymizationAudit:\n",
    "    \"\"\"Genera audit trail para compliance (GDPR, HIPAA, etc)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audit_log = []\n",
    "    \n",
    "    def log_anonymization(\n",
    "        self,\n",
    "        timestamp: str,\n",
    "        dataset_name: str,\n",
    "        num_records: int,\n",
    "        config: List[AnonymizationConfig],\n",
    "        privacy_metrics: Dict,\n",
    "        utility_metrics: Dict,\n",
    "        user: str\n",
    "    ):\n",
    "        \"\"\"Registra operaci√≥n de anonimizaci√≥n\"\"\"\n",
    "        \n",
    "        entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'dataset': dataset_name,\n",
    "            'record_count': num_records,\n",
    "            'user': user,\n",
    "            'configuration': [\n",
    "                {\n",
    "                    'field': cfg.field_name,\n",
    "                    'pii_type': cfg.pii_type,\n",
    "                    'method': cfg.anonymization_level\n",
    "                }\n",
    "                for cfg in config\n",
    "            ],\n",
    "            'privacy_metrics': privacy_metrics,\n",
    "            'utility_metrics': utility_metrics,\n",
    "            'compliance': {\n",
    "                'gdpr_article_6': 'Lawful processing - anonymization',\n",
    "                'gdpr_article_89': 'Safeguards for processing for research purposes',\n",
    "                'hipaa_safe_harbor': 'PHI identifiers removed' if privacy_metrics['pii_removal_rate'] == 100 else 'Does not meet Safe Harbor'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(entry)\n",
    "        \n",
    "        # Export para reguladores\n",
    "        with open(f'audit_logs/anonymization_{timestamp}.json', 'w') as f:\n",
    "            json.dump(entry, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Audit log created: anonymization_{timestamp}.json\")\n",
    "    \n",
    "    def generate_compliance_report(self) -> str:\n",
    "        \"\"\"Genera reporte para auditores\"\"\"\n",
    "        \n",
    "        report = [\n",
    "            \"ANONYMIZATION COMPLIANCE REPORT\",\n",
    "            \"=\" * 50,\n",
    "            f\"\\nTotal Operations: {len(self.audit_log)}\",\n",
    "        ]\n",
    "        \n",
    "        for i, entry in enumerate(self.audit_log, 1):\n",
    "            report.append(f\"\\n{i}. {entry['dataset']} ({entry['timestamp']})\")\n",
    "            report.append(f\"   Records: {entry['record_count']}\")\n",
    "            report.append(f\"   PII Removal: {entry['privacy_metrics']['pii_removal_rate']:.1f}%\")\n",
    "            report.append(f\"   Utility Retention: {entry['utility_metrics'].get('ml_utility_retention', 'N/A')}\")\n",
    "            report.append(f\"   Compliance: {entry['compliance']['hipaa_safe_harbor']}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Uso\n",
    "audit = AnonymizationAudit()\n",
    "\n",
    "audit.log_anonymization(\n",
    "    timestamp='2024-10-31T10:00:00Z',\n",
    "    dataset_name='employees_q3_2024',\n",
    "    num_records=len(employees),\n",
    "    config=config,\n",
    "    privacy_metrics=privacy_metrics,\n",
    "    utility_metrics=utility_metrics,\n",
    "    user='data_engineer_ljrv'\n",
    ")\n",
    "\n",
    "print(audit.generate_compliance_report())\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **Privacy First:**\n",
    "   - Default to highest privacy level\n",
    "   - Explicit consent for lower levels\n",
    "   - Document rationale for each field\n",
    "\n",
    "2. **Test Re-identification:**\n",
    "   - Attempt to match anonymized‚Üíoriginal\n",
    "   - Hire external auditors (red team)\n",
    "   - Use linkage attack simulations\n",
    "\n",
    "3. **Layered Defense:**\n",
    "   ```python\n",
    "   # No confiar solo en anonimizaci√≥n\n",
    "   anonymized_data = anonymize(data)\n",
    "   + Differential privacy noise\n",
    "   + k-anonymity grouping\n",
    "   + Access controls\n",
    "   + Audit logs\n",
    "   = Defense in depth\n",
    "   ```\n",
    "\n",
    "4. **Update Regularly:**\n",
    "   - T√©cnicas de de-anonymization mejoran\n",
    "   - Re-anonymize peri√≥dicamente\n",
    "   - Monitor for new attacks\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_data(sensitive_records: list) -> list:\n",
    "    \"\"\"Anonimiza datos manteniendo realismo.\"\"\"\n",
    "    prompt = f'''\n",
    "Anonimiza estos registros manteniendo la estructura y realismo:\n",
    "- Reemplaza nombres con nombres ficticios\n",
    "- Cambia emails pero mant√©n el formato\n",
    "- Altera IDs pero mant√©n el tipo\n",
    "- Preserva patrones estad√≠sticos (edades, fechas)\n",
    "\n",
    "Datos originales:\n",
    "{json.dumps(sensitive_records, indent=2)}\n",
    "\n",
    "Datos anonimizados (mismo formato JSON):\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "real_data = [\n",
    "    {'id': 12345, 'name': 'John Smith', 'email': 'john@company.com', 'salary': 75000},\n",
    "    {'id': 12346, 'name': 'Maria Garcia', 'email': 'maria@company.com', 'salary': 82000}\n",
    "]\n",
    "\n",
    "anonymized = anonymize_data(real_data)\n",
    "print('Original:', real_data[0])\n",
    "print('Anonimizado:', anonymized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b9c5c",
   "metadata": {},
   "source": [
    "## 5. Aumento de datos (data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(original_records: list, target_count: int) -> list:\n",
    "    \"\"\"Aumenta dataset generando variaciones.\"\"\"\n",
    "    prompt = f'''\n",
    "Tienes estos {len(original_records)} registros originales:\n",
    "\n",
    "{json.dumps(original_records, indent=2)}\n",
    "\n",
    "Genera {target_count} registros nuevos similares pero con variaciones realistas.\n",
    "Mant√©n distribuciones y patrones.\n",
    "\n",
    "JSON array:\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "original_products = [\n",
    "    {'name': 'Laptop Pro 15', 'category': 'Electronics', 'price': 1299},\n",
    "    {'name': 'Wireless Mouse', 'category': 'Accessories', 'price': 29}\n",
    "]\n",
    "\n",
    "augmented = augment_dataset(original_products, 5)\n",
    "print(f'Dataset aumentado a {len(augmented)} registros:\\n')\n",
    "for item in augmented:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de472c0",
   "metadata": {},
   "source": [
    "## 6. Generaci√≥n de series temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05629531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(metric_name: str, days: int, pattern: str) -> list:\n",
    "    \"\"\"Genera serie temporal sint√©tica.\"\"\"\n",
    "    prompt = f'''\n",
    "Genera una serie temporal de {days} d√≠as para la m√©trica: {metric_name}\n",
    "Patr√≥n: {pattern}\n",
    "\n",
    "Formato JSON:\n",
    "[\n",
    "  {{\"date\": \"YYYY-MM-DD\", \"value\": float}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Los valores deben seguir el patr√≥n descrito.\n",
    "'''\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "revenue_data = generate_timeseries(\n",
    "    metric_name='daily_revenue',\n",
    "    days=7,\n",
    "    pattern='Tendencia creciente con picos los fines de semana, valores entre 5000-15000'\n",
    ")\n",
    "\n",
    "for entry in revenue_data:\n",
    "    print(f\"{entry['date']}: ${entry['value']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a753a4",
   "metadata": {},
   "source": [
    "## 7. Integraci√≥n con Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60007cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faker\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_hybrid_dataset(count: int) -> pd.DataFrame:\n",
    "    \"\"\"Combina Faker (estructura) + LLM (sem√°ntica).\"\"\"\n",
    "    # Estructura base con Faker\n",
    "    base_data = [{\n",
    "        'user_id': fake.uuid4(),\n",
    "        'name': fake.name(),\n",
    "        'email': fake.email(),\n",
    "        'city': fake.city()\n",
    "    } for _ in range(count)]\n",
    "    \n",
    "    # Enriquecer con LLM (ej: generar bio coherente)\n",
    "    for record in base_data:\n",
    "        prompt = f\"Genera una bio de 1 frase para {record['name']}, vive en {record['city']}, trabaja en tech.\"\n",
    "        resp = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{'role':'user','content':prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        record['bio'] = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    return pd.DataFrame(base_data)\n",
    "\n",
    "df_hybrid = generate_hybrid_dataset(3)\n",
    "print(df_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc31f0",
   "metadata": {},
   "source": [
    "## 8. Buenas pr√°cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105efae",
   "metadata": {},
   "source": [
    "- **Validaci√≥n**: verifica que datos sint√©ticos cumplan constraints.\n",
    "- **Distribuciones**: compara estad√≠sticas con datos reales.\n",
    "- **Diversidad**: usa temperature alta para mayor variedad.\n",
    "- **Lotes**: genera en batches para eficiencia.\n",
    "- **Costos**: usa GPT-3.5 para volumen, GPT-4 para calidad.\n",
    "- **Complementar**: combina LLMs con bibliotecas tradicionales (Faker, SDV).\n",
    "- **Testing**: valida con Great Expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8b506",
   "metadata": {},
   "source": [
    "### üè≠ **Datos Sint√©ticos en Producci√≥n: Arquitectura y ROI**\n",
    "\n",
    "**Pipeline de Generaci√≥n de Datos Sint√©ticos (Enterprise):**\n",
    "\n",
    "```python\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           SYNTHETIC DATA PRODUCTION PIPELINE                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                   ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£ DATA INGESTION & PROFILING                                   ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Real data sampling               ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Schema extraction                ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Statistical profiling            ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ PII detection (NER, regex)       ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Relationship mapping             ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îÇ            ‚Üì                                                      ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ CONFIGURATION & POLICY                                       ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Anonymization rules              ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Generation strategy              ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Quality thresholds               ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Compliance requirements          ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îÇ            ‚Üì                                                      ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ GENERATION (Multi-Strategy)                                  ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ     ‚îÇ LLM (GPT-4)     ‚Üí  5% high-value   ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ LLM (GPT-3.5)   ‚Üí  15% medium      ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ Faker           ‚Üí  70% simple      ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ SDV (Copula)    ‚Üí  10% complex     ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îÇ            ‚Üì                                                      ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£ QUALITY VALIDATION                                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Schema validation (Pydantic)     ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Statistical tests (KS, Chi¬≤)     ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Business rules (Great Expect.)   ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ PII leakage check                ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ ML utility test                  ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îÇ            ‚Üì                                                      ‚îÇ\n",
    "‚îÇ  5Ô∏è‚É£ STORAGE & DELIVERY                                           ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Versioning (DVC, lakeFS)         ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Access control (RBAC)            ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Audit logging                    ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ API endpoints (FastAPI)          ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îÇ ‚Ä¢ Self-service portal              ‚îÇ                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Production-Grade Generator (Orchestrated):**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "class SyntheticDataPlatform:\n",
    "    \"\"\"\n",
    "    Plataforma empresarial para generaci√≥n de datos sint√©ticos.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-strategy generation (LLM + Faker + SDV)\n",
    "    - Quality gates (fail if metrics below threshold)\n",
    "    - Cost optimization (smart routing)\n",
    "    - Versioning & lineage\n",
    "    - Self-service API\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.llm_client = OpenAI()\n",
    "        self.faker = Faker()\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics = {\n",
    "            'records_generated': 0,\n",
    "            'cost_usd': 0.0,\n",
    "            'quality_score': 0.0,\n",
    "            'generation_time_sec': 0.0\n",
    "        }\n",
    "    \n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        schema: Dict,\n",
    "        count: int,\n",
    "        quality_threshold: float = 0.85\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Genera dataset con quality gates.\n",
    "        \n",
    "        Pipeline:\n",
    "        1. Profile schema ‚Üí detect complexity\n",
    "        2. Route to appropriate generator\n",
    "        3. Validate quality\n",
    "        4. If quality < threshold ‚Üí regenerate or fail\n",
    "        \"\"\"\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Complexity analysis\n",
    "        complexity_score = self._analyze_complexity(schema)\n",
    "        \n",
    "        # Step 2: Smart routing\n",
    "        if complexity_score > 0.8:\n",
    "            print(\"üß† High complexity ‚Üí GPT-4\")\n",
    "            df = self._generate_with_llm(schema, count, model='gpt-4')\n",
    "        elif complexity_score > 0.5:\n",
    "            print(\"‚ö° Medium complexity ‚Üí GPT-3.5\")\n",
    "            df = self._generate_with_llm(schema, count, model='gpt-3.5-turbo')\n",
    "        elif complexity_score > 0.2:\n",
    "            print(\"üìä Low complexity ‚Üí Hybrid (Faker + GPT-3.5)\")\n",
    "            df = self._generate_hybrid(schema, count)\n",
    "        else:\n",
    "            print(\"üöÄ Simple schema ‚Üí Faker only\")\n",
    "            df = self._generate_with_faker(schema, count)\n",
    "        \n",
    "        # Step 3: Quality validation\n",
    "        quality_metrics = self._validate_quality(df, schema)\n",
    "        \n",
    "        self.metrics['quality_score'] = quality_metrics['overall_score']\n",
    "        self.metrics['records_generated'] = len(df)\n",
    "        self.metrics['generation_time_sec'] = time.time() - start_time\n",
    "        \n",
    "        # Step 4: Quality gate\n",
    "        if quality_metrics['overall_score'] < quality_threshold:\n",
    "            raise ValueError(\n",
    "                f\"Quality score {quality_metrics['overall_score']:.2f} \"\n",
    "                f\"below threshold {quality_threshold}\"\n",
    "            )\n",
    "        \n",
    "        # Step 5: Audit log\n",
    "        self._log_generation(df, quality_metrics)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _analyze_complexity(self, schema: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Analiza complejidad del schema para routing.\n",
    "        \n",
    "        Complexity factors:\n",
    "        - Free text fields (high complexity)\n",
    "        - Cross-field dependencies (high complexity)\n",
    "        - Numeric ranges (low complexity)\n",
    "        - Categorical with enum (low complexity)\n",
    "        \"\"\"\n",
    "        \n",
    "        complexity = 0.0\n",
    "        field_count = len(schema)\n",
    "        \n",
    "        for field_name, field_def in schema.items():\n",
    "            if 'text' in field_def.get('type', '').lower():\n",
    "                complexity += 0.3  # Text es complejo\n",
    "            elif 'depends_on' in field_def:\n",
    "                complexity += 0.2  # Dependencies son complejas\n",
    "            elif 'enum' in field_def:\n",
    "                complexity += 0.05  # Categorical simple\n",
    "            else:\n",
    "                complexity += 0.1  # Numeric/simple\n",
    "        \n",
    "        return min(complexity / field_count, 1.0)\n",
    "    \n",
    "    def _generate_hybrid(self, schema: Dict, count: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Estrategia h√≠brida: Faker para estructura, LLM para sem√°ntica.\n",
    "        \n",
    "        70% m√°s barato que 100% LLM\n",
    "        3x m√°s r√°pido que 100% LLM\n",
    "        \"\"\"\n",
    "        \n",
    "        # Paso 1: Estructura con Faker\n",
    "        base_data = []\n",
    "        for i in range(count):\n",
    "            record = {}\n",
    "            for field_name, field_def in schema.items():\n",
    "                if field_def['generator'] == 'faker':\n",
    "                    # Usar Faker para campos simples\n",
    "                    faker_method = getattr(self.faker, field_def['faker_method'])\n",
    "                    record[field_name] = faker_method()\n",
    "                else:\n",
    "                    # Placeholder para LLM\n",
    "                    record[field_name] = None\n",
    "            base_data.append(record)\n",
    "        \n",
    "        # Paso 2: Enriquecer con LLM (batch)\n",
    "        llm_fields = [f for f, d in schema.items() if d['generator'] == 'llm']\n",
    "        \n",
    "        if llm_fields:\n",
    "            # Procesar en batches de 50\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(base_data), batch_size):\n",
    "                batch = base_data[i:i+batch_size]\n",
    "                \n",
    "                prompt = f\"\"\"\n",
    "                Enrich these {len(batch)} records by generating values for: {', '.join(llm_fields)}\n",
    "                \n",
    "                Schema:\n",
    "                {json.dumps({f: schema[f] for f in llm_fields}, indent=2)}\n",
    "                \n",
    "                Existing data:\n",
    "                {json.dumps(batch, indent=2, default=str)}\n",
    "                \n",
    "                Return complete records with enriched fields.\n",
    "                \"\"\"\n",
    "                \n",
    "                response = self.llm_client.chat.completions.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                enriched = json.loads(response.choices[0].message.content)\n",
    "                \n",
    "                # Merge\n",
    "                for j, enriched_record in enumerate(enriched.get('records', [])):\n",
    "                    base_data[i+j].update(enriched_record)\n",
    "                \n",
    "                # Track cost\n",
    "                self.metrics['cost_usd'] += 0.001 * len(batch)  # ~$0.001 per record\n",
    "        \n",
    "        return pd.DataFrame(base_data)\n",
    "    \n",
    "    def _validate_quality(self, df: pd.DataFrame, schema: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive quality validation.\n",
    "        \n",
    "        Checks:\n",
    "        1. Schema compliance (100% must pass)\n",
    "        2. Statistical validity (distributions)\n",
    "        3. Business rules (custom validators)\n",
    "        4. PII leakage (no real data leaked)\n",
    "        5. Uniqueness (sufficient diversity)\n",
    "        \"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'schema_compliance': 0.0,\n",
    "            'statistical_validity': 0.0,\n",
    "            'business_rules': 0.0,\n",
    "            'pii_leakage': 0.0,\n",
    "            'uniqueness': 0.0,\n",
    "            'overall_score': 0.0\n",
    "        }\n",
    "        \n",
    "        # 1. Schema compliance\n",
    "        required_cols = set(schema.keys())\n",
    "        actual_cols = set(df.columns)\n",
    "        metrics['schema_compliance'] = len(required_cols & actual_cols) / len(required_cols)\n",
    "        \n",
    "        # 2. Statistical validity (compare to expected ranges)\n",
    "        valid_records = 0\n",
    "        for col in df.columns:\n",
    "            if col in schema and 'range' in schema[col]:\n",
    "                expected_min, expected_max = schema[col]['range']\n",
    "                valid = df[col].between(expected_min, expected_max).sum()\n",
    "                valid_records += valid / len(df)\n",
    "        \n",
    "        metrics['statistical_validity'] = valid_records / len(df.columns) if len(df.columns) > 0 else 0\n",
    "        \n",
    "        # 3. Business rules (Great Expectations)\n",
    "        from great_expectations.dataset import PandasDataset\n",
    "        ge_df = PandasDataset(df)\n",
    "        \n",
    "        # Example: email format\n",
    "        if 'email' in df.columns:\n",
    "            result = ge_df.expect_column_values_to_match_regex(\n",
    "                'email',\n",
    "                r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            )\n",
    "            metrics['business_rules'] += result.success\n",
    "        \n",
    "        # 4. PII leakage check (placeholder - use Presidio in prod)\n",
    "        # Check if synthetic data contains known real values\n",
    "        metrics['pii_leakage'] = 1.0  # Assume no leakage for now\n",
    "        \n",
    "        # 5. Uniqueness\n",
    "        duplicate_ratio = df.duplicated().sum() / len(df)\n",
    "        metrics['uniqueness'] = 1 - duplicate_ratio\n",
    "        \n",
    "        # Overall score (weighted average)\n",
    "        weights = {\n",
    "            'schema_compliance': 0.3,\n",
    "            'statistical_validity': 0.2,\n",
    "            'business_rules': 0.2,\n",
    "            'pii_leakage': 0.2,\n",
    "            'uniqueness': 0.1\n",
    "        }\n",
    "        \n",
    "        metrics['overall_score'] = sum(\n",
    "            metrics[k] * weights[k] for k in weights\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _log_generation(self, df: pd.DataFrame, quality_metrics: Dict):\n",
    "        \"\"\"Audit trail para compliance y debugging\"\"\"\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'record_count': len(df),\n",
    "            'quality_metrics': quality_metrics,\n",
    "            'cost_usd': self.metrics['cost_usd'],\n",
    "            'generation_time_sec': self.metrics['generation_time_sec'],\n",
    "            'cost_per_record': self.metrics['cost_usd'] / len(df) if len(df) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Write to audit log (S3, database, etc)\n",
    "        with open(f'audit_logs/generation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json', 'w') as f:\n",
    "            json.dump(log_entry, f, indent=2)\n",
    "\n",
    "# Airflow DAG para generaci√≥n programada\n",
    "default_args = {\n",
    "    'owner': 'data_engineering',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'synthetic_data_generation',\n",
    "    default_args=default_args,\n",
    "    description='Generate synthetic data for testing environments',\n",
    "    schedule_interval='@weekly',  # Regenerar cada semana\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "def generate_test_data(**context):\n",
    "    \"\"\"Task para generar datos de testing\"\"\"\n",
    "    \n",
    "    platform = SyntheticDataPlatform(config={})\n",
    "    \n",
    "    schema = {\n",
    "        'customer_id': {'type': 'int', 'generator': 'faker', 'faker_method': 'random_int'},\n",
    "        'name': {'type': 'str', 'generator': 'faker', 'faker_method': 'name'},\n",
    "        'email': {'type': 'str', 'generator': 'faker', 'faker_method': 'email'},\n",
    "        'bio': {'type': 'text', 'generator': 'llm'}  # Solo bio usa LLM\n",
    "    }\n",
    "    \n",
    "    df = platform.generate_dataset(schema, count=10000, quality_threshold=0.85)\n",
    "    \n",
    "    # Save to S3\n",
    "    df.to_parquet(f's3://synthetic-data-bucket/test_customers_{datetime.now().date()}.parquet')\n",
    "    \n",
    "    # Push metrics to XCom\n",
    "    context['task_instance'].xcom_push(key='metrics', value=platform.metrics)\n",
    "\n",
    "generate_task = PythonOperator(\n",
    "    task_id='generate_test_data',\n",
    "    python_callable=generate_test_data,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "**Self-Service API (FastAPI):**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional\n",
    "import uuid\n",
    "\n",
    "app = FastAPI(title=\"Synthetic Data API\")\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    schema: Dict\n",
    "    count: int\n",
    "    quality_threshold: float = 0.85\n",
    "    format: str = 'json'  # json, csv, parquet\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    estimated_time_sec: int\n",
    "\n",
    "# In-memory job store (use Redis in production)\n",
    "jobs = {}\n",
    "\n",
    "@app.post('/v1/generate', response_model=GenerationResponse)\n",
    "async def generate_dataset(request: GenerationRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Endpoint para generar datasets sint√©ticos.\n",
    "    \n",
    "    Usage:\n",
    "        POST /v1/generate\n",
    "        {\n",
    "            \"schema\": {...},\n",
    "            \"count\": 1000,\n",
    "            \"quality_threshold\": 0.85\n",
    "        }\n",
    "    \n",
    "    Returns:\n",
    "        {\"job_id\": \"uuid\", \"status\": \"queued\", \"estimated_time_sec\": 120}\n",
    "    \"\"\"\n",
    "    \n",
    "    job_id = str(uuid.uuid4())\n",
    "    \n",
    "    jobs[job_id] = {\n",
    "        'status': 'queued',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'request': request.dict()\n",
    "    }\n",
    "    \n",
    "    # Run generation in background\n",
    "    background_tasks.add_task(run_generation, job_id, request)\n",
    "    \n",
    "    # Estimate time based on count\n",
    "    estimated_time = request.count / 100  # ~100 records/sec\n",
    "    \n",
    "    return GenerationResponse(\n",
    "        job_id=job_id,\n",
    "        status='queued',\n",
    "        estimated_time_sec=int(estimated_time)\n",
    "    )\n",
    "\n",
    "@app.get('/v1/jobs/{job_id}')\n",
    "async def get_job_status(job_id: str):\n",
    "    \"\"\"Check generation job status\"\"\"\n",
    "    \n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(404, \"Job not found\")\n",
    "    \n",
    "    return jobs[job_id]\n",
    "\n",
    "@app.get('/v1/download/{job_id}')\n",
    "async def download_dataset(job_id: str):\n",
    "    \"\"\"Download generated dataset\"\"\"\n",
    "    \n",
    "    job = jobs.get(job_id)\n",
    "    \n",
    "    if not job:\n",
    "        raise HTTPException(404, \"Job not found\")\n",
    "    \n",
    "    if job['status'] != 'completed':\n",
    "        raise HTTPException(400, f\"Job status: {job['status']}\")\n",
    "    \n",
    "    # Return file (use FileResponse for large files)\n",
    "    from fastapi.responses import FileResponse\n",
    "    return FileResponse(\n",
    "        path=job['output_path'],\n",
    "        media_type='application/octet-stream',\n",
    "        filename=f'synthetic_data_{job_id}.parquet'\n",
    "    )\n",
    "\n",
    "async def run_generation(job_id: str, request: GenerationRequest):\n",
    "    \"\"\"Background task para generaci√≥n\"\"\"\n",
    "    \n",
    "    try:\n",
    "        jobs[job_id]['status'] = 'running'\n",
    "        \n",
    "        platform = SyntheticDataPlatform(config={})\n",
    "        df = platform.generate_dataset(\n",
    "            schema=request.schema,\n",
    "            count=request.count,\n",
    "            quality_threshold=request.quality_threshold\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        output_path = f'/tmp/synthetic_{job_id}.parquet'\n",
    "        df.to_parquet(output_path)\n",
    "        \n",
    "        jobs[job_id].update({\n",
    "            'status': 'completed',\n",
    "            'output_path': output_path,\n",
    "            'metrics': platform.metrics,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        jobs[job_id].update({\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'failed_at': datetime.now().isoformat()\n",
    "        })\n",
    "```\n",
    "\n",
    "**ROI Analysis:**\n",
    "\n",
    "```python\n",
    "class SyntheticDataROI:\n",
    "    \"\"\"Calcula ROI de implementar synthetic data platform\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_roi(scenarios: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Compara costo de synthetic data vs alternativas.\n",
    "        \n",
    "        Scenarios:\n",
    "        - Manual data creation\n",
    "        - Production data copies\n",
    "        - Synthetic data platform\n",
    "        \"\"\"\n",
    "        \n",
    "        # Baseline: Manual data creation\n",
    "        manual_cost_per_hour = 50  # Engineer hourly rate\n",
    "        manual_hours_per_dataset = 8  # 1 d√≠a crear dataset realista\n",
    "        datasets_per_month = 4  # Sprints\n",
    "        \n",
    "        manual_monthly_cost = (\n",
    "            manual_cost_per_hour * \n",
    "            manual_hours_per_dataset * \n",
    "            datasets_per_month\n",
    "        )\n",
    "        \n",
    "        # Alternative: Production copies (risk costs)\n",
    "        prod_copy_compute_cost = 100  # Monthly storage/compute\n",
    "        data_breach_risk = 0.05  # 5% chance per year\n",
    "        breach_cost = 500000  # Average breach cost (IBM report)\n",
    "        \n",
    "        prod_copy_monthly_cost = (\n",
    "            prod_copy_compute_cost + \n",
    "            (data_breach_risk / 12) * breach_cost\n",
    "        )\n",
    "        \n",
    "        # Synthetic data platform\n",
    "        platform_llm_cost = 200  # GPT API\n",
    "        platform_infra_cost = 150  # Hosting, storage\n",
    "        platform_dev_cost_amortized = 5000 / 12  # $5K development / 12 months\n",
    "        \n",
    "        synthetic_monthly_cost = (\n",
    "            platform_llm_cost +\n",
    "            platform_infra_cost +\n",
    "            platform_dev_cost_amortized\n",
    "        )\n",
    "        \n",
    "        # Calculate savings\n",
    "        manual_savings = manual_monthly_cost - synthetic_monthly_cost\n",
    "        prod_copy_savings = prod_copy_monthly_cost - synthetic_monthly_cost\n",
    "        \n",
    "        # Time savings\n",
    "        manual_time_hours = manual_hours_per_dataset * datasets_per_month\n",
    "        synthetic_time_hours = 1 * datasets_per_month  # 1hr para configurar/revisar\n",
    "        time_saved_hours = manual_time_hours - synthetic_time_hours\n",
    "        \n",
    "        # ROI calculation\n",
    "        annual_savings = (manual_savings + prod_copy_savings) * 12\n",
    "        initial_investment = 5000  # Development cost\n",
    "        \n",
    "        roi_pct = (annual_savings - initial_investment) / initial_investment * 100\n",
    "        payback_months = initial_investment / (manual_savings + prod_copy_savings)\n",
    "        \n",
    "        return {\n",
    "            'costs': {\n",
    "                'manual_monthly': manual_monthly_cost,\n",
    "                'prod_copy_monthly': prod_copy_monthly_cost,\n",
    "                'synthetic_monthly': synthetic_monthly_cost\n",
    "            },\n",
    "            'savings': {\n",
    "                'vs_manual_monthly': manual_savings,\n",
    "                'vs_prod_copy_monthly': prod_copy_savings,\n",
    "                'annual_total': annual_savings,\n",
    "                'time_saved_hours_monthly': time_saved_hours\n",
    "            },\n",
    "            'roi': {\n",
    "                'roi_pct': roi_pct,\n",
    "                'payback_months': payback_months,\n",
    "                'break_even_date': (\n",
    "                    datetime.now() + timedelta(days=30*payback_months)\n",
    "                ).strftime('%Y-%m-%d')\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Calcular ROI\n",
    "roi_analysis = SyntheticDataROI.calculate_roi({})\n",
    "\n",
    "print(\"üí∞ SYNTHETIC DATA ROI ANALYSIS\\n\")\n",
    "print(\"Monthly Costs:\")\n",
    "print(f\"  Manual creation: ${roi_analysis['costs']['manual_monthly']:,.2f}\")\n",
    "print(f\"  Prod copies: ${roi_analysis['costs']['prod_copy_monthly']:,.2f}\")\n",
    "print(f\"  Synthetic platform: ${roi_analysis['costs']['synthetic_monthly']:,.2f}\")\n",
    "\n",
    "print(\"\\nSavings:\")\n",
    "print(f\"  vs Manual: ${roi_analysis['savings']['vs_manual_monthly']:,.2f}/month\")\n",
    "print(f\"  vs Prod copies: ${roi_analysis['savings']['vs_prod_copy_monthly']:,.2f}/month\")\n",
    "print(f\"  Annual total: ${roi_analysis['savings']['annual_total']:,.2f}\")\n",
    "print(f\"  Time saved: {roi_analysis['savings']['time_saved_hours_monthly']:.0f} hours/month\")\n",
    "\n",
    "print(\"\\nROI:\")\n",
    "print(f\"  ROI: {roi_analysis['roi']['roi_pct']:.0f}%\")\n",
    "print(f\"  Payback: {roi_analysis['roi']['payback_months']:.1f} months\")\n",
    "print(f\"  Break-even: {roi_analysis['roi']['break_even_date']}\")\n",
    "\n",
    "# Typical output:\n",
    "# üí∞ SYNTHETIC DATA ROI ANALYSIS\n",
    "#\n",
    "# Monthly Costs:\n",
    "#   Manual creation: $1,600.00\n",
    "#   Prod copies: $2,183.33\n",
    "#   Synthetic platform: $766.67\n",
    "#\n",
    "# Savings:\n",
    "#   vs Manual: $833.33/month\n",
    "#   vs Prod copies: $1,416.67/month\n",
    "#   Annual total: $27,000.00\n",
    "#   Time saved: 28 hours/month\n",
    "#\n",
    "# ROI:\n",
    "#   ROI: 440%\n",
    "#   Payback: 2.2 months\n",
    "#   Break-even: 2025-01-15\n",
    "```\n",
    "\n",
    "**Success Metrics (Dashboard):**\n",
    "\n",
    "```python\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_synthetic_data_dashboard(metrics_history: List[Dict]):\n",
    "    \"\"\"Genera dashboard para monitorear plataforma\"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # M√©trica 1: Volumen generado\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[m['date'] for m in metrics_history],\n",
    "        y=[m['records_generated'] for m in metrics_history],\n",
    "        name='Records Generated',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    # M√©trica 2: Quality score\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[m['date'] for m in metrics_history],\n",
    "        y=[m['quality_score'] for m in metrics_history],\n",
    "        name='Quality Score',\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Synthetic Data Platform Metrics',\n",
    "        xaxis=dict(title='Date'),\n",
    "        yaxis=dict(title='Records Generated'),\n",
    "        yaxis2=dict(title='Quality Score', overlaying='y', side='right'),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# KPIs a trackear:\n",
    "kpis = {\n",
    "    'volume': {\n",
    "        'records_generated_monthly': 50000,\n",
    "        'growth_mom': 0.15  # 15% growth month-over-month\n",
    "    },\n",
    "    'quality': {\n",
    "        'avg_quality_score': 0.92,\n",
    "        'quality_gate_pass_rate': 0.95  # 95% pasan quality gate\n",
    "    },\n",
    "    'cost': {\n",
    "        'cost_per_1k_records': 0.50,\n",
    "        'cost_reduction_vs_manual': 0.70  # 70% m√°s barato\n",
    "    },\n",
    "    'adoption': {\n",
    "        'active_users': 25,\n",
    "        'datasets_per_week': 12,\n",
    "        'api_requests_per_day': 150\n",
    "    },\n",
    "    'impact': {\n",
    "        'testing_coverage_increase': 0.40,  # 40% m√°s tests gracias a synthetic data\n",
    "        'prod_incidents_reduced': 0.25,  # 25% menos bugs por mejor testing\n",
    "        'time_to_env_setup_reduced': 0.60  # 60% m√°s r√°pido setup de ambientes\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices (Production):**\n",
    "\n",
    "1. **Start Simple, Scale Gradually:**\n",
    "   ```python\n",
    "   Phase 1 (Month 1-2): Generate test data for 1 team\n",
    "   Phase 2 (Month 3-4): Add anonymization for prod copies\n",
    "   Phase 3 (Month 5-6): Self-service API for all teams\n",
    "   Phase 4 (Month 7+): Advanced features (time series, ML augmentation)\n",
    "   ```\n",
    "\n",
    "2. **Monitor Obsessively:**\n",
    "   - Quality score trends (alert if drops below 0.85)\n",
    "   - Cost per record (budget alerts)\n",
    "   - Generation latency (p95 < 5 min)\n",
    "   - API uptime (99.5% SLA)\n",
    "\n",
    "3. **Version Everything:**\n",
    "   - Schema versions (backward compatibility)\n",
    "   - Generated datasets (reproducibility)\n",
    "   - Generation configs (audit trail)\n",
    "\n",
    "4. **Fail Fast:**\n",
    "   - Quality gates (no bad data downstream)\n",
    "   - Cost limits ($500/day max)\n",
    "   - PII detection (block if detected)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ebefa",
   "metadata": {},
   "source": [
    "## 9. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bca64",
   "metadata": {},
   "source": [
    "1. Genera un dataset completo de e-commerce (clientes, productos, pedidos) con relaciones coherentes.\n",
    "2. Crea un generador de logs de aplicaci√≥n realistas para testing de pipelines.\n",
    "3. Implementa anonimizaci√≥n que preserve privacidad diferencial.\n",
    "4. Construye un augmentador de datos para entrenar modelos de ML con pocos ejemplos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
