{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df53bed",
   "metadata": {},
   "source": [
    "# 🔄 Comparación: OpenAI vs Google Gemini\n",
    "\n",
    "**Notebook de Referencia**: Comparación práctica entre OpenAI GPT y Google Gemini para tareas de ingeniería de datos.\n",
    "\n",
    "## Objetivos\n",
    "- Entender diferencias entre proveedores de LLMs\n",
    "- Comparar precios, latencia y calidad de respuestas\n",
    "- Aprender a implementar fallback entre providers\n",
    "- Elegir el modelo adecuado según el caso de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0707f10",
   "metadata": {},
   "source": [
    "## 1. Setup de Ambos Proveedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configurar APIs\n",
    "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "print('✅ OpenAI y Gemini configurados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11168b00",
   "metadata": {},
   "source": [
    "## 2. Funciones de Consulta Unificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e70c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompt: str, model: str = 'gpt-3.5-turbo') -> dict:\n",
    "    \"\"\"Consulta a OpenAI con timing.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'provider': 'OpenAI',\n",
    "        'model': model,\n",
    "        'response': response.choices[0].message.content,\n",
    "        'latency_ms': round(elapsed * 1000, 2),\n",
    "        'tokens': response.usage.total_tokens if hasattr(response, 'usage') else None\n",
    "    }\n",
    "\n",
    "def query_gemini(prompt: str, model: str = 'gemini-1.5-flash') -> dict:\n",
    "    \"\"\"Consulta a Gemini con timing.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    model_gemini = genai.GenerativeModel(model)\n",
    "    response = model_gemini.generate_content(prompt)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'provider': 'Google Gemini',\n",
    "        'model': model,\n",
    "        'response': response.text,\n",
    "        'latency_ms': round(elapsed * 1000, 2),\n",
    "        'tokens': None  # Gemini no retorna token count directamente\n",
    "    }\n",
    "\n",
    "def query_both(prompt: str) -> None:\n",
    "    \"\"\"Consulta ambos proveedores y compara.\"\"\"\n",
    "    print(f'\\n❓ Prompt: {prompt}\\n')\n",
    "    \n",
    "    # OpenAI\n",
    "    result_openai = query_openai(prompt)\n",
    "    print(f'🟢 {result_openai[\"provider\"]} ({result_openai[\"model\"]}) - {result_openai[\"latency_ms\"]}ms')\n",
    "    print(f'   {result_openai[\"response\"][:200]}...')\n",
    "    \n",
    "    # Gemini\n",
    "    result_gemini = query_gemini(prompt)\n",
    "    print(f'\\n🔵 {result_gemini[\"provider\"]} ({result_gemini[\"model\"]}) - {result_gemini[\"latency_ms\"]}ms')\n",
    "    print(f'   {result_gemini[\"response\"][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab61f14",
   "metadata": {},
   "source": [
    "## 3. Comparación: Tareas Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pregunta conceptual\n",
    "query_both('¿Qué es un Data Lakehouse en 2 frases?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1146df",
   "metadata": {},
   "source": [
    "## 4. Comparación: Generación de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abf918",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompt = '''\n",
    "Genera una query SQL que:\n",
    "- Tabla: ventas (fecha DATE, producto TEXT, cantidad INT, total DECIMAL)\n",
    "- Calcula ventas totales por mes del último año\n",
    "- Ordena de mayor a menor\n",
    "'''\n",
    "\n",
    "query_both(sql_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a40f4",
   "metadata": {},
   "source": [
    "## 5. Comparación: Generación de Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a41d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = '''\n",
    "Genera una función Python que:\n",
    "1. Lee un CSV con pandas\n",
    "2. Filtra filas donde columna \"status\" == \"active\"\n",
    "3. Agrupa por \"category\" y suma \"revenue\"\n",
    "4. Retorna un DataFrame ordenado por revenue descendente\n",
    "\n",
    "Solo el código, sin explicaciones.\n",
    "'''\n",
    "\n",
    "query_both(code_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42581c61",
   "metadata": {},
   "source": [
    "## 6. Tabla Comparativa de Precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c08270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pricing_data = [\n",
    "    {\n",
    "        'Provider': 'OpenAI',\n",
    "        'Model': 'gpt-3.5-turbo',\n",
    "        'Input ($/1M tokens)': 0.50,\n",
    "        'Output ($/1M tokens)': 1.50,\n",
    "        'Use Case': 'Desarrollo, prototipos'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'OpenAI',\n",
    "        'Model': 'gpt-4',\n",
    "        'Input ($/1M tokens)': 10.00,\n",
    "        'Output ($/1M tokens)': 30.00,\n",
    "        'Use Case': 'Producción, tareas complejas'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-flash',\n",
    "        'Input ($/1M tokens)': 0.075,\n",
    "        'Output ($/1M tokens)': 0.30,\n",
    "        'Use Case': 'Desarrollo, alto volumen'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-pro',\n",
    "        'Input ($/1M tokens)': 1.25,\n",
    "        'Output ($/1M tokens)': 5.00,\n",
    "        'Use Case': 'Producción, equilibrio costo-calidad'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-pro (128K+)',\n",
    "        'Input ($/1M tokens)': 2.50,\n",
    "        'Output ($/1M tokens)': 10.00,\n",
    "        'Use Case': 'Contextos largos (documentación)'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_pricing = pd.DataFrame(pricing_data)\n",
    "print('\\n💰 Comparación de Precios (Oct 2025)\\n')\n",
    "print(df_pricing.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f63be",
   "metadata": {},
   "source": [
    "## 7. Estrategia de Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_fallback(prompt: str, prefer_gemini: bool = True) -> dict:\n",
    "    \"\"\"Consulta con fallback automático.\"\"\"\n",
    "    providers = [query_gemini, query_openai] if prefer_gemini else [query_openai, query_gemini]\n",
    "    \n",
    "    for provider_func in providers:\n",
    "        try:\n",
    "            result = provider_func(prompt)\n",
    "            print(f'✅ Respuesta de {result[\"provider\"]}')\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f'⚠️ Error en {provider_func.__name__}: {e}')\n",
    "            continue\n",
    "    \n",
    "    raise Exception('Todos los proveedores fallaron')\n",
    "\n",
    "# Test\n",
    "resultado = query_with_fallback('¿Qué es Apache Kafka?')\n",
    "print(resultado['response'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082f1b5",
   "metadata": {},
   "source": [
    "## 8. Recomendaciones por Caso de Uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db945bd",
   "metadata": {},
   "source": [
    "### ✅ Usa **Gemini 1.5 Flash** para:\n",
    "- Desarrollo y testing\n",
    "- Alto volumen de queries (APIs públicas)\n",
    "- Tareas simples (clasificación, extracción)\n",
    "- Presupuesto limitado\n",
    "- **Ahorro**: 85% vs GPT-3.5, 99% vs GPT-4\n",
    "\n",
    "### ✅ Usa **GPT-3.5 Turbo** para:\n",
    "- Prototipos rápidos con ecosistema OpenAI\n",
    "- Compatibilidad con herramientas existentes\n",
    "- Tareas que requieren función calling\n",
    "\n",
    "### ✅ Usa **Gemini 1.5 Pro** para:\n",
    "- Producción con buen balance costo-calidad\n",
    "- Contextos largos (hasta 1M tokens)\n",
    "- RAG sobre documentación extensa\n",
    "- **Ahorro**: 87% vs GPT-4\n",
    "\n",
    "### ✅ Usa **GPT-4** para:\n",
    "- Tareas críticas que requieren máxima precisión\n",
    "- Razonamiento complejo\n",
    "- Code generation de alta calidad\n",
    "- Cuando el costo no es limitante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e2687",
   "metadata": {},
   "source": [
    "## 9. Benchmark de Latencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(prompts: list, iterations: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark de latencia.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        for _ in range(iterations):\n",
    "            # OpenAI\n",
    "            result_openai = query_openai(prompt)\n",
    "            results.append({\n",
    "                'provider': 'OpenAI GPT-3.5',\n",
    "                'latency_ms': result_openai['latency_ms']\n",
    "            })\n",
    "            \n",
    "            # Gemini\n",
    "            result_gemini = query_gemini(prompt)\n",
    "            results.append({\n",
    "                'provider': 'Gemini Flash',\n",
    "                'latency_ms': result_gemini['latency_ms']\n",
    "            })\n",
    "            \n",
    "            time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results.groupby('provider')['latency_ms'].agg(['mean', 'min', 'max', 'std'])\n",
    "\n",
    "# Ejecutar benchmark\n",
    "test_prompts = [\n",
    "    'Explica ETL en 1 frase',\n",
    "    'SELECT * FROM users WHERE age > 18',\n",
    "    '¿Qué es ACID?'\n",
    "]\n",
    "\n",
    "print('\\n⏱️ Benchmark de Latencia (3 iteraciones)\\n')\n",
    "benchmark_results = benchmark_latency(test_prompts)\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f8d8e",
   "metadata": {},
   "source": [
    "## 10. Configuración Recomendada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración sugerida en .env\n",
    "recommended_config = '''\n",
    "# Desarrollo\n",
    "DEFAULT_LLM_PROVIDER=gemini\n",
    "DEFAULT_GEMINI_MODEL=gemini-1.5-flash\n",
    "FALLBACK_PROVIDER=openai\n",
    "FALLBACK_MODEL=gpt-3.5-turbo\n",
    "\n",
    "# Producción\n",
    "PROD_LLM_PROVIDER=gemini\n",
    "PROD_GEMINI_MODEL=gemini-1.5-pro\n",
    "PROD_FALLBACK_PROVIDER=openai\n",
    "PROD_FALLBACK_MODEL=gpt-4\n",
    "\n",
    "# Límites\n",
    "MAX_TOKENS=4000\n",
    "TIMEOUT_SECONDS=30\n",
    "MAX_RETRIES=3\n",
    "'''\n",
    "\n",
    "print(recommended_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64e90c",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b1616",
   "metadata": {},
   "source": [
    "### 🎯 Resumen\n",
    "\n",
    "**Ganador en Costo**: Gemini Flash (85-99% más barato)\n",
    "**Ganador en Calidad**: GPT-4 (ligeramente superior en tareas complejas)\n",
    "**Mejor Balance**: Gemini Pro (calidad cercana a GPT-4 a 87% menos costo)\n",
    "**Mejor Latencia**: Variable, depende de la región y carga\n",
    "\n",
    "### 💡 Estrategia Recomendada\n",
    "\n",
    "1. **Desarrollo**: Gemini Flash con fallback a GPT-3.5\n",
    "2. **Producción no-crítica**: Gemini Pro\n",
    "3. **Producción crítica**: GPT-4 con caché agresivo\n",
    "4. **Siempre**: Implementa fallback entre proveedores\n",
    "5. **Monitoreo**: Trackea costos, latencia y calidad de respuestas\n",
    "\n",
    "### 🔧 Próximos Pasos\n",
    "\n",
    "- Implementa un router inteligente que elija el modelo según la tarea\n",
    "- Usa modelos pequeños (Flash/3.5) para clasificación\n",
    "- Usa modelos grandes (GPT-4) solo para generación compleja\n",
    "- Considera modelos locales (Ollama) para datos sensibles"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
