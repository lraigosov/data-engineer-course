{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df53bed",
   "metadata": {},
   "source": [
    "# \ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini\n",
    "\n",
    "**Notebook de Referencia**: Comparaci\u00f3n pr\u00e1ctica entre OpenAI GPT y Google Gemini para tareas de ingenier\u00eda de datos.\n",
    "\n",
    "## Objetivos\n",
    "- Entender diferencias entre proveedores de LLMs\n",
    "- Comparar precios, latencia y calidad de respuestas\n",
    "- Aprender a implementar fallback entre providers\n",
    "- Elegir el modelo adecuado seg\u00fan el caso de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0707f10",
   "metadata": {},
   "source": [
    "## 1. Setup de Ambos Proveedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configurar APIs\n",
    "client_openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "print('\u2705 OpenAI y Gemini configurados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11168b00",
   "metadata": {},
   "source": [
    "## 2. Funciones de Consulta Unificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e70c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompt: str, model: str = 'gpt-3.5-turbo') -> dict:\n",
    "    \"\"\"Consulta a OpenAI con timing.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'provider': 'OpenAI',\n",
    "        'model': model,\n",
    "        'response': response.choices[0].message.content,\n",
    "        'latency_ms': round(elapsed * 1000, 2),\n",
    "        'tokens': response.usage.total_tokens if hasattr(response, 'usage') else None\n",
    "    }\n",
    "\n",
    "def query_gemini(prompt: str, model: str = 'gemini-1.5-flash') -> dict:\n",
    "    \"\"\"Consulta a Gemini con timing.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    model_gemini = genai.GenerativeModel(model)\n",
    "    response = model_gemini.generate_content(prompt)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'provider': 'Google Gemini',\n",
    "        'model': model,\n",
    "        'response': response.text,\n",
    "        'latency_ms': round(elapsed * 1000, 2),\n",
    "        'tokens': None  # Gemini no retorna token count directamente\n",
    "    }\n",
    "\n",
    "def query_both(prompt: str) -> None:\n",
    "    \"\"\"Consulta ambos proveedores y compara.\"\"\"\n",
    "    print(f'\\n\u2753 Prompt: {prompt}\\n')\n",
    "    \n",
    "    # OpenAI\n",
    "    result_openai = query_openai(prompt)\n",
    "    print(f'\ud83d\udfe2 {result_openai[\"provider\"]} ({result_openai[\"model\"]}) - {result_openai[\"latency_ms\"]}ms')\n",
    "    print(f'   {result_openai[\"response\"][:200]}...')\n",
    "    \n",
    "    # Gemini\n",
    "    result_gemini = query_gemini(prompt)\n",
    "    print(f'\\n\ud83d\udd35 {result_gemini[\"provider\"]} ({result_gemini[\"model\"]}) - {result_gemini[\"latency_ms\"]}ms')\n",
    "    print(f'   {result_gemini[\"response\"][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab61f14",
   "metadata": {},
   "source": [
    "## 3. Comparaci\u00f3n: Tareas Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pregunta conceptual\n",
    "query_both('\u00bfQu\u00e9 es un Data Lakehouse en 2 frases?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1146df",
   "metadata": {},
   "source": [
    "## 4. Comparaci\u00f3n: Generaci\u00f3n de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abf918",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompt = '''\n",
    "Genera una query SQL que:\n",
    "- Tabla: ventas (fecha DATE, producto TEXT, cantidad INT, total DECIMAL)\n",
    "- Calcula ventas totales por mes del \u00faltimo a\u00f1o\n",
    "- Ordena de mayor a menor\n",
    "'''\n",
    "\n",
    "query_both(sql_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a40f4",
   "metadata": {},
   "source": [
    "## 5. Comparaci\u00f3n: Generaci\u00f3n de C\u00f3digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a41d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = '''\n",
    "Genera una funci\u00f3n Python que:\n",
    "1. Lee un CSV con pandas\n",
    "2. Filtra filas donde columna \"status\" == \"active\"\n",
    "3. Agrupa por \"category\" y suma \"revenue\"\n",
    "4. Retorna un DataFrame ordenado por revenue descendente\n",
    "\n",
    "Solo el c\u00f3digo, sin explicaciones.\n",
    "'''\n",
    "\n",
    "query_both(code_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42581c61",
   "metadata": {},
   "source": [
    "## 6. Tabla Comparativa de Precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c08270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pricing_data = [\n",
    "    {\n",
    "        'Provider': 'OpenAI',\n",
    "        'Model': 'gpt-3.5-turbo',\n",
    "        'Input ($/1M tokens)': 0.50,\n",
    "        'Output ($/1M tokens)': 1.50,\n",
    "        'Use Case': 'Desarrollo, prototipos'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'OpenAI',\n",
    "        'Model': 'gpt-4',\n",
    "        'Input ($/1M tokens)': 10.00,\n",
    "        'Output ($/1M tokens)': 30.00,\n",
    "        'Use Case': 'Producci\u00f3n, tareas complejas'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-flash',\n",
    "        'Input ($/1M tokens)': 0.075,\n",
    "        'Output ($/1M tokens)': 0.30,\n",
    "        'Use Case': 'Desarrollo, alto volumen'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-pro',\n",
    "        'Input ($/1M tokens)': 1.25,\n",
    "        'Output ($/1M tokens)': 5.00,\n",
    "        'Use Case': 'Producci\u00f3n, equilibrio costo-calidad'\n",
    "    },\n",
    "    {\n",
    "        'Provider': 'Google',\n",
    "        'Model': 'gemini-1.5-pro (128K+)',\n",
    "        'Input ($/1M tokens)': 2.50,\n",
    "        'Output ($/1M tokens)': 10.00,\n",
    "        'Use Case': 'Contextos largos (documentaci\u00f3n)'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_pricing = pd.DataFrame(pricing_data)\n",
    "print('\\n\ud83d\udcb0 Comparaci\u00f3n de Precios (Oct 2025)\\n')\n",
    "print(df_pricing.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f63be",
   "metadata": {},
   "source": [
    "## 7. Estrategia de Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_fallback(prompt: str, prefer_gemini: bool = True) -> dict:\n",
    "    \"\"\"Consulta con fallback autom\u00e1tico.\"\"\"\n",
    "    providers = [query_gemini, query_openai] if prefer_gemini else [query_openai, query_gemini]\n",
    "    \n",
    "    for provider_func in providers:\n",
    "        try:\n",
    "            result = provider_func(prompt)\n",
    "            print(f'\u2705 Respuesta de {result[\"provider\"]}')\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f'\u26a0\ufe0f Error en {provider_func.__name__}: {e}')\n",
    "            continue\n",
    "    \n",
    "    raise Exception('Todos los proveedores fallaron')\n",
    "\n",
    "# Test\n",
    "resultado = query_with_fallback('\u00bfQu\u00e9 es Apache Kafka?')\n",
    "print(resultado['response'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082f1b5",
   "metadata": {},
   "source": [
    "## 8. Recomendaciones por Caso de Uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db945bd",
   "metadata": {},
   "source": [
    "### \u2705 Usa **Gemini 1.5 Flash** para:\n",
    "- Desarrollo y testing\n",
    "- Alto volumen de queries (APIs p\u00fablicas)\n",
    "- Tareas simples (clasificaci\u00f3n, extracci\u00f3n)\n",
    "- Presupuesto limitado\n",
    "- **Ahorro**: 85% vs GPT-3.5, 99% vs GPT-4\n",
    "\n",
    "### \u2705 Usa **GPT-3.5 Turbo** para:\n",
    "- Prototipos r\u00e1pidos con ecosistema OpenAI\n",
    "- Compatibilidad con herramientas existentes\n",
    "- Tareas que requieren funci\u00f3n calling\n",
    "\n",
    "### \u2705 Usa **Gemini 1.5 Pro** para:\n",
    "- Producci\u00f3n con buen balance costo-calidad\n",
    "- Contextos largos (hasta 1M tokens)\n",
    "- RAG sobre documentaci\u00f3n extensa\n",
    "- **Ahorro**: 87% vs GPT-4\n",
    "\n",
    "### \u2705 Usa **GPT-4** para:\n",
    "- Tareas cr\u00edticas que requieren m\u00e1xima precisi\u00f3n\n",
    "- Razonamiento complejo\n",
    "- Code generation de alta calidad\n",
    "- Cuando el costo no es limitante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e2687",
   "metadata": {},
   "source": [
    "## 9. Benchmark de Latencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(prompts: list, iterations: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark de latencia.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        for _ in range(iterations):\n",
    "            # OpenAI\n",
    "            result_openai = query_openai(prompt)\n",
    "            results.append({\n",
    "                'provider': 'OpenAI GPT-3.5',\n",
    "                'latency_ms': result_openai['latency_ms']\n",
    "            })\n",
    "            \n",
    "            # Gemini\n",
    "            result_gemini = query_gemini(prompt)\n",
    "            results.append({\n",
    "                'provider': 'Gemini Flash',\n",
    "                'latency_ms': result_gemini['latency_ms']\n",
    "            })\n",
    "            \n",
    "            time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results.groupby('provider')['latency_ms'].agg(['mean', 'min', 'max', 'std'])\n",
    "\n",
    "# Ejecutar benchmark\n",
    "test_prompts = [\n",
    "    'Explica ETL en 1 frase',\n",
    "    'SELECT * FROM users WHERE age > 18',\n",
    "    '\u00bfQu\u00e9 es ACID?'\n",
    "]\n",
    "\n",
    "print('\\n\u23f1\ufe0f Benchmark de Latencia (3 iteraciones)\\n')\n",
    "benchmark_results = benchmark_latency(test_prompts)\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f8d8e",
   "metadata": {},
   "source": [
    "## 10. Configuraci\u00f3n Recomendada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n sugerida en .env\n",
    "recommended_config = '''\n",
    "# Desarrollo\n",
    "DEFAULT_LLM_PROVIDER=gemini\n",
    "DEFAULT_GEMINI_MODEL=gemini-1.5-flash\n",
    "FALLBACK_PROVIDER=openai\n",
    "FALLBACK_MODEL=gpt-3.5-turbo\n",
    "\n",
    "# Producci\u00f3n\n",
    "PROD_LLM_PROVIDER=gemini\n",
    "PROD_GEMINI_MODEL=gemini-1.5-pro\n",
    "PROD_FALLBACK_PROVIDER=openai\n",
    "PROD_FALLBACK_MODEL=gpt-4\n",
    "\n",
    "# L\u00edmites\n",
    "MAX_TOKENS=4000\n",
    "TIMEOUT_SECONDS=30\n",
    "MAX_RETRIES=3\n",
    "'''\n",
    "\n",
    "print(recommended_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64e90c",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b1616",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf Resumen\n",
    "\n",
    "**Ganador en Costo**: Gemini Flash (85-99% m\u00e1s barato)\n",
    "**Ganador en Calidad**: GPT-4 (ligeramente superior en tareas complejas)\n",
    "**Mejor Balance**: Gemini Pro (calidad cercana a GPT-4 a 87% menos costo)\n",
    "**Mejor Latencia**: Variable, depende de la regi\u00f3n y carga\n",
    "\n",
    "### \ud83d\udca1 Estrategia Recomendada\n",
    "\n",
    "1. **Desarrollo**: Gemini Flash con fallback a GPT-3.5\n",
    "2. **Producci\u00f3n no-cr\u00edtica**: Gemini Pro\n",
    "3. **Producci\u00f3n cr\u00edtica**: GPT-4 con cach\u00e9 agresivo\n",
    "4. **Siempre**: Implementa fallback entre proveedores\n",
    "5. **Monitoreo**: Trackea costos, latencia y calidad de respuestas\n",
    "\n",
    "### \ud83d\udd27 Pr\u00f3ximos Pasos\n",
    "\n",
    "- Implementa un router inteligente que elija el modelo seg\u00fan la tarea\n",
    "- Usa modelos peque\u00f1os (Flash/3.5) para clasificaci\u00f3n\n",
    "- Usa modelos grandes (GPT-4) solo para generaci\u00f3n compleja\n",
    "- Considera modelos locales (Ollama) para datos sensibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\u2190 README del Curso](../../README.md)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83e\udd16 Fundamentos de LLMs y Prompting \u2192](01_fundamentos_llms_prompting.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel GenAI:**\n",
    "- [\ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "- [\ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural](02_generacion_sql_nl2sql.ipynb)\n",
    "- [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs](03_generacion_codigo_etl.ipynb)\n",
    "- [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "- [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb)\n",
    "- [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n](06_agentes_automatizacion.ipynb)\n",
    "- [\u2705 Validaci\u00f3n de Datos con LLMs](07_calidad_validacion_llm.ipynb)\n",
    "- [\ud83c\udfb2 Generaci\u00f3n de Datos Sint\u00e9ticos con LLMs](08_sintesis_aumento_datos.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
