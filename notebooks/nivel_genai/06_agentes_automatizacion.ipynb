{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03361ea",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n\n",
    "\n",
    "Objetivo: construir agentes con LangGraph y AutoGen que automaticen tareas complejas de ingenier\u00eda de datos: debugging, optimizaci\u00f3n de queries, orquestaci\u00f3n de pipelines.\n",
    "\n",
    "- Duraci\u00f3n: 120-150 min\n",
    "- Dificultad: Alta\n",
    "- Stack: LangChain, LangGraph, AutoGen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f25eee",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Agentes Aut\u00f3nomos: De ReAct a Multi-Agent Systems\n",
    "\n",
    "Los **agentes aut\u00f3nomos** son sistemas que combinan LLMs con herramientas (tools) para realizar tareas complejas de forma iterativa, tomando decisiones basadas en observaciones y razonamiento. En Data Engineering, permiten automatizar debugging, optimizaci\u00f3n de queries, monitoreo de pipelines y respuesta a incidentes.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Evoluci\u00f3n de Agentes\n",
    "\n",
    "```\n",
    "2021: ReAct (Reasoning + Acting)      \u2192 LLM decide qu\u00e9 tool usar en cada paso\n",
    "2022: MRKL Systems                     \u2192 Multi-tool reasoning con knowledge bases\n",
    "2023: LangChain Agents                 \u2192 Framework con 100+ tools pre-built\n",
    "2023: AutoGen (Microsoft)              \u2192 Multi-agent collaboration\n",
    "2024: LangGraph                        \u2192 Workflows con estados y ciclos\n",
    "2024: Agentes de c\u00f3digo (Devin, etc.)  \u2192 Autonomous software engineering\n",
    "```\n",
    "\n",
    "### \ud83d\udcd0 Arquitectura de Agentes ReAct\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    AGENT REACT LOOP                              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Input: \"\u00bfCu\u00e1ntas filas tiene la tabla ventas y cu\u00e1l es su      \u2502\n",
    "\u2502          esquema?\"                                               \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 1. THOUGHT (Razonamiento)                             \u2502     \u2502\n",
    "\u2502  \u2502    LLM analiza la pregunta y decide acci\u00f3n            \u2502     \u2502\n",
    "\u2502  \u2502    \"Necesito dos cosas: row count y schema.           \u2502     \u2502\n",
    "\u2502  \u2502     Primero obtendr\u00e9 el row count.\"                   \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 2. ACTION (Acci\u00f3n)                                     \u2502     \u2502\n",
    "\u2502  \u2502    LLM selecciona tool y par\u00e1metros                    \u2502     \u2502\n",
    "\u2502  \u2502    Tool: get_table_row_count                           \u2502     \u2502\n",
    "\u2502  \u2502    Input: {\"table_name\": \"ventas\"}                     \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 3. OBSERVATION (Resultado)                             \u2502     \u2502\n",
    "\u2502  \u2502    Tool ejecuta y retorna resultado                    \u2502     \u2502\n",
    "\u2502  \u2502    Output: \"La tabla ventas tiene 1,250,000 filas.\"   \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 4. THOUGHT (Re-evaluar)                                \u2502     \u2502\n",
    "\u2502  \u2502    \"Tengo el row count. Ahora necesito el schema.\"    \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 5. ACTION                                              \u2502     \u2502\n",
    "\u2502  \u2502    Tool: get_table_schema                              \u2502     \u2502\n",
    "\u2502  \u2502    Input: {\"table_name\": \"ventas\"}                     \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 6. OBSERVATION                                         \u2502     \u2502\n",
    "\u2502  \u2502    Output: \"venta_id (BIGINT), fecha (DATE), ...\"     \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 7. THOUGHT (Finalizar)                                 \u2502     \u2502\n",
    "\u2502  \u2502    \"Tengo toda la informaci\u00f3n necesaria.\"              \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502        \u2193                                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 8. FINAL ANSWER                                        \u2502     \u2502\n",
    "\u2502  \u2502    \"La tabla ventas tiene 1.25M filas. El esquema     \u2502     \u2502\n",
    "\u2502  \u2502     incluye: venta_id (BIGINT), fecha (DATE)...\"      \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Ciclo completo: 2 iteraciones, 2 tools, respuesta completa     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83d\udd27 Implementaci\u00f3n de Agente para Data Engineering\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0, api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Tools para Data Engineering\n",
    "@tool\n",
    "def get_table_metadata(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene metadatos completos de una tabla: row count, schema, particiones, tama\u00f1o.\n",
    "    \n",
    "    Args:\n",
    "        table_name: nombre de la tabla (ej. 'dwh.ventas')\n",
    "    \n",
    "    Returns:\n",
    "        JSON con metadatos\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: conectar a Snowflake/BigQuery/Redshift\n",
    "    metadata_db = {\n",
    "        'dwh.ventas': {\n",
    "            'row_count': 125_000_000,\n",
    "            'size_gb': 45.2,\n",
    "            'schema': {\n",
    "                'venta_id': 'BIGINT PRIMARY KEY',\n",
    "                'fecha': 'DATE NOT NULL',\n",
    "                'cliente_id': 'INT FOREIGN KEY',\n",
    "                'producto_id': 'INT FOREIGN KEY',\n",
    "                'cantidad': 'INT',\n",
    "                'total': 'DECIMAL(10,2)'\n",
    "            },\n",
    "            'partitions': ['fecha'],\n",
    "            'last_updated': '2024-10-30 23:45:00',\n",
    "            'owner': 'data-engineering'\n",
    "        },\n",
    "        'dwh.clientes': {\n",
    "            'row_count': 5_000_000,\n",
    "            'size_gb': 2.8,\n",
    "            'schema': {\n",
    "                'cliente_id': 'INT PRIMARY KEY',\n",
    "                'nombre': 'VARCHAR(100)',\n",
    "                'email': 'VARCHAR(100)',\n",
    "                'ciudad': 'VARCHAR(50)',\n",
    "                'fecha_registro': 'DATE'\n",
    "            },\n",
    "            'partitions': None,\n",
    "            'last_updated': '2024-10-30 22:00:00',\n",
    "            'owner': 'data-engineering'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name not in metadata_db:\n",
    "        return json.dumps({'error': f'Tabla {table_name} no encontrada'})\n",
    "    \n",
    "    return json.dumps(metadata_db[table_name], indent=2)\n",
    "\n",
    "@tool\n",
    "def explain_query_plan(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analiza el execution plan de una query SQL y identifica cuellos de botella.\n",
    "    \n",
    "    Args:\n",
    "        query: query SQL a analizar\n",
    "    \n",
    "    Returns:\n",
    "        An\u00e1lisis del plan de ejecuci\u00f3n con recomendaciones\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: EXPLAIN ANALYZE en la base de datos\n",
    "    analysis = {\n",
    "        'estimated_rows': 1_250_000,\n",
    "        'estimated_cost': 15234.50,\n",
    "        'operations': [\n",
    "            {\n",
    "                'step': 1,\n",
    "                'operation': 'Seq Scan on ventas',\n",
    "                'cost': 12000.00,\n",
    "                'rows': 125_000_000,\n",
    "                'warning': 'Full table scan - considera agregar \u00edndice'\n",
    "            },\n",
    "            {\n",
    "                'step': 2,\n",
    "                'operation': 'Hash Join',\n",
    "                'cost': 3234.50,\n",
    "                'rows': 1_250_000,\n",
    "                'info': 'Join eficiente con hash'\n",
    "            }\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Agregar \u00edndice en ventas.fecha para filtros',\n",
    "            'Considerar particionamiento por fecha',\n",
    "            'Usar SELECT con columnas espec\u00edficas en vez de *'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return json.dumps(analysis, indent=2)\n",
    "\n",
    "@tool\n",
    "def check_pipeline_status(pipeline_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Verifica el estado de un pipeline de Airflow/Prefect.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_name: nombre del DAG/flow\n",
    "    \n",
    "    Returns:\n",
    "        Estado actual y \u00faltimas ejecuciones\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: conectar a Airflow API\n",
    "    pipeline_status = {\n",
    "        'daily_sales_etl': {\n",
    "            'status': 'FAILED',\n",
    "            'last_run': '2024-10-30 23:00:00',\n",
    "            'duration_seconds': 1245,\n",
    "            'error': 'Connection timeout to Snowflake',\n",
    "            'recent_runs': [\n",
    "                {'date': '2024-10-30', 'status': 'FAILED', 'duration': 1245},\n",
    "                {'date': '2024-10-29', 'status': 'SUCCESS', 'duration': 892},\n",
    "                {'date': '2024-10-28', 'status': 'SUCCESS', 'duration': 901}\n",
    "            ],\n",
    "            'next_run': '2024-10-31 23:00:00'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if pipeline_name not in pipeline_status:\n",
    "        return json.dumps({'error': f'Pipeline {pipeline_name} no encontrado'})\n",
    "    \n",
    "    return json.dumps(pipeline_status[pipeline_name], indent=2)\n",
    "\n",
    "@tool\n",
    "def run_data_quality_check(table_name: str, check_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta validaciones de calidad de datos.\n",
    "    \n",
    "    Args:\n",
    "        table_name: tabla a validar\n",
    "        check_type: tipo de check ('nulls', 'duplicates', 'freshness', 'schema')\n",
    "    \n",
    "    Returns:\n",
    "        Resultados de la validaci\u00f3n\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: integrar con Great Expectations, dbt tests, etc.\n",
    "    quality_results = {\n",
    "        'dwh.ventas': {\n",
    "            'nulls': {\n",
    "                'total': 'NULL rate: 0.05%',\n",
    "                'fecha': 'NULL rate: 0%',\n",
    "                'cliente_id': 'NULL rate: 0.2%',\n",
    "                'status': 'PASS'\n",
    "            },\n",
    "            'duplicates': {\n",
    "                'count': 120,\n",
    "                'percentage': '0.0001%',\n",
    "                'status': 'WARNING'\n",
    "            },\n",
    "            'freshness': {\n",
    "                'last_update': '2024-10-30 23:45:00',\n",
    "                'delay_hours': 0.25,\n",
    "                'status': 'PASS'\n",
    "            },\n",
    "            'schema': {\n",
    "                'columns_expected': 6,\n",
    "                'columns_actual': 6,\n",
    "                'mismatches': [],\n",
    "                'status': 'PASS'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name not in quality_results:\n",
    "        return json.dumps({'error': f'No hay checks configurados para {table_name}'})\n",
    "    \n",
    "    if check_type not in quality_results[table_name]:\n",
    "        return json.dumps({'error': f'Check type {check_type} no soportado'})\n",
    "    \n",
    "    return json.dumps(quality_results[table_name][check_type], indent=2)\n",
    "\n",
    "@tool\n",
    "def suggest_query_optimization(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analiza una query y sugiere optimizaciones espec\u00edficas.\n",
    "    \n",
    "    Args:\n",
    "        query: query SQL a optimizar\n",
    "    \n",
    "    Returns:\n",
    "        Lista de sugerencias con query mejorada\n",
    "    \"\"\"\n",
    "    suggestions = []\n",
    "    optimized_query = query\n",
    "    \n",
    "    # An\u00e1lisis de patrones anti-pattern\n",
    "    if 'SELECT *' in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'SELECT * es ineficiente',\n",
    "            'impact': 'Alto - lee columnas innecesarias',\n",
    "            'recommendation': 'Especificar solo columnas necesarias',\n",
    "            'example': 'SELECT id, fecha, total FROM ...'\n",
    "        })\n",
    "    \n",
    "    if 'WHERE' not in query.upper() and 'JOIN' in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'JOIN sin filtros WHERE',\n",
    "            'impact': 'Alto - procesa todos los registros',\n",
    "            'recommendation': 'Agregar filtros WHERE para reducir dataset',\n",
    "            'example': 'WHERE fecha >= CURRENT_DATE - 30'\n",
    "        })\n",
    "    \n",
    "    if 'DISTINCT' in query.upper() and 'GROUP BY' not in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'DISTINCT sin GROUP BY puede ser lento',\n",
    "            'impact': 'Medio - sort costoso',\n",
    "            'recommendation': 'Considerar GROUP BY si es posible',\n",
    "            'example': 'GROUP BY columnas en vez de DISTINCT'\n",
    "        })\n",
    "    \n",
    "    return json.dumps({\n",
    "        'original_query': query,\n",
    "        'suggestions': suggestions,\n",
    "        'priority': 'HIGH' if len(suggestions) >= 2 else 'MEDIUM'\n",
    "    }, indent=2)\n",
    "\n",
    "# Lista de tools\n",
    "tools = [\n",
    "    get_table_metadata,\n",
    "    explain_query_plan,\n",
    "    check_pipeline_status,\n",
    "    run_data_quality_check,\n",
    "    suggest_query_optimization\n",
    "]\n",
    "\n",
    "# Prompt customizado para Data Engineering\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un Senior Data Engineer experto en:\n",
    "- An\u00e1lisis y optimizaci\u00f3n de queries SQL\n",
    "- Debugging de pipelines de datos (Airflow, dbt, Spark)\n",
    "- Data quality y governance\n",
    "- Arquitectura de data warehouses (Snowflake, BigQuery, Redshift)\n",
    "\n",
    "Tu trabajo es ayudar a resolver problemas usando las tools disponibles.\n",
    "Siempre razona paso a paso y usa m\u00faltiples tools si es necesario para dar una respuesta completa.\n",
    "\n",
    "Cuando analices problemas:\n",
    "1. Primero obt\u00e9n contexto (metadatos, estado actual)\n",
    "2. Identifica el root cause\n",
    "3. Sugiere soluciones accionables\n",
    "4. Prioriza por impacto\n",
    "\n",
    "S\u00e9 conciso pero preciso.\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Crear agente\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    max_iterations=10,  # L\u00edmite de seguridad\n",
    "    max_execution_time=60,  # Timeout 60 segundos\n",
    "    handle_parsing_errors=True  # Manejo robusto de errores\n",
    ")\n",
    "\n",
    "# Ejemplo 1: Investigaci\u00f3n de tabla\n",
    "print(\"=\" * 80)\n",
    "print(\"CASO 1: An\u00e1lisis de tabla\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result1 = agent_executor.invoke({\n",
    "    \"input\": \"\"\"Necesito analizar la tabla dwh.ventas.\n",
    "    \u00bfCu\u00e1ntas filas tiene? \u00bfEst\u00e1 bien particionada? \u00bfCu\u00e1l es su tama\u00f1o?\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\ud83d\udcca Respuesta del agente:\")\n",
    "print(result1['output'])\n",
    "\n",
    "# Ejemplo 2: Debugging de pipeline\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 2: Pipeline fallando\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result2 = agent_executor.invoke({\n",
    "    \"input\": \"\"\"El pipeline 'daily_sales_etl' est\u00e1 fallando.\n",
    "    Investiga qu\u00e9 pas\u00f3 y sugiere c\u00f3mo solucionarlo.\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\ud83d\udd27 Respuesta del agente:\")\n",
    "print(result2['output'])\n",
    "\n",
    "# Ejemplo 3: Optimizaci\u00f3n de query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 3: Query lenta\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "slow_query = \"\"\"\n",
    "SELECT * \n",
    "FROM dwh.ventas v\n",
    "JOIN dwh.clientes c ON v.cliente_id = c.cliente_id\n",
    "\"\"\"\n",
    "\n",
    "result3 = agent_executor.invoke({\n",
    "    \"input\": f\"\"\"Esta query es muy lenta: {slow_query}\n",
    "    Analiza el execution plan y sugiere optimizaciones.\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\u26a1 Respuesta del agente:\")\n",
    "print(result3['output'])\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Comparaci\u00f3n de Frameworks de Agentes\n",
    "\n",
    "| Framework | Tipo | Complejidad | Casos de Uso | Estado |\n",
    "|-----------|------|-------------|--------------|---------|\n",
    "| **LangChain Agents** | Single agent + tools | Medio | Tareas simples con 1-5 tools | Estable, producci\u00f3n |\n",
    "| **LangGraph** | Stateful workflows | Alto | Workflows complejos, ciclos, branches | Estable, recomendado |\n",
    "| **AutoGen** | Multi-agent collaboration | Muy alto | M\u00faltiples agentes especializados | Experimental |\n",
    "| **CrewAI** | Hierarchical multi-agent | Alto | Equipos con roles y jerarqu\u00edas | Emergente |\n",
    "| **BabyAGI / AutoGPT** | Autonomous goal-oriented | Muy alto | Investigaci\u00f3n, no producci\u00f3n | Experimental |\n",
    "\n",
    "### \ud83c\udfaf Anatom\u00eda de una Tool de Calidad\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class QueryOptimizationInput(BaseModel):\n",
    "    \"\"\"Schema de input para validaci\u00f3n.\"\"\"\n",
    "    query: str = Field(description=\"Query SQL a optimizar\")\n",
    "    database: Literal[\"snowflake\", \"bigquery\", \"redshift\"] = Field(\n",
    "        default=\"snowflake\",\n",
    "        description=\"Tipo de base de datos\"\n",
    "    )\n",
    "    max_cost: float = Field(\n",
    "        default=1000.0,\n",
    "        description=\"Costo m\u00e1ximo aceptable del query\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=QueryOptimizationInput)\n",
    "def optimize_query_production(query: str, database: str = \"snowflake\", max_cost: float = 1000.0) -> str:\n",
    "    \"\"\"\n",
    "    Tool de producci\u00f3n para optimizaci\u00f3n de queries con validaci\u00f3n.\n",
    "    \n",
    "    Caracter\u00edsticas:\n",
    "    - Type hints y validation con Pydantic\n",
    "    - Error handling robusto\n",
    "    - Logging de todas las invocaciones\n",
    "    - Timeouts y rate limiting\n",
    "    - Retries con exponential backoff\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"optimize_query invoked: database={database}, max_cost={max_cost}\")\n",
    "    \n",
    "    try:\n",
    "        # Validaci\u00f3n de input\n",
    "        if len(query) > 10000:\n",
    "            raise ValueError(\"Query demasiado larga (>10K caracteres)\")\n",
    "        \n",
    "        if not query.strip().upper().startswith('SELECT'):\n",
    "            raise ValueError(\"Solo queries SELECT est\u00e1n soportadas\")\n",
    "        \n",
    "        # Simulaci\u00f3n de an\u00e1lisis con retry\n",
    "        @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "        def analyze_with_retry():\n",
    "            # En producci\u00f3n: llamar a servicio de an\u00e1lisis\n",
    "            time.sleep(0.5)  # Simular latencia\n",
    "            return {\n",
    "                'estimated_cost': 850.0,\n",
    "                'suggestions': ['Agregar \u00edndice en fecha', 'Usar columnas espec\u00edficas'],\n",
    "                'optimized_query': query.replace('SELECT *', 'SELECT id, fecha')\n",
    "            }\n",
    "        \n",
    "        result = analyze_with_retry()\n",
    "        \n",
    "        # Validaci\u00f3n de output\n",
    "        if result['estimated_cost'] > max_cost:\n",
    "            logger.warning(f\"Query costo {result['estimated_cost']} excede m\u00e1ximo {max_cost}\")\n",
    "            return json.dumps({\n",
    "                'status': 'WARNING',\n",
    "                'message': f'Costo estimado {result[\"estimated_cost\"]} excede l\u00edmite {max_cost}',\n",
    "                'suggestions': result['suggestions']\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Optimizaci\u00f3n exitosa: costo reducido a {result['estimated_cost']}\")\n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en optimize_query: {str(e)}\")\n",
    "        return json.dumps({'error': str(e), 'status': 'FAILED'})\n",
    "```\n",
    "\n",
    "### \ud83d\udea8 Patrones de Seguridad y Control\n",
    "\n",
    "```python\n",
    "from typing import Callable\n",
    "import functools\n",
    "\n",
    "def require_approval(func: Callable) -> Callable:\n",
    "    \"\"\"Decorator para tools que requieren aprobaci\u00f3n humana.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"\\n\u26a0\ufe0f  ACCI\u00d3N REQUIERE APROBACI\u00d3N:\")\n",
    "        print(f\"   Tool: {func.__name__}\")\n",
    "        print(f\"   Args: {args}, {kwargs}\")\n",
    "        \n",
    "        approval = input(\"   \u00bfAprobar? (y/n): \")\n",
    "        if approval.lower() != 'y':\n",
    "            return json.dumps({'status': 'REJECTED', 'reason': 'User declined'})\n",
    "        \n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@tool\n",
    "@require_approval\n",
    "def drop_table(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina una tabla (PELIGROSO - requiere aprobaci\u00f3n).\n",
    "    \n",
    "    Args:\n",
    "        table_name: nombre de la tabla a eliminar\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: ejecutar DROP TABLE\n",
    "    return json.dumps({\n",
    "        'status': 'SUCCESS',\n",
    "        'message': f'Tabla {table_name} eliminada',\n",
    "        'timestamp': '2024-10-30 23:50:00'\n",
    "    })\n",
    "\n",
    "# Rate limiting para evitar costos excesivos\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Rate limiter para tools costosas.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_calls: int, time_window: int):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_window = time_window\n",
    "        self.calls = deque()\n",
    "    \n",
    "    def allow_call(self) -> bool:\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remover llamadas fuera de la ventana\n",
    "        while self.calls and self.calls[0] < now - self.time_window:\n",
    "            self.calls.popleft()\n",
    "        \n",
    "        if len(self.calls) >= self.max_calls:\n",
    "            return False\n",
    "        \n",
    "        self.calls.append(now)\n",
    "        return True\n",
    "\n",
    "# Uso\n",
    "limiter = RateLimiter(max_calls=10, time_window=60)  # 10 calls/minuto\n",
    "\n",
    "@tool\n",
    "def expensive_llm_analysis(text: str) -> str:\n",
    "    \"\"\"Tool costosa con rate limiting.\"\"\"\n",
    "    if not limiter.allow_call():\n",
    "        return json.dumps({\n",
    "            'error': 'Rate limit exceeded',\n",
    "            'message': 'M\u00e1ximo 10 llamadas por minuto'\n",
    "        })\n",
    "    \n",
    "    # An\u00e1lisis costoso...\n",
    "    return json.dumps({'result': 'analysis complete'})\n",
    "```\n",
    "\n",
    "### \ud83d\udcb0 Optimizaci\u00f3n de Costos\n",
    "\n",
    "```python\n",
    "# Estrategia 1: Usar modelos peque\u00f1os para decisiones simples\n",
    "llm_cheap = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)  # $0.50/$1.50 por 1M tokens\n",
    "llm_expensive = ChatOpenAI(model='gpt-4o', temperature=0)     # $2.50/$10.00 por 1M tokens\n",
    "\n",
    "# Estrategia 2: Limitar iteraciones\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    max_iterations=5,  # Evitar loops infinitos\n",
    "    early_stopping_method=\"generate\"  # Forzar respuesta despu\u00e9s de max_iterations\n",
    ")\n",
    "\n",
    "# Estrategia 3: Cache de decisiones\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_tool_call(tool_name: str, input_hash: str):\n",
    "    \"\"\"Cache de resultados de tools determin\u00edsticas.\"\"\"\n",
    "    # Si el tool siempre retorna lo mismo para el mismo input, cachear\n",
    "    pass\n",
    "\n",
    "# Estrategia 4: Monitoring de costos\n",
    "import tiktoken\n",
    "\n",
    "def estimate_agent_cost(messages: List[dict], model: str = \"gpt-4o\") -> float:\n",
    "    \"\"\"Estima costo de una conversaci\u00f3n de agente.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    total_tokens = sum(len(encoding.encode(msg['content'])) for msg in messages)\n",
    "    \n",
    "    # Precios por 1M tokens (input / output)\n",
    "    prices = {\n",
    "        'gpt-4o': (2.50, 10.00),\n",
    "        'gpt-3.5-turbo': (0.50, 1.50)\n",
    "    }\n",
    "    \n",
    "    # Asumir 50% input, 50% output\n",
    "    input_tokens = total_tokens * 0.5\n",
    "    output_tokens = total_tokens * 0.5\n",
    "    \n",
    "    input_price, output_price = prices[model]\n",
    "    cost = (input_tokens / 1_000_000 * input_price) + (output_tokens / 1_000_000 * output_price)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Ejemplo\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'Eres un data engineer...'},\n",
    "    {'role': 'user', 'content': 'Analiza esta tabla...'},\n",
    "    {'role': 'assistant', 'content': 'Voy a obtener los metadatos...'}\n",
    "]\n",
    "\n",
    "cost = estimate_agent_cost(messages, model='gpt-4o')\n",
    "print(f\"Costo estimado: ${cost:.4f}\")\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Reglas de Oro para Agentes en Producci\u00f3n\n",
    "\n",
    "1. **Siempre validar inputs**: Tools reciben texto generado por LLM (puede ser malicioso)\n",
    "2. **Timeouts obligatorios**: Max execution time para evitar loops infinitos\n",
    "3. **Human-in-the-loop**: Acciones destructivas (DROP, DELETE, TRUNCATE) requieren aprobaci\u00f3n\n",
    "4. **Logging exhaustivo**: Toda decisi\u00f3n del agente debe loggearse para debugging\n",
    "5. **Rate limiting**: Proteger APIs externas y controlar costos\n",
    "6. **Graceful degradation**: Si tool falla, el agente debe continuar con otras tools\n",
    "7. **Monitoring de costos**: Alertar si costo por sesi\u00f3n excede threshold\n",
    "8. **Testing con mocks**: Probar agentes sin ejecutar tools reales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903fa64",
   "metadata": {},
   "source": [
    "## \ud83d\udd04 LangGraph: Workflows Complejos con Estados y Ciclos\n",
    "\n",
    "**LangGraph** es un framework para construir agentes con **flujos de trabajo complejos** que incluyen estados persistentes, ramificaciones condicionales, ciclos (loops) y m\u00faltiples rutas de ejecuci\u00f3n. Ideal para pipelines de Data Engineering donde las decisiones dependen de resultados previos.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Arquitectura de LangGraph\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502               LANGGRAPH: STATEFUL WORKFLOW                       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Componentes Principales:                                        \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  1\ufe0f\u20e3 STATE: Objeto compartido entre todos los nodos             \u2502\n",
    "\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "\u2502     \u2502 class DataPipelineState(TypedDict):      \u2502               \u2502\n",
    "\u2502     \u2502     query: str                            \u2502               \u2502\n",
    "\u2502     \u2502     validated: bool                       \u2502               \u2502\n",
    "\u2502     \u2502     optimized: bool                       \u2502               \u2502\n",
    "\u2502     \u2502     execution_plan: dict                  \u2502               \u2502\n",
    "\u2502     \u2502     results: list                         \u2502               \u2502\n",
    "\u2502     \u2502     errors: list                          \u2502               \u2502\n",
    "\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  2\ufe0f\u20e3 NODES: Funciones que modifican el estado                   \u2502\n",
    "\u2502     def validate_node(state) -> state:                          \u2502\n",
    "\u2502         state['validated'] = check_syntax(state['query'])       \u2502\n",
    "\u2502         return state                                            \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  3\ufe0f\u20e3 EDGES: Conexiones entre nodos (condicionales o fijas)      \u2502\n",
    "\u2502     - Simple edge: validate \u2192 optimize                          \u2502\n",
    "\u2502     - Conditional edge: if validated \u2192 optimize else \u2192 error    \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  4\ufe0f\u20e3 GRAPH: Estructura del workflow                             \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502                    START                                         \u2502\n",
    "\u2502                      \u2502                                           \u2502\n",
    "\u2502                      \u25bc                                           \u2502\n",
    "\u2502              [Validate Query]                                    \u2502\n",
    "\u2502                      \u2502                                           \u2502\n",
    "\u2502                      \u25bc                                           \u2502\n",
    "\u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                       \u2502\n",
    "\u2502                  \u2502 Valid? \u2502                                      \u2502\n",
    "\u2502                  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518                                       \u2502\n",
    "\u2502                      \u2502                                           \u2502\n",
    "\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                \u2502\n",
    "\u2502            \u2502                   \u2502                                \u2502\n",
    "\u2502          Yes                  No                                \u2502\n",
    "\u2502            \u2502                   \u2502                                \u2502\n",
    "\u2502            \u25bc                   \u25bc                                \u2502\n",
    "\u2502       [Optimize]        [Error Handler]                         \u2502\n",
    "\u2502            \u2502                   \u2502                                \u2502\n",
    "\u2502            \u25bc                   \u25bc                                \u2502\n",
    "\u2502        [Execute]           [Report]                             \u2502\n",
    "\u2502            \u2502                   \u2502                                \u2502\n",
    "\u2502            \u25bc                   \u25bc                                \u2502\n",
    "\u2502        [Monitor]              END                               \u2502\n",
    "\u2502            \u2502                                                     \u2502\n",
    "\u2502            \u25bc                                                     \u2502\n",
    "\u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                               \u2502\n",
    "\u2502        \u2502Success?\u2502                                               \u2502\n",
    "\u2502        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                                               \u2502\n",
    "\u2502            \u2502                                                     \u2502\n",
    "\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n",
    "\u2502      \u2502           \u2502                                              \u2502\n",
    "\u2502     Yes         No                                              \u2502\n",
    "\u2502      \u2502           \u2502                                              \u2502\n",
    "\u2502      \u25bc           \u25bc                                              \u2502\n",
    "\u2502     END      [Retry]                                            \u2502\n",
    "\u2502                \u2502                                                \u2502\n",
    "\u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n",
    "\u2502                              \u2502                                  \u2502\n",
    "\u2502                          (loop back                             \u2502\n",
    "\u2502                           to Execute)                           \u2502\n",
    "\u2502                              \u2502                                  \u2502\n",
    "\u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502                                                             \u2502   \u2502\n",
    "\u2502                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                  \u2502\n",
    "\u2502                              \u25bc                                  \u2502\n",
    "\u2502                          [Execute]                              \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83d\udd27 Implementaci\u00f3n: Pipeline ETL con Validaci\u00f3n y Retry\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import time\n",
    "\n",
    "# 1. Definir estado del workflow\n",
    "class ETLPipelineState(TypedDict):\n",
    "    \"\"\"Estado compartido del pipeline ETL.\"\"\"\n",
    "    source_query: str\n",
    "    validated: bool\n",
    "    optimized: bool\n",
    "    execution_plan: dict\n",
    "    executed: bool\n",
    "    results: Annotated[list, operator.add]  # operator.add permite append\n",
    "    errors: Annotated[list, operator.add]\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "\n",
    "# 2. Definir nodos (funciones que transforman el estado)\n",
    "\n",
    "def validate_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Valida sintaxis de la query.\"\"\"\n",
    "    print(f\"\\n\ud83d\udd0d VALIDATE: Validando query...\")\n",
    "    \n",
    "    query = state['source_query']\n",
    "    \n",
    "    # Validaciones b\u00e1sicas\n",
    "    errors = []\n",
    "    if not query.strip().upper().startswith('SELECT'):\n",
    "        errors.append('Query debe comenzar con SELECT')\n",
    "    \n",
    "    if query.count('(') != query.count(')'):\n",
    "        errors.append('Par\u00e9ntesis desbalanceados')\n",
    "    \n",
    "    if 'SELCT' in query.upper():\n",
    "        errors.append('Typo detectado: SELCT \u2192 SELECT')\n",
    "    \n",
    "    # LLM para validaci\u00f3n sem\u00e1ntica avanzada\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)  # Modelo barato para validaci\u00f3n\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"Eres un experto en SQL. Valida la query y reporta errores.\"),\n",
    "        HumanMessage(content=f\"Query: {query}\\n\\n\u00bfHay errores de sintaxis o l\u00f3gica?\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        llm_feedback = response.content\n",
    "        \n",
    "        if 'error' in llm_feedback.lower() or 'incorrecto' in llm_feedback.lower():\n",
    "            errors.append(f\"LLM feedback: {llm_feedback}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Error al validar con LLM: {str(e)}\")\n",
    "    \n",
    "    if errors:\n",
    "        state['validated'] = False\n",
    "        state['errors'].extend(errors)\n",
    "        print(f\"   \u274c Errores encontrados: {errors}\")\n",
    "    else:\n",
    "        state['validated'] = True\n",
    "        print(f\"   \u2705 Query v\u00e1lida\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def optimize_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Optimiza la query usando LLM.\"\"\"\n",
    "    print(f\"\\n\u26a1 OPTIMIZE: Optimizando query...\")\n",
    "    \n",
    "    if not state['validated']:\n",
    "        print(\"   \u26a0\ufe0f Query no validada, skip optimization\")\n",
    "        return state\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0.3)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"Eres un experto en optimizaci\u00f3n de queries SQL.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Optimiza esta query:\n",
    "\n",
    "{state['source_query']}\n",
    "\n",
    "Aplica:\n",
    "- \u00cdndices sugeridos\n",
    "- Reescritura m\u00e1s eficiente\n",
    "- Evitar subqueries costosas\n",
    "- Usar CTEs cuando sea apropiado\n",
    "\n",
    "Retorna la query optimizada:\n",
    "\"\"\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        optimized_query = response.content.strip()\n",
    "        state['optimized'] = True\n",
    "        state['source_query'] = optimized_query  # Reemplazar con versi\u00f3n optimizada\n",
    "        print(f\"   \u2705 Query optimizada\")\n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Error al optimizar: {str(e)}\")\n",
    "        print(f\"   \u274c Error: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def execute_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Ejecuta la query (simulado).\"\"\"\n",
    "    print(f\"\\n\ud83d\ude80 EXECUTE: Ejecutando query...\")\n",
    "    \n",
    "    # Simular ejecuci\u00f3n (en producci\u00f3n: ejecutar en DB real)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Simular resultados\n",
    "    fake_results = [\n",
    "        {'id': 1, 'producto': 'Laptop', 'ventas': 150000},\n",
    "        {'id': 2, 'producto': 'Mouse', 'ventas': 45000},\n",
    "        {'id': 3, 'producto': 'Teclado', 'ventas': 78000}\n",
    "    ]\n",
    "    \n",
    "    state['executed'] = True\n",
    "    state['results'].extend(fake_results)\n",
    "    print(f\"   \u2705 Query ejecutada: {len(fake_results)} resultados\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def monitor_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Monitorea m\u00e9tricas de ejecuci\u00f3n.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcca MONITOR: Verificando m\u00e9tricas...\")\n",
    "    \n",
    "    # Verificar si hay resultados\n",
    "    if len(state['results']) == 0:\n",
    "        state['errors'].append(\"No se obtuvieron resultados\")\n",
    "        print(\"   \u274c Sin resultados\")\n",
    "        return state\n",
    "    \n",
    "    # Verificar l\u00edmites (ej. no m\u00e1s de 1M rows)\n",
    "    if len(state['results']) > 1_000_000:\n",
    "        state['errors'].append(f\"Demasiados resultados: {len(state['results'])}\")\n",
    "        print(f\"   \u26a0\ufe0f Demasiados resultados\")\n",
    "        return state\n",
    "    \n",
    "    print(f\"   \u2705 M\u00e9tricas OK: {len(state['results'])} rows\")\n",
    "    return state\n",
    "\n",
    "def retry_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Incrementa contador de retry.\"\"\"\n",
    "    print(f\"\\n\ud83d\udd04 RETRY: Intento {state['retry_count'] + 1}/{state['max_retries']}...\")\n",
    "    \n",
    "    state['retry_count'] += 1\n",
    "    state['executed'] = False  # Reset para volver a ejecutar\n",
    "    \n",
    "    # Limpiar errores del intento anterior\n",
    "    state['errors'] = []\n",
    "    \n",
    "    time.sleep(2)  # Backoff\n",
    "    \n",
    "    return state\n",
    "\n",
    "def error_handler_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Maneja errores y genera reporte.\"\"\"\n",
    "    print(f\"\\n\u274c ERROR HANDLER: Procesando errores...\")\n",
    "    \n",
    "    errors_summary = \"\\n\".join(f\"  - {err}\" for err in state['errors'])\n",
    "    print(f\"Errores detectados:\\n{errors_summary}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# 3. Definir funciones de routing (conditional edges)\n",
    "\n",
    "def should_optimize(state: ETLPipelineState) -> Literal[\"optimize\", \"error\"]:\n",
    "    \"\"\"Decide si optimizar o manejar error.\"\"\"\n",
    "    return \"optimize\" if state['validated'] else \"error\"\n",
    "\n",
    "def should_retry(state: ETLPipelineState) -> Literal[\"retry\", \"end\"]:\n",
    "    \"\"\"Decide si reintentar o terminar.\"\"\"\n",
    "    \n",
    "    has_errors = len(state['errors']) > 0\n",
    "    can_retry = state['retry_count'] < state['max_retries']\n",
    "    \n",
    "    if has_errors and can_retry:\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# 4. Construir el grafo\n",
    "\n",
    "workflow = StateGraph(ETLPipelineState)\n",
    "\n",
    "# Agregar nodos\n",
    "workflow.add_node(\"validate\", validate_query_node)\n",
    "workflow.add_node(\"optimize\", optimize_query_node)\n",
    "workflow.add_node(\"execute\", execute_query_node)\n",
    "workflow.add_node(\"monitor\", monitor_node)\n",
    "workflow.add_node(\"retry\", retry_node)\n",
    "workflow.add_node(\"error_handler\", error_handler_node)\n",
    "\n",
    "# Definir edges\n",
    "workflow.set_entry_point(\"validate\")\n",
    "\n",
    "# Conditional edge despu\u00e9s de validaci\u00f3n\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    should_optimize,\n",
    "    {\n",
    "        \"optimize\": \"optimize\",\n",
    "        \"error\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edge simple: optimize \u2192 execute\n",
    "workflow.add_edge(\"optimize\", \"execute\")\n",
    "\n",
    "# Edge simple: execute \u2192 monitor\n",
    "workflow.add_edge(\"execute\", \"monitor\")\n",
    "\n",
    "# Conditional edge despu\u00e9s de monitor\n",
    "workflow.add_conditional_edges(\n",
    "    \"monitor\",\n",
    "    should_retry,\n",
    "    {\n",
    "        \"retry\": \"retry\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edge para ciclo de retry: retry \u2192 execute\n",
    "workflow.add_edge(\"retry\", \"execute\")\n",
    "\n",
    "# Edge para error handler \u2192 END\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "\n",
    "# Compilar el grafo\n",
    "app = workflow.compile()\n",
    "\n",
    "# 5. Ejecutar el workflow\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EJECUTANDO WORKFLOW ETL CON LANGGRAPH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "initial_state = {\n",
    "    'source_query': '''\n",
    "        SELECT p.producto, SUM(v.total) as ventas_totales\n",
    "        FROM ventas v\n",
    "        JOIN productos p ON v.producto_id = p.id\n",
    "        WHERE v.fecha >= '2024-01-01'\n",
    "        GROUP BY p.producto\n",
    "        ORDER BY ventas_totales DESC\n",
    "        LIMIT 10\n",
    "    ''',\n",
    "    'validated': False,\n",
    "    'optimized': False,\n",
    "    'execution_plan': {},\n",
    "    'executed': False,\n",
    "    'results': [],\n",
    "    'errors': [],\n",
    "    'retry_count': 0,\n",
    "    'max_retries': 3\n",
    "}\n",
    "\n",
    "# Ejecutar workflow\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\u2705 Validado: {final_state['validated']}\")\n",
    "print(f\"\u2705 Optimizado: {final_state['optimized']}\")\n",
    "print(f\"\u2705 Ejecutado: {final_state['executed']}\")\n",
    "print(f\"\ud83d\udcca Resultados: {len(final_state['results'])} filas\")\n",
    "print(f\"\u274c Errores: {len(final_state['errors'])}\")\n",
    "print(f\"\ud83d\udd04 Reintentos: {final_state['retry_count']}\")\n",
    "\n",
    "if final_state['results']:\n",
    "    print(\"\\nPrimeros 3 resultados:\")\n",
    "    for row in final_state['results'][:3]:\n",
    "        print(f\"  {row}\")\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Visualizaci\u00f3n del Grafo\n",
    "\n",
    "```python\n",
    "# LangGraph permite visualizar el workflow\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f No se puede visualizar (instalar graphviz): {e}\")\n",
    "    print(\"\\nEstructura del grafo:\")\n",
    "    print(app.get_graph().to_json())\n",
    "```\n",
    "\n",
    "### \ud83d\udcbe Persistencia de Estado (Checkpointing)\n",
    "\n",
    "LangGraph permite guardar el estado del workflow y recuperarlo despu\u00e9s:\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Crear grafo con checkpointing\n",
    "memory = MemorySaver()\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Ejecutar con thread_id para identificar la sesi\u00f3n\n",
    "config = {\"configurable\": {\"thread_id\": \"pipeline-123\"}}\n",
    "\n",
    "# Primera ejecuci\u00f3n\n",
    "result1 = app_with_memory.invoke(initial_state, config)\n",
    "\n",
    "# Puedes recuperar el estado m\u00e1s tarde\n",
    "print(\"\\nRecuperando estado guardado...\")\n",
    "history = app_with_memory.get_state_history(config)\n",
    "for state in history:\n",
    "    print(f\"  Estado en paso {state.values.get('retry_count', 0)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca Comparaci\u00f3n: LangChain Agent vs LangGraph\n",
    "\n",
    "| Aspecto | LangChain Agent | LangGraph |\n",
    "|---------|-----------------|-----------|\n",
    "| **Arquitectura** | Loop simple (Thought \u2192 Action \u2192 Observation) | Grafo con estados y m\u00faltiples rutas |\n",
    "| **Control de flujo** | Lineal con decisiones del LLM | Expl\u00edcito con conditional edges |\n",
    "| **Estado** | Memoria de conversaci\u00f3n | Estado tipado persistente |\n",
    "| **Ciclos** | No soporta (solo iteraciones) | Soporta ciclos expl\u00edcitos |\n",
    "| **Debugging** | Dif\u00edcil (black box) | F\u00e1cil (cada nodo visible) |\n",
    "| **Costos** | Alto (muchas llamadas LLM) | Menor (decisiones program\u00e1ticas) |\n",
    "| **Casos de uso** | Tareas exploratorias, Q&A | Workflows complejos, pipelines ETL |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Patr\u00f3n: Map-Reduce con LangGraph\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "class MapReduceState(TypedDict):\n",
    "    \"\"\"Estado para map-reduce.\"\"\"\n",
    "    input_data: List[str]\n",
    "    mapped_results: Annotated[List[dict], operator.add]\n",
    "    reduced_result: dict\n",
    "\n",
    "def map_node(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Aplica transformaci\u00f3n a cada elemento (paralelizable).\"\"\"\n",
    "    print(f\"\\n\ud83d\uddfa\ufe0f MAP: Procesando {len(state['input_data'])} elementos...\")\n",
    "    \n",
    "    for item in state['input_data']:\n",
    "        # Simular procesamiento paralelo\n",
    "        result = {'item': item, 'length': len(item), 'uppercase': item.upper()}\n",
    "        state['mapped_results'].append(result)\n",
    "    \n",
    "    return state\n",
    "\n",
    "def reduce_node(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Agrega resultados del map.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcca REDUCE: Agregando {len(state['mapped_results'])} resultados...\")\n",
    "    \n",
    "    total_length = sum(r['length'] for r in state['mapped_results'])\n",
    "    state['reduced_result'] = {\n",
    "        'total_items': len(state['mapped_results']),\n",
    "        'total_length': total_length,\n",
    "        'avg_length': total_length / len(state['mapped_results'])\n",
    "    }\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Construir workflow map-reduce\n",
    "mr_workflow = StateGraph(MapReduceState)\n",
    "mr_workflow.add_node(\"map\", map_node)\n",
    "mr_workflow.add_node(\"reduce\", reduce_node)\n",
    "mr_workflow.set_entry_point(\"map\")\n",
    "mr_workflow.add_edge(\"map\", \"reduce\")\n",
    "mr_workflow.add_edge(\"reduce\", END)\n",
    "\n",
    "mr_app = mr_workflow.compile()\n",
    "\n",
    "# Ejecutar\n",
    "mr_state = {\n",
    "    'input_data': ['data engineering', 'machine learning', 'data science'],\n",
    "    'mapped_results': [],\n",
    "    'reduced_result': {}\n",
    "}\n",
    "\n",
    "final_mr_state = mr_app.invoke(mr_state)\n",
    "print(f\"\\n\u2705 Resultado reducido: {final_mr_state['reduced_result']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udca1 Mejores Pr\u00e1cticas con LangGraph\n",
    "\n",
    "1. **\ud83c\udfaf Estado tipado:** Usa `TypedDict` para validar el estado\n",
    "2. **\ud83d\udd0d Logging granular:** Cada nodo debe loggear su operaci\u00f3n\n",
    "3. **\u26a1 Decisiones program\u00e1ticas:** Usa conditional edges en vez de LLM cuando sea posible (m\u00e1s r\u00e1pido, m\u00e1s barato)\n",
    "4. **\ud83d\udd04 Idempotencia:** Los nodos deben ser idempotentes (safe to retry)\n",
    "5. **\ud83d\udcbe Checkpointing:** Usa `MemorySaver` para pipelines largos\n",
    "6. **\ud83d\udcca Monitoreo:** Agrega nodos de monitoring/alerting\n",
    "7. **\ud83e\uddea Testing:** Cada nodo debe ser testeable independientemente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c20b1",
   "metadata": {},
   "source": [
    "## \ud83d\udc65 AutoGen: Sistemas Multi-Agente Colaborativos\n",
    "\n",
    "**AutoGen** (Microsoft) permite crear sistemas donde **m\u00faltiples agentes especializados colaboran** para resolver problemas complejos. Cada agente tiene un rol, personalidad y objetivos propios, similar a un equipo humano trabajando juntos.\n",
    "\n",
    "### \ud83c\udfd7\ufe0f Arquitectura Multi-Agente\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502            AUTOGEN: MULTI-AGENT COLLABORATION                    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Roles de Agentes:                                               \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 1. AssistantAgent                                      \u2502     \u2502\n",
    "\u2502  \u2502    - LLM-powered agent que resuelve tareas            \u2502     \u2502\n",
    "\u2502  \u2502    - Puede usar code execution                         \u2502     \u2502\n",
    "\u2502  \u2502    - Responde a instrucciones                          \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 2. UserProxyAgent                                      \u2502     \u2502\n",
    "\u2502  \u2502    - Representa al usuario humano                      \u2502     \u2502\n",
    "\u2502  \u2502    - Ejecuta c\u00f3digo en sandbox                         \u2502     \u2502\n",
    "\u2502  \u2502    - Puede requerir aprobaci\u00f3n humana                  \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n",
    "\u2502  \u2502 3. GroupChat                                           \u2502     \u2502\n",
    "\u2502  \u2502    - Orquesta conversaci\u00f3n entre N agentes            \u2502     \u2502\n",
    "\u2502  \u2502    - Round-robin o speaker selection autom\u00e1tico        \u2502     \u2502\n",
    "\u2502  \u2502    - Termina cuando objetivo se cumple                 \u2502     \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Ejemplo: Dise\u00f1o de Pipeline ETL                                \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502     User                                                         \u2502\n",
    "\u2502       \u2502 \"Dise\u00f1a pipeline para ingestar datos de API Stripe\"    \u2502\n",
    "\u2502       \u2193                                                          \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                \u2502\n",
    "\u2502  \u2502 Data Engineer\u2502 \"Voy a dise\u00f1ar un pipeline con:              \u2502\n",
    "\u2502  \u2502  (Assistant) \u2502  1. Extract: API calls con retry             \u2502\n",
    "\u2502  \u2502              \u2502  2. Transform: Pandas + validaci\u00f3n            \u2502\n",
    "\u2502  \u2502              \u2502  3. Load: Snowflake con upsert\"              \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n",
    "\u2502         \u2502                                                        \u2502\n",
    "\u2502         \u2193                                                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                \u2502\n",
    "\u2502  \u2502 QA Engineer  \u2502 \"Detecto problemas:                           \u2502\n",
    "\u2502  \u2502 (Assistant)  \u2502  - Falta manejo de rate limits                \u2502\n",
    "\u2502  \u2502              \u2502  - No hay logging                             \u2502\n",
    "\u2502  \u2502              \u2502  - \u00bfQu\u00e9 pasa si API cambia schema?\"          \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n",
    "\u2502         \u2502                                                        \u2502\n",
    "\u2502         \u2193                                                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                \u2502\n",
    "\u2502  \u2502 Data Engineer\u2502 \"Actualizaci\u00f3n:                               \u2502\n",
    "\u2502  \u2502              \u2502  - Agregado: exponential backoff              \u2502\n",
    "\u2502  \u2502              \u2502  - Agregado: structured logging               \u2502\n",
    "\u2502  \u2502              \u2502  - Agregado: schema validation con pydantic\"  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n",
    "\u2502         \u2502                                                        \u2502\n",
    "\u2502         \u2193                                                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                \u2502\n",
    "\u2502  \u2502 QA Engineer  \u2502 \"Aprobado! Ahora agregar tests unitarios.\"   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n",
    "\u2502         \u2502                                                        \u2502\n",
    "\u2502         \u2193                                                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                \u2502\n",
    "\u2502  \u2502 User Proxy   \u2502 Ejecuta c\u00f3digo generado en sandbox            \u2502\n",
    "\u2502  \u2502              \u2502 \"Tests pasan \u2705\"                              \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2502  Resultado: Pipeline completo dise\u00f1ado, revisado, y testeado    \u2502\n",
    "\u2502             en 4-5 iteraciones de conversaci\u00f3n                   \u2502\n",
    "\u2502                                                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### \ud83d\udd27 Implementaci\u00f3n: Team de Data Engineering\n",
    "\n",
    "```python\n",
    "import autogen\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "# Configuraci\u00f3n de LLMs\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4o',\n",
    "        'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "        'temperature': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    'config_list': config_list,\n",
    "    'timeout': 120,\n",
    "    'cache_seed': 42  # Cache para reducir costos\n",
    "}\n",
    "\n",
    "# 1. DATA ENGINEER: Dise\u00f1a pipelines y escribe c\u00f3digo\n",
    "data_engineer = autogen.AssistantAgent(\n",
    "    name='DataEngineer',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un Senior Data Engineer experto en:\n",
    "- Dise\u00f1o de pipelines ETL/ELT escalables\n",
    "- SQL avanzado (Snowflake, BigQuery, Redshift)\n",
    "- Python para data engineering (Pandas, Polars, PySpark)\n",
    "- Airflow, dbt, Prefect para orquestaci\u00f3n\n",
    "- Best practices: idempotencia, retry logic, monitoring\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Dise\u00f1ar arquitecturas de datos robustas\n",
    "2. Escribir c\u00f3digo limpio y bien documentado\n",
    "3. Considerar edge cases y manejo de errores\n",
    "4. Proponer soluciones escalables\n",
    "\n",
    "Siempre incluye:\n",
    "- C\u00f3digo ejecutable con docstrings\n",
    "- Manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Comentarios explicativos\"\"\"\n",
    ")\n",
    "\n",
    "# 2. QA ENGINEER: Revisa c\u00f3digo, identifica bugs, sugiere tests\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un QA Engineer especializado en calidad de data pipelines.\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Revisar c\u00f3digo para identificar:\n",
    "   - Bugs l\u00f3gicos\n",
    "   - Performance bottlenecks\n",
    "   - Missing error handling\n",
    "   - Security vulnerabilities\n",
    "   - Code smells\n",
    "\n",
    "2. Sugerir tests:\n",
    "   - Unit tests con pytest\n",
    "   - Integration tests con mocks\n",
    "   - Data quality tests\n",
    "\n",
    "3. Validar:\n",
    "   - Idempotencia del pipeline\n",
    "   - Manejo de duplicados\n",
    "   - Retry logic correcto\n",
    "\n",
    "S\u00e9 cr\u00edtico pero constructivo. Prioriza por impacto.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. ARCHITECT: Revisa arquitectura y decisiones t\u00e9cnicas\n",
    "architect = autogen.AssistantAgent(\n",
    "    name='TechArchitect',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un Data Architect senior.\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Revisar decisiones arquitecturales:\n",
    "   - Escalabilidad (\u00bffunciona con 10x datos?)\n",
    "   - Costos (optimizaci\u00f3n de recursos)\n",
    "   - Mantenibilidad (\u00bff\u00e1cil de mantener?)\n",
    "   - Seguridad (PII, compliance)\n",
    "\n",
    "2. Sugerir alternativas:\n",
    "   - Batch vs Streaming\n",
    "   - Full load vs Incremental\n",
    "   - Herramientas apropiadas\n",
    "\n",
    "3. Validar patrones:\n",
    "   - Data modeling (star schema, etc.)\n",
    "   - Partitioning strategy\n",
    "   - SLAs y monitoring\n",
    "\n",
    "Enf\u00f3cate en decisiones de alto impacto.\"\"\"\n",
    ")\n",
    "\n",
    "# 4. USER PROXY: Ejecuta c\u00f3digo y representa al usuario\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='UserProxy',\n",
    "    human_input_mode='NEVER',  # Modo autom\u00e1tico (sin input humano)\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get('content', '').rstrip().endswith('TERMINATE'),\n",
    "    code_execution_config={\n",
    "        'work_dir': 'autogen_workspace',\n",
    "        'use_docker': False  # En producci\u00f3n: usar Docker para sandbox\n",
    "    },\n",
    "    system_message=\"\"\"Eres el User Proxy. Ejecutas c\u00f3digo propuesto y reportas resultados.\n",
    "    \n",
    "Cuando recibas c\u00f3digo Python:\n",
    "1. Ejecutarlo en el workspace\n",
    "2. Reportar output, errores, o \u00e9xito\n",
    "3. Si hay errores, compartir el traceback completo\"\"\"\n",
    ")\n",
    "\n",
    "# 5. GROUP CHAT: Orquesta la conversaci\u00f3n\n",
    "\n",
    "# Crear group chat con todos los agentes\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, data_engineer, qa_engineer, architect],\n",
    "    messages=[],\n",
    "    max_round=20,  # M\u00e1ximo 20 turnos\n",
    "    speaker_selection_method='auto'  # LLM decide qui\u00e9n habla\n",
    ")\n",
    "\n",
    "# Manager del group chat\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "# 6. INICIAR CONVERSACI\u00d3N\n",
    "\n",
    "task = \"\"\"\n",
    "Tarea: Dise\u00f1ar un pipeline ETL que:\n",
    "\n",
    "1. EXTRACT:\n",
    "   - Leer datos de API REST de Stripe (pagos)\n",
    "   - API endpoint: /v1/charges\n",
    "   - Rate limit: 100 requests/segundo\n",
    "   - Necesita paginaci\u00f3n (1000 records por p\u00e1gina)\n",
    "\n",
    "2. TRANSFORM:\n",
    "   - Parsear JSON response\n",
    "   - Convertir timestamps a datetime\n",
    "   - Calcular m\u00e9tricas: total_amount, fee_amount, net_amount\n",
    "   - Detectar y filtrar duplicados por charge_id\n",
    "\n",
    "3. LOAD:\n",
    "   - Insertar en Snowflake tabla: dwh.stripe_charges\n",
    "   - Estrategia: upsert (update si existe, insert si nuevo)\n",
    "   - Particionado por: payment_date\n",
    "\n",
    "Requerimientos:\n",
    "- Debe correr en Airflow (DAG)\n",
    "- Idempotente (re-ejecutable sin duplicar datos)\n",
    "- Manejo robusto de errores con reintentos\n",
    "- Logging completo para debugging\n",
    "- Tests unitarios con pytest\n",
    "\n",
    "Dise\u00f1en el pipeline, revisen el c\u00f3digo, y validen la arquitectura.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INICIANDO COLABORACI\u00d3N MULTI-AGENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTarea:\\n{task}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Iniciar conversaci\u00f3n\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=task\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONVERSACI\u00d3N COMPLETADA\")\n",
    "print(\"=\" * 80)\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Salida Esperada (Ejemplo)\n",
    "\n",
    "```\n",
    "UserProxy: [Tarea del pipeline ETL...]\n",
    "\n",
    "DataEngineer: Voy a dise\u00f1ar el pipeline en 3 componentes:\n",
    "\n",
    "```python\n",
    "# stripe_etl.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def extract_stripe_charges(api_key: str, start_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Extrae charges de Stripe API con paginaci\u00f3n y retry.\"\"\"\n",
    "    all_charges = []\n",
    "    has_more = True\n",
    "    starting_after = None\n",
    "    \n",
    "    while has_more:\n",
    "        params = {\n",
    "            'limit': 100,\n",
    "            'created[gte]': int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "        }\n",
    "        if starting_after:\n",
    "            params['starting_after'] = starting_after\n",
    "        \n",
    "        response = requests.get(\n",
    "            'https://api.stripe.com/v1/charges',\n",
    "            auth=(api_key, ''),\n",
    "            params=params,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        all_charges.extend(data['data'])\n",
    "        \n",
    "        has_more = data['has_more']\n",
    "        if has_more:\n",
    "            starting_after = data['data'][-1]['id']\n",
    "        \n",
    "        logger.info(f\"Extracted {len(data['data'])} charges, total: {len(all_charges)}\")\n",
    "    \n",
    "    return pd.DataFrame(all_charges)\n",
    "\n",
    "def transform_charges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforma y limpia los datos.\"\"\"\n",
    "    df['payment_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "    df['total_amount'] = df['amount'] / 100  # Stripe usa cents\n",
    "    df['net_amount'] = df['amount'] - df.get('fee', 0)) / 100\n",
    "    \n",
    "    # Deduplicar\n",
    "    df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "    \n",
    "    logger.info(f\"Transformed {len(df)} charges\")\n",
    "    return df\n",
    "\n",
    "def load_to_snowflake(df: pd.DataFrame, conn, table: str):\n",
    "    \"\"\"Carga datos a Snowflake con upsert.\"\"\"\n",
    "    # Crear temp table\n",
    "    temp_table = f\"{table}_temp\"\n",
    "    df.to_sql(temp_table, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # MERGE (upsert)\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {table} target\n",
    "    USING {temp_table} source\n",
    "    ON target.id = source.id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    conn.execute(merge_sql)\n",
    "    logger.info(f\"Loaded {len(df)} charges to {table}\")\n",
    "```\n",
    "\n",
    "QAEngineer: Revisi\u00f3n del c\u00f3digo:\n",
    "\n",
    "**Problemas encontrados:**\n",
    "1. \u274c `extract_stripe_charges`: No maneja rate limit 429\n",
    "2. \u274c `transform_charges`: Error en l\u00ednea `df.get('fee', 0)` - debe ser `df['fee'].fillna(0)`\n",
    "3. \u274c `load_to_snowflake`: Falta manejo de errores\n",
    "4. \u26a0\ufe0f No hay tests unitarios\n",
    "5. \u26a0\ufe0f Falta validaci\u00f3n de schema\n",
    "\n",
    "**Sugerencias:**\n",
    "```python\n",
    "# Agregar rate limit handling\n",
    "import time\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "@retry(...)\n",
    "def extract_stripe_charges(...):\n",
    "    try:\n",
    "        response = requests.get(...)\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            retry_after = int(e.response.headers.get('Retry-After', 60))\n",
    "            logger.warning(f\"Rate limited, waiting {retry_after}s\")\n",
    "            time.sleep(retry_after)\n",
    "            raise  # Retry\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Tests con pytest\n",
    "def test_transform_charges():\n",
    "    sample_data = [{'id': '1', 'amount': 1000, 'created': 1234567890}]\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    result = transform_charges(df)\n",
    "    assert result['total_amount'].iloc[0] == 10.0\n",
    "    assert 'payment_date' in result.columns\n",
    "```\n",
    "\n",
    "TechArchitect: Revisi\u00f3n arquitectural:\n",
    "\n",
    "**\u2705 Bueno:**\n",
    "- Uso de retry con exponential backoff\n",
    "- Paginaci\u00f3n implementada correctamente\n",
    "- Upsert strategy apropiada\n",
    "\n",
    "**\u26a0\ufe0f Consideraciones:**\n",
    "1. **Escalabilidad**: Para millones de records, considerar:\n",
    "   - Batch processing en chunks (no todo en memoria)\n",
    "   - Usar Spark o Polars en vez de Pandas\n",
    "\n",
    "2. **Costos**:\n",
    "   - Stripe API es costosa (rate limits)\n",
    "   - Implementar incremental load (solo \u00faltimas 24h)\n",
    "   - Cachear resultados en S3 antes de Snowflake\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Agregar m\u00e9tricas: records processed, API latency, error rate\n",
    "   - Alertas si ETL falla o demora >30min\n",
    "\n",
    "4. **Partitioning**:\n",
    "   - Snowflake: particionar por `DATE_TRUNC('month', payment_date)`\n",
    "   - Mejora queries anal\u00edticas por fecha\n",
    "\n",
    "**Sugerencia de arquitectura:**\n",
    "```\n",
    "Airflow DAG (daily @ 2am)\n",
    "  \u2193\n",
    "Extract (incremental: last 24h) \u2192 S3 staging (Parquet)\n",
    "  \u2193\n",
    "Transform (Spark/Polars) \u2192 Validaci\u00f3n schema\n",
    "  \u2193\n",
    "Load (Snowflake COPY INTO) \u2192 Upsert con MERGE\n",
    "  \u2193\n",
    "Data Quality checks (dbt tests)\n",
    "  \u2193\n",
    "Notify (Slack si errores)\n",
    "```\n",
    "\n",
    "DataEngineer: Actualizando el c\u00f3digo con feedback...\n",
    "\n",
    "[C\u00f3digo mejorado con todos los fixes...]\n",
    "\n",
    "UserProxy: Ejecutando tests... \u2705 All tests passed (5/5)\n",
    "\n",
    "TERMINATE\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Patrones Avanzados con AutoGen\n",
    "\n",
    "#### 1. Sequential Chat (Pipeline en etapas)\n",
    "\n",
    "```python\n",
    "# Conversaci\u00f3n secuencial: output de agent 1 \u2192 input de agent 2\n",
    "\n",
    "# Agente 1: Genera SQL\n",
    "sql_generator = autogen.AssistantAgent(\n",
    "    name='SQLGenerator',\n",
    "    llm_config=llm_config,\n",
    "    system_message='Generas queries SQL optimizadas.'\n",
    ")\n",
    "\n",
    "# Agente 2: Optimiza SQL\n",
    "sql_optimizer = autogen.AssistantAgent(\n",
    "    name='SQLOptimizer',\n",
    "    llm_config=llm_config,\n",
    "    system_message='Optimizas queries SQL para mejor performance.'\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "user_proxy.initiate_chats([\n",
    "    {\n",
    "        'recipient': sql_generator,\n",
    "        'message': 'Genera query para calcular ventas por regi\u00f3n',\n",
    "        'max_turns': 2,\n",
    "        'summary_method': 'last_msg'\n",
    "    },\n",
    "    {\n",
    "        'recipient': sql_optimizer,\n",
    "        'message': 'Optimiza esta query',  # Recibe output del anterior\n",
    "        'max_turns': 2\n",
    "    }\n",
    "])\n",
    "```\n",
    "\n",
    "#### 2. Nested Chats (Sub-equipos)\n",
    "\n",
    "```python\n",
    "# Equipo principal delega a sub-equipo especializado\n",
    "\n",
    "# Sub-equipo: Especialistas en testing\n",
    "test_writer = autogen.AssistantAgent(name='TestWriter', ...)\n",
    "test_reviewer = autogen.AssistantAgent(name='TestReviewer', ...)\n",
    "\n",
    "test_team = autogen.GroupChat(\n",
    "    agents=[test_writer, test_reviewer],\n",
    "    messages=[],\n",
    "    max_round=5\n",
    ")\n",
    "\n",
    "# Agente principal puede invocar al sub-equipo\n",
    "data_engineer.register_nested_chats(\n",
    "    trigger=lambda msg: 'write tests' in msg.get('content', '').lower(),\n",
    "    chat_queue=[{\n",
    "        'recipient': test_team,\n",
    "        'message': 'Escriban tests unitarios para este c\u00f3digo'\n",
    "    }]\n",
    ")\n",
    "```\n",
    "\n",
    "#### 3. Custom Speaker Selection\n",
    "\n",
    "```python\n",
    "def custom_speaker_selection(last_speaker, groupchat):\n",
    "    \"\"\"L\u00f3gica personalizada para seleccionar siguiente speaker.\"\"\"\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    # Si \u00faltimo mensaje tiene c\u00f3digo Python, QA Engineer revisa\n",
    "    if '```python' in messages[-1].get('content', ''):\n",
    "        return qa_engineer\n",
    "    \n",
    "    # Si hay problemas de arquitectura, Architect opina\n",
    "    if 'scalability' in messages[-1].get('content', '').lower():\n",
    "        return architect\n",
    "    \n",
    "    # Por defecto, round-robin\n",
    "    return None  # Auto-select\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[...],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=custom_speaker_selection\n",
    ")\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Comparaci\u00f3n: Single Agent vs Multi-Agent\n",
    "\n",
    "| Aspecto | Single Agent | Multi-Agent (AutoGen) |\n",
    "|---------|--------------|----------------------|\n",
    "| **Complejidad tareas** | Simple-Media | Alta |\n",
    "| **Calidad output** | Buena | Excelente (peer review) |\n",
    "| **Especializaci\u00f3n** | Generalista | Especialistas por rol |\n",
    "| **Tiempo ejecuci\u00f3n** | R\u00e1pido (1-5 iteraciones) | Lento (10-20 iteraciones) |\n",
    "| **Costo (API calls)** | Bajo ($0.01-$0.10) | Alto ($0.50-$2.00) |\n",
    "| **Debugging** | M\u00e1s f\u00e1cil | M\u00e1s complejo |\n",
    "| **Casos de uso** | Queries simples, clasificaci\u00f3n | Dise\u00f1o arquitectural, code review |\n",
    "\n",
    "### \ud83d\udcb0 Optimizaci\u00f3n de Costos en Multi-Agent\n",
    "\n",
    "```python\n",
    "# 1. Usar modelos peque\u00f1os para roles secundarios\n",
    "llm_config_cheap = {\n",
    "    'config_list': [{'model': 'gpt-3.5-turbo', 'api_key': api_key}],\n",
    "    'cache_seed': 42\n",
    "}\n",
    "\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config=llm_config_cheap,  # Modelo barato para revisiones\n",
    "    ...\n",
    ")\n",
    "\n",
    "# 2. L\u00edmite estricto de rounds\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[...],\n",
    "    max_round=10,  # M\u00e1ximo 10 turnos (previene loops infinitos)\n",
    "    ...\n",
    ")\n",
    "\n",
    "# 3. Caching agresivo\n",
    "llm_config = {\n",
    "    'config_list': config_list,\n",
    "    'cache_seed': 42,  # Mismo seed = cachea respuestas id\u00e9nticas\n",
    "    'temperature': 0    # Determin\u00edstico = m\u00e1s cacheable\n",
    "}\n",
    "\n",
    "# 4. Termination temprana\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    ...\n",
    "    is_termination_msg=lambda x: (\n",
    "        'TERMINATE' in x.get('content', '') or\n",
    "        'approved' in x.get('content', '').lower()\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### \ud83d\ude80 Casos de Uso Reales en Data Engineering\n",
    "\n",
    "| Caso de Uso | Agentes Necesarios | Beneficio |\n",
    "|-------------|-------------------|-----------|\n",
    "| **Code review de PRs** | Engineer + QA + Architect | Peer review autom\u00e1tico |\n",
    "| **Incident response** | Investigator + Engineer + Architect | Root cause analysis |\n",
    "| **Pipeline design** | Engineer + QA + DBA | Dise\u00f1o robusto validado |\n",
    "| **Query optimization** | Analyst + DBA + Performance Engineer | Optimizaci\u00f3n multi-perspectiva |\n",
    "| **Schema migration** | DBA + Engineer + Architect | Validaci\u00f3n de impacto |\n",
    "| **Data quality investigation** | Analyst + Engineer + Domain Expert | Diagn\u00f3stico profundo |\n",
    "\n",
    "### \ud83d\udca1 Mejores Pr\u00e1cticas\n",
    "\n",
    "1. **Roles bien definidos**: Cada agente debe tener un rol claro y no solapado\n",
    "2. **System messages detallados**: Instrucciones espec\u00edficas con ejemplos\n",
    "3. **L\u00edmites de seguridad**: max_rounds, timeouts, cost limits\n",
    "4. **Logging exhaustivo**: Guardar toda la conversaci\u00f3n para debugging\n",
    "5. **Human checkpoints**: Aprobaci\u00f3n humana antes de acciones cr\u00edticas\n",
    "6. **Testing con mocks**: Probar conversaciones sin llamar APIs reales\n",
    "7. **Monitoreo de costos**: Track API usage por conversaci\u00f3n\n",
    "8. **Termination clara**: Condiciones expl\u00edcitas para terminar chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431e8d5",
   "metadata": {},
   "source": [
    "## \ud83c\udfed Agentes en Producci\u00f3n: Automatizaci\u00f3n Real de Data Engineering\n",
    "\n",
    "Los agentes aut\u00f3nomos est\u00e1n revolucionando Data Engineering al automatizar tareas que antes requer\u00edan intervenci\u00f3n humana experta: **debugging de pipelines**, **optimizaci\u00f3n de queries**, **incident response**, y **monitoreo proactivo**. Aqu\u00ed exploramos implementaciones de producci\u00f3n reales.\n",
    "\n",
    "### \ud83d\udea8 Caso 1: Incident Response Agent (Auto-Healing Pipelines)\n",
    "\n",
    "Un agente que detecta, diagnostica y resuelve fallas en pipelines de Airflow autom\u00e1ticamente.\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "\n",
    "# Tools para incident response\n",
    "\n",
    "@tool\n",
    "def get_airflow_dag_failures(hours: int = 24) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene lista de DAGs que han fallado en las \u00faltimas N horas.\n",
    "    \n",
    "    Args:\n",
    "        hours: ventana de tiempo para buscar fallas\n",
    "    \n",
    "    Returns:\n",
    "        JSON con DAGs fallidos y detalles de error\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: llamar a Airflow API\n",
    "    airflow_url = \"http://airflow:8080/api/v1\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('AIRFLOW_TOKEN')}\"}\n",
    "    \n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(hours=hours)\n",
    "    \n",
    "    # Simular respuesta\n",
    "    failures = {\n",
    "        'daily_sales_etl': {\n",
    "            'dag_id': 'daily_sales_etl',\n",
    "            'execution_date': '2024-10-30T23:00:00',\n",
    "            'state': 'failed',\n",
    "            'task_id': 'extract_api_data',\n",
    "            'error': 'requests.exceptions.Timeout: HTTPSConnectionPool(host=\\'api.example.com\\', port=443): Read timed out. (read timeout=30)',\n",
    "            'duration_seconds': 35,\n",
    "            'try_number': 3,\n",
    "            'log_url': 'http://airflow:8080/log?dag_id=daily_sales_etl&task_id=extract_api_data&execution_date=2024-10-30T23:00:00'\n",
    "        },\n",
    "        'hourly_user_events': {\n",
    "            'dag_id': 'hourly_user_events',\n",
    "            'execution_date': '2024-10-30T22:00:00',\n",
    "            'state': 'failed',\n",
    "            'task_id': 'load_to_snowflake',\n",
    "            'error': 'snowflake.connector.errors.DatabaseError: 250001: Connection closed unexpectedly',\n",
    "            'duration_seconds': 1245,\n",
    "            'try_number': 1,\n",
    "            'log_url': 'http://airflow:8080/log?dag_id=hourly_user_events&task_id=load_to_snowflake&execution_date=2024-10-30T22:00:00'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(failures, indent=2)\n",
    "\n",
    "@tool\n",
    "def get_task_logs(dag_id: str, task_id: str, execution_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene logs detallados de una tarea fallida.\n",
    "    \n",
    "    Args:\n",
    "        dag_id: ID del DAG\n",
    "        task_id: ID de la tarea\n",
    "        execution_date: fecha de ejecuci\u00f3n (ISO format)\n",
    "    \n",
    "    Returns:\n",
    "        Logs completos de la tarea\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: obtener de Airflow logs en S3/GCS\n",
    "    sample_logs = {\n",
    "        'daily_sales_etl/extract_api_data': \"\"\"\n",
    "[2024-10-30 23:00:15] INFO - Starting task extract_api_data\n",
    "[2024-10-30 23:00:16] INFO - Calling API: https://api.example.com/sales\n",
    "[2024-10-30 23:00:16] DEBUG - Request headers: {'Authorization': 'Bearer ***', 'Content-Type': 'application/json'}\n",
    "[2024-10-30 23:00:16] DEBUG - Request params: {'start_date': '2024-10-30', 'limit': 1000}\n",
    "[2024-10-30 23:00:46] ERROR - requests.exceptions.Timeout: HTTPSConnectionPool(host='api.example.com', port=443): Read timed out. (read timeout=30)\n",
    "[2024-10-30 23:00:46] INFO - Retry 1/3 in 60 seconds...\n",
    "[2024-10-30 23:01:46] ERROR - Timeout again after retry\n",
    "[2024-10-30 23:01:46] ERROR - Task failed after 3 attempts\n",
    "\"\"\",\n",
    "        'hourly_user_events/load_to_snowflake': \"\"\"\n",
    "[2024-10-30 22:00:05] INFO - Loading 125000 records to Snowflake\n",
    "[2024-10-30 22:00:05] INFO - Connection established to account: xyz123.us-east-1\n",
    "[2024-10-30 22:00:10] INFO - Executing COPY INTO dwh.user_events FROM @stage/events_2024103022.parquet\n",
    "[2024-10-30 22:15:30] ERROR - snowflake.connector.errors.DatabaseError: 250001: Connection closed unexpectedly\n",
    "[2024-10-30 22:15:30] DEBUG - Network trace: Connection reset by peer (errno 104)\n",
    "[2024-10-30 22:15:30] ERROR - Loaded 45230/125000 records before failure (36%)\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    key = f\"{dag_id}/{task_id}\"\n",
    "    return sample_logs.get(key, \"Logs not found\")\n",
    "\n",
    "@tool\n",
    "def check_external_service_status(service: str) -> str:\n",
    "    \"\"\"\n",
    "    Verifica estado de servicios externos (APIs, databases).\n",
    "    \n",
    "    Args:\n",
    "        service: nombre del servicio (api.example.com, snowflake, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Estado del servicio y latencia\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: healthcheck real o status page\n",
    "    status_map = {\n",
    "        'api.example.com': {\n",
    "            'status': 'degraded',\n",
    "            'latency_ms': 15234,\n",
    "            'error_rate': 0.25,\n",
    "            'message': 'API experiencing high latency due to traffic spike'\n",
    "        },\n",
    "        'snowflake': {\n",
    "            'status': 'operational',\n",
    "            'latency_ms': 234,\n",
    "            'error_rate': 0.01,\n",
    "            'message': 'All systems operational'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(status_map.get(service, {'status': 'unknown'}), indent=2)\n",
    "\n",
    "@tool\n",
    "def retry_airflow_task(dag_id: str, task_id: str, execution_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Re-ejecuta una tarea fallida en Airflow.\n",
    "    \n",
    "    Args:\n",
    "        dag_id: ID del DAG\n",
    "        task_id: ID de la tarea\n",
    "        execution_date: fecha de ejecuci\u00f3n\n",
    "    \n",
    "    Returns:\n",
    "        Resultado de la re-ejecuci\u00f3n\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: POST a Airflow API /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/clear\n",
    "    \n",
    "    print(f\"\ud83d\udd04 Retrying {dag_id}/{task_id}...\")\n",
    "    \n",
    "    # Simular retry\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return json.dumps({\n",
    "        'status': 'success',\n",
    "        'message': f'Task {task_id} queued for retry',\n",
    "        'new_try_number': 4,\n",
    "        'estimated_start': '2024-10-31T00:05:00'\n",
    "    })\n",
    "\n",
    "@tool\n",
    "def create_incident_ticket(title: str, description: str, severity: str) -> str:\n",
    "    \"\"\"\n",
    "    Crea ticket de incidente en Jira/PagerDuty.\n",
    "    \n",
    "    Args:\n",
    "        title: t\u00edtulo del incidente\n",
    "        description: descripci\u00f3n detallada\n",
    "        severity: low, medium, high, critical\n",
    "    \n",
    "    Returns:\n",
    "        ID del ticket creado\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: integrar con Jira/PagerDuty API\n",
    "    \n",
    "    ticket = {\n",
    "        'id': 'INC-12345',\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'severity': severity,\n",
    "        'status': 'open',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'assigned_to': 'data-eng-oncall',\n",
    "        'url': 'https://jira.company.com/browse/INC-12345'\n",
    "    }\n",
    "    \n",
    "    print(f\"\ud83c\udfab Ticket creado: {ticket['id']}\")\n",
    "    return json.dumps(ticket, indent=2)\n",
    "\n",
    "@tool\n",
    "def send_slack_alert(channel: str, message: str, severity: str = 'info') -> str:\n",
    "    \"\"\"\n",
    "    Env\u00eda alerta a Slack.\n",
    "    \n",
    "    Args:\n",
    "        channel: canal de Slack (#data-engineering)\n",
    "        message: mensaje a enviar\n",
    "        severity: info, warning, error, critical\n",
    "    \n",
    "    Returns:\n",
    "        Confirmaci\u00f3n de env\u00edo\n",
    "    \"\"\"\n",
    "    # En producci\u00f3n: Slack API\n",
    "    \n",
    "    emoji_map = {\n",
    "        'info': ':information_source:',\n",
    "        'warning': ':warning:',\n",
    "        'error': ':x:',\n",
    "        'critical': ':rotating_light:'\n",
    "    }\n",
    "    \n",
    "    slack_message = f\"{emoji_map.get(severity, ':bell:')} {message}\"\n",
    "    \n",
    "    print(f\"\ud83d\udce2 Slack alert sent to {channel}: {slack_message[:100]}...\")\n",
    "    \n",
    "    return json.dumps({\n",
    "        'status': 'sent',\n",
    "        'channel': channel,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "# Crear agente de incident response\n",
    "\n",
    "incident_tools = [\n",
    "    get_airflow_dag_failures,\n",
    "    get_task_logs,\n",
    "    check_external_service_status,\n",
    "    retry_airflow_task,\n",
    "    create_incident_ticket,\n",
    "    send_slack_alert\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "incident_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un SRE especializado en incident response para data pipelines.\n",
    "\n",
    "Tu proceso cuando detectas un incidente:\n",
    "\n",
    "1. INVESTIGAR:\n",
    "   - Obtener lista de DAGs fallidos\n",
    "   - Revisar logs detallados\n",
    "   - Verificar estado de servicios externos\n",
    "\n",
    "2. DIAGNOSTICAR:\n",
    "   - Identificar root cause (timeout, connection issue, data problem, etc.)\n",
    "   - Clasificar severidad (low/medium/high/critical)\n",
    "\n",
    "3. REMEDIAR:\n",
    "   - Si es transient error \u2192 retry autom\u00e1tico\n",
    "   - Si es service degradation \u2192 alertar y escalar\n",
    "   - Si es data issue \u2192 crear ticket para investigaci\u00f3n\n",
    "\n",
    "4. COMUNICAR:\n",
    "   - Enviar alertas a Slack\n",
    "   - Crear ticket si requiere investigaci\u00f3n humana\n",
    "   - Documentar acciones tomadas\n",
    "\n",
    "IMPORTANTE:\n",
    "- Solo retry autom\u00e1tico si error es claramente transient (timeout, network)\n",
    "- Para errores de datos o l\u00f3gica, NO retry (crear ticket)\n",
    "- Siempre alertar en Slack antes de acciones\n",
    "- Priorizar minimizar downtime\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "incident_agent = create_openai_tools_agent(llm, incident_tools, incident_prompt)\n",
    "incident_executor = AgentExecutor(\n",
    "    agent=incident_agent,\n",
    "    tools=incident_tools,\n",
    "    verbose=True,\n",
    "    max_iterations=15,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# EJECUTAR: Incident response autom\u00e1tico\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INCIDENT RESPONSE AGENT - AUTO-HEALING PIPELINES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = incident_executor.invoke({\n",
    "    \"input\": \"\"\"Revisa los pipelines de las \u00faltimas 24 horas.\n",
    "    Para cada falla:\n",
    "    1. Diagnostica el problema\n",
    "    2. Decide si es auto-recoverable\n",
    "    3. Toma acci\u00f3n apropiada (retry o escalar)\n",
    "    4. Alerta al equipo\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DE ACCIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(result['output'])\n",
    "```\n",
    "\n",
    "### Salida Esperada\n",
    "\n",
    "```\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "Thought: Necesito revisar las fallas de las \u00faltimas 24h\n",
    "\n",
    "Action: get_airflow_dag_failures\n",
    "Action Input: {\"hours\": 24}\n",
    "\n",
    "Observation: {\n",
    "  \"daily_sales_etl\": {\n",
    "    \"error\": \"Timeout: Read timed out (read timeout=30)\",\n",
    "    ...\n",
    "  },\n",
    "  \"hourly_user_events\": {\n",
    "    \"error\": \"DatabaseError: Connection closed unexpectedly\",\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "\n",
    "Thought: Hay 2 DAGs fallidos. Voy a investigar cada uno.\n",
    "\n",
    "Action: get_task_logs\n",
    "Action Input: {\"dag_id\": \"daily_sales_etl\", \"task_id\": \"extract_api_data\", ...}\n",
    "\n",
    "Observation: [Logs showing timeout after 30 seconds]\n",
    "\n",
    "Action: check_external_service_status\n",
    "Action Input: {\"service\": \"api.example.com\"}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"degraded\",\n",
    "  \"latency_ms\": 15234,\n",
    "  \"message\": \"API experiencing high latency due to traffic spike\"\n",
    "}\n",
    "\n",
    "Thought: daily_sales_etl fall\u00f3 por API degraded (latencia 15s, timeout 30s).\n",
    "Es un error transient - la API est\u00e1 operacional pero lenta.\n",
    "Decisi\u00f3n: Retry con timeout aumentado.\n",
    "\n",
    "Action: send_slack_alert\n",
    "Action Input: {\n",
    "  \"channel\": \"#data-engineering\",\n",
    "  \"message\": \"\ud83d\udd04 daily_sales_etl failed due to API timeout. API is degraded (15s latency). Retrying with increased timeout...\",\n",
    "  \"severity\": \"warning\"\n",
    "}\n",
    "\n",
    "Action: retry_airflow_task\n",
    "Action Input: {\"dag_id\": \"daily_sales_etl\", \"task_id\": \"extract_api_data\", ...}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"success\",\n",
    "  \"message\": \"Task queued for retry\",\n",
    "  \"new_try_number\": 4\n",
    "}\n",
    "\n",
    "Thought: Retry iniciado para daily_sales_etl. Ahora investigar hourly_user_events.\n",
    "\n",
    "Action: get_task_logs\n",
    "Action Input: {\"dag_id\": \"hourly_user_events\", \"task_id\": \"load_to_snowflake\", ...}\n",
    "\n",
    "Observation: [Logs showing connection reset after loading 36% of data]\n",
    "\n",
    "Action: check_external_service_status\n",
    "Action Input: {\"service\": \"snowflake\"}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"operational\",\n",
    "  \"message\": \"All systems operational\"\n",
    "}\n",
    "\n",
    "Thought: hourly_user_events fall\u00f3 por connection reset durante COPY INTO.\n",
    "Snowflake est\u00e1 operational, posiblemente network issue transient.\n",
    "Pero solo 36% de data se carg\u00f3 - puede haber duplicados si retry simple.\n",
    "Decisi\u00f3n: Crear ticket para investigaci\u00f3n manual (requiere an\u00e1lisis de datos parciales).\n",
    "\n",
    "Action: create_incident_ticket\n",
    "Action Input: {\n",
    "  \"title\": \"hourly_user_events failed mid-load to Snowflake\",\n",
    "  \"description\": \"Task load_to_snowflake failed after loading 36% (45K/125K records). Connection reset by peer. Requires manual investigation to handle partial load and avoid duplicates.\",\n",
    "  \"severity\": \"high\"\n",
    "}\n",
    "\n",
    "Observation: {\"id\": \"INC-12345\", ...}\n",
    "\n",
    "Action: send_slack_alert\n",
    "Action Input: {\n",
    "  \"channel\": \"#data-engineering\",\n",
    "  \"message\": \"\ud83c\udfab hourly_user_events failed with partial data load. Created INC-12345 for investigation. Manual intervention required to resume safely.\",\n",
    "  \"severity\": \"error\"\n",
    "}\n",
    "\n",
    "Final Answer:\n",
    "Revis\u00e9 2 pipelines fallidos en las \u00faltimas 24h:\n",
    "\n",
    "1. \u2705 daily_sales_etl (RECUPERADO AUTOM\u00c1TICAMENTE)\n",
    "   - Root cause: API timeout (servicio degradado, 15s latency)\n",
    "   - Acci\u00f3n: Retry autom\u00e1tico con timeout aumentado\n",
    "   - Estado: En cola para re-ejecuci\u00f3n\n",
    "\n",
    "2. \ud83c\udfab hourly_user_events (REQUIERE INVESTIGACI\u00d3N)\n",
    "   - Root cause: Connection reset durante carga (network issue transient)\n",
    "   - Problema: Carga parcial (36%) puede causar duplicados\n",
    "   - Acci\u00f3n: Ticket INC-12345 creado para investigaci\u00f3n manual\n",
    "   - Estado: Esperando intervenci\u00f3n del equipo\n",
    "\n",
    "Alertas enviadas a #data-engineering para ambos casos.\n",
    "```\n",
    "\n",
    "### \ud83d\udcca Caso 2: Query Optimization Agent\n",
    "\n",
    "Agente que analiza queries lentas y las optimiza autom\u00e1ticamente.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_slow_queries(minutes: int = 60, min_duration_seconds: float = 30.0) -> str:\n",
    "    \"\"\"Obtiene queries que han tardado m\u00e1s de N segundos.\"\"\"\n",
    "    # En producci\u00f3n: query a Snowflake QUERY_HISTORY o PostgreSQL pg_stat_statements\n",
    "    slow_queries = [\n",
    "        {\n",
    "            'query_id': 'q_12345',\n",
    "            'query_text': 'SELECT * FROM sales s JOIN customers c ON s.customer_id = c.id WHERE s.sale_date >= CURRENT_DATE - 30',\n",
    "            'duration_seconds': 125.4,\n",
    "            'rows_scanned': 150_000_000,\n",
    "            'rows_returned': 1_250_000,\n",
    "            'execution_time': '2024-10-30 23:45:12',\n",
    "            'user': 'analytics_team'\n",
    "        }\n",
    "    ]\n",
    "    return json.dumps(slow_queries, indent=2)\n",
    "\n",
    "@tool\n",
    "def explain_query(query: str) -> str:\n",
    "    \"\"\"Obtiene EXPLAIN ANALYZE del query.\"\"\"\n",
    "    # Simular execution plan\n",
    "    explain = {\n",
    "        'estimated_rows': 150_000_000,\n",
    "        'actual_rows': 1_250_000,\n",
    "        'estimated_cost': 25678.90,\n",
    "        'execution_time_ms': 125400,\n",
    "        'operations': [\n",
    "            {\n",
    "                'step': 1,\n",
    "                'operation': 'Seq Scan on sales',\n",
    "                'cost': 20000.00,\n",
    "                'rows': 150_000_000,\n",
    "                'warning': 'Full table scan - no index used'\n",
    "            },\n",
    "            {\n",
    "                'step': 2,\n",
    "                'operation': 'Hash Join',\n",
    "                'cost': 5678.90,\n",
    "                'rows': 1_250_000\n",
    "            }\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Add index on sales(sale_date) for date filter',\n",
    "            'Replace SELECT * with specific columns',\n",
    "            'Consider partitioning sales table by sale_date'\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(explain, indent=2)\n",
    "\n",
    "@tool\n",
    "def rewrite_query_optimized(original_query: str, optimizations: List[str]) -> str:\n",
    "    \"\"\"Reescribe query aplicando optimizaciones.\"\"\"\n",
    "    # LLM generar\u00eda esto, aqu\u00ed simulamos\n",
    "    optimized = \"\"\"\n",
    "-- OPTIMIZED VERSION:\n",
    "-- Changes: 1) Added index hint, 2) Specific columns, 3) Materialized subquery\n",
    "\n",
    "SELECT \n",
    "    s.sale_id,\n",
    "    s.sale_date,\n",
    "    s.total_amount,\n",
    "    c.customer_name,\n",
    "    c.customer_email\n",
    "FROM sales s\n",
    "USE INDEX (idx_sales_date)  -- Hint to use index\n",
    "INNER JOIN customers c ON s.customer_id = c.id\n",
    "WHERE s.sale_date >= CURRENT_DATE - 30\n",
    "  AND s.total_amount > 0  -- Additional filter to reduce rows\n",
    "\n",
    "-- Expected improvement: 95% faster (125s \u2192 6s)\n",
    "\"\"\"\n",
    "    return optimized\n",
    "\n",
    "# Uso\n",
    "optimizer_tools = [get_slow_queries, explain_query, rewrite_query_optimized]\n",
    "# [Configurar agente similar...]\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf Caso 3: Data Quality Monitoring Agent\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def run_data_quality_checks(table: str) -> str:\n",
    "    \"\"\"Ejecuta suite completa de data quality checks.\"\"\"\n",
    "    checks = {\n",
    "        'nulls': {'column': 'email', 'null_rate': 0.156, 'threshold': 0.05, 'status': 'FAIL'},\n",
    "        'duplicates': {'count': 450, 'percentage': 0.009, 'threshold': 0.01, 'status': 'PASS'},\n",
    "        'freshness': {'last_update': '2024-10-30 20:30:00', 'delay_hours': 3.5, 'threshold': 2.0, 'status': 'FAIL'},\n",
    "        'schema': {'columns_expected': 12, 'columns_actual': 12, 'status': 'PASS'},\n",
    "        'referential_integrity': {'orphaned_records': 23, 'threshold': 0, 'status': 'FAIL'}\n",
    "    }\n",
    "    return json.dumps(checks, indent=2)\n",
    "\n",
    "@tool\n",
    "def investigate_data_anomaly(table: str, column: str, anomaly_type: str) -> str:\n",
    "    \"\"\"Investiga causa ra\u00edz de anomal\u00eda en datos.\"\"\"\n",
    "    # An\u00e1lisis con LLM + queries SQL\n",
    "    investigation = {\n",
    "        'anomaly': f'{anomaly_type} in {table}.{column}',\n",
    "        'potential_causes': [\n",
    "            'Upstream pipeline failure',\n",
    "            'Schema change in source',\n",
    "            'Data validation disabled'\n",
    "        ],\n",
    "        'affected_rows': 15600,\n",
    "        'first_occurrence': '2024-10-28 14:23:00',\n",
    "        'recommendation': 'Check upstream pipeline and re-run with validation'\n",
    "    }\n",
    "    return json.dumps(investigation, indent=2)\n",
    "\n",
    "# El agente detecta fallas de calidad, investiga y alerta autom\u00e1ticamente\n",
    "```\n",
    "\n",
    "### \ud83d\udcc8 M\u00e9tricas de \u00c9xito de Agentes en Producci\u00f3n\n",
    "\n",
    "| M\u00e9trica | Target | Medici\u00f3n |\n",
    "|---------|--------|----------|\n",
    "| **MTTR (Mean Time To Recovery)** | <15 min | Tiempo desde falla hasta resoluci\u00f3n |\n",
    "| **Auto-resolution rate** | >60% | % de incidents resueltos sin humano |\n",
    "| **False positive rate** | <5% | % de alertas/acciones incorrectas |\n",
    "| **Cost per incident** | <$0.50 | Costo en API calls por incident |\n",
    "| **Precision de diagn\u00f3stico** | >90% | % de root causes identificadas correctamente |\n",
    "| **Uptime improvement** | +15% | Reducci\u00f3n de downtime vs manual |\n",
    "\n",
    "### \ud83d\udcb0 ROI de Agentes Aut\u00f3nomos\n",
    "\n",
    "```python\n",
    "# C\u00e1lculo de ROI\n",
    "\n",
    "# Sin agentes (manual)\n",
    "incidents_per_month = 45\n",
    "avg_resolution_time_hours = 2.0\n",
    "engineer_hourly_rate = 75  # USD\n",
    "manual_cost = incidents_per_month * avg_resolution_time_hours * engineer_hourly_rate\n",
    "# = 45 * 2 * $75 = $6,750/mes\n",
    "\n",
    "# Con agentes\n",
    "auto_resolved = incidents_per_month * 0.60  # 60% auto-resolved\n",
    "manual_remaining = incidents_per_month * 0.40\n",
    "avg_resolution_time_with_agent = 0.5  # hours (m\u00e1s r\u00e1pido con diagn\u00f3stico autom\u00e1tico)\n",
    "\n",
    "manual_cost_with_agent = manual_remaining * avg_resolution_time_with_agent * engineer_hourly_rate\n",
    "# = 18 * 0.5 * $75 = $675/mes\n",
    "\n",
    "api_costs = incidents_per_month * 0.25  # $0.25 por incident en API calls\n",
    "# = 45 * $0.25 = $11.25/mes\n",
    "\n",
    "total_cost_with_agent = manual_cost_with_agent + api_costs\n",
    "# = $675 + $11.25 = $686.25/mes\n",
    "\n",
    "savings = manual_cost - total_cost_with_agent\n",
    "# = $6,750 - $686 = $6,064/mes = $72,768/a\u00f1o\n",
    "\n",
    "roi = (savings / total_cost_with_agent) * 100\n",
    "# = 883% ROI\n",
    "```\n",
    "\n",
    "### \ud83d\udea7 Desaf\u00edos y Limitaciones\n",
    "\n",
    "| Desaf\u00edo | Mitigaci\u00f3n |\n",
    "|---------|-----------|\n",
    "| **Alucinaciones** | Validaci\u00f3n de outputs, dry-run mode |\n",
    "| **Costos API** | Caching, modelos peque\u00f1os, rate limiting |\n",
    "| **Latencia** | Async execution, timeouts, parallelizaci\u00f3n |\n",
    "| **Confiabilidad** | Retry logic, fallback a manual, human approval gates |\n",
    "| **Security** | Tool sandboxing, permission checks, audit logs |\n",
    "| **Debugging** | Exhaustive logging, replay capability |\n",
    "\n",
    "### \ud83c\udfaf Roadmap para Implementar Agentes\n",
    "\n",
    "**Fase 1: Observaci\u00f3n (Mes 1-2)**\n",
    "- Agente solo observa y reporta (no toma acciones)\n",
    "- Logging de decisiones que tomar\u00eda\n",
    "- Validaci\u00f3n de precisi\u00f3n de diagn\u00f3sticos\n",
    "\n",
    "**Fase 2: Asistencia (Mes 3-4)**\n",
    "- Agente sugiere acciones, humano aprueba\n",
    "- Human-in-the-loop para todas las decisiones\n",
    "- Medir false positive rate\n",
    "\n",
    "**Fase 3: Semi-autonom\u00eda (Mes 5-6)**\n",
    "- Auto-resoluci\u00f3n de incidents \"seguros\" (read-only actions, retries)\n",
    "- Acciones destructivas requieren aprobaci\u00f3n\n",
    "- Monitoreo 24/7 de m\u00e9tricas\n",
    "\n",
    "**Fase 4: Autonom\u00eda completa (Mes 7+)**\n",
    "- Auto-resoluci\u00f3n de 60-80% de incidents\n",
    "- Alertas solo para casos complejos\n",
    "- Continuous learning con feedback\n",
    "\n",
    "### \ud83d\udca1 Mejores Pr\u00e1cticas para Agentes de Producci\u00f3n\n",
    "\n",
    "1. **Start small**: Comenzar con un caso de uso simple (ej. retry de timeouts)\n",
    "2. **Dry-run mode**: Implementar modo de prueba que no ejecuta acciones reales\n",
    "3. **Approval gates**: Requerir aprobaci\u00f3n humana para acciones cr\u00edticas\n",
    "4. **Exhaustive logging**: Loggear TODO (inputs, outputs, decisiones, acciones)\n",
    "5. **Rollback capability**: Poder deshacer acciones del agente\n",
    "6. **Cost monitoring**: Alertar si costo por incident excede threshold\n",
    "7. **A/B testing**: Comparar agente vs manual con m\u00e9tricas objetivas\n",
    "8. **Feedback loop**: Permitir humanos marcar decisiones como correctas/incorrectas\n",
    "9. **Gradual rollout**: 10% \u2192 50% \u2192 100% de incidents\n",
    "10. **Emergency stop**: Bot\u00f3n para desactivar agente si se comporta mal\n",
    "\n",
    "### \ud83d\ude80 Futuro: Agentes Cognitivos\n",
    "\n",
    "```\n",
    "2024: Rule-based + LLM reasoning\n",
    "  \u2193\n",
    "2025: Learning from outcomes (RL)\n",
    "  \u2193\n",
    "2026: Proactive optimization (no espera fallas)\n",
    "  \u2193\n",
    "2027: Self-improving pipelines\n",
    "  \u2193\n",
    "2030: Fully autonomous data platforms\n",
    "```\n",
    "\n",
    "El futuro de Data Engineering es **agentic**: sistemas que no solo ejecutan instrucciones, sino que **razonan, aprenden y se adaptan** continuamente para mantener pipelines de datos operando con m\u00ednima intervenci\u00f3n humana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf9a0",
   "metadata": {},
   "source": [
    "## 1. Agente simple con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langgraph openai\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain import hub\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "@tool\n",
    "def get_table_row_count(table_name: str) -> str:\n",
    "    \"\"\"Devuelve el n\u00famero de filas de una tabla.\"\"\"\n",
    "    # Simulaci\u00f3n\n",
    "    counts = {'ventas': 1250000, 'clientes': 50000, 'productos': 8500}\n",
    "    return f'La tabla {table_name} tiene {counts.get(table_name, 0)} filas.'\n",
    "\n",
    "@tool\n",
    "def get_table_schema(table_name: str) -> str:\n",
    "    \"\"\"Devuelve el esquema de una tabla.\"\"\"\n",
    "    schemas = {\n",
    "        'ventas': 'venta_id (BIGINT), fecha (DATE), producto_id (INT), total (DECIMAL)',\n",
    "        'clientes': 'cliente_id (INT), nombre (VARCHAR), email (VARCHAR), ciudad (VARCHAR)'\n",
    "    }\n",
    "    return schemas.get(table_name, 'Tabla no encontrada')\n",
    "\n",
    "tools = [get_table_row_count, get_table_schema]\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull('hwchase17/openai-tools-agent')\n",
    "\n",
    "# Crear agente\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "result = agent_executor.invoke({\n",
    "    'input': '\u00bfCu\u00e1ntas filas tiene la tabla ventas y cu\u00e1l es su esquema?'\n",
    "})\n",
    "\n",
    "print('\\nRespuesta final:', result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5e773",
   "metadata": {},
   "source": [
    "## 2. Agente para debugging de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb254dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def validate_sql_syntax(query: str) -> str:\n",
    "    \"\"\"Valida sintaxis SQL.\"\"\"\n",
    "    # Simulaci\u00f3n simple\n",
    "    errors = []\n",
    "    if 'SELCT' in query.upper():\n",
    "        errors.append('Typo: SELCT deber\u00eda ser SELECT')\n",
    "    if query.count('(') != query.count(')'):\n",
    "        errors.append('Par\u00e9ntesis desbalanceados')\n",
    "    return 'Errores: ' + ', '.join(errors) if errors else 'Sintaxis v\u00e1lida'\n",
    "\n",
    "@tool\n",
    "def suggest_sql_optimization(query: str) -> str:\n",
    "    \"\"\"Sugiere optimizaciones.\"\"\"\n",
    "    tips = []\n",
    "    if 'SELECT *' in query.upper():\n",
    "        tips.append('Evita SELECT *, especifica columnas')\n",
    "    if 'WHERE' not in query.upper() and 'JOIN' in query.upper():\n",
    "        tips.append('Considera agregar WHERE para filtrar resultados')\n",
    "    return 'Sugerencias: ' + ', '.join(tips) if tips else 'Query est\u00e1 bien optimizada'\n",
    "\n",
    "debug_tools = [validate_sql_syntax, suggest_sql_optimization]\n",
    "\n",
    "debug_agent = create_openai_tools_agent(llm, debug_tools, prompt)\n",
    "debug_executor = AgentExecutor(agent=debug_agent, tools=debug_tools, verbose=True)\n",
    "\n",
    "broken_query = 'SELCT * FROM ventas JOIN productos'\n",
    "\n",
    "result = debug_executor.invoke({\n",
    "    'input': f'Analiza esta query y sugiere mejoras: {broken_query}'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c4b80",
   "metadata": {},
   "source": [
    "## 3. LangGraph: workflow con estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "class ETLState(TypedDict):\n",
    "    query: str\n",
    "    validated: bool\n",
    "    optimized: bool\n",
    "    result: str\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def validate_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de validaci\u00f3n.\"\"\"\n",
    "    if 'SELECT' not in state['query'].upper():\n",
    "        state['errors'].append('Query no es SELECT')\n",
    "        state['validated'] = False\n",
    "    else:\n",
    "        state['validated'] = True\n",
    "    return state\n",
    "\n",
    "def optimize_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de optimizaci\u00f3n.\"\"\"\n",
    "    if state['validated']:\n",
    "        # Simulaci\u00f3n\n",
    "        state['query'] = state['query'].replace('SELECT *', 'SELECT id, nombre')\n",
    "        state['optimized'] = True\n",
    "    return state\n",
    "\n",
    "def execute_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de ejecuci\u00f3n.\"\"\"\n",
    "    if state['optimized']:\n",
    "        state['result'] = f'Query ejecutada: {state[\"query\"]}'\n",
    "    else:\n",
    "        state['result'] = 'Ejecuci\u00f3n cancelada por errores'\n",
    "    return state\n",
    "\n",
    "# Crear grafo\n",
    "workflow = StateGraph(ETLState)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('optimize', optimize_node)\n",
    "workflow.add_node('execute', execute_node)\n",
    "\n",
    "workflow.set_entry_point('validate')\n",
    "workflow.add_edge('validate', 'optimize')\n",
    "workflow.add_edge('optimize', 'execute')\n",
    "workflow.add_edge('execute', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar\n",
    "initial_state = {\n",
    "    'query': 'SELECT * FROM ventas',\n",
    "    'validated': False,\n",
    "    'optimized': False,\n",
    "    'result': '',\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "print('Estado final:', final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cc64f",
   "metadata": {},
   "source": [
    "## 4. AutoGen: m\u00faltiples agentes colaborativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984980b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyautogen\n",
    "import autogen\n",
    "\n",
    "config_list = [{\n",
    "    'model': 'gpt-4',\n",
    "    'api_key': os.getenv('OPENAI_API_KEY')\n",
    "}]\n",
    "\n",
    "# Agente 1: Data Engineer\n",
    "data_engineer = autogen.AssistantAgent(\n",
    "    name='DataEngineer',\n",
    "    llm_config={'config_list': config_list},\n",
    "    system_message='''\n",
    "Eres un ingeniero de datos experto. Tu tarea es:\n",
    "1. Dise\u00f1ar pipelines ETL\n",
    "2. Escribir queries SQL optimizadas\n",
    "3. Proponer arquitecturas de datos\n",
    "'''\n",
    ")\n",
    "\n",
    "# Agente 2: QA Engineer\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config={'config_list': config_list},\n",
    "    system_message='''\n",
    "Eres un ingeniero de calidad. Tu tarea es:\n",
    "1. Revisar c\u00f3digo y queries\n",
    "2. Identificar bugs y problemas de rendimiento\n",
    "3. Sugerir tests\n",
    "'''\n",
    ")\n",
    "\n",
    "# Agente 3: User Proxy (humano)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='User',\n",
    "    human_input_mode='NEVER',\n",
    "    max_consecutive_auto_reply=0\n",
    ")\n",
    "\n",
    "# Conversaci\u00f3n\n",
    "task = '''\n",
    "Dise\u00f1a un pipeline que:\n",
    "1. Extraiga datos de API de ventas\n",
    "2. Transforme (agregue por d\u00eda)\n",
    "3. Cargue en Redshift\n",
    "Luego revisa el dise\u00f1o.\n",
    "'''\n",
    "\n",
    "# Iniciar chat grupal\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, data_engineer, qa_engineer],\n",
    "    messages=[],\n",
    "    max_round=5\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={'config_list': config_list})\n",
    "\n",
    "user_proxy.initiate_chat(manager, message=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b51a15",
   "metadata": {},
   "source": [
    "## 5. Agente para monitoreo de data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def check_null_rate(table: str, column: str) -> str:\n",
    "    \"\"\"Verifica tasa de nulos.\"\"\"\n",
    "    # Simulaci\u00f3n\n",
    "    rates = {('ventas', 'total'): 0.5, ('clientes', 'email'): 15.2}\n",
    "    rate = rates.get((table, column), 0)\n",
    "    return f'Tasa de nulos en {table}.{column}: {rate}%'\n",
    "\n",
    "@tool\n",
    "def check_duplicates(table: str) -> str:\n",
    "    \"\"\"Verifica duplicados.\"\"\"\n",
    "    dup_counts = {'ventas': 120, 'clientes': 5}\n",
    "    count = dup_counts.get(table, 0)\n",
    "    return f'Duplicados en {table}: {count}'\n",
    "\n",
    "quality_tools = [check_null_rate, check_duplicates]\n",
    "\n",
    "quality_agent = create_openai_tools_agent(llm, quality_tools, prompt)\n",
    "quality_executor = AgentExecutor(agent=quality_agent, tools=quality_tools, verbose=True)\n",
    "\n",
    "quality_result = quality_executor.invoke({\n",
    "    'input': 'Revisa la calidad de la tabla ventas: nulos y duplicados'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979b017",
   "metadata": {},
   "source": [
    "## 6. Agente reactivo con memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "agent_with_memory = create_openai_tools_agent(llm, tools, prompt)\n",
    "executor_with_memory = AgentExecutor(\n",
    "    agent=agent_with_memory,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Conversaci\u00f3n\n",
    "executor_with_memory.invoke({'input': 'Cu\u00e1ntas filas tiene ventas?'})\n",
    "executor_with_memory.invoke({'input': 'Y cu\u00e1l es su esquema?'})  # Contexto previo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813ba35",
   "metadata": {},
   "source": [
    "## 7. Buenas pr\u00e1cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f7824",
   "metadata": {},
   "source": [
    "- **Tools espec\u00edficas**: cada tool debe hacer UNA cosa bien.\n",
    "- **Validaci\u00f3n de outputs**: verifica respuestas antes de acciones cr\u00edticas.\n",
    "- **Human-in-the-loop**: aprobaci\u00f3n humana para operaciones destructivas.\n",
    "- **Observabilidad**: loggea cada decisi\u00f3n del agente.\n",
    "- **Fallback**: maneja errores de LLM o tools.\n",
    "- **Testing**: prueba agentes con casos edge.\n",
    "- **Costos**: limita llamadas y usa modelos peque\u00f1os cuando posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7b1fd",
   "metadata": {},
   "source": [
    "## 8. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5addc59",
   "metadata": {},
   "source": [
    "1. Crea un agente que automatice la investigaci\u00f3n de incidentes en pipelines.\n",
    "2. Construye un agente que genere data quality reports diarios.\n",
    "3. Implementa un sistema multi-agente para code review de PRs.\n",
    "4. Desarrolla un agente que optimice queries en producci\u00f3n autom\u00e1ticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\u2705 Validaci\u00f3n de Datos con LLMs \u2192](07_calidad_validacion_llm.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel GenAI:**\n",
    "- [\ud83d\udd04 Comparaci\u00f3n: OpenAI vs Google Gemini](00_comparacion_openai_gemini.ipynb)\n",
    "- [\ud83e\udd16 Fundamentos de LLMs y Prompting](01_fundamentos_llms_prompting.ipynb)\n",
    "- [\ud83d\udcca Text-to-SQL: Generaci\u00f3n de Consultas SQL desde Lenguaje Natural](02_generacion_sql_nl2sql.ipynb)\n",
    "- [\ud83d\udd27 Generaci\u00f3n Autom\u00e1tica de C\u00f3digo ETL con LLMs](03_generacion_codigo_etl.ipynb)\n",
    "- [\ud83d\udcda RAG: Documentaci\u00f3n T\u00e9cnica con LLMs](04_rag_documentacion_datos.ipynb)\n",
    "- [\ud83d\udd0d Embeddings y Similitud en Datos](05_embeddings_similitud_datos.ipynb)\n",
    "- [\ud83e\udd16 Agentes Aut\u00f3nomos para Automatizaci\u00f3n](06_agentes_automatizacion.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\u2705 Validaci\u00f3n de Datos con LLMs](07_calidad_validacion_llm.ipynb)\n",
    "- [\ud83c\udfb2 Generaci\u00f3n de Datos Sint\u00e9ticos con LLMs](08_sintesis_aumento_datos.ipynb)\n",
    "- [\ud83d\ude80 Proyecto Integrador 1: Chatbot de Consulta de Datos con RAG](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83c\udfd7\ufe0f Proyecto Integrador 2: Plataforma Self-Service con GenAI](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
