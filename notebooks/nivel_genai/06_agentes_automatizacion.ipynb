{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03361ea",
   "metadata": {},
   "source": [
    "# ü§ñ Agentes Aut√≥nomos para Automatizaci√≥n\n",
    "\n",
    "Objetivo: construir agentes con LangGraph y AutoGen que automaticen tareas complejas de ingenier√≠a de datos: debugging, optimizaci√≥n de queries, orquestaci√≥n de pipelines.\n",
    "\n",
    "- Duraci√≥n: 120-150 min\n",
    "- Dificultad: Alta\n",
    "- Stack: LangChain, LangGraph, AutoGen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f25eee",
   "metadata": {},
   "source": [
    "## üß† Agentes Aut√≥nomos: De ReAct a Multi-Agent Systems\n",
    "\n",
    "Los **agentes aut√≥nomos** son sistemas que combinan LLMs con herramientas (tools) para realizar tareas complejas de forma iterativa, tomando decisiones basadas en observaciones y razonamiento. En Data Engineering, permiten automatizar debugging, optimizaci√≥n de queries, monitoreo de pipelines y respuesta a incidentes.\n",
    "\n",
    "### üèóÔ∏è Evoluci√≥n de Agentes\n",
    "\n",
    "```\n",
    "2021: ReAct (Reasoning + Acting)      ‚Üí LLM decide qu√© tool usar en cada paso\n",
    "2022: MRKL Systems                     ‚Üí Multi-tool reasoning con knowledge bases\n",
    "2023: LangChain Agents                 ‚Üí Framework con 100+ tools pre-built\n",
    "2023: AutoGen (Microsoft)              ‚Üí Multi-agent collaboration\n",
    "2024: LangGraph                        ‚Üí Workflows con estados y ciclos\n",
    "2024: Agentes de c√≥digo (Devin, etc.)  ‚Üí Autonomous software engineering\n",
    "```\n",
    "\n",
    "### üìê Arquitectura de Agentes ReAct\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    AGENT REACT LOOP                              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Input: \"¬øCu√°ntas filas tiene la tabla ventas y cu√°l es su      ‚îÇ\n",
    "‚îÇ          esquema?\"                                               ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. THOUGHT (Razonamiento)                             ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    LLM analiza la pregunta y decide acci√≥n            ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    \"Necesito dos cosas: row count y schema.           ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ     Primero obtendr√© el row count.\"                   ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. ACTION (Acci√≥n)                                     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    LLM selecciona tool y par√°metros                    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Tool: get_table_row_count                           ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Input: {\"table_name\": \"ventas\"}                     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. OBSERVATION (Resultado)                             ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Tool ejecuta y retorna resultado                    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Output: \"La tabla ventas tiene 1,250,000 filas.\"   ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. THOUGHT (Re-evaluar)                                ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    \"Tengo el row count. Ahora necesito el schema.\"    ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 5. ACTION                                              ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Tool: get_table_schema                              ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Input: {\"table_name\": \"ventas\"}                     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 6. OBSERVATION                                         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    Output: \"venta_id (BIGINT), fecha (DATE), ...\"     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 7. THOUGHT (Finalizar)                                 ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    \"Tengo toda la informaci√≥n necesaria.\"              ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚Üì                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 8. FINAL ANSWER                                        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    \"La tabla ventas tiene 1.25M filas. El esquema     ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ     incluye: venta_id (BIGINT), fecha (DATE)...\"      ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Ciclo completo: 2 iteraciones, 2 tools, respuesta completa     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üîß Implementaci√≥n de Agente para Data Engineering\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0, api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Tools para Data Engineering\n",
    "@tool\n",
    "def get_table_metadata(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene metadatos completos de una tabla: row count, schema, particiones, tama√±o.\n",
    "    \n",
    "    Args:\n",
    "        table_name: nombre de la tabla (ej. 'dwh.ventas')\n",
    "    \n",
    "    Returns:\n",
    "        JSON con metadatos\n",
    "    \"\"\"\n",
    "    # En producci√≥n: conectar a Snowflake/BigQuery/Redshift\n",
    "    metadata_db = {\n",
    "        'dwh.ventas': {\n",
    "            'row_count': 125_000_000,\n",
    "            'size_gb': 45.2,\n",
    "            'schema': {\n",
    "                'venta_id': 'BIGINT PRIMARY KEY',\n",
    "                'fecha': 'DATE NOT NULL',\n",
    "                'cliente_id': 'INT FOREIGN KEY',\n",
    "                'producto_id': 'INT FOREIGN KEY',\n",
    "                'cantidad': 'INT',\n",
    "                'total': 'DECIMAL(10,2)'\n",
    "            },\n",
    "            'partitions': ['fecha'],\n",
    "            'last_updated': '2024-10-30 23:45:00',\n",
    "            'owner': 'data-engineering'\n",
    "        },\n",
    "        'dwh.clientes': {\n",
    "            'row_count': 5_000_000,\n",
    "            'size_gb': 2.8,\n",
    "            'schema': {\n",
    "                'cliente_id': 'INT PRIMARY KEY',\n",
    "                'nombre': 'VARCHAR(100)',\n",
    "                'email': 'VARCHAR(100)',\n",
    "                'ciudad': 'VARCHAR(50)',\n",
    "                'fecha_registro': 'DATE'\n",
    "            },\n",
    "            'partitions': None,\n",
    "            'last_updated': '2024-10-30 22:00:00',\n",
    "            'owner': 'data-engineering'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name not in metadata_db:\n",
    "        return json.dumps({'error': f'Tabla {table_name} no encontrada'})\n",
    "    \n",
    "    return json.dumps(metadata_db[table_name], indent=2)\n",
    "\n",
    "@tool\n",
    "def explain_query_plan(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analiza el execution plan de una query SQL y identifica cuellos de botella.\n",
    "    \n",
    "    Args:\n",
    "        query: query SQL a analizar\n",
    "    \n",
    "    Returns:\n",
    "        An√°lisis del plan de ejecuci√≥n con recomendaciones\n",
    "    \"\"\"\n",
    "    # En producci√≥n: EXPLAIN ANALYZE en la base de datos\n",
    "    analysis = {\n",
    "        'estimated_rows': 1_250_000,\n",
    "        'estimated_cost': 15234.50,\n",
    "        'operations': [\n",
    "            {\n",
    "                'step': 1,\n",
    "                'operation': 'Seq Scan on ventas',\n",
    "                'cost': 12000.00,\n",
    "                'rows': 125_000_000,\n",
    "                'warning': 'Full table scan - considera agregar √≠ndice'\n",
    "            },\n",
    "            {\n",
    "                'step': 2,\n",
    "                'operation': 'Hash Join',\n",
    "                'cost': 3234.50,\n",
    "                'rows': 1_250_000,\n",
    "                'info': 'Join eficiente con hash'\n",
    "            }\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Agregar √≠ndice en ventas.fecha para filtros',\n",
    "            'Considerar particionamiento por fecha',\n",
    "            'Usar SELECT con columnas espec√≠ficas en vez de *'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return json.dumps(analysis, indent=2)\n",
    "\n",
    "@tool\n",
    "def check_pipeline_status(pipeline_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Verifica el estado de un pipeline de Airflow/Prefect.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_name: nombre del DAG/flow\n",
    "    \n",
    "    Returns:\n",
    "        Estado actual y √∫ltimas ejecuciones\n",
    "    \"\"\"\n",
    "    # En producci√≥n: conectar a Airflow API\n",
    "    pipeline_status = {\n",
    "        'daily_sales_etl': {\n",
    "            'status': 'FAILED',\n",
    "            'last_run': '2024-10-30 23:00:00',\n",
    "            'duration_seconds': 1245,\n",
    "            'error': 'Connection timeout to Snowflake',\n",
    "            'recent_runs': [\n",
    "                {'date': '2024-10-30', 'status': 'FAILED', 'duration': 1245},\n",
    "                {'date': '2024-10-29', 'status': 'SUCCESS', 'duration': 892},\n",
    "                {'date': '2024-10-28', 'status': 'SUCCESS', 'duration': 901}\n",
    "            ],\n",
    "            'next_run': '2024-10-31 23:00:00'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if pipeline_name not in pipeline_status:\n",
    "        return json.dumps({'error': f'Pipeline {pipeline_name} no encontrado'})\n",
    "    \n",
    "    return json.dumps(pipeline_status[pipeline_name], indent=2)\n",
    "\n",
    "@tool\n",
    "def run_data_quality_check(table_name: str, check_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta validaciones de calidad de datos.\n",
    "    \n",
    "    Args:\n",
    "        table_name: tabla a validar\n",
    "        check_type: tipo de check ('nulls', 'duplicates', 'freshness', 'schema')\n",
    "    \n",
    "    Returns:\n",
    "        Resultados de la validaci√≥n\n",
    "    \"\"\"\n",
    "    # En producci√≥n: integrar con Great Expectations, dbt tests, etc.\n",
    "    quality_results = {\n",
    "        'dwh.ventas': {\n",
    "            'nulls': {\n",
    "                'total': 'NULL rate: 0.05%',\n",
    "                'fecha': 'NULL rate: 0%',\n",
    "                'cliente_id': 'NULL rate: 0.2%',\n",
    "                'status': 'PASS'\n",
    "            },\n",
    "            'duplicates': {\n",
    "                'count': 120,\n",
    "                'percentage': '0.0001%',\n",
    "                'status': 'WARNING'\n",
    "            },\n",
    "            'freshness': {\n",
    "                'last_update': '2024-10-30 23:45:00',\n",
    "                'delay_hours': 0.25,\n",
    "                'status': 'PASS'\n",
    "            },\n",
    "            'schema': {\n",
    "                'columns_expected': 6,\n",
    "                'columns_actual': 6,\n",
    "                'mismatches': [],\n",
    "                'status': 'PASS'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name not in quality_results:\n",
    "        return json.dumps({'error': f'No hay checks configurados para {table_name}'})\n",
    "    \n",
    "    if check_type not in quality_results[table_name]:\n",
    "        return json.dumps({'error': f'Check type {check_type} no soportado'})\n",
    "    \n",
    "    return json.dumps(quality_results[table_name][check_type], indent=2)\n",
    "\n",
    "@tool\n",
    "def suggest_query_optimization(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analiza una query y sugiere optimizaciones espec√≠ficas.\n",
    "    \n",
    "    Args:\n",
    "        query: query SQL a optimizar\n",
    "    \n",
    "    Returns:\n",
    "        Lista de sugerencias con query mejorada\n",
    "    \"\"\"\n",
    "    suggestions = []\n",
    "    optimized_query = query\n",
    "    \n",
    "    # An√°lisis de patrones anti-pattern\n",
    "    if 'SELECT *' in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'SELECT * es ineficiente',\n",
    "            'impact': 'Alto - lee columnas innecesarias',\n",
    "            'recommendation': 'Especificar solo columnas necesarias',\n",
    "            'example': 'SELECT id, fecha, total FROM ...'\n",
    "        })\n",
    "    \n",
    "    if 'WHERE' not in query.upper() and 'JOIN' in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'JOIN sin filtros WHERE',\n",
    "            'impact': 'Alto - procesa todos los registros',\n",
    "            'recommendation': 'Agregar filtros WHERE para reducir dataset',\n",
    "            'example': 'WHERE fecha >= CURRENT_DATE - 30'\n",
    "        })\n",
    "    \n",
    "    if 'DISTINCT' in query.upper() and 'GROUP BY' not in query.upper():\n",
    "        suggestions.append({\n",
    "            'issue': 'DISTINCT sin GROUP BY puede ser lento',\n",
    "            'impact': 'Medio - sort costoso',\n",
    "            'recommendation': 'Considerar GROUP BY si es posible',\n",
    "            'example': 'GROUP BY columnas en vez de DISTINCT'\n",
    "        })\n",
    "    \n",
    "    return json.dumps({\n",
    "        'original_query': query,\n",
    "        'suggestions': suggestions,\n",
    "        'priority': 'HIGH' if len(suggestions) >= 2 else 'MEDIUM'\n",
    "    }, indent=2)\n",
    "\n",
    "# Lista de tools\n",
    "tools = [\n",
    "    get_table_metadata,\n",
    "    explain_query_plan,\n",
    "    check_pipeline_status,\n",
    "    run_data_quality_check,\n",
    "    suggest_query_optimization\n",
    "]\n",
    "\n",
    "# Prompt customizado para Data Engineering\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un Senior Data Engineer experto en:\n",
    "- An√°lisis y optimizaci√≥n de queries SQL\n",
    "- Debugging de pipelines de datos (Airflow, dbt, Spark)\n",
    "- Data quality y governance\n",
    "- Arquitectura de data warehouses (Snowflake, BigQuery, Redshift)\n",
    "\n",
    "Tu trabajo es ayudar a resolver problemas usando las tools disponibles.\n",
    "Siempre razona paso a paso y usa m√∫ltiples tools si es necesario para dar una respuesta completa.\n",
    "\n",
    "Cuando analices problemas:\n",
    "1. Primero obt√©n contexto (metadatos, estado actual)\n",
    "2. Identifica el root cause\n",
    "3. Sugiere soluciones accionables\n",
    "4. Prioriza por impacto\n",
    "\n",
    "S√© conciso pero preciso.\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Crear agente\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    max_iterations=10,  # L√≠mite de seguridad\n",
    "    max_execution_time=60,  # Timeout 60 segundos\n",
    "    handle_parsing_errors=True  # Manejo robusto de errores\n",
    ")\n",
    "\n",
    "# Ejemplo 1: Investigaci√≥n de tabla\n",
    "print(\"=\" * 80)\n",
    "print(\"CASO 1: An√°lisis de tabla\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result1 = agent_executor.invoke({\n",
    "    \"input\": \"\"\"Necesito analizar la tabla dwh.ventas.\n",
    "    ¬øCu√°ntas filas tiene? ¬øEst√° bien particionada? ¬øCu√°l es su tama√±o?\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Respuesta del agente:\")\n",
    "print(result1['output'])\n",
    "\n",
    "# Ejemplo 2: Debugging de pipeline\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 2: Pipeline fallando\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result2 = agent_executor.invoke({\n",
    "    \"input\": \"\"\"El pipeline 'daily_sales_etl' est√° fallando.\n",
    "    Investiga qu√© pas√≥ y sugiere c√≥mo solucionarlo.\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\nüîß Respuesta del agente:\")\n",
    "print(result2['output'])\n",
    "\n",
    "# Ejemplo 3: Optimizaci√≥n de query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 3: Query lenta\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "slow_query = \"\"\"\n",
    "SELECT * \n",
    "FROM dwh.ventas v\n",
    "JOIN dwh.clientes c ON v.cliente_id = c.cliente_id\n",
    "\"\"\"\n",
    "\n",
    "result3 = agent_executor.invoke({\n",
    "    \"input\": f\"\"\"Esta query es muy lenta: {slow_query}\n",
    "    Analiza el execution plan y sugiere optimizaciones.\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n‚ö° Respuesta del agente:\")\n",
    "print(result3['output'])\n",
    "```\n",
    "\n",
    "### üìä Comparaci√≥n de Frameworks de Agentes\n",
    "\n",
    "| Framework | Tipo | Complejidad | Casos de Uso | Estado |\n",
    "|-----------|------|-------------|--------------|---------|\n",
    "| **LangChain Agents** | Single agent + tools | Medio | Tareas simples con 1-5 tools | Estable, producci√≥n |\n",
    "| **LangGraph** | Stateful workflows | Alto | Workflows complejos, ciclos, branches | Estable, recomendado |\n",
    "| **AutoGen** | Multi-agent collaboration | Muy alto | M√∫ltiples agentes especializados | Experimental |\n",
    "| **CrewAI** | Hierarchical multi-agent | Alto | Equipos con roles y jerarqu√≠as | Emergente |\n",
    "| **BabyAGI / AutoGPT** | Autonomous goal-oriented | Muy alto | Investigaci√≥n, no producci√≥n | Experimental |\n",
    "\n",
    "### üéØ Anatom√≠a de una Tool de Calidad\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class QueryOptimizationInput(BaseModel):\n",
    "    \"\"\"Schema de input para validaci√≥n.\"\"\"\n",
    "    query: str = Field(description=\"Query SQL a optimizar\")\n",
    "    database: Literal[\"snowflake\", \"bigquery\", \"redshift\"] = Field(\n",
    "        default=\"snowflake\",\n",
    "        description=\"Tipo de base de datos\"\n",
    "    )\n",
    "    max_cost: float = Field(\n",
    "        default=1000.0,\n",
    "        description=\"Costo m√°ximo aceptable del query\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=QueryOptimizationInput)\n",
    "def optimize_query_production(query: str, database: str = \"snowflake\", max_cost: float = 1000.0) -> str:\n",
    "    \"\"\"\n",
    "    Tool de producci√≥n para optimizaci√≥n de queries con validaci√≥n.\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Type hints y validation con Pydantic\n",
    "    - Error handling robusto\n",
    "    - Logging de todas las invocaciones\n",
    "    - Timeouts y rate limiting\n",
    "    - Retries con exponential backoff\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"optimize_query invoked: database={database}, max_cost={max_cost}\")\n",
    "    \n",
    "    try:\n",
    "        # Validaci√≥n de input\n",
    "        if len(query) > 10000:\n",
    "            raise ValueError(\"Query demasiado larga (>10K caracteres)\")\n",
    "        \n",
    "        if not query.strip().upper().startswith('SELECT'):\n",
    "            raise ValueError(\"Solo queries SELECT est√°n soportadas\")\n",
    "        \n",
    "        # Simulaci√≥n de an√°lisis con retry\n",
    "        @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "        def analyze_with_retry():\n",
    "            # En producci√≥n: llamar a servicio de an√°lisis\n",
    "            time.sleep(0.5)  # Simular latencia\n",
    "            return {\n",
    "                'estimated_cost': 850.0,\n",
    "                'suggestions': ['Agregar √≠ndice en fecha', 'Usar columnas espec√≠ficas'],\n",
    "                'optimized_query': query.replace('SELECT *', 'SELECT id, fecha')\n",
    "            }\n",
    "        \n",
    "        result = analyze_with_retry()\n",
    "        \n",
    "        # Validaci√≥n de output\n",
    "        if result['estimated_cost'] > max_cost:\n",
    "            logger.warning(f\"Query costo {result['estimated_cost']} excede m√°ximo {max_cost}\")\n",
    "            return json.dumps({\n",
    "                'status': 'WARNING',\n",
    "                'message': f'Costo estimado {result[\"estimated_cost\"]} excede l√≠mite {max_cost}',\n",
    "                'suggestions': result['suggestions']\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Optimizaci√≥n exitosa: costo reducido a {result['estimated_cost']}\")\n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en optimize_query: {str(e)}\")\n",
    "        return json.dumps({'error': str(e), 'status': 'FAILED'})\n",
    "```\n",
    "\n",
    "### üö® Patrones de Seguridad y Control\n",
    "\n",
    "```python\n",
    "from typing import Callable\n",
    "import functools\n",
    "\n",
    "def require_approval(func: Callable) -> Callable:\n",
    "    \"\"\"Decorator para tools que requieren aprobaci√≥n humana.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"\\n‚ö†Ô∏è  ACCI√ìN REQUIERE APROBACI√ìN:\")\n",
    "        print(f\"   Tool: {func.__name__}\")\n",
    "        print(f\"   Args: {args}, {kwargs}\")\n",
    "        \n",
    "        approval = input(\"   ¬øAprobar? (y/n): \")\n",
    "        if approval.lower() != 'y':\n",
    "            return json.dumps({'status': 'REJECTED', 'reason': 'User declined'})\n",
    "        \n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@tool\n",
    "@require_approval\n",
    "def drop_table(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina una tabla (PELIGROSO - requiere aprobaci√≥n).\n",
    "    \n",
    "    Args:\n",
    "        table_name: nombre de la tabla a eliminar\n",
    "    \"\"\"\n",
    "    # En producci√≥n: ejecutar DROP TABLE\n",
    "    return json.dumps({\n",
    "        'status': 'SUCCESS',\n",
    "        'message': f'Tabla {table_name} eliminada',\n",
    "        'timestamp': '2024-10-30 23:50:00'\n",
    "    })\n",
    "\n",
    "# Rate limiting para evitar costos excesivos\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Rate limiter para tools costosas.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_calls: int, time_window: int):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_window = time_window\n",
    "        self.calls = deque()\n",
    "    \n",
    "    def allow_call(self) -> bool:\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remover llamadas fuera de la ventana\n",
    "        while self.calls and self.calls[0] < now - self.time_window:\n",
    "            self.calls.popleft()\n",
    "        \n",
    "        if len(self.calls) >= self.max_calls:\n",
    "            return False\n",
    "        \n",
    "        self.calls.append(now)\n",
    "        return True\n",
    "\n",
    "# Uso\n",
    "limiter = RateLimiter(max_calls=10, time_window=60)  # 10 calls/minuto\n",
    "\n",
    "@tool\n",
    "def expensive_llm_analysis(text: str) -> str:\n",
    "    \"\"\"Tool costosa con rate limiting.\"\"\"\n",
    "    if not limiter.allow_call():\n",
    "        return json.dumps({\n",
    "            'error': 'Rate limit exceeded',\n",
    "            'message': 'M√°ximo 10 llamadas por minuto'\n",
    "        })\n",
    "    \n",
    "    # An√°lisis costoso...\n",
    "    return json.dumps({'result': 'analysis complete'})\n",
    "```\n",
    "\n",
    "### üí∞ Optimizaci√≥n de Costos\n",
    "\n",
    "```python\n",
    "# Estrategia 1: Usar modelos peque√±os para decisiones simples\n",
    "llm_cheap = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)  # $0.50/$1.50 por 1M tokens\n",
    "llm_expensive = ChatOpenAI(model='gpt-4o', temperature=0)     # $2.50/$10.00 por 1M tokens\n",
    "\n",
    "# Estrategia 2: Limitar iteraciones\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    max_iterations=5,  # Evitar loops infinitos\n",
    "    early_stopping_method=\"generate\"  # Forzar respuesta despu√©s de max_iterations\n",
    ")\n",
    "\n",
    "# Estrategia 3: Cache de decisiones\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_tool_call(tool_name: str, input_hash: str):\n",
    "    \"\"\"Cache de resultados de tools determin√≠sticas.\"\"\"\n",
    "    # Si el tool siempre retorna lo mismo para el mismo input, cachear\n",
    "    pass\n",
    "\n",
    "# Estrategia 4: Monitoring de costos\n",
    "import tiktoken\n",
    "\n",
    "def estimate_agent_cost(messages: List[dict], model: str = \"gpt-4o\") -> float:\n",
    "    \"\"\"Estima costo de una conversaci√≥n de agente.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    total_tokens = sum(len(encoding.encode(msg['content'])) for msg in messages)\n",
    "    \n",
    "    # Precios por 1M tokens (input / output)\n",
    "    prices = {\n",
    "        'gpt-4o': (2.50, 10.00),\n",
    "        'gpt-3.5-turbo': (0.50, 1.50)\n",
    "    }\n",
    "    \n",
    "    # Asumir 50% input, 50% output\n",
    "    input_tokens = total_tokens * 0.5\n",
    "    output_tokens = total_tokens * 0.5\n",
    "    \n",
    "    input_price, output_price = prices[model]\n",
    "    cost = (input_tokens / 1_000_000 * input_price) + (output_tokens / 1_000_000 * output_price)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Ejemplo\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'Eres un data engineer...'},\n",
    "    {'role': 'user', 'content': 'Analiza esta tabla...'},\n",
    "    {'role': 'assistant', 'content': 'Voy a obtener los metadatos...'}\n",
    "]\n",
    "\n",
    "cost = estimate_agent_cost(messages, model='gpt-4o')\n",
    "print(f\"Costo estimado: ${cost:.4f}\")\n",
    "```\n",
    "\n",
    "### üéØ Reglas de Oro para Agentes en Producci√≥n\n",
    "\n",
    "1. **Siempre validar inputs**: Tools reciben texto generado por LLM (puede ser malicioso)\n",
    "2. **Timeouts obligatorios**: Max execution time para evitar loops infinitos\n",
    "3. **Human-in-the-loop**: Acciones destructivas (DROP, DELETE, TRUNCATE) requieren aprobaci√≥n\n",
    "4. **Logging exhaustivo**: Toda decisi√≥n del agente debe loggearse para debugging\n",
    "5. **Rate limiting**: Proteger APIs externas y controlar costos\n",
    "6. **Graceful degradation**: Si tool falla, el agente debe continuar con otras tools\n",
    "7. **Monitoring de costos**: Alertar si costo por sesi√≥n excede threshold\n",
    "8. **Testing con mocks**: Probar agentes sin ejecutar tools reales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903fa64",
   "metadata": {},
   "source": [
    "## üîÑ LangGraph: Workflows Complejos con Estados y Ciclos\n",
    "\n",
    "**LangGraph** es un framework para construir agentes con **flujos de trabajo complejos** que incluyen estados persistentes, ramificaciones condicionales, ciclos (loops) y m√∫ltiples rutas de ejecuci√≥n. Ideal para pipelines de Data Engineering donde las decisiones dependen de resultados previos.\n",
    "\n",
    "### üèóÔ∏è Arquitectura de LangGraph\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ               LANGGRAPH: STATEFUL WORKFLOW                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Componentes Principales:                                        ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£ STATE: Objeto compartido entre todos los nodos             ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
    "‚îÇ     ‚îÇ class DataPipelineState(TypedDict):      ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     query: str                            ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     validated: bool                       ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     optimized: bool                       ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     execution_plan: dict                  ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     results: list                         ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îÇ     errors: list                          ‚îÇ               ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ NODES: Funciones que modifican el estado                   ‚îÇ\n",
    "‚îÇ     def validate_node(state) -> state:                          ‚îÇ\n",
    "‚îÇ         state['validated'] = check_syntax(state['query'])       ‚îÇ\n",
    "‚îÇ         return state                                            ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ EDGES: Conexiones entre nodos (condicionales o fijas)      ‚îÇ\n",
    "‚îÇ     - Simple edge: validate ‚Üí optimize                          ‚îÇ\n",
    "‚îÇ     - Conditional edge: if validated ‚Üí optimize else ‚Üí error    ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£ GRAPH: Estructura del workflow                             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ     START                                                        ‚îÇ\n",
    "‚îÇ       ‚Üì                                                          ‚îÇ\n",
    "‚îÇ   [Validate Query]                                               ‚îÇ\n",
    "‚îÇ       ‚Üì                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê                                                      ‚îÇ\n",
    "‚îÇ   ‚îÇ Valid? ‚îÇ                                                     ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ\n",
    "‚îÇ       ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ   Yes ‚Üì         No ‚Üì                                             ‚îÇ\n",
    "‚îÇ [Optimize]   [Error Handler]                                     ‚îÇ\n",
    "‚îÇ       ‚Üì             ‚Üì                                            ‚îÇ\n",
    "‚îÇ   [Execute]     [Report]                                         ‚îÇ\n",
    "‚îÇ       ‚Üì             ‚Üì                                            ‚îÇ\n",
    "‚îÇ   [Monitor]      END                                             ‚îÇ\n",
    "‚îÇ       ‚Üì                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                     ‚îÇ\n",
    "‚îÇ   ‚îÇSuccess?‚îÇ                                                     ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                     ‚îÇ\n",
    "‚îÇ       ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ   Yes ‚Üì         No ‚Üì                                             ‚îÇ\n",
    "‚îÇ    END       [Retry] ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
    "‚îÇ                 ‚Üì        ‚îÇ                                       ‚îÇ\n",
    "‚îÇ              (ciclo back)‚îÇ                                       ‚îÇ\n",
    "‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üîß Implementaci√≥n: Pipeline ETL con Validaci√≥n y Retry\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import time\n",
    "\n",
    "# 1. Definir estado del workflow\n",
    "class ETLPipelineState(TypedDict):\n",
    "    \"\"\"Estado compartido del pipeline ETL.\"\"\"\n",
    "    source_query: str\n",
    "    validated: bool\n",
    "    optimized: bool\n",
    "    execution_plan: dict\n",
    "    executed: bool\n",
    "    results: Annotated[list, operator.add]  # operator.add permite append\n",
    "    errors: Annotated[list, operator.add]\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "\n",
    "# 2. Definir nodos (funciones que transforman el estado)\n",
    "\n",
    "def validate_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Valida sintaxis de la query.\"\"\"\n",
    "    print(f\"\\nüîç VALIDATE: Validando query...\")\n",
    "    \n",
    "    query = state['source_query']\n",
    "    \n",
    "    # Validaciones b√°sicas\n",
    "    errors = []\n",
    "    if not query.strip().upper().startswith('SELECT'):\n",
    "        errors.append('Query debe comenzar con SELECT')\n",
    "    \n",
    "    if query.count('(') != query.count(')'):\n",
    "        errors.append('Par√©ntesis desbalanceados')\n",
    "    \n",
    "    if 'SELCT' in query.upper():\n",
    "        errors.append('Typo detectado: SELCT ‚Üí SELECT')\n",
    "    \n",
    "    # LLM para validaci√≥n sem√°ntica avanzada\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)  # Modelo barato para validaci√≥n\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"Eres un experto en SQL. Valida la query y reporta errores.\"),\n",
    "        HumanMessage(content=f\"Query: {query}\\n\\n¬øHay errores de sintaxis o l√≥gica?\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        llm_feedback = response.content\n",
    "        \n",
    "        if 'error' in llm_feedback.lower() or 'incorrecto' in llm_feedback.lower():\n",
    "            errors.append(f'LLM validation: {llm_feedback[:100]}...')\n",
    "    except Exception as e:\n",
    "        errors.append(f'Error en validaci√≥n LLM: {str(e)}')\n",
    "    \n",
    "    # Actualizar estado\n",
    "    if errors:\n",
    "        state['validated'] = False\n",
    "        state['errors'].extend(errors)\n",
    "        print(f\"   ‚ùå Validaci√≥n fall√≥: {len(errors)} errores\")\n",
    "    else:\n",
    "        state['validated'] = True\n",
    "        print(f\"   ‚úÖ Validaci√≥n exitosa\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def optimize_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Optimiza la query usando LLM.\"\"\"\n",
    "    print(f\"\\n‚ö° OPTIMIZE: Optimizando query...\")\n",
    "    \n",
    "    if not state['validated']:\n",
    "        print(\"   ‚è≠Ô∏è  Saltando optimizaci√≥n (query no v√°lida)\")\n",
    "        return state\n",
    "    \n",
    "    query = state['source_query']\n",
    "    \n",
    "    # LLM para optimizaci√≥n\n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"Eres un experto en optimizaci√≥n de SQL.\n",
    "Reescribe la query para mejor performance:\n",
    "- Reemplaza SELECT * con columnas espec√≠ficas\n",
    "- Agrega filtros WHERE apropiados\n",
    "- Optimiza JOINs\n",
    "- Sugiere √≠ndices\n",
    "\n",
    "Retorna solo la query optimizada.\"\"\"),\n",
    "        HumanMessage(content=f\"Query original:\\n{query}\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        optimized_query = response.content.strip()\n",
    "        \n",
    "        # Actualizar estado\n",
    "        state['source_query'] = optimized_query\n",
    "        state['optimized'] = True\n",
    "        state['results'].append(f'Query optimizada: {optimized_query[:100]}...')\n",
    "        print(f\"   ‚úÖ Optimizaci√≥n completada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f'Error en optimizaci√≥n: {str(e)}')\n",
    "        print(f\"   ‚ùå Optimizaci√≥n fall√≥: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def generate_execution_plan_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Genera plan de ejecuci√≥n detallado.\"\"\"\n",
    "    print(f\"\\nüìã PLAN: Generando execution plan...\")\n",
    "    \n",
    "    if not state['optimized']:\n",
    "        print(\"   ‚è≠Ô∏è  Saltando plan (query no optimizada)\")\n",
    "        return state\n",
    "    \n",
    "    # Simular EXPLAIN ANALYZE\n",
    "    execution_plan = {\n",
    "        'estimated_rows': 125000,\n",
    "        'estimated_cost': 1234.56,\n",
    "        'estimated_time_seconds': 15.2,\n",
    "        'operations': [\n",
    "            {'step': 1, 'operation': 'Index Scan', 'cost': 234.56},\n",
    "            {'step': 2, 'operation': 'Hash Join', 'cost': 1000.00}\n",
    "        ],\n",
    "        'indexes_used': ['idx_ventas_fecha', 'idx_clientes_id'],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    state['execution_plan'] = execution_plan\n",
    "    state['results'].append(f'Plan generado: costo {execution_plan[\"estimated_cost\"]}')\n",
    "    print(f\"   ‚úÖ Plan listo: {execution_plan['estimated_rows']} filas estimadas\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def execute_query_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Ejecuta la query (simulado).\"\"\"\n",
    "    print(f\"\\nüöÄ EXECUTE: Ejecutando query...\")\n",
    "    \n",
    "    if not state['execution_plan']:\n",
    "        print(\"   ‚è≠Ô∏è  Saltando ejecuci√≥n (sin plan)\")\n",
    "        state['executed'] = False\n",
    "        return state\n",
    "    \n",
    "    # Simular ejecuci√≥n\n",
    "    time.sleep(1)  # Simular latencia\n",
    "    \n",
    "    # Simular error aleatorio (30% probabilidad)\n",
    "    import random\n",
    "    if random.random() < 0.3:\n",
    "        error_msg = 'Connection timeout to database'\n",
    "        state['errors'].append(error_msg)\n",
    "        state['executed'] = False\n",
    "        print(f\"   ‚ùå Ejecuci√≥n fall√≥: {error_msg}\")\n",
    "    else:\n",
    "        state['executed'] = True\n",
    "        state['results'].append('Query ejecutada exitosamente')\n",
    "        print(f\"   ‚úÖ Ejecuci√≥n exitosa\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def retry_handler_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Maneja reintentos con exponential backoff.\"\"\"\n",
    "    print(f\"\\nüîÑ RETRY: Intento {state['retry_count'] + 1}/{state['max_retries']}...\")\n",
    "    \n",
    "    state['retry_count'] += 1\n",
    "    \n",
    "    # Exponential backoff\n",
    "    wait_time = 2 ** state['retry_count']\n",
    "    print(f\"   ‚è≥ Esperando {wait_time} segundos antes de reintentar...\")\n",
    "    time.sleep(wait_time)\n",
    "    \n",
    "    # Reset flags para reintentar\n",
    "    state['executed'] = False\n",
    "    state['errors'] = []  # Limpiar errores previos\n",
    "    \n",
    "    return state\n",
    "\n",
    "def error_handler_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Maneja errores finales.\"\"\"\n",
    "    print(f\"\\n‚ùå ERROR HANDLER: Pipeline fall√≥\")\n",
    "    \n",
    "    error_summary = {\n",
    "        'total_errors': len(state['errors']),\n",
    "        'errors': state['errors'],\n",
    "        'retry_count': state['retry_count'],\n",
    "        'status': 'FAILED'\n",
    "    }\n",
    "    \n",
    "    state['results'].append(f'Pipeline fall√≥ despu√©s de {state[\"retry_count\"]} reintentos')\n",
    "    print(f\"   Total errores: {len(state['errors'])}\")\n",
    "    for error in state['errors']:\n",
    "        print(f\"     - {error}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def success_handler_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Maneja √©xito final.\"\"\"\n",
    "    print(f\"\\n‚úÖ SUCCESS: Pipeline completado\")\n",
    "    \n",
    "    state['results'].append('Pipeline completado exitosamente')\n",
    "    print(f\"   Total pasos: {len(state['results'])}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# 3. Funci√≥n de routing condicional\n",
    "\n",
    "def should_retry(state: ETLPipelineState) -> Literal[\"retry\", \"error_handler\"]:\n",
    "    \"\"\"Decide si reintentar o terminar con error.\"\"\"\n",
    "    if not state['executed'] and state['retry_count'] < state['max_retries']:\n",
    "        return \"retry\"\n",
    "    elif not state['executed']:\n",
    "        return \"error_handler\"\n",
    "    else:\n",
    "        # No deber√≠a llegar aqu√≠\n",
    "        return \"error_handler\"\n",
    "\n",
    "def should_execute(state: ETLPipelineState) -> Literal[\"execute\", \"error_handler\"]:\n",
    "    \"\"\"Decide si ejecutar o manejar error.\"\"\"\n",
    "    if state['validated'] and state['optimized']:\n",
    "        return \"execute\"\n",
    "    else:\n",
    "        return \"error_handler\"\n",
    "\n",
    "def check_execution_result(state: ETLPipelineState) -> Literal[\"success\", \"retry_or_fail\"]:\n",
    "    \"\"\"Decide siguiente paso despu√©s de ejecuci√≥n.\"\"\"\n",
    "    if state['executed']:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"retry_or_fail\"\n",
    "\n",
    "# 4. Construir el grafo\n",
    "\n",
    "workflow = StateGraph(ETLPipelineState)\n",
    "\n",
    "# Agregar nodos\n",
    "workflow.add_node(\"validate\", validate_query_node)\n",
    "workflow.add_node(\"optimize\", optimize_query_node)\n",
    "workflow.add_node(\"plan\", generate_execution_plan_node)\n",
    "workflow.add_node(\"execute\", execute_query_node)\n",
    "workflow.add_node(\"retry\", retry_handler_node)\n",
    "workflow.add_node(\"error_handler\", error_handler_node)\n",
    "workflow.add_node(\"success\", success_handler_node)\n",
    "\n",
    "# Definir flujo\n",
    "workflow.set_entry_point(\"validate\")\n",
    "\n",
    "# Edges condicionales\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    should_execute,\n",
    "    {\n",
    "        \"execute\": \"optimize\",  # Si valid√≥, optimizar\n",
    "        \"error_handler\": \"error_handler\"  # Si no valid√≥, error\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"optimize\", \"plan\")\n",
    "workflow.add_edge(\"plan\", \"execute\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"execute\",\n",
    "    check_execution_result,\n",
    "    {\n",
    "        \"success\": \"success\",\n",
    "        \"retry_or_fail\": \"retry\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retry\",\n",
    "    should_retry,\n",
    "    {\n",
    "        \"retry\": \"execute\",  # Ciclo de vuelta a execute\n",
    "        \"error_handler\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "workflow.add_edge(\"success\", END)\n",
    "\n",
    "# Compilar el grafo\n",
    "app = workflow.compile()\n",
    "\n",
    "# 5. Ejecutar el workflow\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EJECUTANDO WORKFLOW ETL CON LANGGRAPH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "initial_state = ETLPipelineState(\n",
    "    source_query=\"SELECT * FROM dwh.ventas WHERE fecha >= '2024-01-01'\",\n",
    "    validated=False,\n",
    "    optimized=False,\n",
    "    execution_plan={},\n",
    "    executed=False,\n",
    "    results=[],\n",
    "    errors=[],\n",
    "    retry_count=0,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# Invocar workflow\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "# Resultados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Validado: {final_state['validated']}\")\n",
    "print(f\"‚ö° Optimizado: {final_state['optimized']}\")\n",
    "print(f\"üöÄ Ejecutado: {final_state['executed']}\")\n",
    "print(f\"üîÑ Reintentos: {final_state['retry_count']}\")\n",
    "print(f\"\\nüìä Resultados ({len(final_state['results'])}):\")\n",
    "for i, result in enumerate(final_state['results'], 1):\n",
    "    print(f\"  {i}. {result}\")\n",
    "\n",
    "if final_state['errors']:\n",
    "    print(f\"\\n‚ùå Errores ({len(final_state['errors'])}):\")\n",
    "    for i, error in enumerate(final_state['errors'], 1):\n",
    "        print(f\"  {i}. {error}\")\n",
    "```\n",
    "\n",
    "### üé® Visualizaci√≥n del Grafo\n",
    "\n",
    "```python\n",
    "# LangGraph permite visualizar el workflow\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # Requiere pygraphviz: conda install pygraphviz\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo visualizar el grafo: {e}\")\n",
    "    print(\"\\nEstructura del grafo:\")\n",
    "    print(app.get_graph().to_json())\n",
    "```\n",
    "\n",
    "### üîÑ Patr√≥n: Human-in-the-Loop con Interrupciones\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Agregar checkpointing para pausar/reanudar\n",
    "memory = MemorySaver()\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Nodo que requiere aprobaci√≥n humana\n",
    "def approval_node(state: ETLPipelineState) -> ETLPipelineState:\n",
    "    \"\"\"Pausa para aprobaci√≥n humana.\"\"\"\n",
    "    print(f\"\\n‚è∏Ô∏è  APROBACI√ìN REQUERIDA:\")\n",
    "    print(f\"   Query optimizada: {state['source_query'][:200]}...\")\n",
    "    print(f\"   Costo estimado: {state['execution_plan'].get('estimated_cost', 0)}\")\n",
    "    \n",
    "    # En producci√≥n: enviar notificaci√≥n (Slack, email)\n",
    "    # y esperar respuesta as√≠ncrona\n",
    "    \n",
    "    # Aqu√≠ el workflow se pausa hasta que se reanude manualmente\n",
    "    return state\n",
    "\n",
    "# Agregar nodo de aprobaci√≥n al workflow\n",
    "workflow.add_node(\"approval\", approval_node)\n",
    "workflow.add_edge(\"plan\", \"approval\")\n",
    "workflow.add_edge(\"approval\", \"execute\")\n",
    "\n",
    "# Ejecutar con checkpointing\n",
    "config = {\"configurable\": {\"thread_id\": \"etl-pipeline-123\"}}\n",
    "\n",
    "# Primera ejecuci√≥n: llega hasta approval y se pausa\n",
    "result = app_with_memory.invoke(initial_state, config)\n",
    "\n",
    "# Simulaci√≥n: usuario aprueba despu√©s\n",
    "# Segunda ejecuci√≥n: contin√∫a desde donde se paus√≥\n",
    "result = app_with_memory.invoke(None, config)  # None = continuar con estado guardado\n",
    "```\n",
    "\n",
    "### üìä Comparaci√≥n: LangChain Agent vs LangGraph\n",
    "\n",
    "| Aspecto | LangChain Agent | LangGraph |\n",
    "|---------|----------------|-----------|\n",
    "| **Complejidad** | Simple (linear tool calls) | Compleja (estados, branches, ciclos) |\n",
    "| **Estado** | Ef√≠mero (solo en memoria del agente) | Persistente (compartido entre nodos) |\n",
    "| **Flujo** | Secuencial ReAct loop | DAG con condicionales y ciclos |\n",
    "| **Reintentos** | Manual (re-invoke agent) | Built-in con conditional edges |\n",
    "| **Debugging** | Dif√≠cil (caja negra) | F√°cil (cada nodo es observable) |\n",
    "| **Testing** | Dif√≠cil (todo el agente) | F√°cil (cada nodo por separado) |\n",
    "| **Human-in-the-loop** | Hack con callbacks | Native con checkpointing |\n",
    "| **Caso de uso** | Tareas simples 1-5 tools | Workflows complejos de producci√≥n |\n",
    "\n",
    "### üéØ Patr√≥n: Map-Reduce con LangGraph\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "class MapReduceState(TypedDict):\n",
    "    \"\"\"Estado para pattern map-reduce.\"\"\"\n",
    "    input_data: List[str]  # Lista de queries a procesar\n",
    "    mapped_results: Annotated[list, operator.add]\n",
    "    reduced_result: dict\n",
    "\n",
    "def map_node(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Procesa cada elemento en paralelo.\"\"\"\n",
    "    print(f\"\\nüó∫Ô∏è  MAP: Procesando {len(state['input_data'])} elementos...\")\n",
    "    \n",
    "    # En producci√≥n: usar ThreadPoolExecutor o asyncio\n",
    "    for i, query in enumerate(state['input_data']):\n",
    "        result = f\"Processed: {query[:50]}...\"\n",
    "        state['mapped_results'].append(result)\n",
    "        print(f\"   {i+1}/{len(state['input_data'])}: {result}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def reduce_node(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Agrega resultados.\"\"\"\n",
    "    print(f\"\\nüìä REDUCE: Agregando {len(state['mapped_results'])} resultados...\")\n",
    "    \n",
    "    state['reduced_result'] = {\n",
    "        'total_processed': len(state['mapped_results']),\n",
    "        'summary': 'All queries processed successfully'\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Total procesado: {state['reduced_result']['total_processed']}\")\n",
    "    return state\n",
    "\n",
    "# Construir workflow map-reduce\n",
    "mr_workflow = StateGraph(MapReduceState)\n",
    "mr_workflow.add_node(\"map\", map_node)\n",
    "mr_workflow.add_node(\"reduce\", reduce_node)\n",
    "mr_workflow.set_entry_point(\"map\")\n",
    "mr_workflow.add_edge(\"map\", \"reduce\")\n",
    "mr_workflow.add_edge(\"reduce\", END)\n",
    "\n",
    "mr_app = mr_workflow.compile()\n",
    "\n",
    "# Ejecutar\n",
    "queries = [\n",
    "    \"SELECT * FROM table1\",\n",
    "    \"SELECT * FROM table2\",\n",
    "    \"SELECT * FROM table3\"\n",
    "]\n",
    "\n",
    "result = mr_app.invoke(MapReduceState(\n",
    "    input_data=queries,\n",
    "    mapped_results=[],\n",
    "    reduced_result={}\n",
    "))\n",
    "\n",
    "print(f\"\\nResultado final: {result['reduced_result']}\")\n",
    "```\n",
    "\n",
    "### üöÄ Casos de Uso Reales en Data Engineering\n",
    "\n",
    "| Caso de Uso | Nodos del Workflow | Complejidad |\n",
    "|-------------|-------------------|-------------|\n",
    "| **ETL con validaci√≥n** | Extract ‚Üí Validate ‚Üí Transform ‚Üí Load ‚Üí Monitor | Media |\n",
    "| **Incident response** | Detect ‚Üí Investigate ‚Üí Diagnose ‚Üí Fix ‚Üí Verify | Alta |\n",
    "| **Query optimization** | Analyze ‚Üí Suggest ‚Üí Test ‚Üí Apply ‚Üí Benchmark | Media |\n",
    "| **Data quality monitoring** | Check nulls ‚Üí Check duplicates ‚Üí Check freshness ‚Üí Alert ‚Üí Report | Baja |\n",
    "| **Schema migration** | Analyze impact ‚Üí Generate DDL ‚Üí Approve ‚Üí Execute ‚Üí Rollback if fail | Alta |\n",
    "| **Pipeline orchestration** | Schedule ‚Üí Run stages ‚Üí Handle failures ‚Üí Retry ‚Üí Notify | Alta |\n",
    "\n",
    "### üí° Mejores Pr√°cticas con LangGraph\n",
    "\n",
    "1. **Estado m√≠nimo**: Solo incluir datos necesarios (no cachear todo)\n",
    "2. **Nodos peque√±os**: Cada nodo = 1 responsabilidad clara\n",
    "3. **Idempotencia**: Nodos deben ser re-ejecutables sin side effects\n",
    "4. **Error handling**: Cada nodo debe capturar y loggear sus propios errores\n",
    "5. **Checkpointing**: Usar en workflows largos para recovery\n",
    "6. **Testing**: Testear cada nodo por separado antes de integrar\n",
    "7. **Observability**: Loggear entrada/salida de cada nodo\n",
    "8. **Timeouts**: L√≠mite de tiempo por nodo para evitar hangs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c20b1",
   "metadata": {},
   "source": [
    "## üë• AutoGen: Sistemas Multi-Agente Colaborativos\n",
    "\n",
    "**AutoGen** (Microsoft) permite crear sistemas donde **m√∫ltiples agentes especializados colaboran** para resolver problemas complejos. Cada agente tiene un rol, personalidad y objetivos propios, similar a un equipo humano trabajando juntos.\n",
    "\n",
    "### üèóÔ∏è Arquitectura Multi-Agente\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ            AUTOGEN: MULTI-AGENT COLLABORATION                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Roles de Agentes:                                               ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. AssistantAgent                                      ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - LLM-powered agent que resuelve tareas            ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Puede usar code execution                         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Responde a instrucciones                          ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. UserProxyAgent                                      ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Representa al usuario humano                      ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Ejecuta c√≥digo en sandbox                         ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Puede requerir aprobaci√≥n humana                  ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. GroupChat                                           ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Orquesta conversaci√≥n entre N agentes            ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Round-robin o speaker selection autom√°tico        ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îÇ    - Termina cuando objetivo se cumple                 ‚îÇ     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Ejemplo: Dise√±o de Pipeline ETL                                ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ     User                                                         ‚îÇ\n",
    "‚îÇ       ‚îÇ \"Dise√±a pipeline para ingestar datos de API Stripe\"    ‚îÇ\n",
    "‚îÇ       ‚Üì                                                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ  ‚îÇ Data Engineer‚îÇ \"Voy a dise√±ar un pipeline con:              ‚îÇ\n",
    "‚îÇ  ‚îÇ  (Assistant) ‚îÇ  1. Extract: API calls con retry             ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  2. Transform: Pandas + validaci√≥n            ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  3. Load: Snowflake con upsert\"              ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ         ‚îÇ                                                        ‚îÇ\n",
    "‚îÇ         ‚Üì                                                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ  ‚îÇ QA Engineer  ‚îÇ \"Detecto problemas:                           ‚îÇ\n",
    "‚îÇ  ‚îÇ (Assistant)  ‚îÇ  - Falta manejo de rate limits                ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  - No hay logging                             ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  - ¬øQu√© pasa si API cambia schema?\"          ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ         ‚îÇ                                                        ‚îÇ\n",
    "‚îÇ         ‚Üì                                                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ  ‚îÇ Data Engineer‚îÇ \"Actualizaci√≥n:                               ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  - Agregado: exponential backoff              ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  - Agregado: structured logging               ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ  - Agregado: schema validation con pydantic\"  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ         ‚îÇ                                                        ‚îÇ\n",
    "‚îÇ         ‚Üì                                                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ  ‚îÇ QA Engineer  ‚îÇ \"Aprobado! Ahora agregar tests unitarios.\"   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ         ‚îÇ                                                        ‚îÇ\n",
    "‚îÇ         ‚Üì                                                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ  ‚îÇ User Proxy   ‚îÇ Ejecuta c√≥digo generado en sandbox            ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ \"Tests pasan ‚úÖ\"                              ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  Resultado: Pipeline completo dise√±ado, revisado, y testeado    ‚îÇ\n",
    "‚îÇ             en 4-5 iteraciones de conversaci√≥n                   ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üîß Implementaci√≥n: Team de Data Engineering\n",
    "\n",
    "```python\n",
    "import autogen\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "# Configuraci√≥n de LLMs\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4o',\n",
    "        'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "        'temperature': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    'config_list': config_list,\n",
    "    'timeout': 120,\n",
    "    'cache_seed': 42  # Cache para reducir costos\n",
    "}\n",
    "\n",
    "# 1. DATA ENGINEER: Dise√±a pipelines y escribe c√≥digo\n",
    "data_engineer = autogen.AssistantAgent(\n",
    "    name='DataEngineer',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un Senior Data Engineer experto en:\n",
    "- Dise√±o de pipelines ETL/ELT escalables\n",
    "- SQL avanzado (Snowflake, BigQuery, Redshift)\n",
    "- Python para data engineering (Pandas, Polars, PySpark)\n",
    "- Airflow, dbt, Prefect para orquestaci√≥n\n",
    "- Best practices: idempotencia, retry logic, monitoring\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Dise√±ar arquitecturas de datos robustas\n",
    "2. Escribir c√≥digo limpio y bien documentado\n",
    "3. Considerar edge cases y manejo de errores\n",
    "4. Proponer soluciones escalables\n",
    "\n",
    "Siempre incluye:\n",
    "- C√≥digo ejecutable con docstrings\n",
    "- Manejo de errores con try/except\n",
    "- Logging detallado\n",
    "- Comentarios explicativos\"\"\"\n",
    ")\n",
    "\n",
    "# 2. QA ENGINEER: Revisa c√≥digo, identifica bugs, sugiere tests\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un QA Engineer especializado en calidad de data pipelines.\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Revisar c√≥digo para identificar:\n",
    "   - Bugs l√≥gicos\n",
    "   - Performance bottlenecks\n",
    "   - Missing error handling\n",
    "   - Security vulnerabilities\n",
    "   - Code smells\n",
    "\n",
    "2. Sugerir tests:\n",
    "   - Unit tests con pytest\n",
    "   - Integration tests con mocks\n",
    "   - Data quality tests\n",
    "\n",
    "3. Validar:\n",
    "   - Idempotencia del pipeline\n",
    "   - Manejo de duplicados\n",
    "   - Retry logic correcto\n",
    "\n",
    "S√© cr√≠tico pero constructivo. Prioriza por impacto.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. ARCHITECT: Revisa arquitectura y decisiones t√©cnicas\n",
    "architect = autogen.AssistantAgent(\n",
    "    name='TechArchitect',\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Eres un Data Architect senior.\n",
    "\n",
    "Tu trabajo es:\n",
    "1. Revisar decisiones arquitecturales:\n",
    "   - Escalabilidad (¬øfunciona con 10x datos?)\n",
    "   - Costos (optimizaci√≥n de recursos)\n",
    "   - Mantenibilidad (¬øf√°cil de mantener?)\n",
    "   - Seguridad (PII, compliance)\n",
    "\n",
    "2. Sugerir alternativas:\n",
    "   - Batch vs Streaming\n",
    "   - Full load vs Incremental\n",
    "   - Herramientas apropiadas\n",
    "\n",
    "3. Validar patrones:\n",
    "   - Data modeling (star schema, etc.)\n",
    "   - Partitioning strategy\n",
    "   - SLAs y monitoring\n",
    "\n",
    "Enf√≥cate en decisiones de alto impacto.\"\"\"\n",
    ")\n",
    "\n",
    "# 4. USER PROXY: Ejecuta c√≥digo y representa al usuario\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='UserProxy',\n",
    "    human_input_mode='NEVER',  # Modo autom√°tico (sin input humano)\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get('content', '').rstrip().endswith('TERMINATE'),\n",
    "    code_execution_config={\n",
    "        'work_dir': 'autogen_workspace',\n",
    "        'use_docker': False  # En producci√≥n: usar Docker para sandbox\n",
    "    },\n",
    "    system_message=\"\"\"Eres el User Proxy. Ejecutas c√≥digo propuesto y reportas resultados.\n",
    "    \n",
    "Cuando recibas c√≥digo Python:\n",
    "1. Ejecutarlo en el workspace\n",
    "2. Reportar output, errores, o √©xito\n",
    "3. Si hay errores, compartir el traceback completo\"\"\"\n",
    ")\n",
    "\n",
    "# 5. GROUP CHAT: Orquesta la conversaci√≥n\n",
    "\n",
    "# Crear group chat con todos los agentes\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, data_engineer, qa_engineer, architect],\n",
    "    messages=[],\n",
    "    max_round=20,  # M√°ximo 20 turnos\n",
    "    speaker_selection_method='auto'  # LLM decide qui√©n habla\n",
    ")\n",
    "\n",
    "# Manager del group chat\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "# 6. INICIAR CONVERSACI√ìN\n",
    "\n",
    "task = \"\"\"\n",
    "Tarea: Dise√±ar un pipeline ETL que:\n",
    "\n",
    "1. EXTRACT:\n",
    "   - Leer datos de API REST de Stripe (pagos)\n",
    "   - API endpoint: /v1/charges\n",
    "   - Rate limit: 100 requests/segundo\n",
    "   - Necesita paginaci√≥n (1000 records por p√°gina)\n",
    "\n",
    "2. TRANSFORM:\n",
    "   - Parsear JSON response\n",
    "   - Convertir timestamps a datetime\n",
    "   - Calcular m√©tricas: total_amount, fee_amount, net_amount\n",
    "   - Detectar y filtrar duplicados por charge_id\n",
    "\n",
    "3. LOAD:\n",
    "   - Insertar en Snowflake tabla: dwh.stripe_charges\n",
    "   - Estrategia: upsert (update si existe, insert si nuevo)\n",
    "   - Particionado por: payment_date\n",
    "\n",
    "Requerimientos:\n",
    "- Debe correr en Airflow (DAG)\n",
    "- Idempotente (re-ejecutable sin duplicar datos)\n",
    "- Manejo robusto de errores con reintentos\n",
    "- Logging completo para debugging\n",
    "- Tests unitarios con pytest\n",
    "\n",
    "Dise√±en el pipeline, revisen el c√≥digo, y validen la arquitectura.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INICIANDO COLABORACI√ìN MULTI-AGENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTarea:\\n{task}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Iniciar conversaci√≥n\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=task\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONVERSACI√ìN COMPLETADA\")\n",
    "print(\"=\" * 80)\n",
    "```\n",
    "\n",
    "### üìä Salida Esperada (Ejemplo)\n",
    "\n",
    "```\n",
    "UserProxy: [Tarea del pipeline ETL...]\n",
    "\n",
    "DataEngineer: Voy a dise√±ar el pipeline en 3 componentes:\n",
    "\n",
    "```python\n",
    "# stripe_etl.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def extract_stripe_charges(api_key: str, start_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Extrae charges de Stripe API con paginaci√≥n y retry.\"\"\"\n",
    "    all_charges = []\n",
    "    has_more = True\n",
    "    starting_after = None\n",
    "    \n",
    "    while has_more:\n",
    "        params = {\n",
    "            'limit': 100,\n",
    "            'created[gte]': int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "        }\n",
    "        if starting_after:\n",
    "            params['starting_after'] = starting_after\n",
    "        \n",
    "        response = requests.get(\n",
    "            'https://api.stripe.com/v1/charges',\n",
    "            auth=(api_key, ''),\n",
    "            params=params,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        all_charges.extend(data['data'])\n",
    "        \n",
    "        has_more = data['has_more']\n",
    "        if has_more:\n",
    "            starting_after = data['data'][-1]['id']\n",
    "        \n",
    "        logger.info(f\"Extracted {len(data['data'])} charges, total: {len(all_charges)}\")\n",
    "    \n",
    "    return pd.DataFrame(all_charges)\n",
    "\n",
    "def transform_charges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforma y limpia los datos.\"\"\"\n",
    "    df['payment_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "    df['total_amount'] = df['amount'] / 100  # Stripe usa cents\n",
    "    df['net_amount'] = df['amount'] - df.get('fee', 0)) / 100\n",
    "    \n",
    "    # Deduplicar\n",
    "    df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "    \n",
    "    logger.info(f\"Transformed {len(df)} charges\")\n",
    "    return df\n",
    "\n",
    "def load_to_snowflake(df: pd.DataFrame, conn, table: str):\n",
    "    \"\"\"Carga datos a Snowflake con upsert.\"\"\"\n",
    "    # Crear temp table\n",
    "    temp_table = f\"{table}_temp\"\n",
    "    df.to_sql(temp_table, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # MERGE (upsert)\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {table} target\n",
    "    USING {temp_table} source\n",
    "    ON target.id = source.id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    conn.execute(merge_sql)\n",
    "    logger.info(f\"Loaded {len(df)} charges to {table}\")\n",
    "```\n",
    "\n",
    "QAEngineer: Revisi√≥n del c√≥digo:\n",
    "\n",
    "**Problemas encontrados:**\n",
    "1. ‚ùå `extract_stripe_charges`: No maneja rate limit 429\n",
    "2. ‚ùå `transform_charges`: Error en l√≠nea `df.get('fee', 0)` - debe ser `df['fee'].fillna(0)`\n",
    "3. ‚ùå `load_to_snowflake`: Falta manejo de errores\n",
    "4. ‚ö†Ô∏è No hay tests unitarios\n",
    "5. ‚ö†Ô∏è Falta validaci√≥n de schema\n",
    "\n",
    "**Sugerencias:**\n",
    "```python\n",
    "# Agregar rate limit handling\n",
    "import time\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "@retry(...)\n",
    "def extract_stripe_charges(...):\n",
    "    try:\n",
    "        response = requests.get(...)\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            retry_after = int(e.response.headers.get('Retry-After', 60))\n",
    "            logger.warning(f\"Rate limited, waiting {retry_after}s\")\n",
    "            time.sleep(retry_after)\n",
    "            raise  # Retry\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Tests con pytest\n",
    "def test_transform_charges():\n",
    "    sample_data = [{'id': '1', 'amount': 1000, 'created': 1234567890}]\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    result = transform_charges(df)\n",
    "    assert result['total_amount'].iloc[0] == 10.0\n",
    "    assert 'payment_date' in result.columns\n",
    "```\n",
    "\n",
    "TechArchitect: Revisi√≥n arquitectural:\n",
    "\n",
    "**‚úÖ Bueno:**\n",
    "- Uso de retry con exponential backoff\n",
    "- Paginaci√≥n implementada correctamente\n",
    "- Upsert strategy apropiada\n",
    "\n",
    "**‚ö†Ô∏è Consideraciones:**\n",
    "1. **Escalabilidad**: Para millones de records, considerar:\n",
    "   - Batch processing en chunks (no todo en memoria)\n",
    "   - Usar Spark o Polars en vez de Pandas\n",
    "\n",
    "2. **Costos**:\n",
    "   - Stripe API es costosa (rate limits)\n",
    "   - Implementar incremental load (solo √∫ltimas 24h)\n",
    "   - Cachear resultados en S3 antes de Snowflake\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Agregar m√©tricas: records processed, API latency, error rate\n",
    "   - Alertas si ETL falla o demora >30min\n",
    "\n",
    "4. **Partitioning**:\n",
    "   - Snowflake: particionar por `DATE_TRUNC('month', payment_date)`\n",
    "   - Mejora queries anal√≠ticas por fecha\n",
    "\n",
    "**Sugerencia de arquitectura:**\n",
    "```\n",
    "Airflow DAG (daily @ 2am)\n",
    "  ‚Üì\n",
    "Extract (incremental: last 24h) ‚Üí S3 staging (Parquet)\n",
    "  ‚Üì\n",
    "Transform (Spark/Polars) ‚Üí Validaci√≥n schema\n",
    "  ‚Üì\n",
    "Load (Snowflake COPY INTO) ‚Üí Upsert con MERGE\n",
    "  ‚Üì\n",
    "Data Quality checks (dbt tests)\n",
    "  ‚Üì\n",
    "Notify (Slack si errores)\n",
    "```\n",
    "\n",
    "DataEngineer: Actualizando el c√≥digo con feedback...\n",
    "\n",
    "[C√≥digo mejorado con todos los fixes...]\n",
    "\n",
    "UserProxy: Ejecutando tests... ‚úÖ All tests passed (5/5)\n",
    "\n",
    "TERMINATE\n",
    "```\n",
    "\n",
    "### üéØ Patrones Avanzados con AutoGen\n",
    "\n",
    "#### 1. Sequential Chat (Pipeline en etapas)\n",
    "\n",
    "```python\n",
    "# Conversaci√≥n secuencial: output de agent 1 ‚Üí input de agent 2\n",
    "\n",
    "# Agente 1: Genera SQL\n",
    "sql_generator = autogen.AssistantAgent(\n",
    "    name='SQLGenerator',\n",
    "    llm_config=llm_config,\n",
    "    system_message='Generas queries SQL optimizadas.'\n",
    ")\n",
    "\n",
    "# Agente 2: Optimiza SQL\n",
    "sql_optimizer = autogen.AssistantAgent(\n",
    "    name='SQLOptimizer',\n",
    "    llm_config=llm_config,\n",
    "    system_message='Optimizas queries SQL para mejor performance.'\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "user_proxy.initiate_chats([\n",
    "    {\n",
    "        'recipient': sql_generator,\n",
    "        'message': 'Genera query para calcular ventas por regi√≥n',\n",
    "        'max_turns': 2,\n",
    "        'summary_method': 'last_msg'\n",
    "    },\n",
    "    {\n",
    "        'recipient': sql_optimizer,\n",
    "        'message': 'Optimiza esta query',  # Recibe output del anterior\n",
    "        'max_turns': 2\n",
    "    }\n",
    "])\n",
    "```\n",
    "\n",
    "#### 2. Nested Chats (Sub-equipos)\n",
    "\n",
    "```python\n",
    "# Equipo principal delega a sub-equipo especializado\n",
    "\n",
    "# Sub-equipo: Especialistas en testing\n",
    "test_writer = autogen.AssistantAgent(name='TestWriter', ...)\n",
    "test_reviewer = autogen.AssistantAgent(name='TestReviewer', ...)\n",
    "\n",
    "test_team = autogen.GroupChat(\n",
    "    agents=[test_writer, test_reviewer],\n",
    "    messages=[],\n",
    "    max_round=5\n",
    ")\n",
    "\n",
    "# Agente principal puede invocar al sub-equipo\n",
    "data_engineer.register_nested_chats(\n",
    "    trigger=lambda msg: 'write tests' in msg.get('content', '').lower(),\n",
    "    chat_queue=[{\n",
    "        'recipient': test_team,\n",
    "        'message': 'Escriban tests unitarios para este c√≥digo'\n",
    "    }]\n",
    ")\n",
    "```\n",
    "\n",
    "#### 3. Custom Speaker Selection\n",
    "\n",
    "```python\n",
    "def custom_speaker_selection(last_speaker, groupchat):\n",
    "    \"\"\"L√≥gica personalizada para seleccionar siguiente speaker.\"\"\"\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    # Si √∫ltimo mensaje tiene c√≥digo Python, QA Engineer revisa\n",
    "    if '```python' in messages[-1].get('content', ''):\n",
    "        return qa_engineer\n",
    "    \n",
    "    # Si hay problemas de arquitectura, Architect opina\n",
    "    if 'scalability' in messages[-1].get('content', '').lower():\n",
    "        return architect\n",
    "    \n",
    "    # Por defecto, round-robin\n",
    "    return None  # Auto-select\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[...],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=custom_speaker_selection\n",
    ")\n",
    "```\n",
    "\n",
    "### üìä Comparaci√≥n: Single Agent vs Multi-Agent\n",
    "\n",
    "| Aspecto | Single Agent | Multi-Agent (AutoGen) |\n",
    "|---------|--------------|----------------------|\n",
    "| **Complejidad tareas** | Simple-Media | Alta |\n",
    "| **Calidad output** | Buena | Excelente (peer review) |\n",
    "| **Especializaci√≥n** | Generalista | Especialistas por rol |\n",
    "| **Tiempo ejecuci√≥n** | R√°pido (1-5 iteraciones) | Lento (10-20 iteraciones) |\n",
    "| **Costo (API calls)** | Bajo ($0.01-$0.10) | Alto ($0.50-$2.00) |\n",
    "| **Debugging** | M√°s f√°cil | M√°s complejo |\n",
    "| **Casos de uso** | Queries simples, clasificaci√≥n | Dise√±o arquitectural, code review |\n",
    "\n",
    "### üí∞ Optimizaci√≥n de Costos en Multi-Agent\n",
    "\n",
    "```python\n",
    "# 1. Usar modelos peque√±os para roles secundarios\n",
    "llm_config_cheap = {\n",
    "    'config_list': [{'model': 'gpt-3.5-turbo', 'api_key': api_key}],\n",
    "    'cache_seed': 42\n",
    "}\n",
    "\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config=llm_config_cheap,  # Modelo barato para revisiones\n",
    "    ...\n",
    ")\n",
    "\n",
    "# 2. L√≠mite estricto de rounds\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[...],\n",
    "    max_round=10,  # M√°ximo 10 turnos (previene loops infinitos)\n",
    "    ...\n",
    ")\n",
    "\n",
    "# 3. Caching agresivo\n",
    "llm_config = {\n",
    "    'config_list': config_list,\n",
    "    'cache_seed': 42,  # Mismo seed = cachea respuestas id√©nticas\n",
    "    'temperature': 0    # Determin√≠stico = m√°s cacheable\n",
    "}\n",
    "\n",
    "# 4. Termination temprana\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    ...\n",
    "    is_termination_msg=lambda x: (\n",
    "        'TERMINATE' in x.get('content', '') or\n",
    "        'approved' in x.get('content', '').lower()\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### üöÄ Casos de Uso Reales en Data Engineering\n",
    "\n",
    "| Caso de Uso | Agentes Necesarios | Beneficio |\n",
    "|-------------|-------------------|-----------|\n",
    "| **Code review de PRs** | Engineer + QA + Architect | Peer review autom√°tico |\n",
    "| **Incident response** | Investigator + Engineer + Architect | Root cause analysis |\n",
    "| **Pipeline design** | Engineer + QA + DBA | Dise√±o robusto validado |\n",
    "| **Query optimization** | Analyst + DBA + Performance Engineer | Optimizaci√≥n multi-perspectiva |\n",
    "| **Schema migration** | DBA + Engineer + Architect | Validaci√≥n de impacto |\n",
    "| **Data quality investigation** | Analyst + Engineer + Domain Expert | Diagn√≥stico profundo |\n",
    "\n",
    "### üí° Mejores Pr√°cticas\n",
    "\n",
    "1. **Roles bien definidos**: Cada agente debe tener un rol claro y no solapado\n",
    "2. **System messages detallados**: Instrucciones espec√≠ficas con ejemplos\n",
    "3. **L√≠mites de seguridad**: max_rounds, timeouts, cost limits\n",
    "4. **Logging exhaustivo**: Guardar toda la conversaci√≥n para debugging\n",
    "5. **Human checkpoints**: Aprobaci√≥n humana antes de acciones cr√≠ticas\n",
    "6. **Testing con mocks**: Probar conversaciones sin llamar APIs reales\n",
    "7. **Monitoreo de costos**: Track API usage por conversaci√≥n\n",
    "8. **Termination clara**: Condiciones expl√≠citas para terminar chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431e8d5",
   "metadata": {},
   "source": [
    "## üè≠ Agentes en Producci√≥n: Automatizaci√≥n Real de Data Engineering\n",
    "\n",
    "Los agentes aut√≥nomos est√°n revolucionando Data Engineering al automatizar tareas que antes requer√≠an intervenci√≥n humana experta: **debugging de pipelines**, **optimizaci√≥n de queries**, **incident response**, y **monitoreo proactivo**. Aqu√≠ exploramos implementaciones de producci√≥n reales.\n",
    "\n",
    "### üö® Caso 1: Incident Response Agent (Auto-Healing Pipelines)\n",
    "\n",
    "Un agente que detecta, diagnostica y resuelve fallas en pipelines de Airflow autom√°ticamente.\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "\n",
    "# Tools para incident response\n",
    "\n",
    "@tool\n",
    "def get_airflow_dag_failures(hours: int = 24) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene lista de DAGs que han fallado en las √∫ltimas N horas.\n",
    "    \n",
    "    Args:\n",
    "        hours: ventana de tiempo para buscar fallas\n",
    "    \n",
    "    Returns:\n",
    "        JSON con DAGs fallidos y detalles de error\n",
    "    \"\"\"\n",
    "    # En producci√≥n: llamar a Airflow API\n",
    "    airflow_url = \"http://airflow:8080/api/v1\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('AIRFLOW_TOKEN')}\"}\n",
    "    \n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(hours=hours)\n",
    "    \n",
    "    # Simular respuesta\n",
    "    failures = {\n",
    "        'daily_sales_etl': {\n",
    "            'dag_id': 'daily_sales_etl',\n",
    "            'execution_date': '2024-10-30T23:00:00',\n",
    "            'state': 'failed',\n",
    "            'task_id': 'extract_api_data',\n",
    "            'error': 'requests.exceptions.Timeout: HTTPSConnectionPool(host=\\'api.example.com\\', port=443): Read timed out. (read timeout=30)',\n",
    "            'duration_seconds': 35,\n",
    "            'try_number': 3,\n",
    "            'log_url': 'http://airflow:8080/log?dag_id=daily_sales_etl&task_id=extract_api_data&execution_date=2024-10-30T23:00:00'\n",
    "        },\n",
    "        'hourly_user_events': {\n",
    "            'dag_id': 'hourly_user_events',\n",
    "            'execution_date': '2024-10-30T22:00:00',\n",
    "            'state': 'failed',\n",
    "            'task_id': 'load_to_snowflake',\n",
    "            'error': 'snowflake.connector.errors.DatabaseError: 250001: Connection closed unexpectedly',\n",
    "            'duration_seconds': 1245,\n",
    "            'try_number': 1,\n",
    "            'log_url': 'http://airflow:8080/log?dag_id=hourly_user_events&task_id=load_to_snowflake&execution_date=2024-10-30T22:00:00'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(failures, indent=2)\n",
    "\n",
    "@tool\n",
    "def get_task_logs(dag_id: str, task_id: str, execution_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene logs detallados de una tarea fallida.\n",
    "    \n",
    "    Args:\n",
    "        dag_id: ID del DAG\n",
    "        task_id: ID de la tarea\n",
    "        execution_date: fecha de ejecuci√≥n (ISO format)\n",
    "    \n",
    "    Returns:\n",
    "        Logs completos de la tarea\n",
    "    \"\"\"\n",
    "    # En producci√≥n: obtener de Airflow logs en S3/GCS\n",
    "    sample_logs = {\n",
    "        'daily_sales_etl/extract_api_data': \"\"\"\n",
    "[2024-10-30 23:00:15] INFO - Starting task extract_api_data\n",
    "[2024-10-30 23:00:16] INFO - Calling API: https://api.example.com/sales\n",
    "[2024-10-30 23:00:16] DEBUG - Request headers: {'Authorization': 'Bearer ***', 'Content-Type': 'application/json'}\n",
    "[2024-10-30 23:00:16] DEBUG - Request params: {'start_date': '2024-10-30', 'limit': 1000}\n",
    "[2024-10-30 23:00:46] ERROR - requests.exceptions.Timeout: HTTPSConnectionPool(host='api.example.com', port=443): Read timed out. (read timeout=30)\n",
    "[2024-10-30 23:00:46] INFO - Retry 1/3 in 60 seconds...\n",
    "[2024-10-30 23:01:46] ERROR - Timeout again after retry\n",
    "[2024-10-30 23:01:46] ERROR - Task failed after 3 attempts\n",
    "\"\"\",\n",
    "        'hourly_user_events/load_to_snowflake': \"\"\"\n",
    "[2024-10-30 22:00:05] INFO - Loading 125000 records to Snowflake\n",
    "[2024-10-30 22:00:05] INFO - Connection established to account: xyz123.us-east-1\n",
    "[2024-10-30 22:00:10] INFO - Executing COPY INTO dwh.user_events FROM @stage/events_2024103022.parquet\n",
    "[2024-10-30 22:15:30] ERROR - snowflake.connector.errors.DatabaseError: 250001: Connection closed unexpectedly\n",
    "[2024-10-30 22:15:30] DEBUG - Network trace: Connection reset by peer (errno 104)\n",
    "[2024-10-30 22:15:30] ERROR - Loaded 45230/125000 records before failure (36%)\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    key = f\"{dag_id}/{task_id}\"\n",
    "    return sample_logs.get(key, \"Logs not found\")\n",
    "\n",
    "@tool\n",
    "def check_external_service_status(service: str) -> str:\n",
    "    \"\"\"\n",
    "    Verifica estado de servicios externos (APIs, databases).\n",
    "    \n",
    "    Args:\n",
    "        service: nombre del servicio (api.example.com, snowflake, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Estado del servicio y latencia\n",
    "    \"\"\"\n",
    "    # En producci√≥n: healthcheck real o status page\n",
    "    status_map = {\n",
    "        'api.example.com': {\n",
    "            'status': 'degraded',\n",
    "            'latency_ms': 15234,\n",
    "            'error_rate': 0.25,\n",
    "            'message': 'API experiencing high latency due to traffic spike'\n",
    "        },\n",
    "        'snowflake': {\n",
    "            'status': 'operational',\n",
    "            'latency_ms': 234,\n",
    "            'error_rate': 0.01,\n",
    "            'message': 'All systems operational'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(status_map.get(service, {'status': 'unknown'}), indent=2)\n",
    "\n",
    "@tool\n",
    "def retry_airflow_task(dag_id: str, task_id: str, execution_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Re-ejecuta una tarea fallida en Airflow.\n",
    "    \n",
    "    Args:\n",
    "        dag_id: ID del DAG\n",
    "        task_id: ID de la tarea\n",
    "        execution_date: fecha de ejecuci√≥n\n",
    "    \n",
    "    Returns:\n",
    "        Resultado de la re-ejecuci√≥n\n",
    "    \"\"\"\n",
    "    # En producci√≥n: POST a Airflow API /dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/clear\n",
    "    \n",
    "    print(f\"üîÑ Retrying {dag_id}/{task_id}...\")\n",
    "    \n",
    "    # Simular retry\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return json.dumps({\n",
    "        'status': 'success',\n",
    "        'message': f'Task {task_id} queued for retry',\n",
    "        'new_try_number': 4,\n",
    "        'estimated_start': '2024-10-31T00:05:00'\n",
    "    })\n",
    "\n",
    "@tool\n",
    "def create_incident_ticket(title: str, description: str, severity: str) -> str:\n",
    "    \"\"\"\n",
    "    Crea ticket de incidente en Jira/PagerDuty.\n",
    "    \n",
    "    Args:\n",
    "        title: t√≠tulo del incidente\n",
    "        description: descripci√≥n detallada\n",
    "        severity: low, medium, high, critical\n",
    "    \n",
    "    Returns:\n",
    "        ID del ticket creado\n",
    "    \"\"\"\n",
    "    # En producci√≥n: integrar con Jira/PagerDuty API\n",
    "    \n",
    "    ticket = {\n",
    "        'id': 'INC-12345',\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'severity': severity,\n",
    "        'status': 'open',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'assigned_to': 'data-eng-oncall',\n",
    "        'url': 'https://jira.company.com/browse/INC-12345'\n",
    "    }\n",
    "    \n",
    "    print(f\"üé´ Ticket creado: {ticket['id']}\")\n",
    "    return json.dumps(ticket, indent=2)\n",
    "\n",
    "@tool\n",
    "def send_slack_alert(channel: str, message: str, severity: str = 'info') -> str:\n",
    "    \"\"\"\n",
    "    Env√≠a alerta a Slack.\n",
    "    \n",
    "    Args:\n",
    "        channel: canal de Slack (#data-engineering)\n",
    "        message: mensaje a enviar\n",
    "        severity: info, warning, error, critical\n",
    "    \n",
    "    Returns:\n",
    "        Confirmaci√≥n de env√≠o\n",
    "    \"\"\"\n",
    "    # En producci√≥n: Slack API\n",
    "    \n",
    "    emoji_map = {\n",
    "        'info': ':information_source:',\n",
    "        'warning': ':warning:',\n",
    "        'error': ':x:',\n",
    "        'critical': ':rotating_light:'\n",
    "    }\n",
    "    \n",
    "    slack_message = f\"{emoji_map.get(severity, ':bell:')} {message}\"\n",
    "    \n",
    "    print(f\"üì¢ Slack alert sent to {channel}: {slack_message[:100]}...\")\n",
    "    \n",
    "    return json.dumps({\n",
    "        'status': 'sent',\n",
    "        'channel': channel,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "# Crear agente de incident response\n",
    "\n",
    "incident_tools = [\n",
    "    get_airflow_dag_failures,\n",
    "    get_task_logs,\n",
    "    check_external_service_status,\n",
    "    retry_airflow_task,\n",
    "    create_incident_ticket,\n",
    "    send_slack_alert\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "incident_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un SRE especializado en incident response para data pipelines.\n",
    "\n",
    "Tu proceso cuando detectas un incidente:\n",
    "\n",
    "1. INVESTIGAR:\n",
    "   - Obtener lista de DAGs fallidos\n",
    "   - Revisar logs detallados\n",
    "   - Verificar estado de servicios externos\n",
    "\n",
    "2. DIAGNOSTICAR:\n",
    "   - Identificar root cause (timeout, connection issue, data problem, etc.)\n",
    "   - Clasificar severidad (low/medium/high/critical)\n",
    "\n",
    "3. REMEDIAR:\n",
    "   - Si es transient error ‚Üí retry autom√°tico\n",
    "   - Si es service degradation ‚Üí alertar y escalar\n",
    "   - Si es data issue ‚Üí crear ticket para investigaci√≥n\n",
    "\n",
    "4. COMUNICAR:\n",
    "   - Enviar alertas a Slack\n",
    "   - Crear ticket si requiere investigaci√≥n humana\n",
    "   - Documentar acciones tomadas\n",
    "\n",
    "IMPORTANTE:\n",
    "- Solo retry autom√°tico si error es claramente transient (timeout, network)\n",
    "- Para errores de datos o l√≥gica, NO retry (crear ticket)\n",
    "- Siempre alertar en Slack antes de acciones\n",
    "- Priorizar minimizar downtime\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "incident_agent = create_openai_tools_agent(llm, incident_tools, incident_prompt)\n",
    "incident_executor = AgentExecutor(\n",
    "    agent=incident_agent,\n",
    "    tools=incident_tools,\n",
    "    verbose=True,\n",
    "    max_iterations=15,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# EJECUTAR: Incident response autom√°tico\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INCIDENT RESPONSE AGENT - AUTO-HEALING PIPELINES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = incident_executor.invoke({\n",
    "    \"input\": \"\"\"Revisa los pipelines de las √∫ltimas 24 horas.\n",
    "    Para cada falla:\n",
    "    1. Diagnostica el problema\n",
    "    2. Decide si es auto-recoverable\n",
    "    3. Toma acci√≥n apropiada (retry o escalar)\n",
    "    4. Alerta al equipo\"\"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DE ACCIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(result['output'])\n",
    "```\n",
    "\n",
    "### Salida Esperada\n",
    "\n",
    "```\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "Thought: Necesito revisar las fallas de las √∫ltimas 24h\n",
    "\n",
    "Action: get_airflow_dag_failures\n",
    "Action Input: {\"hours\": 24}\n",
    "\n",
    "Observation: {\n",
    "  \"daily_sales_etl\": {\n",
    "    \"error\": \"Timeout: Read timed out (read timeout=30)\",\n",
    "    ...\n",
    "  },\n",
    "  \"hourly_user_events\": {\n",
    "    \"error\": \"DatabaseError: Connection closed unexpectedly\",\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "\n",
    "Thought: Hay 2 DAGs fallidos. Voy a investigar cada uno.\n",
    "\n",
    "Action: get_task_logs\n",
    "Action Input: {\"dag_id\": \"daily_sales_etl\", \"task_id\": \"extract_api_data\", ...}\n",
    "\n",
    "Observation: [Logs showing timeout after 30 seconds]\n",
    "\n",
    "Action: check_external_service_status\n",
    "Action Input: {\"service\": \"api.example.com\"}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"degraded\",\n",
    "  \"latency_ms\": 15234,\n",
    "  \"message\": \"API experiencing high latency due to traffic spike\"\n",
    "}\n",
    "\n",
    "Thought: daily_sales_etl fall√≥ por API degraded (latencia 15s, timeout 30s).\n",
    "Es un error transient - la API est√° operacional pero lenta.\n",
    "Decisi√≥n: Retry con timeout aumentado.\n",
    "\n",
    "Action: send_slack_alert\n",
    "Action Input: {\n",
    "  \"channel\": \"#data-engineering\",\n",
    "  \"message\": \"üîÑ daily_sales_etl failed due to API timeout. API is degraded (15s latency). Retrying with increased timeout...\",\n",
    "  \"severity\": \"warning\"\n",
    "}\n",
    "\n",
    "Action: retry_airflow_task\n",
    "Action Input: {\"dag_id\": \"daily_sales_etl\", \"task_id\": \"extract_api_data\", ...}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"success\",\n",
    "  \"message\": \"Task queued for retry\",\n",
    "  \"new_try_number\": 4\n",
    "}\n",
    "\n",
    "Thought: Retry iniciado para daily_sales_etl. Ahora investigar hourly_user_events.\n",
    "\n",
    "Action: get_task_logs\n",
    "Action Input: {\"dag_id\": \"hourly_user_events\", \"task_id\": \"load_to_snowflake\", ...}\n",
    "\n",
    "Observation: [Logs showing connection reset after loading 36% of data]\n",
    "\n",
    "Action: check_external_service_status\n",
    "Action Input: {\"service\": \"snowflake\"}\n",
    "\n",
    "Observation: {\n",
    "  \"status\": \"operational\",\n",
    "  \"message\": \"All systems operational\"\n",
    "}\n",
    "\n",
    "Thought: hourly_user_events fall√≥ por connection reset durante COPY INTO.\n",
    "Snowflake est√° operational, posiblemente network issue transient.\n",
    "Pero solo 36% de data se carg√≥ - puede haber duplicados si retry simple.\n",
    "Decisi√≥n: Crear ticket para investigaci√≥n manual (requiere an√°lisis de datos parciales).\n",
    "\n",
    "Action: create_incident_ticket\n",
    "Action Input: {\n",
    "  \"title\": \"hourly_user_events failed mid-load to Snowflake\",\n",
    "  \"description\": \"Task load_to_snowflake failed after loading 36% (45K/125K records). Connection reset by peer. Requires manual investigation to handle partial load and avoid duplicates.\",\n",
    "  \"severity\": \"high\"\n",
    "}\n",
    "\n",
    "Observation: {\"id\": \"INC-12345\", ...}\n",
    "\n",
    "Action: send_slack_alert\n",
    "Action Input: {\n",
    "  \"channel\": \"#data-engineering\",\n",
    "  \"message\": \"üé´ hourly_user_events failed with partial data load. Created INC-12345 for investigation. Manual intervention required to resume safely.\",\n",
    "  \"severity\": \"error\"\n",
    "}\n",
    "\n",
    "Final Answer:\n",
    "Revis√© 2 pipelines fallidos en las √∫ltimas 24h:\n",
    "\n",
    "1. ‚úÖ daily_sales_etl (RECUPERADO AUTOM√ÅTICAMENTE)\n",
    "   - Root cause: API timeout (servicio degradado, 15s latency)\n",
    "   - Acci√≥n: Retry autom√°tico con timeout aumentado\n",
    "   - Estado: En cola para re-ejecuci√≥n\n",
    "\n",
    "2. üé´ hourly_user_events (REQUIERE INVESTIGACI√ìN)\n",
    "   - Root cause: Connection reset durante carga (network issue transient)\n",
    "   - Problema: Carga parcial (36%) puede causar duplicados\n",
    "   - Acci√≥n: Ticket INC-12345 creado para investigaci√≥n manual\n",
    "   - Estado: Esperando intervenci√≥n del equipo\n",
    "\n",
    "Alertas enviadas a #data-engineering para ambos casos.\n",
    "```\n",
    "\n",
    "### üìä Caso 2: Query Optimization Agent\n",
    "\n",
    "Agente que analiza queries lentas y las optimiza autom√°ticamente.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_slow_queries(minutes: int = 60, min_duration_seconds: float = 30.0) -> str:\n",
    "    \"\"\"Obtiene queries que han tardado m√°s de N segundos.\"\"\"\n",
    "    # En producci√≥n: query a Snowflake QUERY_HISTORY o PostgreSQL pg_stat_statements\n",
    "    slow_queries = [\n",
    "        {\n",
    "            'query_id': 'q_12345',\n",
    "            'query_text': 'SELECT * FROM sales s JOIN customers c ON s.customer_id = c.id WHERE s.sale_date >= CURRENT_DATE - 30',\n",
    "            'duration_seconds': 125.4,\n",
    "            'rows_scanned': 150_000_000,\n",
    "            'rows_returned': 1_250_000,\n",
    "            'execution_time': '2024-10-30 23:45:12',\n",
    "            'user': 'analytics_team'\n",
    "        }\n",
    "    ]\n",
    "    return json.dumps(slow_queries, indent=2)\n",
    "\n",
    "@tool\n",
    "def explain_query(query: str) -> str:\n",
    "    \"\"\"Obtiene EXPLAIN ANALYZE del query.\"\"\"\n",
    "    # Simular execution plan\n",
    "    explain = {\n",
    "        'estimated_rows': 150_000_000,\n",
    "        'actual_rows': 1_250_000,\n",
    "        'estimated_cost': 25678.90,\n",
    "        'execution_time_ms': 125400,\n",
    "        'operations': [\n",
    "            {\n",
    "                'step': 1,\n",
    "                'operation': 'Seq Scan on sales',\n",
    "                'cost': 20000.00,\n",
    "                'rows': 150_000_000,\n",
    "                'warning': 'Full table scan - no index used'\n",
    "            },\n",
    "            {\n",
    "                'step': 2,\n",
    "                'operation': 'Hash Join',\n",
    "                'cost': 5678.90,\n",
    "                'rows': 1_250_000\n",
    "            }\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Add index on sales(sale_date) for date filter',\n",
    "            'Replace SELECT * with specific columns',\n",
    "            'Consider partitioning sales table by sale_date'\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(explain, indent=2)\n",
    "\n",
    "@tool\n",
    "def rewrite_query_optimized(original_query: str, optimizations: List[str]) -> str:\n",
    "    \"\"\"Reescribe query aplicando optimizaciones.\"\"\"\n",
    "    # LLM generar√≠a esto, aqu√≠ simulamos\n",
    "    optimized = \"\"\"\n",
    "-- OPTIMIZED VERSION:\n",
    "-- Changes: 1) Added index hint, 2) Specific columns, 3) Materialized subquery\n",
    "\n",
    "SELECT \n",
    "    s.sale_id,\n",
    "    s.sale_date,\n",
    "    s.total_amount,\n",
    "    c.customer_name,\n",
    "    c.customer_email\n",
    "FROM sales s\n",
    "USE INDEX (idx_sales_date)  -- Hint to use index\n",
    "INNER JOIN customers c ON s.customer_id = c.id\n",
    "WHERE s.sale_date >= CURRENT_DATE - 30\n",
    "  AND s.total_amount > 0  -- Additional filter to reduce rows\n",
    "\n",
    "-- Expected improvement: 95% faster (125s ‚Üí 6s)\n",
    "\"\"\"\n",
    "    return optimized\n",
    "\n",
    "# Uso\n",
    "optimizer_tools = [get_slow_queries, explain_query, rewrite_query_optimized]\n",
    "# [Configurar agente similar...]\n",
    "```\n",
    "\n",
    "### üéØ Caso 3: Data Quality Monitoring Agent\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def run_data_quality_checks(table: str) -> str:\n",
    "    \"\"\"Ejecuta suite completa de data quality checks.\"\"\"\n",
    "    checks = {\n",
    "        'nulls': {'column': 'email', 'null_rate': 0.156, 'threshold': 0.05, 'status': 'FAIL'},\n",
    "        'duplicates': {'count': 450, 'percentage': 0.009, 'threshold': 0.01, 'status': 'PASS'},\n",
    "        'freshness': {'last_update': '2024-10-30 20:30:00', 'delay_hours': 3.5, 'threshold': 2.0, 'status': 'FAIL'},\n",
    "        'schema': {'columns_expected': 12, 'columns_actual': 12, 'status': 'PASS'},\n",
    "        'referential_integrity': {'orphaned_records': 23, 'threshold': 0, 'status': 'FAIL'}\n",
    "    }\n",
    "    return json.dumps(checks, indent=2)\n",
    "\n",
    "@tool\n",
    "def investigate_data_anomaly(table: str, column: str, anomaly_type: str) -> str:\n",
    "    \"\"\"Investiga causa ra√≠z de anomal√≠a en datos.\"\"\"\n",
    "    # An√°lisis con LLM + queries SQL\n",
    "    investigation = {\n",
    "        'anomaly': f'{anomaly_type} in {table}.{column}',\n",
    "        'potential_causes': [\n",
    "            'Upstream pipeline failure',\n",
    "            'Schema change in source',\n",
    "            'Data validation disabled'\n",
    "        ],\n",
    "        'affected_rows': 15600,\n",
    "        'first_occurrence': '2024-10-28 14:23:00',\n",
    "        'recommendation': 'Check upstream pipeline and re-run with validation'\n",
    "    }\n",
    "    return json.dumps(investigation, indent=2)\n",
    "\n",
    "# El agente detecta fallas de calidad, investiga y alerta autom√°ticamente\n",
    "```\n",
    "\n",
    "### üìà M√©tricas de √âxito de Agentes en Producci√≥n\n",
    "\n",
    "| M√©trica | Target | Medici√≥n |\n",
    "|---------|--------|----------|\n",
    "| **MTTR (Mean Time To Recovery)** | <15 min | Tiempo desde falla hasta resoluci√≥n |\n",
    "| **Auto-resolution rate** | >60% | % de incidents resueltos sin humano |\n",
    "| **False positive rate** | <5% | % de alertas/acciones incorrectas |\n",
    "| **Cost per incident** | <$0.50 | Costo en API calls por incident |\n",
    "| **Precision de diagn√≥stico** | >90% | % de root causes identificadas correctamente |\n",
    "| **Uptime improvement** | +15% | Reducci√≥n de downtime vs manual |\n",
    "\n",
    "### üí∞ ROI de Agentes Aut√≥nomos\n",
    "\n",
    "```python\n",
    "# C√°lculo de ROI\n",
    "\n",
    "# Sin agentes (manual)\n",
    "incidents_per_month = 45\n",
    "avg_resolution_time_hours = 2.0\n",
    "engineer_hourly_rate = 75  # USD\n",
    "manual_cost = incidents_per_month * avg_resolution_time_hours * engineer_hourly_rate\n",
    "# = 45 * 2 * $75 = $6,750/mes\n",
    "\n",
    "# Con agentes\n",
    "auto_resolved = incidents_per_month * 0.60  # 60% auto-resolved\n",
    "manual_remaining = incidents_per_month * 0.40\n",
    "avg_resolution_time_with_agent = 0.5  # hours (m√°s r√°pido con diagn√≥stico autom√°tico)\n",
    "\n",
    "manual_cost_with_agent = manual_remaining * avg_resolution_time_with_agent * engineer_hourly_rate\n",
    "# = 18 * 0.5 * $75 = $675/mes\n",
    "\n",
    "api_costs = incidents_per_month * 0.25  # $0.25 por incident en API calls\n",
    "# = 45 * $0.25 = $11.25/mes\n",
    "\n",
    "total_cost_with_agent = manual_cost_with_agent + api_costs\n",
    "# = $675 + $11.25 = $686.25/mes\n",
    "\n",
    "savings = manual_cost - total_cost_with_agent\n",
    "# = $6,750 - $686 = $6,064/mes = $72,768/a√±o\n",
    "\n",
    "roi = (savings / total_cost_with_agent) * 100\n",
    "# = 883% ROI\n",
    "```\n",
    "\n",
    "### üöß Desaf√≠os y Limitaciones\n",
    "\n",
    "| Desaf√≠o | Mitigaci√≥n |\n",
    "|---------|-----------|\n",
    "| **Alucinaciones** | Validaci√≥n de outputs, dry-run mode |\n",
    "| **Costos API** | Caching, modelos peque√±os, rate limiting |\n",
    "| **Latencia** | Async execution, timeouts, parallelizaci√≥n |\n",
    "| **Confiabilidad** | Retry logic, fallback a manual, human approval gates |\n",
    "| **Security** | Tool sandboxing, permission checks, audit logs |\n",
    "| **Debugging** | Exhaustive logging, replay capability |\n",
    "\n",
    "### üéØ Roadmap para Implementar Agentes\n",
    "\n",
    "**Fase 1: Observaci√≥n (Mes 1-2)**\n",
    "- Agente solo observa y reporta (no toma acciones)\n",
    "- Logging de decisiones que tomar√≠a\n",
    "- Validaci√≥n de precisi√≥n de diagn√≥sticos\n",
    "\n",
    "**Fase 2: Asistencia (Mes 3-4)**\n",
    "- Agente sugiere acciones, humano aprueba\n",
    "- Human-in-the-loop para todas las decisiones\n",
    "- Medir false positive rate\n",
    "\n",
    "**Fase 3: Semi-autonom√≠a (Mes 5-6)**\n",
    "- Auto-resoluci√≥n de incidents \"seguros\" (read-only actions, retries)\n",
    "- Acciones destructivas requieren aprobaci√≥n\n",
    "- Monitoreo 24/7 de m√©tricas\n",
    "\n",
    "**Fase 4: Autonom√≠a completa (Mes 7+)**\n",
    "- Auto-resoluci√≥n de 60-80% de incidents\n",
    "- Alertas solo para casos complejos\n",
    "- Continuous learning con feedback\n",
    "\n",
    "### üí° Mejores Pr√°cticas para Agentes de Producci√≥n\n",
    "\n",
    "1. **Start small**: Comenzar con un caso de uso simple (ej. retry de timeouts)\n",
    "2. **Dry-run mode**: Implementar modo de prueba que no ejecuta acciones reales\n",
    "3. **Approval gates**: Requerir aprobaci√≥n humana para acciones cr√≠ticas\n",
    "4. **Exhaustive logging**: Loggear TODO (inputs, outputs, decisiones, acciones)\n",
    "5. **Rollback capability**: Poder deshacer acciones del agente\n",
    "6. **Cost monitoring**: Alertar si costo por incident excede threshold\n",
    "7. **A/B testing**: Comparar agente vs manual con m√©tricas objetivas\n",
    "8. **Feedback loop**: Permitir humanos marcar decisiones como correctas/incorrectas\n",
    "9. **Gradual rollout**: 10% ‚Üí 50% ‚Üí 100% de incidents\n",
    "10. **Emergency stop**: Bot√≥n para desactivar agente si se comporta mal\n",
    "\n",
    "### üöÄ Futuro: Agentes Cognitivos\n",
    "\n",
    "```\n",
    "2024: Rule-based + LLM reasoning\n",
    "  ‚Üì\n",
    "2025: Learning from outcomes (RL)\n",
    "  ‚Üì\n",
    "2026: Proactive optimization (no espera fallas)\n",
    "  ‚Üì\n",
    "2027: Self-improving pipelines\n",
    "  ‚Üì\n",
    "2030: Fully autonomous data platforms\n",
    "```\n",
    "\n",
    "El futuro de Data Engineering es **agentic**: sistemas que no solo ejecutan instrucciones, sino que **razonan, aprenden y se adaptan** continuamente para mantener pipelines de datos operando con m√≠nima intervenci√≥n humana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf9a0",
   "metadata": {},
   "source": [
    "## 1. Agente simple con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langgraph openai\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import tool\n",
    "from langchain import hub\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "@tool\n",
    "def get_table_row_count(table_name: str) -> str:\n",
    "    \"\"\"Devuelve el n√∫mero de filas de una tabla.\"\"\"\n",
    "    # Simulaci√≥n\n",
    "    counts = {'ventas': 1250000, 'clientes': 50000, 'productos': 8500}\n",
    "    return f'La tabla {table_name} tiene {counts.get(table_name, 0)} filas.'\n",
    "\n",
    "@tool\n",
    "def get_table_schema(table_name: str) -> str:\n",
    "    \"\"\"Devuelve el esquema de una tabla.\"\"\"\n",
    "    schemas = {\n",
    "        'ventas': 'venta_id (BIGINT), fecha (DATE), producto_id (INT), total (DECIMAL)',\n",
    "        'clientes': 'cliente_id (INT), nombre (VARCHAR), email (VARCHAR), ciudad (VARCHAR)'\n",
    "    }\n",
    "    return schemas.get(table_name, 'Tabla no encontrada')\n",
    "\n",
    "tools = [get_table_row_count, get_table_schema]\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull('hwchase17/openai-tools-agent')\n",
    "\n",
    "# Crear agente\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "result = agent_executor.invoke({\n",
    "    'input': '¬øCu√°ntas filas tiene la tabla ventas y cu√°l es su esquema?'\n",
    "})\n",
    "\n",
    "print('\\nRespuesta final:', result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5e773",
   "metadata": {},
   "source": [
    "## 2. Agente para debugging de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb254dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def validate_sql_syntax(query: str) -> str:\n",
    "    \"\"\"Valida sintaxis SQL.\"\"\"\n",
    "    # Simulaci√≥n simple\n",
    "    errors = []\n",
    "    if 'SELCT' in query.upper():\n",
    "        errors.append('Typo: SELCT deber√≠a ser SELECT')\n",
    "    if query.count('(') != query.count(')'):\n",
    "        errors.append('Par√©ntesis desbalanceados')\n",
    "    return 'Errores: ' + ', '.join(errors) if errors else 'Sintaxis v√°lida'\n",
    "\n",
    "@tool\n",
    "def suggest_sql_optimization(query: str) -> str:\n",
    "    \"\"\"Sugiere optimizaciones.\"\"\"\n",
    "    tips = []\n",
    "    if 'SELECT *' in query.upper():\n",
    "        tips.append('Evita SELECT *, especifica columnas')\n",
    "    if 'WHERE' not in query.upper() and 'JOIN' in query.upper():\n",
    "        tips.append('Considera agregar WHERE para filtrar resultados')\n",
    "    return 'Sugerencias: ' + ', '.join(tips) if tips else 'Query est√° bien optimizada'\n",
    "\n",
    "debug_tools = [validate_sql_syntax, suggest_sql_optimization]\n",
    "\n",
    "debug_agent = create_openai_tools_agent(llm, debug_tools, prompt)\n",
    "debug_executor = AgentExecutor(agent=debug_agent, tools=debug_tools, verbose=True)\n",
    "\n",
    "broken_query = 'SELCT * FROM ventas JOIN productos'\n",
    "\n",
    "result = debug_executor.invoke({\n",
    "    'input': f'Analiza esta query y sugiere mejoras: {broken_query}'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c4b80",
   "metadata": {},
   "source": [
    "## 3. LangGraph: workflow con estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "class ETLState(TypedDict):\n",
    "    query: str\n",
    "    validated: bool\n",
    "    optimized: bool\n",
    "    result: str\n",
    "    errors: Annotated[list, operator.add]\n",
    "\n",
    "def validate_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de validaci√≥n.\"\"\"\n",
    "    if 'SELECT' not in state['query'].upper():\n",
    "        state['errors'].append('Query no es SELECT')\n",
    "        state['validated'] = False\n",
    "    else:\n",
    "        state['validated'] = True\n",
    "    return state\n",
    "\n",
    "def optimize_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de optimizaci√≥n.\"\"\"\n",
    "    if state['validated']:\n",
    "        # Simulaci√≥n\n",
    "        state['query'] = state['query'].replace('SELECT *', 'SELECT id, nombre')\n",
    "        state['optimized'] = True\n",
    "    return state\n",
    "\n",
    "def execute_node(state: ETLState) -> ETLState:\n",
    "    \"\"\"Nodo de ejecuci√≥n.\"\"\"\n",
    "    if state['optimized']:\n",
    "        state['result'] = f'Query ejecutada: {state[\"query\"]}'\n",
    "    else:\n",
    "        state['result'] = 'Ejecuci√≥n cancelada por errores'\n",
    "    return state\n",
    "\n",
    "# Crear grafo\n",
    "workflow = StateGraph(ETLState)\n",
    "workflow.add_node('validate', validate_node)\n",
    "workflow.add_node('optimize', optimize_node)\n",
    "workflow.add_node('execute', execute_node)\n",
    "\n",
    "workflow.set_entry_point('validate')\n",
    "workflow.add_edge('validate', 'optimize')\n",
    "workflow.add_edge('optimize', 'execute')\n",
    "workflow.add_edge('execute', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Ejecutar\n",
    "initial_state = {\n",
    "    'query': 'SELECT * FROM ventas',\n",
    "    'validated': False,\n",
    "    'optimized': False,\n",
    "    'result': '',\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "print('Estado final:', final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cc64f",
   "metadata": {},
   "source": [
    "## 4. AutoGen: m√∫ltiples agentes colaborativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984980b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyautogen\n",
    "import autogen\n",
    "\n",
    "config_list = [{\n",
    "    'model': 'gpt-4',\n",
    "    'api_key': os.getenv('OPENAI_API_KEY')\n",
    "}]\n",
    "\n",
    "# Agente 1: Data Engineer\n",
    "data_engineer = autogen.AssistantAgent(\n",
    "    name='DataEngineer',\n",
    "    llm_config={'config_list': config_list},\n",
    "    system_message='''\n",
    "Eres un ingeniero de datos experto. Tu tarea es:\n",
    "1. Dise√±ar pipelines ETL\n",
    "2. Escribir queries SQL optimizadas\n",
    "3. Proponer arquitecturas de datos\n",
    "'''\n",
    ")\n",
    "\n",
    "# Agente 2: QA Engineer\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name='QAEngineer',\n",
    "    llm_config={'config_list': config_list},\n",
    "    system_message='''\n",
    "Eres un ingeniero de calidad. Tu tarea es:\n",
    "1. Revisar c√≥digo y queries\n",
    "2. Identificar bugs y problemas de rendimiento\n",
    "3. Sugerir tests\n",
    "'''\n",
    ")\n",
    "\n",
    "# Agente 3: User Proxy (humano)\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name='User',\n",
    "    human_input_mode='NEVER',\n",
    "    max_consecutive_auto_reply=0\n",
    ")\n",
    "\n",
    "# Conversaci√≥n\n",
    "task = '''\n",
    "Dise√±a un pipeline que:\n",
    "1. Extraiga datos de API de ventas\n",
    "2. Transforme (agregue por d√≠a)\n",
    "3. Cargue en Redshift\n",
    "Luego revisa el dise√±o.\n",
    "'''\n",
    "\n",
    "# Iniciar chat grupal\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, data_engineer, qa_engineer],\n",
    "    messages=[],\n",
    "    max_round=5\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={'config_list': config_list})\n",
    "\n",
    "user_proxy.initiate_chat(manager, message=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b51a15",
   "metadata": {},
   "source": [
    "## 5. Agente para monitoreo de data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def check_null_rate(table: str, column: str) -> str:\n",
    "    \"\"\"Verifica tasa de nulos.\"\"\"\n",
    "    # Simulaci√≥n\n",
    "    rates = {('ventas', 'total'): 0.5, ('clientes', 'email'): 15.2}\n",
    "    rate = rates.get((table, column), 0)\n",
    "    return f'Tasa de nulos en {table}.{column}: {rate}%'\n",
    "\n",
    "@tool\n",
    "def check_duplicates(table: str) -> str:\n",
    "    \"\"\"Verifica duplicados.\"\"\"\n",
    "    dup_counts = {'ventas': 120, 'clientes': 5}\n",
    "    count = dup_counts.get(table, 0)\n",
    "    return f'Duplicados en {table}: {count}'\n",
    "\n",
    "quality_tools = [check_null_rate, check_duplicates]\n",
    "\n",
    "quality_agent = create_openai_tools_agent(llm, quality_tools, prompt)\n",
    "quality_executor = AgentExecutor(agent=quality_agent, tools=quality_tools, verbose=True)\n",
    "\n",
    "quality_result = quality_executor.invoke({\n",
    "    'input': 'Revisa la calidad de la tabla ventas: nulos y duplicados'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979b017",
   "metadata": {},
   "source": [
    "## 6. Agente reactivo con memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "agent_with_memory = create_openai_tools_agent(llm, tools, prompt)\n",
    "executor_with_memory = AgentExecutor(\n",
    "    agent=agent_with_memory,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Conversaci√≥n\n",
    "executor_with_memory.invoke({'input': 'Cu√°ntas filas tiene ventas?'})\n",
    "executor_with_memory.invoke({'input': 'Y cu√°l es su esquema?'})  # Contexto previo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813ba35",
   "metadata": {},
   "source": [
    "## 7. Buenas pr√°cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f7824",
   "metadata": {},
   "source": [
    "- **Tools espec√≠ficas**: cada tool debe hacer UNA cosa bien.\n",
    "- **Validaci√≥n de outputs**: verifica respuestas antes de acciones cr√≠ticas.\n",
    "- **Human-in-the-loop**: aprobaci√≥n humana para operaciones destructivas.\n",
    "- **Observabilidad**: loggea cada decisi√≥n del agente.\n",
    "- **Fallback**: maneja errores de LLM o tools.\n",
    "- **Testing**: prueba agentes con casos edge.\n",
    "- **Costos**: limita llamadas y usa modelos peque√±os cuando posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7b1fd",
   "metadata": {},
   "source": [
    "## 8. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5addc59",
   "metadata": {},
   "source": [
    "1. Crea un agente que automatice la investigaci√≥n de incidentes en pipelines.\n",
    "2. Construye un agente que genere data quality reports diarios.\n",
    "3. Implementa un sistema multi-agente para code review de PRs.\n",
    "4. Desarrolla un agente que optimice queries en producci√≥n autom√°ticamente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
