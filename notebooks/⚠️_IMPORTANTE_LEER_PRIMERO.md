# âš ï¸ IMPORTANTE - LEE ESTO ANTES DE EMPEZAR

## ðŸš¨ SOBRE EL USO DE JUPYTER NOTEBOOKS

### âœ… Notebooks son EXCELENTES para:
- ðŸ“š **Aprendizaje y enseÃ±anza** (como este curso)
- ðŸ”¬ **ExploraciÃ³n y anÃ¡lisis de datos**
- ðŸ§ª **Prototipado rÃ¡pido**
- ðŸ“Š **VisualizaciÃ³n interactiva**
- ðŸ“ **DocumentaciÃ³n tÃ©cnica ejecutable**
- ðŸ§‘â€ðŸ« **Presentaciones y demos**

### âŒ Notebooks NO son recomendables para:
- ðŸ­ **Sistemas en producciÃ³n**
- ðŸ”„ **Pipelines ETL automatizados**
- âš™ï¸ **Workflows orquestados (Airflow, Prefect)**
- ðŸš€ **Aplicaciones escalables**
- ðŸ” **Servicios con requisitos de seguridad estrictos**
- ðŸ“¦ **CÃ³digo que requiere testing exhaustivo**
- ðŸ” **Procesos que se ejecutan repetidamente**

## ðŸ—ï¸ MEJORES PRÃCTICAS PARA PRODUCCIÃ“N

### En lugar de notebooks, usa:

```
notebooks/ (Desarrollo y exploraciÃ³n)
    â†“
    Prototipo validado
    â†“
src/ (CÃ³digo de producciÃ³n)
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py          â† MÃ³dulos Python estructurados
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ schemas.py          â† Pydantic models
â”‚   â””â”€â”€ entities.py
â”œâ”€â”€ services/
â”‚   â””â”€â”€ data_service.py     â† Business logic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ db.py               â† Database connectors
â”‚   â””â”€â”€ validators.py       â† Data validation
â””â”€â”€ config/
    â””â”€â”€ settings.py         â† Configuration management

tests/                      â† Testing comprehensivo
â”œâ”€â”€ unit/
â”œâ”€â”€ integration/
â””â”€â”€ e2e/

.github/workflows/          â† CI/CD automation
â””â”€â”€ deploy.yml

Dockerfile                  â† Containerization
docker-compose.yml

airflow/dags/               â† Orchestration
â””â”€â”€ sales_pipeline_dag.py
```

## ðŸ”„ FLUJO DE TRABAJO RECOMENDADO

### 1ï¸âƒ£ EXPLORACIÃ“N (Notebooks âœ…)
```python
# notebooks/exploration/01_explore_sales_data.ipynb
import pandas as pd

# Cargar datos
df = pd.read_csv('data/ventas.csv')

# Explorar
df.describe()
df.plot()

# Probar transformaciones
df['total_clean'] = df['total'].fillna(0)
```

### 2ï¸âƒ£ PROTOTIPADO (Notebooks âœ…)
```python
# notebooks/prototypes/02_sales_etl_prototype.ipynb

# Probar pipeline completo
def extract():
    return pd.read_csv('data/ventas.csv')

def transform(df):
    df['total_clean'] = df['total'].fillna(0)
    df['fecha'] = pd.to_datetime(df['fecha'])
    return df

def load(df):
    df.to_parquet('output/ventas_clean.parquet')

# Ejecutar y validar
df = extract()
df = transform(df)
load(df)
```

### 3ï¸âƒ£ PRODUCCIÃ“N (Scripts Python âœ…)
```python
# src/pipelines/sales_etl.py
from typing import Optional
import pandas as pd
from pydantic import BaseModel
import logging

logger = logging.getLogger(__name__)

class SalesETL:
    """Pipeline ETL para ventas - VersiÃ³n producciÃ³n"""
    
    def __init__(self, config: dict):
        self.config = config
    
    def extract(self) -> pd.DataFrame:
        """Extract sales data from source"""
        logger.info("Starting extraction...")
        try:
            df = pd.read_csv(self.config['source_path'])
            logger.info(f"Extracted {len(df)} records")
            return df
        except Exception as e:
            logger.error(f"Extraction failed: {e}")
            raise
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform sales data"""
        logger.info("Starting transformation...")
        df['total_clean'] = df['total'].fillna(0)
        df['fecha'] = pd.to_datetime(df['fecha'])
        
        # Validaciones
        assert not df['total_clean'].isnull().any(), "Nulls found after cleaning"
        
        return df
    
    def load(self, df: pd.DataFrame) -> None:
        """Load to destination"""
        logger.info("Starting load...")
        df.to_parquet(
            self.config['destination_path'],
            compression='snappy',
            index=False
        )
        logger.info(f"Loaded {len(df)} records successfully")
    
    def run(self) -> None:
        """Execute full pipeline"""
        try:
            df = self.extract()
            df = self.transform(df)
            self.load(df)
            logger.info("Pipeline completed successfully")
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            raise

# Entry point
if __name__ == '__main__':
    config = {
        'source_path': 'data/ventas.csv',
        'destination_path': 'output/ventas_clean.parquet'
    }
    
    pipeline = SalesETL(config)
    pipeline.run()
```

### 4ï¸âƒ£ ORQUESTACIÃ“N (Airflow DAG âœ…)
```python
# airflow/dags/sales_pipeline_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def run_sales_etl():
    from src.pipelines.sales_etl import SalesETL
    
    pipeline = SalesETL(config)
    pipeline.run()

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'sales_etl_pipeline',
    default_args=default_args,
    description='Sales ETL pipeline',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    catchup=False,
    tags=['sales', 'etl']
) as dag:
    
    run_pipeline = PythonOperator(
        task_id='run_sales_etl',
        python_callable=run_sales_etl
    )
```

## ðŸ“Š COMPARACIÃ“N: NOTEBOOK vs PRODUCCIÃ“N

| Aspecto | Notebook (Desarrollo) | Script Python (ProducciÃ³n) |
|---------|----------------------|----------------------------|
| **Estructura** | CÃ©lulas secuenciales | MÃ³dulos y funciones |
| **Reusabilidad** | Baja (copy-paste) | Alta (imports) |
| **Testing** | DifÃ­cil | FÃ¡cil (pytest) |
| **CI/CD** | No integrable | Completamente integrable |
| **Logging** | Print statements | Logging estructurado |
| **Error Handling** | BÃ¡sico | Robusto con retry |
| **Version Control** | ProblemÃ¡tico (JSON) | Excelente (Python puro) |
| **Schedulling** | Manual | Automatizado (Airflow) |
| **Monitoring** | No | SÃ­ (Datadog, Prometheus) |
| **Escalabilidad** | Limitada | Alta |
| **Mantenibilidad** | Baja | Alta |

## âš ï¸ PROBLEMAS COMUNES DE NOTEBOOKS EN PRODUCCIÃ“N

### 1. Orden de ejecuciÃ³n no garantizado
```python
# Celda 10 ejecutada antes que Celda 5 â†’ resultados inconsistentes
```

### 2. Estado oculto (hidden state)
```python
# Variables definidas en celdas eliminadas siguen en memoria
```

### 3. DifÃ­cil de testear
```python
# Â¿CÃ³mo hacer pytest de una celda especÃ­fica?
```

### 4. No hay control de versiones efectivo
```python
# Git diff de notebooks es ilegible (JSON)
```

### 5. No se puede orquestar fÃ¡cilmente
```python
# Â¿CÃ³mo correr un notebook con retry en Airflow?
```

## âœ… RESUMEN

### Este curso usa notebooks porque:
- âœ… Facilita el aprendizaje paso a paso
- âœ… Permite visualizar resultados inmediatamente
- âœ… Combina cÃ³digo y explicaciones
- âœ… Es interactivo y fÃ¡cil de compartir

### En tu trabajo real usa:
- âœ… Scripts Python modulares (`.py`)
- âœ… Tests automatizados (pytest)
- âœ… CI/CD pipelines (GitHub Actions)
- âœ… Orquestadores (Airflow, Prefect)
- âœ… Contenedores (Docker)
- âœ… Monitoreo (Datadog, New Relic)

## ðŸŽ“ APRENDE AQUÃ, APLICA CORRECTAMENTE

**Los notebooks son una herramienta excelente para aprender Data Engineering, pero recuerda siempre:**

> **"Learn with notebooks, deploy with production code."**

---

**Autor:** LuisRai (Luis J. Raigoso V.)  
**Curso:** Data Engineering - Modular Course  
**Repositorio:** https://github.com/lraigosov/data-engineer-course

Â© 2024-2025 LuisRai - Todos los derechos reservados
