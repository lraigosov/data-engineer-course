{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f82e7",
   "metadata": {},
   "source": [
    "# ⚡ Mid - 01. Orquestación de Pipelines con Apache Airflow\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los conceptos de DAGs (Directed Acyclic Graphs)\n",
    "- [ ] Crear y configurar DAGs de Apache Airflow\n",
    "- [ ] Implementar operators y tasks\n",
    "- [ ] Manejar dependencias entre tareas\n",
    "- [ ] Monitorear y debuggear pipelines\n",
    "- [ ] Implementar buenas prácticas de orquestación\n",
    "\n",
    "**Duración Estimada:** 120 minutos  \n",
    "**Nivel de Dificultad:** Intermedio  \n",
    "**Prerrequisitos:** Notebooks nivel Junior completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720de3",
   "metadata": {},
   "source": [
    "## 🎯 ¿Qué es Apache Airflow?\n",
    "\n",
    "**Apache Airflow** es una plataforma open-source para:\n",
    "\n",
    "✅ **Orquestar** pipelines de datos complejos  \n",
    "✅ **Programar** ejecuciones automáticas  \n",
    "✅ **Monitorear** el estado de los pipelines  \n",
    "✅ **Manejar** dependencias entre tareas  \n",
    "✅ **Reintenta r** tareas fallidas automáticamente  \n",
    "\n",
    "### 🏗️ Conceptos Clave:\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Pipeline completo definido como código\n",
    "- **Operator**: Unidad de trabajo (tarea individual)\n",
    "- **Task**: Instancia de un operator\n",
    "- **Scheduler**: Programa la ejecución de DAGs\n",
    "- **Executor**: Ejecuta las tareas\n",
    "- **Web UI**: Interfaz para monitoreo\n",
    "\n",
    "### 🌟 Ventajas de Airflow:\n",
    "\n",
    "```\n",
    "✅ Código Python nativo\n",
    "✅ Pipelines como código (versionables)\n",
    "✅ Rico ecosistema de operators\n",
    "✅ Escalable y extensible\n",
    "✅ Monitoreo visual intuitivo\n",
    "✅ Manejo robusto de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778f4f",
   "metadata": {},
   "source": [
    "## 📦 Setup y Configuración\n",
    "\n",
    "### Instalación de Airflow\n",
    "\n",
    "```bash\n",
    "# Instalar Airflow (ejecutar en terminal)\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar scheduler (en otra terminal)\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar instalación de Airflow\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import airflow\n",
    "    print(f\"✅ Apache Airflow instalado\")\n",
    "    print(f\"   Versión: {airflow.__version__}\")\n",
    "    print(f\"   Ubicación: {airflow.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ Apache Airflow no está instalado\")\n",
    "    print(\"\\n📝 Para instalar, ejecuta en terminal:\")\n",
    "    print(\"   pip install apache-airflow==2.7.0\")\n",
    "\n",
    "# Librerías que usaremos\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n✅ Librerías auxiliares importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa132a6",
   "metadata": {},
   "source": [
    "### 🔧 Verificación de Entorno Airflow\n",
    "\n",
    "**Concepto:** Airflow requiere instalación separada y configuración inicial antes de usar.\n",
    "\n",
    "**Componentes principales:**\n",
    "- **Webserver:** UI en puerto 8080 para monitoreo visual\n",
    "- **Scheduler:** Daemon que programa y dispara DAGs según cron expressions\n",
    "- **Metadata DB:** SQLite (dev) o PostgreSQL (prod) para estado de ejecuciones\n",
    "- **Executor:** LocalExecutor, CeleryExecutor, KubernetesExecutor\n",
    "\n",
    "**Arquitectura:**\n",
    "```\n",
    "DAG Files (.py) → Scheduler → Executor → Tasks → Metadata DB → Webserver UI\n",
    "```\n",
    "\n",
    "**Nota:** Este notebook usa ejemplos de código; en producción, DAGs se colocan en `~/airflow/dags/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8040e5",
   "metadata": {},
   "source": [
    "## 🔨 Creando tu Primer DAG\n",
    "\n",
    "### Estructura Básica de un DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG básico (guardar como archivo .py en la carpeta dags/)\n",
    "dag_basico = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['tu_email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "}\n",
    "\n",
    "# Definición del DAG\n",
    "dag = DAG(\n",
    "    'mi_primer_dag',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG simple de demostración',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo', 'tutorial'],\n",
    ")\n",
    "\n",
    "# Función Python simple\n",
    "def imprimir_fecha(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
    "    return f\"Completado en {execution_date}\"\n",
    "\n",
    "# Tareas del DAG\n",
    "tarea_inicio = BashOperator(\n",
    "    task_id='inicio',\n",
    "    bash_command='echo \"Iniciando pipeline...\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=imprimir_fecha,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_fin = BashOperator(\n",
    "    task_id='finalizar',\n",
    "    bash_command='echo \"Pipeline completado!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Definir dependencias\n",
    "tarea_inicio >> tarea_python >> tarea_fin\n",
    "\"\"\"\n",
    "\n",
    "print(\"📋 Ejemplo de DAG básico:\")\n",
    "print(dag_basico)\n",
    "print(\"\\n💾 Guarda este código en: $AIRFLOW_HOME/dags/mi_primer_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2d8dd",
   "metadata": {},
   "source": [
    "### 📋 DAG Definition: Default Args y Configuración\n",
    "\n",
    "**Concepto:** Un DAG es un grafo dirigido acíclico que define un pipeline completo como código Python.\n",
    "\n",
    "**default_args:** Configuración compartida por todas las tasks del DAG:\n",
    "- **owner:** Responsable del DAG (para auditoría)\n",
    "- **retries:** Número de reintentos automáticos en caso de fallo\n",
    "- **retry_delay:** Tiempo de espera entre reintentos\n",
    "- **start_date:** Primera fecha elegible para ejecución\n",
    "- **depends_on_past:** Si True, espera éxito de ejecución anterior\n",
    "\n",
    "**Parámetros del DAG:**\n",
    "- **dag_id:** Identificador único del pipeline\n",
    "- **schedule_interval:** Cron expression o timedelta para frecuencia\n",
    "- **catchup:** Si False, no ejecuta periodos pasados al activar DAG\n",
    "\n",
    "**Principio:** DAGs son declarativos e idempotentes - misma ejecución produce mismo resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ec61",
   "metadata": {},
   "source": [
    "## 🎭 Operators Principales\n",
    "\n",
    "### 1. PythonOperator - Ejecutar Funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de funciones para PythonOperator\n",
    "def extraer_datos(**context):\n",
    "    \"\"\"\n",
    "    Simula extracción de datos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    print(\"🔽 Extrayendo datos...\")\n",
    "    \n",
    "    # Simular datos extraídos\n",
    "    datos = [\n",
    "        {'id': i, 'valor': random.randint(100, 1000), 'fecha': str(datetime.now().date())}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "    \n",
    "    print(f\"✅ Extraídos {len(datos)} registros\")\n",
    "    \n",
    "    # Pasar datos a la siguiente tarea usando XCom\n",
    "    return datos\n",
    "\n",
    "def transformar_datos(**context):\n",
    "    \"\"\"\n",
    "    Transforma los datos extraídos\n",
    "    \"\"\"\n",
    "    # Obtener datos de la tarea anterior usando XCom\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    \n",
    "    print(\"⚙️ Transformando datos...\")\n",
    "    \n",
    "    # Transformación simple\n",
    "    datos_transformados = [\n",
    "        {\n",
    "            **d,\n",
    "            'valor_duplicado': d['valor'] * 2,\n",
    "            'categoria': 'Alto' if d['valor'] > 500 else 'Bajo'\n",
    "        }\n",
    "        for d in datos\n",
    "    ]\n",
    "    \n",
    "    print(f\"✅ Transformados {len(datos_transformados)} registros\")\n",
    "    return datos_transformados\n",
    "\n",
    "def cargar_datos(**context):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='transformar')\n",
    "    \n",
    "    print(\"📤 Cargando datos...\")\n",
    "    \n",
    "    # Guardar en archivo JSON (simulando carga a BD)\n",
    "    output_path = '../datasets/processed/datos_pipeline.json'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Datos guardados en {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Simular ejecución\n",
    "print(\"🧪 Simulando ejecución del pipeline:\\n\")\n",
    "\n",
    "context_mock = {\n",
    "    'execution_date': datetime.now(),\n",
    "    'ti': type('obj', (object,), {\n",
    "        'xcom_pull': lambda self, task_ids: [\n",
    "            {'id': 1, 'valor': 750, 'fecha': '2024-01-01'},\n",
    "            {'id': 2, 'valor': 250, 'fecha': '2024-01-01'}\n",
    "        ]\n",
    "    })()\n",
    "}\n",
    "\n",
    "datos_ext = extraer_datos(**context_mock)\n",
    "print(f\"\\nDatos extraídos (muestra): {datos_ext[:2]}\")\n",
    "\n",
    "# Modificar context para transformar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_ext\n",
    "datos_trans = transformar_datos(**context_mock)\n",
    "print(f\"\\nDatos transformados (muestra): {datos_trans[:2]}\")\n",
    "\n",
    "# Cargar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_trans\n",
    "ruta_salida = cargar_datos(**context_mock)\n",
    "print(f\"\\n✅ Pipeline simulado completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c32cb6",
   "metadata": {},
   "source": [
    "### 🐍 PythonOperator y XCom: Compartir Datos Entre Tasks\n",
    "\n",
    "**Concepto:** PythonOperator ejecuta funciones Python arbitrarias como tasks en el DAG.\n",
    "\n",
    "**XCom (Cross-Communication):**\n",
    "- Mecanismo para pasar datos entre tasks\n",
    "- `return valor`: guarda en XCom automáticamente\n",
    "- `ti.xcom_pull(task_ids='anterior')`: recupera datos de task previa\n",
    "- Almacenado en metadata DB (límite ~48KB por seguridad)\n",
    "\n",
    "**Context dict:** Inyectado automáticamente en funciones con `**context`:\n",
    "- `ti` (TaskInstance): objeto para XCom y metadata\n",
    "- `execution_date`: fecha lógica de ejecución\n",
    "- `dag_run`: información de la ejecución del DAG\n",
    "- `params`: parámetros configurables\n",
    "\n",
    "**Buena práctica:** Para datasets grandes, usar XCom solo para paths/URLs, almacenar datos en S3/HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee553d4",
   "metadata": {},
   "source": [
    "### 2. BashOperator - Ejecutar Comandos Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b21370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de uso de BashOperator\n",
    "bash_examples = {\n",
    "    'verificar_archivo': {\n",
    "        'task_id': 'verificar_datos',\n",
    "        'bash_command': 'test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"',\n",
    "        'descripcion': 'Verifica que un archivo exista antes de procesarlo'\n",
    "    },\n",
    "    'limpiar_temp': {\n",
    "        'task_id': 'limpiar_archivos',\n",
    "        'bash_command': 'rm -rf /tmp/data_temp/*',\n",
    "        'descripcion': 'Limpia archivos temporales'\n",
    "    },\n",
    "    'ejecutar_script': {\n",
    "        'task_id': 'procesar_sql',\n",
    "        'bash_command': 'psql -U user -d database -f /scripts/query.sql',\n",
    "        'descripcion': 'Ejecuta un script SQL'\n",
    "    },\n",
    "    'mover_archivo': {\n",
    "        'task_id': 'archivar_datos',\n",
    "        'bash_command': 'mv /data/raw/{{ ds }}.csv /data/processed/',\n",
    "        'descripcion': 'Mueve archivo procesado con templating'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🔧 Ejemplos de BashOperator:\\n\")\n",
    "for nombre, config in bash_examples.items():\n",
    "    print(f\"📌 {nombre}:\")\n",
    "    print(f\"   Task ID: {config['task_id']}\")\n",
    "    print(f\"   Comando: {config['bash_command']}\")\n",
    "    print(f\"   Uso: {config['descripcion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531badb",
   "metadata": {},
   "source": [
    "### 💻 BashOperator: Integración con Sistema Operativo\n",
    "\n",
    "**Concepto:** Ejecuta comandos shell/bash directamente desde Airflow, útil para scripts legacy o herramientas CLI.\n",
    "\n",
    "**Casos de uso típicos:**\n",
    "- Verificar existencia de archivos antes de procesar\n",
    "- Ejecutar scripts SQL con `psql` o `mysql`\n",
    "- Mover/copiar archivos entre directorios\n",
    "- Limpiar archivos temporales\n",
    "- Ejecutar comandos de cloud CLI (aws s3 sync, gsutil)\n",
    "\n",
    "**Jinja Templating:**\n",
    "- Variables de Airflow disponibles en bash_command\n",
    "- `{{ ds }}`: execution_date como YYYY-MM-DD\n",
    "- `{{ dag_run.conf }}`: parámetros de ejecución manual\n",
    "- `{{ var.value.my_var }}`: Airflow Variables\n",
    "\n",
    "**Ventaja:** Reutilizar scripts bash existentes sin reescribir en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2772ca",
   "metadata": {},
   "source": [
    "## 🔗 Manejo de Dependencias\n",
    "\n",
    "### Diferentes Formas de Definir Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7724a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias_ejemplos = \"\"\"\n",
    "# Forma 1: Usando >> (recomendado - más legible)\n",
    "tarea_a >> tarea_b >> tarea_c\n",
    "# Significa: A debe completarse antes de B, B antes de C\n",
    "\n",
    "# Forma 2: Usando <<\n",
    "tarea_c << tarea_b << tarea_a\n",
    "# Mismo resultado que Forma 1, pero leyendo al revés\n",
    "\n",
    "# Forma 3: set_downstream() y set_upstream()\n",
    "tarea_a.set_downstream(tarea_b)\n",
    "tarea_b.set_downstream(tarea_c)\n",
    "\n",
    "# Dependencias múltiples (paralelo)\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
    "# Significa: A, B y C se ejecutan en paralelo después de inicio\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> tarea_a\n",
    "tarea_inicio >> tarea_b\n",
    "tarea_a >> tarea_c\n",
    "tarea_b >> tarea_c\n",
    "tarea_c >> tarea_fin\n",
    "# Significa: A y B en paralelo, luego C, luego fin\n",
    "\n",
    "# Usando listas para múltiples dependencias\n",
    "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔗 Patrones de Dependencias en Airflow:\")\n",
    "print(dependencias_ejemplos)\n",
    "\n",
    "# Visualización de patrones comunes\n",
    "print(\"\\n📊 Patrones de Flujo Comunes:\\n\")\n",
    "\n",
    "patrones = {\n",
    "    'Lineal': \"\"\"\n",
    "        A → B → C → D\n",
    "    \"\"\",\n",
    "    'Fan-out (Paralelo)': \"\"\"\n",
    "             ┌→ B ┐\n",
    "        A → ├→ C ├→ F\n",
    "             └→ D ┘\n",
    "    \"\"\",\n",
    "    'Fan-in (Convergente)': \"\"\"\n",
    "        A ┐\n",
    "        B ├→ D → E\n",
    "        C ┘\n",
    "    \"\"\",\n",
    "    'Diamante': \"\"\"\n",
    "             ┌→ B ┐\n",
    "        A → │     ├→ D\n",
    "             └→ C ┘\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for nombre, diagrama in patrones.items():\n",
    "    print(f\"🔸 {nombre}:{diagrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686755b",
   "metadata": {},
   "source": [
    "### 🔗 Dependency Management: Orquestando el Flujo\n",
    "\n",
    "**Concepto:** Dependencias definen el orden de ejecución de tasks en el DAG.\n",
    "\n",
    "**Operadores de dependencia:**\n",
    "- `>>` (bitshift right): `A >> B` = \"B depende de A\"\n",
    "- `<<` (bitshift left): `C << B` = \"C depende de B\"\n",
    "- `[A, B] >> C`: C espera a que A y B completen\n",
    "\n",
    "**Patrones de flujo:**\n",
    "- **Lineal:** A → B → C (secuencial, sin paralelismo)\n",
    "- **Fan-out:** A → [B, C, D] (ejecución paralela)\n",
    "- **Fan-in:** [A, B, C] → D (convergencia, espera todas)\n",
    "- **Diamante:** A → [B, C] → D (paralelo + convergencia)\n",
    "\n",
    "**Trigger Rules:**\n",
    "- `all_success` (default): espera que todas las upstream tasks tengan éxito\n",
    "- `all_failed`: ejecuta solo si todas upstream fallan\n",
    "- `one_success`: ejecuta si al menos una upstream tiene éxito\n",
    "- `none_failed`: ejecuta si ninguna upstream falló (permite skipped)\n",
    "\n",
    "**Uso:** Optimizar paralelismo maximiza throughput del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57575e0",
   "metadata": {},
   "source": [
    "## 🎨 DAG Completo: ETL de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG completo de ejemplo para guardar en dags/\n",
    "dag_ecommerce = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ecommerce_team',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_ecommerce_ventas',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para procesar ventas de e-commerce',\n",
    "    schedule_interval='0 2 * * *',  # Diario a las 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'ventas', 'etl'],\n",
    ")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def verificar_datos_disponibles(**context):\n",
    "    \"\"\"Verifica si hay datos nuevos para procesar\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    archivo = f'/data/raw/ventas_{fecha_archivo}.csv'\n",
    "    \n",
    "    if os.path.exists(archivo):\n",
    "        print(f\"✅ Archivo encontrado: {archivo}\")\n",
    "        return 'extraer_ventas'\n",
    "    else:\n",
    "        print(f\"❌ No hay datos para {fecha_archivo}\")\n",
    "        return 'no_data_skip'\n",
    "\n",
    "def extraer_ventas(**context):\n",
    "    \"\"\"Extrae ventas desde archivos CSV\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"🔽 Extrayendo ventas del {fecha_archivo}\")\n",
    "    \n",
    "    # Simular lectura\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id': range(1, 101),\n",
    "        'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Monitor'],\n",
    "        'cantidad': [1, 2, 1] * 33 + [1],\n",
    "        'precio': [1000, 25, 75] * 33 + [300],\n",
    "        'fecha': execution_date\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Extraídas {len(df)} ventas\")\n",
    "    return df.to_json()\n",
    "\n",
    "def transformar_ventas(**context):\n",
    "    \"\"\"Transforma y enriquece datos de ventas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='extraer_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    print(\"⚙️ Transformando datos de ventas\")\n",
    "    \n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio']\n",
    "    \n",
    "    # Categorizar por monto\n",
    "    df['categoria_venta'] = df['total'].apply(\n",
    "        lambda x: 'Premium' if x >= 500 else 'Standard'\n",
    "    )\n",
    "    \n",
    "    # Limpiar duplicados\n",
    "    df = df.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    print(f\"✅ Transformados {len(df)} registros\")\n",
    "    return df.to_json()\n",
    "\n",
    "def calcular_metricas(**context):\n",
    "    \"\"\"Calcula métricas agregadas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    metricas = {\n",
    "        'total_ventas': len(df),\n",
    "        'ingreso_total': df['total'].sum(),\n",
    "        'ticket_promedio': df['total'].mean(),\n",
    "        'producto_mas_vendido': df['producto'].mode()[0],\n",
    "        'fecha_proceso': context['execution_date'].isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Métricas calculadas: {metricas}\")\n",
    "    return metricas\n",
    "\n",
    "def cargar_warehouse(**context):\n",
    "    \"\"\"Carga datos en el data warehouse\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    \n",
    "    print(\"📤 Cargando datos al warehouse...\")\n",
    "    # Aquí iría la conexión real a BD\n",
    "    # connection.execute(\"INSERT INTO ventas ...\")\n",
    "    \n",
    "    print(\"✅ Datos cargados exitosamente\")\n",
    "\n",
    "def enviar_notificacion(**context):\n",
    "    \"\"\"Envía notificación de finalización\"\"\"\n",
    "    ti = context['ti']\n",
    "    metricas = ti.xcom_pull(task_ids='calcular_metricas')\n",
    "    \n",
    "    mensaje = f\"\"\"\n",
    "    Pipeline ETL Completado\n",
    "    -----------------------\n",
    "    Fecha: {metricas['fecha_proceso']}\n",
    "    Total ventas: {metricas['total_ventas']}\n",
    "    Ingreso total: ${metricas['ingreso_total']:,.2f}\n",
    "    Ticket promedio: ${metricas['ticket_promedio']:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"📧 Enviando notificación:\\n{mensaje}\")\n",
    "\n",
    "# --- TAREAS ---\n",
    "\n",
    "inicio = DummyOperator(\n",
    "    task_id='inicio',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "verificar = BranchPythonOperator(\n",
    "    task_id='verificar_datos',\n",
    "    python_callable=verificar_datos_disponibles,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "no_data = DummyOperator(\n",
    "    task_id='no_data_skip',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extraer = PythonOperator(\n",
    "    task_id='extraer_ventas',\n",
    "    python_callable=extraer_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transformar = PythonOperator(\n",
    "    task_id='transformar_ventas',\n",
    "    python_callable=transformar_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "metricas = PythonOperator(\n",
    "    task_id='calcular_metricas',\n",
    "    python_callable=calcular_metricas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cargar = PythonOperator(\n",
    "    task_id='cargar_warehouse',\n",
    "    python_callable=cargar_warehouse,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notificar = PythonOperator(\n",
    "    task_id='enviar_notificacion',\n",
    "    python_callable=enviar_notificacion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fin = DummyOperator(\n",
    "    task_id='fin',\n",
    "    trigger_rule='none_failed_min_one_success',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# --- FLUJO ---\n",
    "inicio >> verificar >> [extraer, no_data]\n",
    "extraer >> transformar >> [metricas, cargar]\n",
    "[metricas, cargar] >> notificar >> fin\n",
    "no_data >> fin\n",
    "'''\n",
    "\n",
    "print(\"📋 DAG Completo de E-commerce:\")\n",
    "print(\"\\n💾 Guarda este código en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\")\n",
    "print(\"\\n🎯 Características del DAG:\")\n",
    "print(\"   • Branching (decisiones condicionales)\")\n",
    "print(\"   • Procesamiento paralelo\")\n",
    "print(\"   • XCom para pasar datos entre tareas\")\n",
    "print(\"   • Manejo de escenarios sin datos\")\n",
    "print(\"   • Notificaciones automáticas\")\n",
    "print(\"   • Trigger rules personalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac22e98",
   "metadata": {},
   "source": [
    "### 🏗️ DAG Completo: Patrón ETL Productivo\n",
    "\n",
    "**Concepto:** DAG que implementa pipeline ETL completo con branching, paralelismo y notificaciones.\n",
    "\n",
    "**Componentes avanzados:**\n",
    "- **DummyOperator:** Tareas placeholder para organización lógica\n",
    "- **BranchPythonOperator:** Ejecución condicional basada en lógica\n",
    "- **Trigger rules:** Control fino de cuándo ejecutar tasks downstream\n",
    "\n",
    "**Flujo del DAG:**\n",
    "1. **Inicio** → Verificar datos disponibles\n",
    "2. **Branch:** Si hay datos → Extraer | Si no → Skip\n",
    "3. **Paralelo:** Transformar + Calcular métricas\n",
    "4. **Convergencia:** Cargar al warehouse\n",
    "5. **Notificación:** Enviar resumen de ejecución\n",
    "\n",
    "**Patrón Branch:**\n",
    "```python\n",
    "def verificar(**context):\n",
    "    if condicion:\n",
    "        return 'tarea_si'\n",
    "    return 'tarea_no'\n",
    "```\n",
    "\n",
    "**Producción:** Este patrón es base para pipelines enterprise con alerting y observabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995b6a7",
   "metadata": {},
   "source": [
    "## 🎓 Resumen y Mejores Prácticas\n",
    "\n",
    "### ✅ Mejores Prácticas:\n",
    "\n",
    "1. **Idempotencia**: Las tareas deben producir el mismo resultado si se ejecutan múltiples veces\n",
    "2. **Atomicidad**: Cada tarea debe ser una unidad indivisible de trabajo\n",
    "3. **No estado compartido**: Usar XCom para comunicación entre tareas\n",
    "4. **Logging apropiado**: Registrar información útil para debugging\n",
    "5. **Manejo de errores**: Implementar retries y alertas\n",
    "6. **Documentación**: Docstrings claros y tags descriptivos\n",
    "7. **Testing**: Probar DAGs antes de producción\n",
    "\n",
    "### 🔜 Próximos Temas:\n",
    "\n",
    "- Sensors y event-driven workflows\n",
    "- TaskGroups para organización\n",
    "- Dynamic DAGs\n",
    "- Integración con Cloud (AWS, GCP, Azure)\n",
    "- Monitoreo y alertas avanzadas\n",
    "\n",
    "---\n",
    "\n",
    "**¡Has dominado los fundamentos de Apache Airflow!** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
