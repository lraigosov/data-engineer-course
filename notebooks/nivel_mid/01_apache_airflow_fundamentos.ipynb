{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f82e7",
   "metadata": {},
   "source": [
    "# âš¡ Mid - 01. OrquestaciÃ³n de Pipelines con Apache Airflow\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los conceptos de DAGs (Directed Acyclic Graphs)\n",
    "- [ ] Crear y configurar DAGs de Apache Airflow\n",
    "- [ ] Implementar operators y tasks\n",
    "- [ ] Manejar dependencias entre tareas\n",
    "- [ ] Monitorear y debuggear pipelines\n",
    "- [ ] Implementar buenas prÃ¡cticas de orquestaciÃ³n\n",
    "\n",
    "**DuraciÃ³n Estimada:** 120 minutos  \n",
    "**Nivel de Dificultad:** Intermedio  \n",
    "**Prerrequisitos:** Notebooks nivel Junior completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720de3",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Â¿QuÃ© es Apache Airflow?\n",
    "\n",
    "**Apache Airflow** es una plataforma open-source para:\n",
    "\n",
    "âœ… **Orquestar** pipelines de datos complejos  \n",
    "âœ… **Programar** ejecuciones automÃ¡ticas  \n",
    "âœ… **Monitorear** el estado de los pipelines  \n",
    "âœ… **Manejar** dependencias entre tareas  \n",
    "âœ… **Reintenta r** tareas fallidas automÃ¡ticamente  \n",
    "\n",
    "### ðŸ—ï¸ Conceptos Clave:\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Pipeline completo definido como cÃ³digo\n",
    "- **Operator**: Unidad de trabajo (tarea individual)\n",
    "- **Task**: Instancia de un operator\n",
    "- **Scheduler**: Programa la ejecuciÃ³n de DAGs\n",
    "- **Executor**: Ejecuta las tareas\n",
    "- **Web UI**: Interfaz para monitoreo\n",
    "\n",
    "### ðŸŒŸ Ventajas de Airflow:\n",
    "\n",
    "```\n",
    "âœ… CÃ³digo Python nativo\n",
    "âœ… Pipelines como cÃ³digo (versionables)\n",
    "âœ… Rico ecosistema de operators\n",
    "âœ… Escalable y extensible\n",
    "âœ… Monitoreo visual intuitivo\n",
    "âœ… Manejo robusto de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778f4f",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Setup y ConfiguraciÃ³n\n",
    "\n",
    "### InstalaciÃ³n de Airflow\n",
    "\n",
    "```bash\n",
    "# Instalar Airflow (ejecutar en terminal)\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar scheduler (en otra terminal)\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar instalaciÃ³n de Airflow\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import airflow\n",
    "    print(f\"âœ… Apache Airflow instalado\")\n",
    "    print(f\"   VersiÃ³n: {airflow.__version__}\")\n",
    "    print(f\"   UbicaciÃ³n: {airflow.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Apache Airflow no estÃ¡ instalado\")\n",
    "    print(\"\\nðŸ“ Para instalar, ejecuta en terminal:\")\n",
    "    print(\"   pip install apache-airflow==2.7.0\")\n",
    "\n",
    "# LibrerÃ­as que usaremos\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\nâœ… LibrerÃ­as auxiliares importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8040e5",
   "metadata": {},
   "source": [
    "## ðŸ”¨ Creando tu Primer DAG\n",
    "\n",
    "### Estructura BÃ¡sica de un DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG bÃ¡sico (guardar como archivo .py en la carpeta dags/)\n",
    "dag_basico = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['tu_email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "}\n",
    "\n",
    "# DefiniciÃ³n del DAG\n",
    "dag = DAG(\n",
    "    'mi_primer_dag',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG simple de demostraciÃ³n',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo', 'tutorial'],\n",
    ")\n",
    "\n",
    "# FunciÃ³n Python simple\n",
    "def imprimir_fecha(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
    "    return f\"Completado en {execution_date}\"\n",
    "\n",
    "# Tareas del DAG\n",
    "tarea_inicio = BashOperator(\n",
    "    task_id='inicio',\n",
    "    bash_command='echo \"Iniciando pipeline...\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=imprimir_fecha,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_fin = BashOperator(\n",
    "    task_id='finalizar',\n",
    "    bash_command='echo \"Pipeline completado!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Definir dependencias\n",
    "tarea_inicio >> tarea_python >> tarea_fin\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“‹ Ejemplo de DAG bÃ¡sico:\")\n",
    "print(dag_basico)\n",
    "print(\"\\nðŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/mi_primer_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ec61",
   "metadata": {},
   "source": [
    "## ðŸŽ­ Operators Principales\n",
    "\n",
    "### 1. PythonOperator - Ejecutar Funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de funciones para PythonOperator\n",
    "def extraer_datos(**context):\n",
    "    \"\"\"\n",
    "    Simula extracciÃ³n de datos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    print(\"ðŸ”½ Extrayendo datos...\")\n",
    "    \n",
    "    # Simular datos extraÃ­dos\n",
    "    datos = [\n",
    "        {'id': i, 'valor': random.randint(100, 1000), 'fecha': str(datetime.now().date())}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ… ExtraÃ­dos {len(datos)} registros\")\n",
    "    \n",
    "    # Pasar datos a la siguiente tarea usando XCom\n",
    "    return datos\n",
    "\n",
    "def transformar_datos(**context):\n",
    "    \"\"\"\n",
    "    Transforma los datos extraÃ­dos\n",
    "    \"\"\"\n",
    "    # Obtener datos de la tarea anterior usando XCom\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    \n",
    "    print(\"âš™ï¸ Transformando datos...\")\n",
    "    \n",
    "    # TransformaciÃ³n simple\n",
    "    datos_transformados = [\n",
    "        {\n",
    "            **d,\n",
    "            'valor_duplicado': d['valor'] * 2,\n",
    "            'categoria': 'Alto' if d['valor'] > 500 else 'Bajo'\n",
    "        }\n",
    "        for d in datos\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ… Transformados {len(datos_transformados)} registros\")\n",
    "    return datos_transformados\n",
    "\n",
    "def cargar_datos(**context):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='transformar')\n",
    "    \n",
    "    print(\"ðŸ“¤ Cargando datos...\")\n",
    "    \n",
    "    # Guardar en archivo JSON (simulando carga a BD)\n",
    "    output_path = '../datasets/processed/datos_pipeline.json'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Datos guardados en {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Simular ejecuciÃ³n\n",
    "print(\"ðŸ§ª Simulando ejecuciÃ³n del pipeline:\\n\")\n",
    "\n",
    "context_mock = {\n",
    "    'execution_date': datetime.now(),\n",
    "    'ti': type('obj', (object,), {\n",
    "        'xcom_pull': lambda self, task_ids: [\n",
    "            {'id': 1, 'valor': 750, 'fecha': '2024-01-01'},\n",
    "            {'id': 2, 'valor': 250, 'fecha': '2024-01-01'}\n",
    "        ]\n",
    "    })()\n",
    "}\n",
    "\n",
    "datos_ext = extraer_datos(**context_mock)\n",
    "print(f\"\\nDatos extraÃ­dos (muestra): {datos_ext[:2]}\")\n",
    "\n",
    "# Modificar context para transformar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_ext\n",
    "datos_trans = transformar_datos(**context_mock)\n",
    "print(f\"\\nDatos transformados (muestra): {datos_trans[:2]}\")\n",
    "\n",
    "# Cargar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_trans\n",
    "ruta_salida = cargar_datos(**context_mock)\n",
    "print(f\"\\nâœ… Pipeline simulado completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee553d4",
   "metadata": {},
   "source": [
    "### 2. BashOperator - Ejecutar Comandos Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b21370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de uso de BashOperator\n",
    "bash_examples = {\n",
    "    'verificar_archivo': {\n",
    "        'task_id': 'verificar_datos',\n",
    "        'bash_command': 'test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"',\n",
    "        'descripcion': 'Verifica que un archivo exista antes de procesarlo'\n",
    "    },\n",
    "    'limpiar_temp': {\n",
    "        'task_id': 'limpiar_archivos',\n",
    "        'bash_command': 'rm -rf /tmp/data_temp/*',\n",
    "        'descripcion': 'Limpia archivos temporales'\n",
    "    },\n",
    "    'ejecutar_script': {\n",
    "        'task_id': 'procesar_sql',\n",
    "        'bash_command': 'psql -U user -d database -f /scripts/query.sql',\n",
    "        'descripcion': 'Ejecuta un script SQL'\n",
    "    },\n",
    "    'mover_archivo': {\n",
    "        'task_id': 'archivar_datos',\n",
    "        'bash_command': 'mv /data/raw/{{ ds }}.csv /data/processed/',\n",
    "        'descripcion': 'Mueve archivo procesado con templating'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ”§ Ejemplos de BashOperator:\\n\")\n",
    "for nombre, config in bash_examples.items():\n",
    "    print(f\"ðŸ“Œ {nombre}:\")\n",
    "    print(f\"   Task ID: {config['task_id']}\")\n",
    "    print(f\"   Comando: {config['bash_command']}\")\n",
    "    print(f\"   Uso: {config['descripcion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2772ca",
   "metadata": {},
   "source": [
    "## ðŸ”— Manejo de Dependencias\n",
    "\n",
    "### Diferentes Formas de Definir Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7724a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias_ejemplos = \"\"\"\n",
    "# Forma 1: Usando >> (recomendado - mÃ¡s legible)\n",
    "tarea_a >> tarea_b >> tarea_c\n",
    "# Significa: A debe completarse antes de B, B antes de C\n",
    "\n",
    "# Forma 2: Usando <<\n",
    "tarea_c << tarea_b << tarea_a\n",
    "# Mismo resultado que Forma 1, pero leyendo al revÃ©s\n",
    "\n",
    "# Forma 3: set_downstream() y set_upstream()\n",
    "tarea_a.set_downstream(tarea_b)\n",
    "tarea_b.set_downstream(tarea_c)\n",
    "\n",
    "# Dependencias mÃºltiples (paralelo)\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
    "# Significa: A, B y C se ejecutan en paralelo despuÃ©s de inicio\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> tarea_a\n",
    "tarea_inicio >> tarea_b\n",
    "tarea_a >> tarea_c\n",
    "tarea_b >> tarea_c\n",
    "tarea_c >> tarea_fin\n",
    "# Significa: A y B en paralelo, luego C, luego fin\n",
    "\n",
    "# Usando listas para mÃºltiples dependencias\n",
    "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”— Patrones de Dependencias en Airflow:\")\n",
    "print(dependencias_ejemplos)\n",
    "\n",
    "# VisualizaciÃ³n de patrones comunes\n",
    "print(\"\\nðŸ“Š Patrones de Flujo Comunes:\\n\")\n",
    "\n",
    "patrones = {\n",
    "    'Lineal': \"\"\"\n",
    "        A â†’ B â†’ C â†’ D\n",
    "    \"\"\",\n",
    "    'Fan-out (Paralelo)': \"\"\"\n",
    "             â”Œâ†’ B â”\n",
    "        A â†’ â”œâ†’ C â”œâ†’ F\n",
    "             â””â†’ D â”˜\n",
    "    \"\"\",\n",
    "    'Fan-in (Convergente)': \"\"\"\n",
    "        A â”\n",
    "        B â”œâ†’ D â†’ E\n",
    "        C â”˜\n",
    "    \"\"\",\n",
    "    'Diamante': \"\"\"\n",
    "             â”Œâ†’ B â”\n",
    "        A â†’ â”‚     â”œâ†’ D\n",
    "             â””â†’ C â”˜\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for nombre, diagrama in patrones.items():\n",
    "    print(f\"ðŸ”¸ {nombre}:{diagrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57575e0",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ DAG Completo: ETL de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG completo de ejemplo para guardar en dags/\n",
    "dag_ecommerce = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ecommerce_team',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_ecommerce_ventas',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para procesar ventas de e-commerce',\n",
    "    schedule_interval='0 2 * * *',  # Diario a las 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'ventas', 'etl'],\n",
    ")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def verificar_datos_disponibles(**context):\n",
    "    \"\"\"Verifica si hay datos nuevos para procesar\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    archivo = f'/data/raw/ventas_{fecha_archivo}.csv'\n",
    "    \n",
    "    if os.path.exists(archivo):\n",
    "        print(f\"âœ… Archivo encontrado: {archivo}\")\n",
    "        return 'extraer_ventas'\n",
    "    else:\n",
    "        print(f\"âŒ No hay datos para {fecha_archivo}\")\n",
    "        return 'no_data_skip'\n",
    "\n",
    "def extraer_ventas(**context):\n",
    "    \"\"\"Extrae ventas desde archivos CSV\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"ðŸ”½ Extrayendo ventas del {fecha_archivo}\")\n",
    "    \n",
    "    # Simular lectura\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id': range(1, 101),\n",
    "        'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Monitor'],\n",
    "        'cantidad': [1, 2, 1] * 33 + [1],\n",
    "        'precio': [1000, 25, 75] * 33 + [300],\n",
    "        'fecha': execution_date\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… ExtraÃ­das {len(df)} ventas\")\n",
    "    return df.to_json()\n",
    "\n",
    "def transformar_ventas(**context):\n",
    "    \"\"\"Transforma y enriquece datos de ventas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='extraer_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    print(\"âš™ï¸ Transformando datos de ventas\")\n",
    "    \n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio']\n",
    "    \n",
    "    # Categorizar por monto\n",
    "    df['categoria_venta'] = df['total'].apply(\n",
    "        lambda x: 'Premium' if x >= 500 else 'Standard'\n",
    "    )\n",
    "    \n",
    "    # Limpiar duplicados\n",
    "    df = df.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    print(f\"âœ… Transformados {len(df)} registros\")\n",
    "    return df.to_json()\n",
    "\n",
    "def calcular_metricas(**context):\n",
    "    \"\"\"Calcula mÃ©tricas agregadas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    metricas = {\n",
    "        'total_ventas': len(df),\n",
    "        'ingreso_total': df['total'].sum(),\n",
    "        'ticket_promedio': df['total'].mean(),\n",
    "        'producto_mas_vendido': df['producto'].mode()[0],\n",
    "        'fecha_proceso': context['execution_date'].isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š MÃ©tricas calculadas: {metricas}\")\n",
    "    return metricas\n",
    "\n",
    "def cargar_warehouse(**context):\n",
    "    \"\"\"Carga datos en el data warehouse\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    \n",
    "    print(\"ðŸ“¤ Cargando datos al warehouse...\")\n",
    "    # AquÃ­ irÃ­a la conexiÃ³n real a BD\n",
    "    # connection.execute(\"INSERT INTO ventas ...\")\n",
    "    \n",
    "    print(\"âœ… Datos cargados exitosamente\")\n",
    "\n",
    "def enviar_notificacion(**context):\n",
    "    \"\"\"EnvÃ­a notificaciÃ³n de finalizaciÃ³n\"\"\"\n",
    "    ti = context['ti']\n",
    "    metricas = ti.xcom_pull(task_ids='calcular_metricas')\n",
    "    \n",
    "    mensaje = f\"\"\"\n",
    "    Pipeline ETL Completado\n",
    "    -----------------------\n",
    "    Fecha: {metricas['fecha_proceso']}\n",
    "    Total ventas: {metricas['total_ventas']}\n",
    "    Ingreso total: ${metricas['ingreso_total']:,.2f}\n",
    "    Ticket promedio: ${metricas['ticket_promedio']:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ“§ Enviando notificaciÃ³n:\\n{mensaje}\")\n",
    "\n",
    "# --- TAREAS ---\n",
    "\n",
    "inicio = DummyOperator(\n",
    "    task_id='inicio',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "verificar = BranchPythonOperator(\n",
    "    task_id='verificar_datos',\n",
    "    python_callable=verificar_datos_disponibles,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "no_data = DummyOperator(\n",
    "    task_id='no_data_skip',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extraer = PythonOperator(\n",
    "    task_id='extraer_ventas',\n",
    "    python_callable=extraer_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transformar = PythonOperator(\n",
    "    task_id='transformar_ventas',\n",
    "    python_callable=transformar_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "metricas = PythonOperator(\n",
    "    task_id='calcular_metricas',\n",
    "    python_callable=calcular_metricas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cargar = PythonOperator(\n",
    "    task_id='cargar_warehouse',\n",
    "    python_callable=cargar_warehouse,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notificar = PythonOperator(\n",
    "    task_id='enviar_notificacion',\n",
    "    python_callable=enviar_notificacion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fin = DummyOperator(\n",
    "    task_id='fin',\n",
    "    trigger_rule='none_failed_min_one_success',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# --- FLUJO ---\n",
    "inicio >> verificar >> [extraer, no_data]\n",
    "extraer >> transformar >> [metricas, cargar]\n",
    "[metricas, cargar] >> notificar >> fin\n",
    "no_data >> fin\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“‹ DAG Completo de E-commerce:\")\n",
    "print(\"\\nðŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\")\n",
    "print(\"\\nðŸŽ¯ CaracterÃ­sticas del DAG:\")\n",
    "print(\"   â€¢ Branching (decisiones condicionales)\")\n",
    "print(\"   â€¢ Procesamiento paralelo\")\n",
    "print(\"   â€¢ XCom para pasar datos entre tareas\")\n",
    "print(\"   â€¢ Manejo de escenarios sin datos\")\n",
    "print(\"   â€¢ Notificaciones automÃ¡ticas\")\n",
    "print(\"   â€¢ Trigger rules personalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995b6a7",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Resumen y Mejores PrÃ¡cticas\n",
    "\n",
    "### âœ… Mejores PrÃ¡cticas:\n",
    "\n",
    "1. **Idempotencia**: Las tareas deben producir el mismo resultado si se ejecutan mÃºltiples veces\n",
    "2. **Atomicidad**: Cada tarea debe ser una unidad indivisible de trabajo\n",
    "3. **No estado compartido**: Usar XCom para comunicaciÃ³n entre tareas\n",
    "4. **Logging apropiado**: Registrar informaciÃ³n Ãºtil para debugging\n",
    "5. **Manejo de errores**: Implementar retries y alertas\n",
    "6. **DocumentaciÃ³n**: Docstrings claros y tags descriptivos\n",
    "7. **Testing**: Probar DAGs antes de producciÃ³n\n",
    "\n",
    "### ðŸ”œ PrÃ³ximos Temas:\n",
    "\n",
    "- Sensors y event-driven workflows\n",
    "- TaskGroups para organizaciÃ³n\n",
    "- Dynamic DAGs\n",
    "- IntegraciÃ³n con Cloud (AWS, GCP, Azure)\n",
    "- Monitoreo y alertas avanzadas\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Has dominado los fundamentos de Apache Airflow!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
