{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f82e7",
   "metadata": {},
   "source": [
    "# ‚ö° Mid - 01. Orquestaci√≥n de Pipelines con Apache Airflow\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los conceptos de DAGs (Directed Acyclic Graphs)\n",
    "- [ ] Crear y configurar DAGs de Apache Airflow\n",
    "- [ ] Implementar operators y tasks\n",
    "- [ ] Manejar dependencias entre tareas\n",
    "- [ ] Monitorear y debuggear pipelines\n",
    "- [ ] Implementar buenas pr√°cticas de orquestaci√≥n\n",
    "\n",
    "**Duraci√≥n Estimada:** 120 minutos  \n",
    "**Nivel de Dificultad:** Intermedio  \n",
    "**Prerrequisitos:** Notebooks nivel Junior completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720de3",
   "metadata": {},
   "source": [
    "## üéØ ¬øQu√© es Apache Airflow?\n",
    "\n",
    "**Apache Airflow** es una plataforma open-source para:\n",
    "\n",
    "‚úÖ **Orquestar** pipelines de datos complejos  \n",
    "‚úÖ **Programar** ejecuciones autom√°ticas  \n",
    "‚úÖ **Monitorear** el estado de los pipelines  \n",
    "‚úÖ **Manejar** dependencias entre tareas  \n",
    "‚úÖ **Reintenta r** tareas fallidas autom√°ticamente  \n",
    "\n",
    "### üèóÔ∏è Conceptos Clave:\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Pipeline completo definido como c√≥digo\n",
    "- **Operator**: Unidad de trabajo (tarea individual)\n",
    "- **Task**: Instancia de un operator\n",
    "- **Scheduler**: Programa la ejecuci√≥n de DAGs\n",
    "- **Executor**: Ejecuta las tareas\n",
    "- **Web UI**: Interfaz para monitoreo\n",
    "\n",
    "### üåü Ventajas de Airflow:\n",
    "\n",
    "```\n",
    "‚úÖ C√≥digo Python nativo\n",
    "‚úÖ Pipelines como c√≥digo (versionables)\n",
    "‚úÖ Rico ecosistema de operators\n",
    "‚úÖ Escalable y extensible\n",
    "‚úÖ Monitoreo visual intuitivo\n",
    "‚úÖ Manejo robusto de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778f4f",
   "metadata": {},
   "source": [
    "## üì¶ Setup y Configuraci√≥n\n",
    "\n",
    "### Instalaci√≥n de Airflow\n",
    "\n",
    "```bash\n",
    "# Instalar Airflow (ejecutar en terminal)\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar scheduler (en otra terminal)\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar instalaci√≥n de Airflow\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import airflow\n",
    "    print(f\"‚úÖ Apache Airflow instalado\")\n",
    "    print(f\"   Versi√≥n: {airflow.__version__}\")\n",
    "    print(f\"   Ubicaci√≥n: {airflow.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Apache Airflow no est√° instalado\")\n",
    "    print(\"\\nüìù Para instalar, ejecuta en terminal:\")\n",
    "    print(\"   pip install apache-airflow==2.7.0\")\n",
    "\n",
    "# Librer√≠as que usaremos\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n‚úÖ Librer√≠as auxiliares importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa132a6",
   "metadata": {},
   "source": [
    "### üîß Verificaci√≥n de Entorno Airflow\n",
    "\n",
    "**Concepto:** Airflow requiere instalaci√≥n separada y configuraci√≥n inicial antes de usar.\n",
    "\n",
    "**Componentes principales:**\n",
    "- **Webserver:** UI en puerto 8080 para monitoreo visual\n",
    "- **Scheduler:** Daemon que programa y dispara DAGs seg√∫n cron expressions\n",
    "- **Metadata DB:** SQLite (dev) o PostgreSQL (prod) para estado de ejecuciones\n",
    "- **Executor:** LocalExecutor, CeleryExecutor, KubernetesExecutor\n",
    "\n",
    "**Arquitectura:**\n",
    "```\n",
    "DAG Files (.py) ‚Üí Scheduler ‚Üí Executor ‚Üí Tasks ‚Üí Metadata DB ‚Üí Webserver UI\n",
    "```\n",
    "\n",
    "**Nota:** Este notebook usa ejemplos de c√≥digo; en producci√≥n, DAGs se colocan en `~/airflow/dags/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8040e5",
   "metadata": {},
   "source": [
    "## üî® Creando tu Primer DAG\n",
    "\n",
    "### Estructura B√°sica de un DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG b√°sico (guardar como archivo .py en la carpeta dags/)\n",
    "dag_basico = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['tu_email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "}\n",
    "\n",
    "# Definici√≥n del DAG\n",
    "dag = DAG(\n",
    "    'mi_primer_dag',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG simple de demostraci√≥n',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo', 'tutorial'],\n",
    ")\n",
    "\n",
    "# Funci√≥n Python simple\n",
    "def imprimir_fecha(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
    "    return f\"Completado en {execution_date}\"\n",
    "\n",
    "# Tareas del DAG\n",
    "tarea_inicio = BashOperator(\n",
    "    task_id='inicio',\n",
    "    bash_command='echo \"Iniciando pipeline...\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=imprimir_fecha,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_fin = BashOperator(\n",
    "    task_id='finalizar',\n",
    "    bash_command='echo \"Pipeline completado!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Definir dependencias\n",
    "tarea_inicio >> tarea_python >> tarea_fin\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Ejemplo de DAG b√°sico:\")\n",
    "print(dag_basico)\n",
    "print(\"\\nüíæ Guarda este c√≥digo en: $AIRFLOW_HOME/dags/mi_primer_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2d8dd",
   "metadata": {},
   "source": [
    "### üìã DAG Definition: Default Args y Configuraci√≥n\n",
    "\n",
    "**Concepto:** Un DAG es un grafo dirigido ac√≠clico que define un pipeline completo como c√≥digo Python.\n",
    "\n",
    "**default_args:** Configuraci√≥n compartida por todas las tasks del DAG:\n",
    "- **owner:** Responsable del DAG (para auditor√≠a)\n",
    "- **retries:** N√∫mero de reintentos autom√°ticos en caso de fallo\n",
    "- **retry_delay:** Tiempo de espera entre reintentos\n",
    "- **start_date:** Primera fecha elegible para ejecuci√≥n\n",
    "- **depends_on_past:** Si True, espera √©xito de ejecuci√≥n anterior\n",
    "\n",
    "**Par√°metros del DAG:**\n",
    "- **dag_id:** Identificador √∫nico del pipeline\n",
    "- **schedule_interval:** Cron expression o timedelta para frecuencia\n",
    "- **catchup:** Si False, no ejecuta periodos pasados al activar DAG\n",
    "\n",
    "**Principio:** DAGs son declarativos e idempotentes - misma ejecuci√≥n produce mismo resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ec61",
   "metadata": {},
   "source": [
    "## üé≠ Operators Principales\n",
    "\n",
    "### 1. PythonOperator - Ejecutar Funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de funciones para PythonOperator\n",
    "def extraer_datos(**context):\n",
    "    \"\"\"\n",
    "    Simula extracci√≥n de datos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    print(\"üîΩ Extrayendo datos...\")\n",
    "    \n",
    "    # Simular datos extra√≠dos\n",
    "    datos = [\n",
    "        {'id': i, 'valor': random.randint(100, 1000), 'fecha': str(datetime.now().date())}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Extra√≠dos {len(datos)} registros\")\n",
    "    \n",
    "    # Pasar datos a la siguiente tarea usando XCom\n",
    "    return datos\n",
    "\n",
    "def transformar_datos(**context):\n",
    "    \"\"\"\n",
    "    Transforma los datos extra√≠dos\n",
    "    \"\"\"\n",
    "    # Obtener datos de la tarea anterior usando XCom\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    \n",
    "    print(\"‚öôÔ∏è Transformando datos...\")\n",
    "    \n",
    "    # Transformaci√≥n simple\n",
    "    datos_transformados = [\n",
    "        {\n",
    "            **d,\n",
    "            'valor_duplicado': d['valor'] * 2,\n",
    "            'categoria': 'Alto' if d['valor'] > 500 else 'Bajo'\n",
    "        }\n",
    "        for d in datos\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Transformados {len(datos_transformados)} registros\")\n",
    "    return datos_transformados\n",
    "\n",
    "def cargar_datos(**context):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='transformar')\n",
    "    \n",
    "    print(\"üì§ Cargando datos...\")\n",
    "    \n",
    "    # Guardar en archivo JSON (simulando carga a BD)\n",
    "    output_path = '../datasets/processed/datos_pipeline.json'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Datos guardados en {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Simular ejecuci√≥n\n",
    "print(\"üß™ Simulando ejecuci√≥n del pipeline:\\n\")\n",
    "\n",
    "context_mock = {\n",
    "    'execution_date': datetime.now(),\n",
    "    'ti': type('obj', (object,), {\n",
    "        'xcom_pull': lambda self, task_ids: [\n",
    "            {'id': 1, 'valor': 750, 'fecha': '2024-01-01'},\n",
    "            {'id': 2, 'valor': 250, 'fecha': '2024-01-01'}\n",
    "        ]\n",
    "    })()\n",
    "}\n",
    "\n",
    "datos_ext = extraer_datos(**context_mock)\n",
    "print(f\"\\nDatos extra√≠dos (muestra): {datos_ext[:2]}\")\n",
    "\n",
    "# Modificar context para transformar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_ext\n",
    "datos_trans = transformar_datos(**context_mock)\n",
    "print(f\"\\nDatos transformados (muestra): {datos_trans[:2]}\")\n",
    "\n",
    "# Cargar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_trans\n",
    "ruta_salida = cargar_datos(**context_mock)\n",
    "print(f\"\\n‚úÖ Pipeline simulado completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c32cb6",
   "metadata": {},
   "source": [
    "### üêç PythonOperator y XCom: Compartir Datos Entre Tasks\n",
    "\n",
    "**Concepto:** PythonOperator ejecuta funciones Python arbitrarias como tasks en el DAG.\n",
    "\n",
    "**XCom (Cross-Communication):**\n",
    "- Mecanismo para pasar datos entre tasks\n",
    "- `return valor`: guarda en XCom autom√°ticamente\n",
    "- `ti.xcom_pull(task_ids='anterior')`: recupera datos de task previa\n",
    "- Almacenado en metadata DB (l√≠mite ~48KB por seguridad)\n",
    "\n",
    "**Context dict:** Inyectado autom√°ticamente en funciones con `**context`:\n",
    "- `ti` (TaskInstance): objeto para XCom y metadata\n",
    "- `execution_date`: fecha l√≥gica de ejecuci√≥n\n",
    "- `dag_run`: informaci√≥n de la ejecuci√≥n del DAG\n",
    "- `params`: par√°metros configurables\n",
    "\n",
    "**Buena pr√°ctica:** Para datasets grandes, usar XCom solo para paths/URLs, almacenar datos en S3/HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee553d4",
   "metadata": {},
   "source": [
    "### 2. BashOperator - Ejecutar Comandos Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b21370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de uso de BashOperator\n",
    "bash_examples = {\n",
    "    'verificar_archivo': {\n",
    "        'task_id': 'verificar_datos',\n",
    "        'bash_command': 'test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"',\n",
    "        'descripcion': 'Verifica que un archivo exista antes de procesarlo'\n",
    "    },\n",
    "    'limpiar_temp': {\n",
    "        'task_id': 'limpiar_archivos',\n",
    "        'bash_command': 'rm -rf /tmp/data_temp/*',\n",
    "        'descripcion': 'Limpia archivos temporales'\n",
    "    },\n",
    "    'ejecutar_script': {\n",
    "        'task_id': 'procesar_sql',\n",
    "        'bash_command': 'psql -U user -d database -f /scripts/query.sql',\n",
    "        'descripcion': 'Ejecuta un script SQL'\n",
    "    },\n",
    "    'mover_archivo': {\n",
    "        'task_id': 'archivar_datos',\n",
    "        'bash_command': 'mv /data/raw/{{ ds }}.csv /data/processed/',\n",
    "        'descripcion': 'Mueve archivo procesado con templating'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Ejemplos de BashOperator:\\n\")\n",
    "for nombre, config in bash_examples.items():\n",
    "    print(f\"üìå {nombre}:\")\n",
    "    print(f\"   Task ID: {config['task_id']}\")\n",
    "    print(f\"   Comando: {config['bash_command']}\")\n",
    "    print(f\"   Uso: {config['descripcion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531badb",
   "metadata": {},
   "source": [
    "### üíª BashOperator: Integraci√≥n con Sistema Operativo\n",
    "\n",
    "**Concepto:** Ejecuta comandos shell/bash directamente desde Airflow, √∫til para scripts legacy o herramientas CLI.\n",
    "\n",
    "**Casos de uso t√≠picos:**\n",
    "- Verificar existencia de archivos antes de procesar\n",
    "- Ejecutar scripts SQL con `psql` o `mysql`\n",
    "- Mover/copiar archivos entre directorios\n",
    "- Limpiar archivos temporales\n",
    "- Ejecutar comandos de cloud CLI (aws s3 sync, gsutil)\n",
    "\n",
    "**Jinja Templating:**\n",
    "- Variables de Airflow disponibles en bash_command\n",
    "- `{{ ds }}`: execution_date como YYYY-MM-DD\n",
    "- `{{ dag_run.conf }}`: par√°metros de ejecuci√≥n manual\n",
    "- `{{ var.value.my_var }}`: Airflow Variables\n",
    "\n",
    "**Ventaja:** Reutilizar scripts bash existentes sin reescribir en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2772ca",
   "metadata": {},
   "source": [
    "## üîó Manejo de Dependencias\n",
    "\n",
    "### Diferentes Formas de Definir Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7724a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias_ejemplos = \"\"\"\n",
    "# Forma 1: Usando >> (recomendado - m√°s legible)\n",
    "tarea_a >> tarea_b >> tarea_c\n",
    "# Significa: A debe completarse antes de B, B antes de C\n",
    "\n",
    "# Forma 2: Usando <<\n",
    "tarea_c << tarea_b << tarea_a\n",
    "# Mismo resultado que Forma 1, pero leyendo al rev√©s\n",
    "\n",
    "# Forma 3: set_downstream() y set_upstream()\n",
    "tarea_a.set_downstream(tarea_b)\n",
    "tarea_b.set_downstream(tarea_c)\n",
    "\n",
    "# Dependencias m√∫ltiples (paralelo)\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
    "# Significa: A, B y C se ejecutan en paralelo despu√©s de inicio\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> tarea_a\n",
    "tarea_inicio >> tarea_b\n",
    "tarea_a >> tarea_c\n",
    "tarea_b >> tarea_c\n",
    "tarea_c >> tarea_fin\n",
    "# Significa: A y B en paralelo, luego C, luego fin\n",
    "\n",
    "# Usando listas para m√∫ltiples dependencias\n",
    "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîó Patrones de Dependencias en Airflow:\")\n",
    "print(dependencias_ejemplos)\n",
    "\n",
    "# Visualizaci√≥n de patrones comunes\n",
    "print(\"\\nüìä Patrones de Flujo Comunes:\\n\")\n",
    "\n",
    "patrones = {\n",
    "    'Lineal': \"\"\"\n",
    "        A ‚Üí B ‚Üí C ‚Üí D\n",
    "    \"\"\",\n",
    "    'Fan-out (Paralelo)': \"\"\"\n",
    "             ‚îå‚Üí B ‚îê\n",
    "        A ‚Üí ‚îú‚Üí C ‚îú‚Üí F\n",
    "             ‚îî‚Üí D ‚îò\n",
    "    \"\"\",\n",
    "    'Fan-in (Convergente)': \"\"\"\n",
    "        A ‚îê\n",
    "        B ‚îú‚Üí D ‚Üí E\n",
    "        C ‚îò\n",
    "    \"\"\",\n",
    "    'Diamante': \"\"\"\n",
    "             ‚îå‚Üí B ‚îê\n",
    "        A ‚Üí ‚îÇ     ‚îú‚Üí D\n",
    "             ‚îî‚Üí C ‚îò\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for nombre, diagrama in patrones.items():\n",
    "    print(f\"üî∏ {nombre}:{diagrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686755b",
   "metadata": {},
   "source": [
    "### üîó Dependency Management: Orquestando el Flujo\n",
    "\n",
    "**Concepto:** Dependencias definen el orden de ejecuci√≥n de tasks en el DAG.\n",
    "\n",
    "**Operadores de dependencia:**\n",
    "- `>>` (bitshift right): `A >> B` = \"B depende de A\"\n",
    "- `<<` (bitshift left): `C << B` = \"C depende de B\"\n",
    "- `[A, B] >> C`: C espera a que A y B completen\n",
    "\n",
    "**Patrones de flujo:**\n",
    "- **Lineal:** A ‚Üí B ‚Üí C (secuencial, sin paralelismo)\n",
    "- **Fan-out:** A ‚Üí [B, C, D] (ejecuci√≥n paralela)\n",
    "- **Fan-in:** [A, B, C] ‚Üí D (convergencia, espera todas)\n",
    "- **Diamante:** A ‚Üí [B, C] ‚Üí D (paralelo + convergencia)\n",
    "\n",
    "**Trigger Rules:**\n",
    "- `all_success` (default): espera que todas las upstream tasks tengan √©xito\n",
    "- `all_failed`: ejecuta solo si todas upstream fallan\n",
    "- `one_success`: ejecuta si al menos una upstream tiene √©xito\n",
    "- `none_failed`: ejecuta si ninguna upstream fall√≥ (permite skipped)\n",
    "\n",
    "**Uso:** Optimizar paralelismo maximiza throughput del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57575e0",
   "metadata": {},
   "source": [
    "## üé® DAG Completo: ETL de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG completo de ejemplo para guardar en dags/\n",
    "dag_ecommerce = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ecommerce_team',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_ecommerce_ventas',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para procesar ventas de e-commerce',\n",
    "    schedule_interval='0 2 * * *',  # Diario a las 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'ventas', 'etl'],\n",
    ")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def verificar_datos_disponibles(**context):\n",
    "    \"\"\"Verifica si hay datos nuevos para procesar\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    archivo = f'/data/raw/ventas_{fecha_archivo}.csv'\n",
    "    \n",
    "    if os.path.exists(archivo):\n",
    "        print(f\"‚úÖ Archivo encontrado: {archivo}\")\n",
    "        return 'extraer_ventas'\n",
    "    else:\n",
    "        print(f\"‚ùå No hay datos para {fecha_archivo}\")\n",
    "        return 'no_data_skip'\n",
    "\n",
    "def extraer_ventas(**context):\n",
    "    \"\"\"Extrae ventas desde archivos CSV\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"üîΩ Extrayendo ventas del {fecha_archivo}\")\n",
    "    \n",
    "    # Simular lectura\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id': range(1, 101),\n",
    "        'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Monitor'],\n",
    "        'cantidad': [1, 2, 1] * 33 + [1],\n",
    "        'precio': [1000, 25, 75] * 33 + [300],\n",
    "        'fecha': execution_date\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Extra√≠das {len(df)} ventas\")\n",
    "    return df.to_json()\n",
    "\n",
    "def transformar_ventas(**context):\n",
    "    \"\"\"Transforma y enriquece datos de ventas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='extraer_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    print(\"‚öôÔ∏è Transformando datos de ventas\")\n",
    "    \n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio']\n",
    "    \n",
    "    # Categorizar por monto\n",
    "    df['categoria_venta'] = df['total'].apply(\n",
    "        lambda x: 'Premium' if x >= 500 else 'Standard'\n",
    "    )\n",
    "    \n",
    "    # Limpiar duplicados\n",
    "    df = df.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    print(f\"‚úÖ Transformados {len(df)} registros\")\n",
    "    return df.to_json()\n",
    "\n",
    "def calcular_metricas(**context):\n",
    "    \"\"\"Calcula m√©tricas agregadas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    metricas = {\n",
    "        'total_ventas': len(df),\n",
    "        'ingreso_total': df['total'].sum(),\n",
    "        'ticket_promedio': df['total'].mean(),\n",
    "        'producto_mas_vendido': df['producto'].mode()[0],\n",
    "        'fecha_proceso': context['execution_date'].isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä M√©tricas calculadas: {metricas}\")\n",
    "    return metricas\n",
    "\n",
    "def cargar_warehouse(**context):\n",
    "    \"\"\"Carga datos en el data warehouse\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    \n",
    "    print(\"üì§ Cargando datos al warehouse...\")\n",
    "    # Aqu√≠ ir√≠a la conexi√≥n real a BD\n",
    "    # connection.execute(\"INSERT INTO ventas ...\")\n",
    "    \n",
    "    print(\"‚úÖ Datos cargados exitosamente\")\n",
    "\n",
    "def enviar_notificacion(**context):\n",
    "    \"\"\"Env√≠a notificaci√≥n de finalizaci√≥n\"\"\"\n",
    "    ti = context['ti']\n",
    "    metricas = ti.xcom_pull(task_ids='calcular_metricas')\n",
    "    \n",
    "    mensaje = f\"\"\"\n",
    "    Pipeline ETL Completado\n",
    "    -----------------------\n",
    "    Fecha: {metricas['fecha_proceso']}\n",
    "    Total ventas: {metricas['total_ventas']}\n",
    "    Ingreso total: ${metricas['ingreso_total']:,.2f}\n",
    "    Ticket promedio: ${metricas['ticket_promedio']:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üìß Enviando notificaci√≥n:\\n{mensaje}\")\n",
    "\n",
    "# --- TAREAS ---\n",
    "\n",
    "inicio = DummyOperator(\n",
    "    task_id='inicio',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "verificar = BranchPythonOperator(\n",
    "    task_id='verificar_datos',\n",
    "    python_callable=verificar_datos_disponibles,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "no_data = DummyOperator(\n",
    "    task_id='no_data_skip',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extraer = PythonOperator(\n",
    "    task_id='extraer_ventas',\n",
    "    python_callable=extraer_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transformar = PythonOperator(\n",
    "    task_id='transformar_ventas',\n",
    "    python_callable=transformar_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "metricas = PythonOperator(\n",
    "    task_id='calcular_metricas',\n",
    "    python_callable=calcular_metricas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cargar = PythonOperator(\n",
    "    task_id='cargar_warehouse',\n",
    "    python_callable=cargar_warehouse,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notificar = PythonOperator(\n",
    "    task_id='enviar_notificacion',\n",
    "    python_callable=enviar_notificacion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fin = DummyOperator(\n",
    "    task_id='fin',\n",
    "    trigger_rule='none_failed_min_one_success',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# --- FLUJO ---\n",
    "inicio >> verificar >> [extraer, no_data]\n",
    "extraer >> transformar >> [metricas, cargar]\n",
    "[metricas, cargar] >> notificar >> fin\n",
    "no_data >> fin\n",
    "'''\n",
    "\n",
    "print(\"üìã DAG Completo de E-commerce:\")\n",
    "print(\"\\nüíæ Guarda este c√≥digo en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\")\n",
    "print(\"\\nüéØ Caracter√≠sticas del DAG:\")\n",
    "print(\"   ‚Ä¢ Branching (decisiones condicionales)\")\n",
    "print(\"   ‚Ä¢ Procesamiento paralelo\")\n",
    "print(\"   ‚Ä¢ XCom para pasar datos entre tareas\")\n",
    "print(\"   ‚Ä¢ Manejo de escenarios sin datos\")\n",
    "print(\"   ‚Ä¢ Notificaciones autom√°ticas\")\n",
    "print(\"   ‚Ä¢ Trigger rules personalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac22e98",
   "metadata": {},
   "source": [
    "### üèóÔ∏è DAG Completo: Patr√≥n ETL Productivo\n",
    "\n",
    "**Concepto:** DAG que implementa pipeline ETL completo con branching, paralelismo y notificaciones.\n",
    "\n",
    "**Componentes avanzados:**\n",
    "- **DummyOperator:** Tareas placeholder para organizaci√≥n l√≥gica\n",
    "- **BranchPythonOperator:** Ejecuci√≥n condicional basada en l√≥gica\n",
    "- **Trigger rules:** Control fino de cu√°ndo ejecutar tasks downstream\n",
    "\n",
    "**Flujo del DAG:**\n",
    "1. **Inicio** ‚Üí Verificar datos disponibles\n",
    "2. **Branch:** Si hay datos ‚Üí Extraer | Si no ‚Üí Skip\n",
    "3. **Paralelo:** Transformar + Calcular m√©tricas\n",
    "4. **Convergencia:** Cargar al warehouse\n",
    "5. **Notificaci√≥n:** Enviar resumen de ejecuci√≥n\n",
    "\n",
    "**Patr√≥n Branch:**\n",
    "```python\n",
    "def verificar(**context):\n",
    "    if condicion:\n",
    "        return 'tarea_si'\n",
    "    return 'tarea_no'\n",
    "```\n",
    "\n",
    "**Producci√≥n:** Este patr√≥n es base para pipelines enterprise con alerting y observabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995b6a7",
   "metadata": {},
   "source": [
    "## üéì Resumen y Mejores Pr√°cticas\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas:\n",
    "\n",
    "1. **Idempotencia**: Las tareas deben producir el mismo resultado si se ejecutan m√∫ltiples veces\n",
    "2. **Atomicidad**: Cada tarea debe ser una unidad indivisible de trabajo\n",
    "3. **No estado compartido**: Usar XCom para comunicaci√≥n entre tareas\n",
    "4. **Logging apropiado**: Registrar informaci√≥n √∫til para debugging\n",
    "5. **Manejo de errores**: Implementar retries y alertas\n",
    "6. **Documentaci√≥n**: Docstrings claros y tags descriptivos\n",
    "7. **Testing**: Probar DAGs antes de producci√≥n\n",
    "\n",
    "### üîú Pr√≥ximos Temas:\n",
    "\n",
    "- Sensors y event-driven workflows\n",
    "- TaskGroups para organizaci√≥n\n",
    "- Dynamic DAGs\n",
    "- Integraci√≥n con Cloud (AWS, GCP, Azure)\n",
    "- Monitoreo y alertas avanzadas\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Has dominado los fundamentos de Apache Airflow!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
