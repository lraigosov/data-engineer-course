{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f82e7",
   "metadata": {},
   "source": [
    "# \u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los conceptos de DAGs (Directed Acyclic Graphs)\n",
    "- [ ] Crear y configurar DAGs de Apache Airflow\n",
    "- [ ] Implementar operators y tasks\n",
    "- [ ] Manejar dependencias entre tareas\n",
    "- [ ] Monitorear y debuggear pipelines\n",
    "- [ ] Implementar buenas pr\u00e1cticas de orquestaci\u00f3n\n",
    "\n",
    "**Duraci\u00f3n Estimada:** 120 minutos  \n",
    "**Nivel de Dificultad:** Intermedio  \n",
    "**Prerrequisitos:** Notebooks nivel Junior completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc384c5",
   "metadata": {},
   "source": [
    "## \u26a0\ufe0f RECORDATORIO IMPORTANTE\n",
    "\n",
    "### \ud83d\udea8 NOTEBOOKS vs PRODUCCI\u00d3N\n",
    "\n",
    "Este curso usa notebooks para **ense\u00f1anza**, pero en tu trabajo real:\n",
    "\n",
    "**\u274c NO uses notebooks para pipelines en producci\u00f3n**\n",
    "\n",
    "**\u2705 USA:**\n",
    "- Scripts Python modulares en `src/`\n",
    "- Airflow DAGs en `dags/` (archivos `.py`, NO `.ipynb`)\n",
    "- Docker para containerizaci\u00f3n\n",
    "- CI/CD para deployment autom\u00e1tico\n",
    "- Tests con pytest\n",
    "\n",
    "**\ud83d\udcd6 Consulta:** `notebooks/\u26a0\ufe0f_IMPORTANTE_LEER_PRIMERO.md` para detalles\n",
    "\n",
    "**En este nivel aprender\u00e1s:** C\u00f3mo convertir c\u00f3digo de notebooks a DAGs de Airflow de producci\u00f3n.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | \u00a9 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720de3",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf \u00bfQu\u00e9 es Apache Airflow?\n",
    "\n",
    "**Apache Airflow** es una plataforma open-source para:\n",
    "\n",
    "\u2705 **Orquestar** pipelines de datos complejos  \n",
    "\u2705 **Programar** ejecuciones autom\u00e1ticas  \n",
    "\u2705 **Monitorear** el estado de los pipelines  \n",
    "\u2705 **Manejar** dependencias entre tareas  \n",
    "\u2705 **Reintenta r** tareas fallidas autom\u00e1ticamente  \n",
    "\n",
    "### \ud83c\udfd7\ufe0f Conceptos Clave:\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Pipeline completo definido como c\u00f3digo\n",
    "- **Operator**: Unidad de trabajo (tarea individual)\n",
    "- **Task**: Instancia de un operator\n",
    "- **Scheduler**: Programa la ejecuci\u00f3n de DAGs\n",
    "- **Executor**: Ejecuta las tareas\n",
    "- **Web UI**: Interfaz para monitoreo\n",
    "\n",
    "### \ud83c\udf1f Ventajas de Airflow:\n",
    "\n",
    "```\n",
    "\u2705 C\u00f3digo Python nativo\n",
    "\u2705 Pipelines como c\u00f3digo (versionables)\n",
    "\u2705 Rico ecosistema de operators\n",
    "\u2705 Escalable y extensible\n",
    "\u2705 Monitoreo visual intuitivo\n",
    "\u2705 Manejo robusto de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778f4f",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Setup y Configuraci\u00f3n\n",
    "\n",
    "### Instalaci\u00f3n de Airflow\n",
    "\n",
    "```bash\n",
    "# Instalar Airflow (ejecutar en terminal)\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar scheduler (en otra terminal)\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar instalaci\u00f3n de Airflow\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import airflow\n",
    "    print(f\"\u2705 Apache Airflow instalado\")\n",
    "    print(f\"   Versi\u00f3n: {airflow.__version__}\")\n",
    "    print(f\"   Ubicaci\u00f3n: {airflow.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"\u274c Apache Airflow no est\u00e1 instalado\")\n",
    "    print(\"\\n\ud83d\udcdd Para instalar, ejecuta en terminal:\")\n",
    "    print(\"   pip install apache-airflow==2.7.0\")\n",
    "\n",
    "# Librer\u00edas que usaremos\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n\u2705 Librer\u00edas auxiliares importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa132a6",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 Verificaci\u00f3n de Entorno Airflow\n",
    "\n",
    "**Concepto:** Airflow requiere instalaci\u00f3n separada y configuraci\u00f3n inicial antes de usar.\n",
    "\n",
    "**Componentes principales:**\n",
    "- **Webserver:** UI en puerto 8080 para monitoreo visual\n",
    "- **Scheduler:** Daemon que programa y dispara DAGs seg\u00fan cron expressions\n",
    "- **Metadata DB:** SQLite (dev) o PostgreSQL (prod) para estado de ejecuciones\n",
    "- **Executor:** LocalExecutor, CeleryExecutor, KubernetesExecutor\n",
    "\n",
    "**Arquitectura:**\n",
    "```\n",
    "DAG Files (.py) \u2192 Scheduler \u2192 Executor \u2192 Tasks \u2192 Metadata DB \u2192 Webserver UI\n",
    "```\n",
    "\n",
    "**Nota:** Este notebook usa ejemplos de c\u00f3digo; en producci\u00f3n, DAGs se colocan en `~/airflow/dags/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8040e5",
   "metadata": {},
   "source": [
    "## \ud83d\udd28 Creando tu Primer DAG\n",
    "\n",
    "### Estructura B\u00e1sica de un DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG b\u00e1sico (guardar como archivo .py en la carpeta dags/)\n",
    "dag_basico = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['tu_email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "}\n",
    "\n",
    "# Definici\u00f3n del DAG\n",
    "dag = DAG(\n",
    "    'mi_primer_dag',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG simple de demostraci\u00f3n',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo', 'tutorial'],\n",
    ")\n",
    "\n",
    "# Funci\u00f3n Python simple\n",
    "def imprimir_fecha(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
    "    return f\"Completado en {execution_date}\"\n",
    "\n",
    "# Tareas del DAG\n",
    "tarea_inicio = BashOperator(\n",
    "    task_id='inicio',\n",
    "    bash_command='echo \"Iniciando pipeline...\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=imprimir_fecha,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_fin = BashOperator(\n",
    "    task_id='finalizar',\n",
    "    bash_command='echo \"Pipeline completado!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Definir dependencias\n",
    "tarea_inicio >> tarea_python >> tarea_fin\n",
    "\"\"\"\n",
    "\n",
    "print(\"\ud83d\udccb Ejemplo de DAG b\u00e1sico:\")\n",
    "print(dag_basico)\n",
    "print(\"\\n\ud83d\udcbe Guarda este c\u00f3digo en: $AIRFLOW_HOME/dags/mi_primer_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2d8dd",
   "metadata": {},
   "source": [
    "### \ud83d\udccb DAG Definition: Default Args y Configuraci\u00f3n\n",
    "\n",
    "**Concepto:** Un DAG es un grafo dirigido ac\u00edclico que define un pipeline completo como c\u00f3digo Python.\n",
    "\n",
    "**default_args:** Configuraci\u00f3n compartida por todas las tasks del DAG:\n",
    "- **owner:** Responsable del DAG (para auditor\u00eda)\n",
    "- **retries:** N\u00famero de reintentos autom\u00e1ticos en caso de fallo\n",
    "- **retry_delay:** Tiempo de espera entre reintentos\n",
    "- **start_date:** Primera fecha elegible para ejecuci\u00f3n\n",
    "- **depends_on_past:** Si True, espera \u00e9xito de ejecuci\u00f3n anterior\n",
    "\n",
    "**Par\u00e1metros del DAG:**\n",
    "- **dag_id:** Identificador \u00fanico del pipeline\n",
    "- **schedule_interval:** Cron expression o timedelta para frecuencia\n",
    "- **catchup:** Si False, no ejecuta periodos pasados al activar DAG\n",
    "\n",
    "**Principio:** DAGs son declarativos e idempotentes - misma ejecuci\u00f3n produce mismo resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ec61",
   "metadata": {},
   "source": [
    "## \ud83c\udfad Operators Principales\n",
    "\n",
    "### 1. PythonOperator - Ejecutar Funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de funciones para PythonOperator\n",
    "def extraer_datos(**context):\n",
    "    \"\"\"\n",
    "    Simula extracci\u00f3n de datos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    print(\"\ud83d\udd3d Extrayendo datos...\")\n",
    "    \n",
    "    # Simular datos extra\u00eddos\n",
    "    datos = [\n",
    "        {'id': i, 'valor': random.randint(100, 1000), 'fecha': str(datetime.now().date())}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\u2705 Extra\u00eddos {len(datos)} registros\")\n",
    "    \n",
    "    # Pasar datos a la siguiente tarea usando XCom\n",
    "    return datos\n",
    "\n",
    "def transformar_datos(**context):\n",
    "    \"\"\"\n",
    "    Transforma los datos extra\u00eddos\n",
    "    \"\"\"\n",
    "    # Obtener datos de la tarea anterior usando XCom\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    \n",
    "    print(\"\u2699\ufe0f Transformando datos...\")\n",
    "    \n",
    "    # Transformaci\u00f3n simple\n",
    "    datos_transformados = [\n",
    "        {\n",
    "            **d,\n",
    "            'valor_duplicado': d['valor'] * 2,\n",
    "            'categoria': 'Alto' if d['valor'] > 500 else 'Bajo'\n",
    "        }\n",
    "        for d in datos\n",
    "    ]\n",
    "    \n",
    "    print(f\"\u2705 Transformados {len(datos_transformados)} registros\")\n",
    "    return datos_transformados\n",
    "\n",
    "def cargar_datos(**context):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='transformar')\n",
    "    \n",
    "    print(\"\ud83d\udce4 Cargando datos...\")\n",
    "    \n",
    "    # Guardar en archivo JSON (simulando carga a BD)\n",
    "    output_path = '../datasets/processed/datos_pipeline.json'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "    \n",
    "    print(f\"\u2705 Datos guardados en {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Simular ejecuci\u00f3n\n",
    "print(\"\ud83e\uddea Simulando ejecuci\u00f3n del pipeline:\\n\")\n",
    "\n",
    "context_mock = {\n",
    "    'execution_date': datetime.now(),\n",
    "    'ti': type('obj', (object,), {\n",
    "        'xcom_pull': lambda self, task_ids: [\n",
    "            {'id': 1, 'valor': 750, 'fecha': '2024-01-01'},\n",
    "            {'id': 2, 'valor': 250, 'fecha': '2024-01-01'}\n",
    "        ]\n",
    "    })()\n",
    "}\n",
    "\n",
    "datos_ext = extraer_datos(**context_mock)\n",
    "print(f\"\\nDatos extra\u00eddos (muestra): {datos_ext[:2]}\")\n",
    "\n",
    "# Modificar context para transformar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_ext\n",
    "datos_trans = transformar_datos(**context_mock)\n",
    "print(f\"\\nDatos transformados (muestra): {datos_trans[:2]}\")\n",
    "\n",
    "# Cargar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_trans\n",
    "ruta_salida = cargar_datos(**context_mock)\n",
    "print(f\"\\n\u2705 Pipeline simulado completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c32cb6",
   "metadata": {},
   "source": [
    "### \ud83d\udc0d PythonOperator y XCom: Compartir Datos Entre Tasks\n",
    "\n",
    "**Concepto:** PythonOperator ejecuta funciones Python arbitrarias como tasks en el DAG.\n",
    "\n",
    "**XCom (Cross-Communication):**\n",
    "- Mecanismo para pasar datos entre tasks\n",
    "- `return valor`: guarda en XCom autom\u00e1ticamente\n",
    "- `ti.xcom_pull(task_ids='anterior')`: recupera datos de task previa\n",
    "- Almacenado en metadata DB (l\u00edmite ~48KB por seguridad)\n",
    "\n",
    "**Context dict:** Inyectado autom\u00e1ticamente en funciones con `**context`:\n",
    "- `ti` (TaskInstance): objeto para XCom y metadata\n",
    "- `execution_date`: fecha l\u00f3gica de ejecuci\u00f3n\n",
    "- `dag_run`: informaci\u00f3n de la ejecuci\u00f3n del DAG\n",
    "- `params`: par\u00e1metros configurables\n",
    "\n",
    "**Buena pr\u00e1ctica:** Para datasets grandes, usar XCom solo para paths/URLs, almacenar datos en S3/HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee553d4",
   "metadata": {},
   "source": [
    "### 2. BashOperator - Ejecutar Comandos Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b21370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de uso de BashOperator\n",
    "bash_examples = {\n",
    "    'verificar_archivo': {\n",
    "        'task_id': 'verificar_datos',\n",
    "        'bash_command': 'test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"',\n",
    "        'descripcion': 'Verifica que un archivo exista antes de procesarlo'\n",
    "    },\n",
    "    'limpiar_temp': {\n",
    "        'task_id': 'limpiar_archivos',\n",
    "        'bash_command': 'rm -rf /tmp/data_temp/*',\n",
    "        'descripcion': 'Limpia archivos temporales'\n",
    "    },\n",
    "    'ejecutar_script': {\n",
    "        'task_id': 'procesar_sql',\n",
    "        'bash_command': 'psql -U user -d database -f /scripts/query.sql',\n",
    "        'descripcion': 'Ejecuta un script SQL'\n",
    "    },\n",
    "    'mover_archivo': {\n",
    "        'task_id': 'archivar_datos',\n",
    "        'bash_command': 'mv /data/raw/{{ ds }}.csv /data/processed/',\n",
    "        'descripcion': 'Mueve archivo procesado con templating'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd27 Ejemplos de BashOperator:\\n\")\n",
    "for nombre, config in bash_examples.items():\n",
    "    print(f\"\ud83d\udccc {nombre}:\")\n",
    "    print(f\"   Task ID: {config['task_id']}\")\n",
    "    print(f\"   Comando: {config['bash_command']}\")\n",
    "    print(f\"   Uso: {config['descripcion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531badb",
   "metadata": {},
   "source": [
    "### \ud83d\udcbb BashOperator: Integraci\u00f3n con Sistema Operativo\n",
    "\n",
    "**Concepto:** Ejecuta comandos shell/bash directamente desde Airflow, \u00fatil para scripts legacy o herramientas CLI.\n",
    "\n",
    "**Casos de uso t\u00edpicos:**\n",
    "- Verificar existencia de archivos antes de procesar\n",
    "- Ejecutar scripts SQL con `psql` o `mysql`\n",
    "- Mover/copiar archivos entre directorios\n",
    "- Limpiar archivos temporales\n",
    "- Ejecutar comandos de cloud CLI (aws s3 sync, gsutil)\n",
    "\n",
    "**Jinja Templating:**\n",
    "- Variables de Airflow disponibles en bash_command\n",
    "- `{{ ds }}`: execution_date como YYYY-MM-DD\n",
    "- `{{ dag_run.conf }}`: par\u00e1metros de ejecuci\u00f3n manual\n",
    "- `{{ var.value.my_var }}`: Airflow Variables\n",
    "\n",
    "**Ventaja:** Reutilizar scripts bash existentes sin reescribir en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2772ca",
   "metadata": {},
   "source": [
    "## \ud83d\udd17 Manejo de Dependencias\n",
    "\n",
    "### Diferentes Formas de Definir Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7724a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias_ejemplos = \"\"\"\n",
    "# Forma 1: Usando >> (recomendado - m\u00e1s legible)\n",
    "tarea_a >> tarea_b >> tarea_c\n",
    "# Significa: A debe completarse antes de B, B antes de C\n",
    "\n",
    "# Forma 2: Usando <<\n",
    "tarea_c << tarea_b << tarea_a\n",
    "# Mismo resultado que Forma 1, pero leyendo al rev\u00e9s\n",
    "\n",
    "# Forma 3: set_downstream() y set_upstream()\n",
    "tarea_a.set_downstream(tarea_b)\n",
    "tarea_b.set_downstream(tarea_c)\n",
    "\n",
    "# Dependencias m\u00faltiples (paralelo)\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
    "# Significa: A, B y C se ejecutan en paralelo despu\u00e9s de inicio\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> tarea_a\n",
    "tarea_inicio >> tarea_b\n",
    "tarea_a >> tarea_c\n",
    "tarea_b >> tarea_c\n",
    "tarea_c >> tarea_fin\n",
    "# Significa: A y B en paralelo, luego C, luego fin\n",
    "\n",
    "# Usando listas para m\u00faltiples dependencias\n",
    "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
    "\"\"\"\n",
    "\n",
    "print(\"\ud83d\udd17 Patrones de Dependencias en Airflow:\")\n",
    "print(dependencias_ejemplos)\n",
    "\n",
    "# Visualizaci\u00f3n de patrones comunes\n",
    "print(\"\\n\ud83d\udcca Patrones de Flujo Comunes:\\n\")\n",
    "\n",
    "patrones = {\n",
    "    'Lineal': \"\"\"\n",
    "        A \u2192 B \u2192 C \u2192 D\n",
    "    \"\"\",\n",
    "    'Fan-out (Paralelo)': \"\"\"\n",
    "             \u250c\u2192 B \u2510\n",
    "        A \u2192 \u251c\u2192 C \u251c\u2192 F\n",
    "             \u2514\u2192 D \u2518\n",
    "    \"\"\",\n",
    "    'Fan-in (Convergente)': \"\"\"\n",
    "        A \u2510\n",
    "        B \u251c\u2192 D \u2192 E\n",
    "        C \u2518\n",
    "    \"\"\",\n",
    "    'Diamante': \"\"\"\n",
    "             \u250c\u2192 B \u2510\n",
    "        A \u2192 \u2502     \u251c\u2192 D\n",
    "             \u2514\u2192 C \u2518\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for nombre, diagrama in patrones.items():\n",
    "    print(f\"\ud83d\udd38 {nombre}:{diagrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686755b",
   "metadata": {},
   "source": [
    "### \ud83d\udd17 Dependency Management: Orquestando el Flujo\n",
    "\n",
    "**Concepto:** Dependencias definen el orden de ejecuci\u00f3n de tasks en el DAG.\n",
    "\n",
    "**Operadores de dependencia:**\n",
    "- `>>` (bitshift right): `A >> B` = \"B depende de A\"\n",
    "- `<<` (bitshift left): `C << B` = \"C depende de B\"\n",
    "- `[A, B] >> C`: C espera a que A y B completen\n",
    "\n",
    "**Patrones de flujo:**\n",
    "- **Lineal:** A \u2192 B \u2192 C (secuencial, sin paralelismo)\n",
    "- **Fan-out:** A \u2192 [B, C, D] (ejecuci\u00f3n paralela)\n",
    "- **Fan-in:** [A, B, C] \u2192 D (convergencia, espera todas)\n",
    "- **Diamante:** A \u2192 [B, C] \u2192 D (paralelo + convergencia)\n",
    "\n",
    "**Trigger Rules:**\n",
    "- `all_success` (default): espera que todas las upstream tasks tengan \u00e9xito\n",
    "- `all_failed`: ejecuta solo si todas upstream fallan\n",
    "- `one_success`: ejecuta si al menos una upstream tiene \u00e9xito\n",
    "- `none_failed`: ejecuta si ninguna upstream fall\u00f3 (permite skipped)\n",
    "\n",
    "**Uso:** Optimizar paralelismo maximiza throughput del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57575e0",
   "metadata": {},
   "source": [
    "## \ud83c\udfa8 DAG Completo: ETL de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG completo de ejemplo para guardar en dags/\n",
    "dag_ecommerce = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ecommerce_team',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_ecommerce_ventas',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para procesar ventas de e-commerce',\n",
    "    schedule_interval='0 2 * * *',  # Diario a las 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'ventas', 'etl'],\n",
    ")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def verificar_datos_disponibles(**context):\n",
    "    \"\"\"Verifica si hay datos nuevos para procesar\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    archivo = f'/data/raw/ventas_{fecha_archivo}.csv'\n",
    "    \n",
    "    if os.path.exists(archivo):\n",
    "        print(f\"\u2705 Archivo encontrado: {archivo}\")\n",
    "        return 'extraer_ventas'\n",
    "    else:\n",
    "        print(f\"\u274c No hay datos para {fecha_archivo}\")\n",
    "        return 'no_data_skip'\n",
    "\n",
    "def extraer_ventas(**context):\n",
    "    \"\"\"Extrae ventas desde archivos CSV\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"\ud83d\udd3d Extrayendo ventas del {fecha_archivo}\")\n",
    "    \n",
    "    # Simular lectura\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id': range(1, 101),\n",
    "        'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Monitor'],\n",
    "        'cantidad': [1, 2, 1] * 33 + [1],\n",
    "        'precio': [1000, 25, 75] * 33 + [300],\n",
    "        'fecha': execution_date\n",
    "    })\n",
    "    \n",
    "    print(f\"\u2705 Extra\u00eddas {len(df)} ventas\")\n",
    "    return df.to_json()\n",
    "\n",
    "def transformar_ventas(**context):\n",
    "    \"\"\"Transforma y enriquece datos de ventas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='extraer_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    print(\"\u2699\ufe0f Transformando datos de ventas\")\n",
    "    \n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio']\n",
    "    \n",
    "    # Categorizar por monto\n",
    "    df['categoria_venta'] = df['total'].apply(\n",
    "        lambda x: 'Premium' if x >= 500 else 'Standard'\n",
    "    )\n",
    "    \n",
    "    # Limpiar duplicados\n",
    "    df = df.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    print(f\"\u2705 Transformados {len(df)} registros\")\n",
    "    return df.to_json()\n",
    "\n",
    "def calcular_metricas(**context):\n",
    "    \"\"\"Calcula m\u00e9tricas agregadas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    metricas = {\n",
    "        'total_ventas': len(df),\n",
    "        'ingreso_total': df['total'].sum(),\n",
    "        'ticket_promedio': df['total'].mean(),\n",
    "        'producto_mas_vendido': df['producto'].mode()[0],\n",
    "        'fecha_proceso': context['execution_date'].isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"\ud83d\udcca M\u00e9tricas calculadas: {metricas}\")\n",
    "    return metricas\n",
    "\n",
    "def cargar_warehouse(**context):\n",
    "    \"\"\"Carga datos en el data warehouse\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    \n",
    "    print(\"\ud83d\udce4 Cargando datos al warehouse...\")\n",
    "    # Aqu\u00ed ir\u00eda la conexi\u00f3n real a BD\n",
    "    # connection.execute(\"INSERT INTO ventas ...\")\n",
    "    \n",
    "    print(\"\u2705 Datos cargados exitosamente\")\n",
    "\n",
    "def enviar_notificacion(**context):\n",
    "    \"\"\"Env\u00eda notificaci\u00f3n de finalizaci\u00f3n\"\"\"\n",
    "    ti = context['ti']\n",
    "    metricas = ti.xcom_pull(task_ids='calcular_metricas')\n",
    "    \n",
    "    mensaje = f\"\"\"\n",
    "    Pipeline ETL Completado\n",
    "    -----------------------\n",
    "    Fecha: {metricas['fecha_proceso']}\n",
    "    Total ventas: {metricas['total_ventas']}\n",
    "    Ingreso total: ${metricas['ingreso_total']:,.2f}\n",
    "    Ticket promedio: ${metricas['ticket_promedio']:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\ud83d\udce7 Enviando notificaci\u00f3n:\\n{mensaje}\")\n",
    "\n",
    "# --- TAREAS ---\n",
    "\n",
    "inicio = DummyOperator(\n",
    "    task_id='inicio',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "verificar = BranchPythonOperator(\n",
    "    task_id='verificar_datos',\n",
    "    python_callable=verificar_datos_disponibles,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "no_data = DummyOperator(\n",
    "    task_id='no_data_skip',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extraer = PythonOperator(\n",
    "    task_id='extraer_ventas',\n",
    "    python_callable=extraer_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transformar = PythonOperator(\n",
    "    task_id='transformar_ventas',\n",
    "    python_callable=transformar_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "metricas = PythonOperator(\n",
    "    task_id='calcular_metricas',\n",
    "    python_callable=calcular_metricas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cargar = PythonOperator(\n",
    "    task_id='cargar_warehouse',\n",
    "    python_callable=cargar_warehouse,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notificar = PythonOperator(\n",
    "    task_id='enviar_notificacion',\n",
    "    python_callable=enviar_notificacion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fin = DummyOperator(\n",
    "    task_id='fin',\n",
    "    trigger_rule='none_failed_min_one_success',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# --- FLUJO ---\n",
    "inicio >> verificar >> [extraer, no_data]\n",
    "extraer >> transformar >> [metricas, cargar]\n",
    "[metricas, cargar] >> notificar >> fin\n",
    "no_data >> fin\n",
    "'''\n",
    "\n",
    "print(\"\ud83d\udccb DAG Completo de E-commerce:\")\n",
    "print(\"\\n\ud83d\udcbe Guarda este c\u00f3digo en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\")\n",
    "print(\"\\n\ud83c\udfaf Caracter\u00edsticas del DAG:\")\n",
    "print(\"   \u2022 Branching (decisiones condicionales)\")\n",
    "print(\"   \u2022 Procesamiento paralelo\")\n",
    "print(\"   \u2022 XCom para pasar datos entre tareas\")\n",
    "print(\"   \u2022 Manejo de escenarios sin datos\")\n",
    "print(\"   \u2022 Notificaciones autom\u00e1ticas\")\n",
    "print(\"   \u2022 Trigger rules personalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac22e98",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f DAG Completo: Patr\u00f3n ETL Productivo\n",
    "\n",
    "**Concepto:** DAG que implementa pipeline ETL completo con branching, paralelismo y notificaciones.\n",
    "\n",
    "**Componentes avanzados:**\n",
    "- **DummyOperator:** Tareas placeholder para organizaci\u00f3n l\u00f3gica\n",
    "- **BranchPythonOperator:** Ejecuci\u00f3n condicional basada en l\u00f3gica\n",
    "- **Trigger rules:** Control fino de cu\u00e1ndo ejecutar tasks downstream\n",
    "\n",
    "**Flujo del DAG:**\n",
    "1. **Inicio** \u2192 Verificar datos disponibles\n",
    "2. **Branch:** Si hay datos \u2192 Extraer | Si no \u2192 Skip\n",
    "3. **Paralelo:** Transformar + Calcular m\u00e9tricas\n",
    "4. **Convergencia:** Cargar al warehouse\n",
    "5. **Notificaci\u00f3n:** Enviar resumen de ejecuci\u00f3n\n",
    "\n",
    "**Patr\u00f3n Branch:**\n",
    "```python\n",
    "def verificar(**context):\n",
    "    if condicion:\n",
    "        return 'tarea_si'\n",
    "    return 'tarea_no'\n",
    "```\n",
    "\n",
    "**Producci\u00f3n:** Este patr\u00f3n es base para pipelines enterprise con alerting y observabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995b6a7",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Resumen y Mejores Pr\u00e1cticas\n",
    "\n",
    "### \u2705 Mejores Pr\u00e1cticas:\n",
    "\n",
    "1. **Idempotencia**: Las tareas deben producir el mismo resultado si se ejecutan m\u00faltiples veces\n",
    "2. **Atomicidad**: Cada tarea debe ser una unidad indivisible de trabajo\n",
    "3. **No estado compartido**: Usar XCom para comunicaci\u00f3n entre tareas\n",
    "4. **Logging apropiado**: Registrar informaci\u00f3n \u00fatil para debugging\n",
    "5. **Manejo de errores**: Implementar retries y alertas\n",
    "6. **Documentaci\u00f3n**: Docstrings claros y tags descriptivos\n",
    "7. **Testing**: Probar DAGs antes de producci\u00f3n\n",
    "\n",
    "### \ud83d\udd1c Pr\u00f3ximos Temas:\n",
    "\n",
    "- Sensors y event-driven workflows\n",
    "- TaskGroups para organizaci\u00f3n\n",
    "- Dynamic DAGs\n",
    "- Integraci\u00f3n con Cloud (AWS, GCP, Azure)\n",
    "- Monitoreo y alertas avanzadas\n",
    "\n",
    "---\n",
    "\n",
    "**\u00a1Has dominado los fundamentos de Apache Airflow!** \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\u2190 README del Curso](../../README.md)\n",
    "\n",
    "**Siguiente \u2192:** [Streaming con Apache Kafka: Fundamentos \u2192](02_streaming_kafka.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
