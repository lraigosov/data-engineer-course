{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f82e7",
   "metadata": {},
   "source": [
    "# âš¡ Mid - 01. OrquestaciÃ³n de Pipelines con Apache Airflow\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "- [ ] Comprender los conceptos de DAGs (Directed Acyclic Graphs)\n",
    "- [ ] Crear y configurar DAGs de Apache Airflow\n",
    "- [ ] Implementar operators y tasks\n",
    "- [ ] Manejar dependencias entre tareas\n",
    "- [ ] Monitorear y debuggear pipelines\n",
    "- [ ] Implementar buenas prÃ¡cticas de orquestaciÃ³n\n",
    "\n",
    "**DuraciÃ³n Estimada:** 120 minutos  \n",
    "**Nivel de Dificultad:** Intermedio  \n",
    "**Prerrequisitos:** Notebooks nivel Junior completos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc384c5",
   "metadata": {},
   "source": [
    "## âš ï¸ RECORDATORIO IMPORTANTE\n",
    "\n",
    "### ğŸš¨ NOTEBOOKS vs PRODUCCIÃ“N\n",
    "\n",
    "Este curso usa notebooks para **enseÃ±anza**, pero en tu trabajo real:\n",
    "\n",
    "**âŒ NO uses notebooks para pipelines en producciÃ³n**\n",
    "\n",
    "**âœ… USA:**\n",
    "- Scripts Python modulares en `src/`\n",
    "- Airflow DAGs en `dags/` (archivos `.py`, NO `.ipynb`)\n",
    "- Docker para containerizaciÃ³n\n",
    "- CI/CD para deployment automÃ¡tico\n",
    "- Tests con pytest\n",
    "\n",
    "**ğŸ“– Consulta:** `notebooks/âš ï¸_IMPORTANTE_LEER_PRIMERO.md` para detalles\n",
    "\n",
    "**En este nivel aprenderÃ¡s:** CÃ³mo convertir cÃ³digo de notebooks a DAGs de Airflow de producciÃ³n.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | Â© 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720de3",
   "metadata": {},
   "source": [
    "## ğŸ¯ Â¿QuÃ© es Apache Airflow?\n",
    "\n",
    "**Apache Airflow** es una plataforma open-source para:\n",
    "\n",
    "âœ… **Orquestar** pipelines de datos complejos  \n",
    "âœ… **Programar** ejecuciones automÃ¡ticas  \n",
    "âœ… **Monitorear** el estado de los pipelines  \n",
    "âœ… **Manejar** dependencias entre tareas  \n",
    "âœ… **Reintenta r** tareas fallidas automÃ¡ticamente  \n",
    "\n",
    "### ğŸ—ï¸ Conceptos Clave:\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Pipeline completo definido como cÃ³digo\n",
    "- **Operator**: Unidad de trabajo (tarea individual)\n",
    "- **Task**: Instancia de un operator\n",
    "- **Scheduler**: Programa la ejecuciÃ³n de DAGs\n",
    "- **Executor**: Ejecuta las tareas\n",
    "- **Web UI**: Interfaz para monitoreo\n",
    "\n",
    "### ğŸŒŸ Ventajas de Airflow:\n",
    "\n",
    "```\n",
    "âœ… CÃ³digo Python nativo\n",
    "âœ… Pipelines como cÃ³digo (versionables)\n",
    "âœ… Rico ecosistema de operators\n",
    "âœ… Escalable y extensible\n",
    "âœ… Monitoreo visual intuitivo\n",
    "âœ… Manejo robusto de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840d149",
   "metadata": {},
   "source": [
    "## ğŸ’¡ AclaraciÃ³n Importante sobre la InstalaciÃ³n\n",
    "\n",
    "### Â¿Necesito instalar Apache Airflow para este notebook?\n",
    "\n",
    "**Respuesta corta: NO es obligatorio, pero SÃ es altamente recomendado.**\n",
    "\n",
    "#### ğŸ“š Para seguir este notebook (modo aprendizaje):\n",
    "\n",
    "**âœ… NO necesitas instalar Airflow porque:**\n",
    "- Este notebook usa **ejemplos de cÃ³digo como strings** para enseÃ±ar sintaxis\n",
    "- Los ejemplos estÃ¡n diseÃ±ados para ser **leÃ­dos y comprendidos**, no ejecutados\n",
    "- Puedes aprender los conceptos, patrones y mejores prÃ¡cticas sin instalaciÃ³n\n",
    "- Las celdas de cÃ³digo funcionan sin Airflow instalado (solo muestran ejemplos)\n",
    "\n",
    "**âš¡ Sin embargo, SÃ se recomienda instalarlo si:**\n",
    "- Quieres **practicar activamente** creando tus propios DAGs\n",
    "- Deseas **ver la Web UI** de Airflow y el monitoreo visual\n",
    "- Necesitas **ejecutar DAGs reales** y experimentar con el scheduler\n",
    "- Planeas usar Airflow en tu trabajo actual o futuro\n",
    "\n",
    "#### ğŸš€ Para usar Airflow en producciÃ³n (modo profesional):\n",
    "\n",
    "**âœ… DEBES instalarlo y configurarlo porque:**\n",
    "- Airflow es una **plataforma completa** que requiere servicios corriendo (webserver, scheduler, executor)\n",
    "- Los DAGs deben estar en archivos `.py` en la carpeta `$AIRFLOW_HOME/dags/`\n",
    "- Necesitas base de datos (SQLite para dev, PostgreSQL para prod)\n",
    "- Requiere configuraciÃ³n de `airflow.cfg` para conexiones, variables, etc.\n",
    "\n",
    "#### ğŸ¯ RecomendaciÃ³n para este curso:\n",
    "\n",
    "```\n",
    "1ï¸âƒ£ Primera pasada: Lee el notebook SIN instalar Airflow\n",
    "   â†’ EnfÃ³cate en entender conceptos, sintaxis y patrones\n",
    "\n",
    "2ï¸âƒ£ Segunda pasada: Instala Airflow y practica\n",
    "   â†’ Crea los DAGs en archivos .py\n",
    "   â†’ Ejecuta en la UI y observa el comportamiento\n",
    "   â†’ Experimenta con modificaciones\n",
    "\n",
    "3ï¸âƒ£ Proyecto final: Implementa un pipeline real\n",
    "   â†’ Usa los patrones aprendidos\n",
    "   â†’ Sigue mejores prÃ¡cticas de producciÃ³n\n",
    "```\n",
    "\n",
    "#### ğŸ“‹ InstalaciÃ³n opcional (si decides instalar):\n",
    "\n",
    "```bash\n",
    "# Crear entorno virtual (recomendado)\n",
    "python -m venv venv_airflow\n",
    "source venv_airflow/bin/activate  # En Windows: venv_airflow\\Scripts\\activate\n",
    "\n",
    "# Instalar Airflow\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar\n",
    "export AIRFLOW_HOME=~/airflow  # En Windows: set AIRFLOW_HOME=%USERPROFILE%\\airflow\n",
    "airflow db init\n",
    "airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com\n",
    "```\n",
    "\n",
    "**ğŸ”— DocumentaciÃ³n oficial:** https://airflow.apache.org/docs/apache-airflow/stable/start.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778f4f",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup y ConfiguraciÃ³n\n",
    "\n",
    "### InstalaciÃ³n de Airflow\n",
    "\n",
    "```bash\n",
    "# Instalar Airflow (ejecutar en terminal)\n",
    "pip install apache-airflow==2.7.0\n",
    "\n",
    "# Inicializar base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar scheduler (en otra terminal)\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9640ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Apache Airflow no estÃ¡ instalado\n",
      "\n",
      "ğŸ“ Para instalar, ejecuta en terminal:\n",
      "   pip install apache-airflow==2.7.0\n",
      "\n",
      "âœ… LibrerÃ­as auxiliares importadas\n"
     ]
    }
   ],
   "source": [
    "# Verificar instalaciÃ³n de Airflow\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import airflow\n",
    "    print(f\"âœ… Apache Airflow instalado\")\n",
    "    print(f\"   VersiÃ³n: {airflow.__version__}\")\n",
    "    print(f\"   UbicaciÃ³n: {airflow.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Apache Airflow no estÃ¡ instalado\")\n",
    "    print(\"\\nğŸ“ Para instalar, ejecuta en terminal:\")\n",
    "    print(\"   pip install apache-airflow==2.7.0\")\n",
    "\n",
    "# LibrerÃ­as que usaremos\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\nâœ… LibrerÃ­as auxiliares importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa132a6",
   "metadata": {},
   "source": [
    "### ğŸ”§ VerificaciÃ³n de Entorno Airflow\n",
    "\n",
    "**Concepto:** Airflow requiere instalaciÃ³n separada y configuraciÃ³n inicial antes de usar.\n",
    "\n",
    "**Componentes principales:**\n",
    "- **Webserver:** UI en puerto 8080 para monitoreo visual\n",
    "- **Scheduler:** Daemon que programa y dispara DAGs segÃºn cron expressions\n",
    "- **Metadata DB:** SQLite (dev) o PostgreSQL (prod) para estado de ejecuciones\n",
    "- **Executor:** LocalExecutor, CeleryExecutor, KubernetesExecutor\n",
    "\n",
    "**Arquitectura:**\n",
    "```\n",
    "DAG Files (.py) â†’ Scheduler â†’ Executor â†’ Tasks â†’ Metadata DB â†’ Webserver UI\n",
    "```\n",
    "\n",
    "**Nota:** Este notebook usa ejemplos de cÃ³digo; en producciÃ³n, DAGs se colocan en `~/airflow/dags/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8040e5",
   "metadata": {},
   "source": [
    "## ğŸ”¨ Creando tu Primer DAG\n",
    "\n",
    "### Estructura BÃ¡sica de un DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1167c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Ejemplo de DAG bÃ¡sico:\n",
      "\n",
      "from airflow import DAG\n",
      "from airflow.operators.python import PythonOperator\n",
      "from airflow.operators.bash import BashOperator\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Argumentos por defecto para el DAG\n",
      "default_args = {\n",
      "    'owner': 'data_engineer',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['tu_email@example.com'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'start_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "# DefiniciÃ³n del DAG\n",
      "dag = DAG(\n",
      "    'mi_primer_dag',\n",
      "    default_args=default_args,\n",
      "    description='Un DAG simple de demostraciÃ³n',\n",
      "    schedule_interval=timedelta(days=1),\n",
      "    catchup=False,\n",
      "    tags=['ejemplo', 'tutorial'],\n",
      ")\n",
      "\n",
      "# FunciÃ³n Python simple\n",
      "def imprimir_fecha(**context):\n",
      "    execution_date = context['execution_date']\n",
      "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
      "    return f\"Completado en {execution_date}\"\n",
      "\n",
      "# Tareas del DAG\n",
      "tarea_inicio = BashOperator(\n",
      "    task_id='inicio',\n",
      "    bash_command='echo \"Iniciando pipeline...\"',\n",
      "    dag=dag,\n",
      ")\n",
      "\n",
      "tarea_python = PythonOperator(\n",
      "    task_id='procesar_datos',\n",
      "    python_callable=imprimir_fecha,\n",
      "    provide_context=True,\n",
      "    dag=dag,\n",
      ")\n",
      "\n",
      "tarea_fin = BashOperator(\n",
      "    task_id='finalizar',\n",
      "    bash_command='echo \"Pipeline completado!\"',\n",
      "    dag=dag,\n",
      ")\n",
      "\n",
      "# Definir dependencias\n",
      "tarea_inicio >> tarea_python >> tarea_fin\n",
      "\n",
      "\n",
      "ğŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/mi_primer_dag.py\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de DAG bÃ¡sico (guardar como archivo .py en la carpeta dags/)\n",
    "dag_basico = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['tu_email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "}\n",
    "\n",
    "# DefiniciÃ³n del DAG\n",
    "dag = DAG(\n",
    "    'mi_primer_dag',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG simple de demostraciÃ³n',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo', 'tutorial'],\n",
    ")\n",
    "\n",
    "# FunciÃ³n Python simple\n",
    "def imprimir_fecha(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Ejecutando DAG en: {execution_date}\")\n",
    "    return f\"Completado en {execution_date}\"\n",
    "\n",
    "# Tareas del DAG\n",
    "tarea_inicio = BashOperator(\n",
    "    task_id='inicio',\n",
    "    bash_command='echo \"Iniciando pipeline...\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=imprimir_fecha,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_fin = BashOperator(\n",
    "    task_id='finalizar',\n",
    "    bash_command='echo \"Pipeline completado!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Definir dependencias\n",
    "tarea_inicio >> tarea_python >> tarea_fin\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“‹ Ejemplo de DAG bÃ¡sico:\")\n",
    "print(dag_basico)\n",
    "print(\"\\nğŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/mi_primer_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2d8dd",
   "metadata": {},
   "source": [
    "### ğŸ“‹ DAG Definition: Default Args y ConfiguraciÃ³n\n",
    "\n",
    "**Concepto:** Un DAG es un grafo dirigido acÃ­clico que define un pipeline completo como cÃ³digo Python.\n",
    "\n",
    "**default_args:** ConfiguraciÃ³n compartida por todas las tasks del DAG:\n",
    "- **owner:** Responsable del DAG (para auditorÃ­a)\n",
    "- **retries:** NÃºmero de reintentos automÃ¡ticos en caso de fallo\n",
    "- **retry_delay:** Tiempo de espera entre reintentos\n",
    "- **start_date:** Primera fecha elegible para ejecuciÃ³n\n",
    "- **depends_on_past:** Si True, espera Ã©xito de ejecuciÃ³n anterior\n",
    "\n",
    "**ParÃ¡metros del DAG:**\n",
    "- **dag_id:** Identificador Ãºnico del pipeline\n",
    "- **schedule_interval:** Cron expression o timedelta para frecuencia\n",
    "- **catchup:** Si False, no ejecuta periodos pasados al activar DAG\n",
    "\n",
    "**Principio:** DAGs son declarativos e idempotentes - misma ejecuciÃ³n produce mismo resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ec61",
   "metadata": {},
   "source": [
    "## ğŸ­ Operators Principales\n",
    "\n",
    "### 1. PythonOperator - Ejecutar Funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6223a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Simulando ejecuciÃ³n del pipeline:\n",
      "\n",
      "ğŸ”½ Extrayendo datos...\n",
      "âœ… ExtraÃ­dos 10 registros\n",
      "\n",
      "Datos extraÃ­dos (muestra): [{'id': 1, 'valor': 386, 'fecha': '2025-12-07'}, {'id': 2, 'valor': 195, 'fecha': '2025-12-07'}]\n",
      "âš™ï¸ Transformando datos...\n",
      "âœ… Transformados 10 registros\n",
      "\n",
      "Datos transformados (muestra): [{'id': 1, 'valor': 386, 'fecha': '2025-12-07', 'valor_duplicado': 772, 'categoria': 'Bajo'}, {'id': 2, 'valor': 195, 'fecha': '2025-12-07', 'valor_duplicado': 390, 'categoria': 'Bajo'}]\n",
      "ğŸ“¤ Cargando datos...\n",
      "âœ… Datos guardados en ../datasets/processed/datos_pipeline.json\n",
      "\n",
      "âœ… Pipeline simulado completado\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de funciones para PythonOperator\n",
    "def extraer_datos(**context):\n",
    "    \"\"\"\n",
    "    Simula extracciÃ³n de datos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    print(\"ğŸ”½ Extrayendo datos...\")\n",
    "    \n",
    "    # Simular datos extraÃ­dos\n",
    "    datos = [\n",
    "        {'id': i, 'valor': random.randint(100, 1000), 'fecha': str(datetime.now().date())}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ… ExtraÃ­dos {len(datos)} registros\")\n",
    "    \n",
    "    # Pasar datos a la siguiente tarea usando XCom\n",
    "    return datos\n",
    "\n",
    "def transformar_datos(**context):\n",
    "    \"\"\"\n",
    "    Transforma los datos extraÃ­dos\n",
    "    \"\"\"\n",
    "    # Obtener datos de la tarea anterior usando XCom\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    \n",
    "    print(\"âš™ï¸ Transformando datos...\")\n",
    "    \n",
    "    # TransformaciÃ³n simple\n",
    "    datos_transformados = [\n",
    "        {\n",
    "            **d,\n",
    "            'valor_duplicado': d['valor'] * 2,\n",
    "            'categoria': 'Alto' if d['valor'] > 500 else 'Bajo'\n",
    "        }\n",
    "        for d in datos\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ… Transformados {len(datos_transformados)} registros\")\n",
    "    return datos_transformados\n",
    "\n",
    "def cargar_datos(**context):\n",
    "    \"\"\"\n",
    "    Carga los datos transformados\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    datos = ti.xcom_pull(task_ids='transformar')\n",
    "    \n",
    "    print(\"ğŸ“¤ Cargando datos...\")\n",
    "    \n",
    "    # Guardar en archivo JSON (simulando carga a BD)\n",
    "    output_path = '../datasets/processed/datos_pipeline.json'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Datos guardados en {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Simular ejecuciÃ³n\n",
    "print(\"ğŸ§ª Simulando ejecuciÃ³n del pipeline:\\n\")\n",
    "\n",
    "context_mock = {\n",
    "    'execution_date': datetime.now(),\n",
    "    'ti': type('obj', (object,), {\n",
    "        'xcom_pull': lambda self, task_ids: [\n",
    "            {'id': 1, 'valor': 750, 'fecha': '2024-01-01'},\n",
    "            {'id': 2, 'valor': 250, 'fecha': '2024-01-01'}\n",
    "        ]\n",
    "    })()\n",
    "}\n",
    "\n",
    "datos_ext = extraer_datos(**context_mock)\n",
    "print(f\"\\nDatos extraÃ­dos (muestra): {datos_ext[:2]}\")\n",
    "\n",
    "# Modificar context para transformar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_ext\n",
    "datos_trans = transformar_datos(**context_mock)\n",
    "print(f\"\\nDatos transformados (muestra): {datos_trans[:2]}\")\n",
    "\n",
    "# Cargar\n",
    "context_mock['ti'].xcom_pull = lambda task_ids: datos_trans\n",
    "ruta_salida = cargar_datos(**context_mock)\n",
    "print(f\"\\nâœ… Pipeline simulado completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c32cb6",
   "metadata": {},
   "source": [
    "### ğŸ PythonOperator y XCom: Compartir Datos Entre Tasks\n",
    "\n",
    "**Concepto:** PythonOperator ejecuta funciones Python arbitrarias como tasks en el DAG.\n",
    "\n",
    "**XCom (Cross-Communication):**\n",
    "- Mecanismo para pasar datos entre tasks\n",
    "- `return valor`: guarda en XCom automÃ¡ticamente\n",
    "- `ti.xcom_pull(task_ids='anterior')`: recupera datos de task previa\n",
    "- Almacenado en metadata DB (lÃ­mite ~48KB por seguridad)\n",
    "\n",
    "**Context dict:** Inyectado automÃ¡ticamente en funciones con `**context`:\n",
    "- `ti` (TaskInstance): objeto para XCom y metadata\n",
    "- `execution_date`: fecha lÃ³gica de ejecuciÃ³n\n",
    "- `dag_run`: informaciÃ³n de la ejecuciÃ³n del DAG\n",
    "- `params`: parÃ¡metros configurables\n",
    "\n",
    "**Buena prÃ¡ctica:** Para datasets grandes, usar XCom solo para paths/URLs, almacenar datos en S3/HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee553d4",
   "metadata": {},
   "source": [
    "### 2. BashOperator - Ejecutar Comandos Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b21370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Ejemplos de BashOperator:\n",
      "\n",
      "ğŸ“Œ verificar_archivo:\n",
      "   Task ID: verificar_datos\n",
      "   Comando: test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"\n",
      "   Uso: Verifica que un archivo exista antes de procesarlo\n",
      "\n",
      "ğŸ“Œ limpiar_temp:\n",
      "   Task ID: limpiar_archivos\n",
      "   Comando: rm -rf /tmp/data_temp/*\n",
      "   Uso: Limpia archivos temporales\n",
      "\n",
      "ğŸ“Œ ejecutar_script:\n",
      "   Task ID: procesar_sql\n",
      "   Comando: psql -U user -d database -f /scripts/query.sql\n",
      "   Uso: Ejecuta un script SQL\n",
      "\n",
      "ğŸ“Œ mover_archivo:\n",
      "   Task ID: archivar_datos\n",
      "   Comando: mv /data/raw/{{ ds }}.csv /data/processed/\n",
      "   Uso: Mueve archivo procesado con templating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de uso de BashOperator\n",
    "bash_examples = {\n",
    "    'verificar_archivo': {\n",
    "        'task_id': 'verificar_datos',\n",
    "        'bash_command': 'test -f /path/to/data.csv && echo \"Archivo existe\" || echo \"Archivo no encontrado\"',\n",
    "        'descripcion': 'Verifica que un archivo exista antes de procesarlo'\n",
    "    },\n",
    "    'limpiar_temp': {\n",
    "        'task_id': 'limpiar_archivos',\n",
    "        'bash_command': 'rm -rf /tmp/data_temp/*',\n",
    "        'descripcion': 'Limpia archivos temporales'\n",
    "    },\n",
    "    'ejecutar_script': {\n",
    "        'task_id': 'procesar_sql',\n",
    "        'bash_command': 'psql -U user -d database -f /scripts/query.sql',\n",
    "        'descripcion': 'Ejecuta un script SQL'\n",
    "    },\n",
    "    'mover_archivo': {\n",
    "        'task_id': 'archivar_datos',\n",
    "        'bash_command': 'mv /data/raw/{{ ds }}.csv /data/processed/',\n",
    "        'descripcion': 'Mueve archivo procesado con templating'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ Ejemplos de BashOperator:\\n\")\n",
    "for nombre, config in bash_examples.items():\n",
    "    print(f\"ğŸ“Œ {nombre}:\")\n",
    "    print(f\"   Task ID: {config['task_id']}\")\n",
    "    print(f\"   Comando: {config['bash_command']}\")\n",
    "    print(f\"   Uso: {config['descripcion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531badb",
   "metadata": {},
   "source": [
    "### ğŸ’» BashOperator: IntegraciÃ³n con Sistema Operativo\n",
    "\n",
    "**Concepto:** Ejecuta comandos shell/bash directamente desde Airflow, Ãºtil para scripts legacy o herramientas CLI.\n",
    "\n",
    "**Casos de uso tÃ­picos:**\n",
    "- Verificar existencia de archivos antes de procesar\n",
    "- Ejecutar scripts SQL con `psql` o `mysql`\n",
    "- Mover/copiar archivos entre directorios\n",
    "- Limpiar archivos temporales\n",
    "- Ejecutar comandos de cloud CLI (aws s3 sync, gsutil)\n",
    "\n",
    "**Jinja Templating:**\n",
    "- Variables de Airflow disponibles en bash_command\n",
    "- `{{ ds }}`: execution_date como YYYY-MM-DD\n",
    "- `{{ dag_run.conf }}`: parÃ¡metros de ejecuciÃ³n manual\n",
    "- `{{ var.value.my_var }}`: Airflow Variables\n",
    "\n",
    "**Ventaja:** Reutilizar scripts bash existentes sin reescribir en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2772ca",
   "metadata": {},
   "source": [
    "## ğŸ”— Manejo de Dependencias\n",
    "\n",
    "### Diferentes Formas de Definir Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7724a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Patrones de Dependencias en Airflow:\n",
      "\n",
      "# Forma 1: Usando >> (recomendado - mÃ¡s legible)\n",
      "tarea_a >> tarea_b >> tarea_c\n",
      "# Significa: A debe completarse antes de B, B antes de C\n",
      "\n",
      "# Forma 2: Usando <<\n",
      "tarea_c << tarea_b << tarea_a\n",
      "# Mismo resultado que Forma 1, pero leyendo al revÃ©s\n",
      "\n",
      "# Forma 3: set_downstream() y set_upstream()\n",
      "tarea_a.set_downstream(tarea_b)\n",
      "tarea_b.set_downstream(tarea_c)\n",
      "\n",
      "# Dependencias mÃºltiples (paralelo)\n",
      "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
      "# Significa: A, B y C se ejecutan en paralelo despuÃ©s de inicio\n",
      "\n",
      "# Dependencias complejas\n",
      "tarea_inicio >> tarea_a\n",
      "tarea_inicio >> tarea_b\n",
      "tarea_a >> tarea_c\n",
      "tarea_b >> tarea_c\n",
      "tarea_c >> tarea_fin\n",
      "# Significa: A y B en paralelo, luego C, luego fin\n",
      "\n",
      "# Usando listas para mÃºltiples dependencias\n",
      "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
      "\n",
      "\n",
      "ğŸ“Š Patrones de Flujo Comunes:\n",
      "\n",
      "ğŸ”¸ Lineal:\n",
      "        A â†’ B â†’ C â†’ D\n",
      "    \n",
      "ğŸ”¸ Fan-out (Paralelo):\n",
      "             â”Œâ†’ B â”\n",
      "        A â†’ â”œâ†’ C â”œâ†’ F\n",
      "             â””â†’ D â”˜\n",
      "    \n",
      "ğŸ”¸ Fan-in (Convergente):\n",
      "        A â”\n",
      "        B â”œâ†’ D â†’ E\n",
      "        C â”˜\n",
      "    \n",
      "ğŸ”¸ Diamante:\n",
      "             â”Œâ†’ B â”\n",
      "        A â†’ â”‚     â”œâ†’ D\n",
      "             â””â†’ C â”˜\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dependencias_ejemplos = \"\"\"\n",
    "# Forma 1: Usando >> (recomendado - mÃ¡s legible)\n",
    "tarea_a >> tarea_b >> tarea_c\n",
    "# Significa: A debe completarse antes de B, B antes de C\n",
    "\n",
    "# Forma 2: Usando <<\n",
    "tarea_c << tarea_b << tarea_a\n",
    "# Mismo resultado que Forma 1, pero leyendo al revÃ©s\n",
    "\n",
    "# Forma 3: set_downstream() y set_upstream()\n",
    "tarea_a.set_downstream(tarea_b)\n",
    "tarea_b.set_downstream(tarea_c)\n",
    "\n",
    "# Dependencias mÃºltiples (paralelo)\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_fin\n",
    "# Significa: A, B y C se ejecutan en paralelo despuÃ©s de inicio\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> tarea_a\n",
    "tarea_inicio >> tarea_b\n",
    "tarea_a >> tarea_c\n",
    "tarea_b >> tarea_c\n",
    "tarea_c >> tarea_fin\n",
    "# Significa: A y B en paralelo, luego C, luego fin\n",
    "\n",
    "# Usando listas para mÃºltiples dependencias\n",
    "[tarea_a, tarea_b] >> tarea_c >> [tarea_d, tarea_e]\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ”— Patrones de Dependencias en Airflow:\")\n",
    "print(dependencias_ejemplos)\n",
    "\n",
    "# VisualizaciÃ³n de patrones comunes\n",
    "print(\"\\nğŸ“Š Patrones de Flujo Comunes:\\n\")\n",
    "\n",
    "patrones = {\n",
    "    'Lineal': \"\"\"\n",
    "        A â†’ B â†’ C â†’ D\n",
    "    \"\"\",\n",
    "    'Fan-out (Paralelo)': \"\"\"\n",
    "             â”Œâ†’ B â”\n",
    "        A â†’ â”œâ†’ C â”œâ†’ F\n",
    "             â””â†’ D â”˜\n",
    "    \"\"\",\n",
    "    'Fan-in (Convergente)': \"\"\"\n",
    "        A â”\n",
    "        B â”œâ†’ D â†’ E\n",
    "        C â”˜\n",
    "    \"\"\",\n",
    "    'Diamante': \"\"\"\n",
    "             â”Œâ†’ B â”\n",
    "        A â†’ â”‚     â”œâ†’ D\n",
    "             â””â†’ C â”˜\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for nombre, diagrama in patrones.items():\n",
    "    print(f\"ğŸ”¸ {nombre}:{diagrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686755b",
   "metadata": {},
   "source": [
    "### ğŸ”— Dependency Management: Orquestando el Flujo\n",
    "\n",
    "**Concepto:** Dependencias definen el orden de ejecuciÃ³n de tasks en el DAG.\n",
    "\n",
    "**Operadores de dependencia:**\n",
    "- `>>` (bitshift right): `A >> B` = \"B depende de A\"\n",
    "- `<<` (bitshift left): `C << B` = \"C depende de B\"\n",
    "- `[A, B] >> C`: C espera a que A y B completen\n",
    "\n",
    "**Patrones de flujo:**\n",
    "- **Lineal:** A â†’ B â†’ C (secuencial, sin paralelismo)\n",
    "- **Fan-out:** A â†’ [B, C, D] (ejecuciÃ³n paralela)\n",
    "- **Fan-in:** [A, B, C] â†’ D (convergencia, espera todas)\n",
    "- **Diamante:** A â†’ [B, C] â†’ D (paralelo + convergencia)\n",
    "\n",
    "**Trigger Rules:**\n",
    "- `all_success` (default): espera que todas las upstream tasks tengan Ã©xito\n",
    "- `all_failed`: ejecuta solo si todas upstream fallan\n",
    "- `one_success`: ejecuta si al menos una upstream tiene Ã©xito\n",
    "- `none_failed`: ejecuta si ninguna upstream fallÃ³ (permite skipped)\n",
    "\n",
    "**Uso:** Optimizar paralelismo maximiza throughput del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57575e0",
   "metadata": {},
   "source": [
    "## ğŸ¨ DAG Completo: ETL de E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46dd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ DAG Completo de E-commerce:\n",
      "\n",
      "ğŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\n",
      "\n",
      "ğŸ¯ CaracterÃ­sticas del DAG:\n",
      "   â€¢ Branching (decisiones condicionales)\n",
      "   â€¢ Procesamiento paralelo\n",
      "   â€¢ XCom para pasar datos entre tareas\n",
      "   â€¢ Manejo de escenarios sin datos\n",
      "   â€¢ Notificaciones automÃ¡ticas\n",
      "   â€¢ Trigger rules personalizados\n"
     ]
    }
   ],
   "source": [
    "# DAG completo de ejemplo para guardar en dags/\n",
    "dag_ecommerce = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ecommerce_team',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_ecommerce_ventas',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para procesar ventas de e-commerce',\n",
    "    schedule_interval='0 2 * * *',  # Diario a las 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'ventas', 'etl'],\n",
    ")\n",
    "\n",
    "# --- FUNCIONES ---\n",
    "\n",
    "def verificar_datos_disponibles(**context):\n",
    "    \"\"\"Verifica si hay datos nuevos para procesar\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    archivo = f'/data/raw/ventas_{fecha_archivo}.csv'\n",
    "    \n",
    "    if os.path.exists(archivo):\n",
    "        print(f\"âœ… Archivo encontrado: {archivo}\")\n",
    "        return 'extraer_ventas'\n",
    "    else:\n",
    "        print(f\"âŒ No hay datos para {fecha_archivo}\")\n",
    "        return 'no_data_skip'\n",
    "\n",
    "def extraer_ventas(**context):\n",
    "    \"\"\"Extrae ventas desde archivos CSV\"\"\"\n",
    "    execution_date = context['execution_date']\n",
    "    fecha_archivo = execution_date.strftime('%Y%m%d')\n",
    "    \n",
    "    print(f\"ğŸ”½ Extrayendo ventas del {fecha_archivo}\")\n",
    "    \n",
    "    # Simular lectura\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id': range(1, 101),\n",
    "        'producto': ['Laptop', 'Mouse', 'Teclado'] * 33 + ['Monitor'],\n",
    "        'cantidad': [1, 2, 1] * 33 + [1],\n",
    "        'precio': [1000, 25, 75] * 33 + [300],\n",
    "        'fecha': execution_date\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… ExtraÃ­das {len(df)} ventas\")\n",
    "    return df.to_json()\n",
    "\n",
    "def transformar_ventas(**context):\n",
    "    \"\"\"Transforma y enriquece datos de ventas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='extraer_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    print(\"âš™ï¸ Transformando datos de ventas\")\n",
    "    \n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio']\n",
    "    \n",
    "    # Categorizar por monto\n",
    "    df['categoria_venta'] = df['total'].apply(\n",
    "        lambda x: 'Premium' if x >= 500 else 'Standard'\n",
    "    )\n",
    "    \n",
    "    # Limpiar duplicados\n",
    "    df = df.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    print(f\"âœ… Transformados {len(df)} registros\")\n",
    "    return df.to_json()\n",
    "\n",
    "def calcular_metricas(**context):\n",
    "    \"\"\"Calcula mÃ©tricas agregadas\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    df = pd.read_json(ventas_json)\n",
    "    \n",
    "    metricas = {\n",
    "        'total_ventas': len(df),\n",
    "        'ingreso_total': df['total'].sum(),\n",
    "        'ticket_promedio': df['total'].mean(),\n",
    "        'producto_mas_vendido': df['producto'].mode()[0],\n",
    "        'fecha_proceso': context['execution_date'].isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š MÃ©tricas calculadas: {metricas}\")\n",
    "    return metricas\n",
    "\n",
    "def cargar_warehouse(**context):\n",
    "    \"\"\"Carga datos en el data warehouse\"\"\"\n",
    "    ti = context['ti']\n",
    "    ventas_json = ti.xcom_pull(task_ids='transformar_ventas')\n",
    "    \n",
    "    print(\"ğŸ“¤ Cargando datos al warehouse...\")\n",
    "    # AquÃ­ irÃ­a la conexiÃ³n real a BD\n",
    "    # connection.execute(\"INSERT INTO ventas ...\")\n",
    "    \n",
    "    print(\"âœ… Datos cargados exitosamente\")\n",
    "\n",
    "def enviar_notificacion(**context):\n",
    "    \"\"\"EnvÃ­a notificaciÃ³n de finalizaciÃ³n\"\"\"\n",
    "    ti = context['ti']\n",
    "    metricas = ti.xcom_pull(task_ids='calcular_metricas')\n",
    "    \n",
    "    mensaje = f\"\"\"\n",
    "    Pipeline ETL Completado\n",
    "    -----------------------\n",
    "    Fecha: {metricas['fecha_proceso']}\n",
    "    Total ventas: {metricas['total_ventas']}\n",
    "    Ingreso total: ${metricas['ingreso_total']:,.2f}\n",
    "    Ticket promedio: ${metricas['ticket_promedio']:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“§ Enviando notificaciÃ³n:\\n{mensaje}\")\n",
    "\n",
    "# --- TAREAS ---\n",
    "\n",
    "inicio = EmptyOperator(\n",
    "    task_id='inicio',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "verificar = BranchPythonOperator(\n",
    "    task_id='verificar_datos',\n",
    "    python_callable=verificar_datos_disponibles,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "no_data = EmptyOperator(\n",
    "    task_id='no_data_skip',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extraer = PythonOperator(\n",
    "    task_id='extraer_ventas',\n",
    "    python_callable=extraer_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transformar = PythonOperator(\n",
    "    task_id='transformar_ventas',\n",
    "    python_callable=transformar_ventas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "metricas = PythonOperator(\n",
    "    task_id='calcular_metricas',\n",
    "    python_callable=calcular_metricas,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cargar = PythonOperator(\n",
    "    task_id='cargar_warehouse',\n",
    "    python_callable=cargar_warehouse,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notificar = PythonOperator(\n",
    "    task_id='enviar_notificacion',\n",
    "    python_callable=enviar_notificacion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fin = EmptyOperator(\n",
    "    task_id='fin',\n",
    "    trigger_rule='none_failed_min_one_success',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# --- FLUJO ---\n",
    "inicio >> verificar >> [extraer, no_data]\n",
    "extraer >> transformar >> [metricas, cargar]\n",
    "[metricas, cargar] >> notificar >> fin\n",
    "no_data >> fin\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“‹ DAG Completo de E-commerce:\")\n",
    "print(\"\\nğŸ’¾ Guarda este cÃ³digo en: $AIRFLOW_HOME/dags/etl_ecommerce_ventas.py\")\n",
    "print(\"\\nğŸ¯ CaracterÃ­sticas del DAG:\")\n",
    "print(\"   â€¢ Branching (decisiones condicionales)\")\n",
    "print(\"   â€¢ Procesamiento paralelo\")\n",
    "print(\"   â€¢ XCom para pasar datos entre tareas\")\n",
    "print(\"   â€¢ Manejo de escenarios sin datos\")\n",
    "print(\"   â€¢ Notificaciones automÃ¡ticas\")\n",
    "print(\"   â€¢ Trigger rules personalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac22e98",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ DAG Completo: PatrÃ³n ETL Productivo\n",
    "\n",
    "**Concepto:** DAG que implementa pipeline ETL completo con branching, paralelismo y notificaciones.\n",
    "\n",
    "**Componentes avanzados:**\n",
    "- **EmptyOperator:** Tareas placeholder para organizaciÃ³n lÃ³gica (antes DummyOperator en versiones antiguas)\n",
    "- **BranchPythonOperator:** EjecuciÃ³n condicional basada en lÃ³gica\n",
    "- **Trigger rules:** Control fino de cuÃ¡ndo ejecutar tasks downstream\n",
    "\n",
    "**Flujo del DAG:**\n",
    "1. **Inicio** â†’ Verificar datos disponibles\n",
    "2. **Branch:** Si hay datos â†’ Extraer | Si no â†’ Skip\n",
    "3. **Paralelo:** Transformar + Calcular mÃ©tricas\n",
    "4. **Convergencia:** Cargar al warehouse\n",
    "5. **NotificaciÃ³n:** Enviar resumen de ejecuciÃ³n\n",
    "\n",
    "**PatrÃ³n Branch:**\n",
    "```python\n",
    "def verificar(**context):\n",
    "    if condicion:\n",
    "        return 'tarea_si'\n",
    "    return 'tarea_no'\n",
    "```\n",
    "\n",
    "**ProducciÃ³n:** Este patrÃ³n es base para pipelines enterprise con alerting y observabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995b6a7",
   "metadata": {},
   "source": [
    "## ğŸ“ Resumen y Mejores PrÃ¡cticas\n",
    "\n",
    "### âœ… Mejores PrÃ¡cticas:\n",
    "\n",
    "1. **Idempotencia**: Las tareas deben producir el mismo resultado si se ejecutan mÃºltiples veces\n",
    "2. **Atomicidad**: Cada tarea debe ser una unidad indivisible de trabajo\n",
    "3. **No estado compartido**: Usar XCom para comunicaciÃ³n entre tareas\n",
    "4. **Logging apropiado**: Registrar informaciÃ³n Ãºtil para debugging\n",
    "5. **Manejo de errores**: Implementar retries y alertas\n",
    "6. **DocumentaciÃ³n**: Docstrings claros y tags descriptivos\n",
    "7. **Testing**: Probar DAGs antes de producciÃ³n\n",
    "\n",
    "### ğŸ”œ PrÃ³ximos Temas:\n",
    "\n",
    "- Sensors y event-driven workflows\n",
    "- TaskGroups para organizaciÃ³n\n",
    "- Dynamic DAGs\n",
    "- IntegraciÃ³n con Cloud (AWS, GCP, Azure)\n",
    "- Monitoreo y alertas avanzadas\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Has dominado los fundamentos de Apache Airflow!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [â† README del Curso](../../README.md)\n",
    "\n",
    "**Siguiente â†’:** [Streaming con Apache Kafka: Fundamentos â†’](02_streaming_kafka.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Mid:**\n",
    "- [âš¡ Mid - 01. OrquestaciÃ³n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [â˜ï¸ AWS para IngenierÃ­a de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [â˜ï¸ GCP para IngenierÃ­a de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [â˜ï¸ Azure para IngenierÃ­a de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [ğŸ—„ï¸ Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [â™»ï¸ DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [ğŸŒ Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [ğŸ§© OptimizaciÃ³n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [ğŸš€ Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [ğŸ§ª Proyecto Integrador Mid 1: API â†’ DB â†’ Parquet con OrquestaciÃ³n](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸ”„ Proyecto Integrador Mid 2: Kafka â†’ Streaming â†’ Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
