{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ec1db3",
   "metadata": {},
   "source": [
    "# \ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos\n",
    "\n",
    "Objetivo: entender y aplicar t\u00e9cnicas de optimizaci\u00f3n de consultas (\u00edndices, estad\u00edsticas, JOIN strategies) y estrategias de particionado (bases relacionales y data lakes) para mejorar desempe\u00f1o y costos.\n",
    "\n",
    "- Duraci\u00f3n: 90\u2013120 min\n",
    "- Dificultad: Media\n",
    "- Prerrequisitos: SQL intermedio, nociones de modelado y almacenamiento columnar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f7060",
   "metadata": {},
   "source": [
    "### \ud83d\ude80 **Query Optimization: Del Problema al Performance**\n",
    "\n",
    "**\u00bfPor qu\u00e9 Optimizar?**\n",
    "- Query lento = Costos altos (Athena cobra por TB escaneado)\n",
    "- Timeouts en dashboards \u2192 Mala UX\n",
    "- Lock contention \u2192 Bloqueos en OLTP\n",
    "- Escalabilidad limitada\n",
    "\n",
    "**Jerarqu\u00eda de Optimizaci\u00f3n:**\n",
    "\n",
    "```\n",
    "1. Schema Design (40% impacto)\n",
    "   \u251c\u2500 Normalizaci\u00f3n vs Desnormalizaci\u00f3n\n",
    "   \u251c\u2500 Tipos de datos correctos (INT vs VARCHAR)\n",
    "   \u2514\u2500 Particionamiento estrat\u00e9gico\n",
    "\n",
    "2. \u00cdndices (30% impacto)\n",
    "   \u251c\u2500 B-Tree para lookups\n",
    "   \u251c\u2500 Columnar para analytics\n",
    "   \u2514\u2500 Covering indexes\n",
    "\n",
    "3. Query Rewrite (20% impacto)\n",
    "   \u251c\u2500 Predicados pushdown\n",
    "   \u251c\u2500 JOIN order optimization\n",
    "   \u2514\u2500 Eliminar subqueries correlacionadas\n",
    "\n",
    "4. Hardware/Config (10% impacto)\n",
    "   \u251c\u2500 Memoria (work_mem, buffer pool)\n",
    "   \u251c\u2500 Parallel workers\n",
    "   \u2514\u2500 Connection pooling\n",
    "```\n",
    "\n",
    "**Metodolog\u00eda de Optimizaci\u00f3n:**\n",
    "\n",
    "1. **Measure**: EXPLAIN ANALYZE (costos reales)\n",
    "2. **Identify**: Seq Scan largo, Hash Join costoso\n",
    "3. **Hypothesize**: \"Falta \u00edndice en columna X\"\n",
    "4. **Test**: CREATE INDEX y re-medir\n",
    "5. **Validate**: A/B test en producci\u00f3n\n",
    "\n",
    "**Costos en Query Plans:**\n",
    "```sql\n",
    "EXPLAIN (ANALYZE, BUFFERS) SELECT ...\n",
    "\n",
    "Seq Scan on ventas  (cost=0.00..180.00 rows=10000 width=42) (actual time=0.05..5.32 rows=9856)\n",
    "                     ^^^^^^^^^^^^^^^^    ^^^^^^^^^^^^^        ^^^^^^^^^^^^^^^^^^^^\n",
    "                     Estimado            Estimado             REAL (con ANALYZE)\n",
    "```\n",
    "\n",
    "**M\u00e9tricas Clave:**\n",
    "- **Rows**: Estimado vs real (si difieren mucho \u2192 ANALYZE tabla)\n",
    "- **Cost**: Unidades arbitrarias (comparar entre planes)\n",
    "- **Time**: ms reales (actual time)\n",
    "- **Buffers**: Bloques le\u00eddos (shared hit=cache, read=disk)\n",
    "\n",
    "**Red Flags:**\n",
    "- \ud83d\udea9 Seq Scan en tablas >1M filas\n",
    "- \ud83d\udea9 Nested Loop con outer side > 10K filas\n",
    "- \ud83d\udea9 Sort en memoria \u2192 Disk (work_mem insuficiente)\n",
    "- \ud83d\udea9 Rows estimado != rows actual (10x+ diferencia)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058c3f1",
   "metadata": {},
   "source": [
    "## 1. EXPLAIN / EXPLAIN ANALYZE (demo con SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fcfa1",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d **EXPLAIN ANALYZE: Decodificando Query Plans**\n",
    "\n",
    "**Tipos de Scans:**\n",
    "\n",
    "1. **Sequential Scan (Seq Scan):**\n",
    "   ```\n",
    "   Seq Scan on ventas  (cost=0.00..180.00 rows=10000)\n",
    "   ```\n",
    "   - Lee tabla completa, fila por fila\n",
    "   - **Cu\u00e1ndo es OK**: Tablas peque\u00f1as (<100K filas), queries que leen >25% de la tabla\n",
    "   - **Malo**: Tablas grandes con filtros selectivos\n",
    "\n",
    "2. **Index Scan:**\n",
    "   ```\n",
    "   Index Scan using idx_cliente on ventas  (cost=0.42..8.44 rows=1)\n",
    "   ```\n",
    "   - Lee \u00edndice \u2192 busca filas en tabla\n",
    "   - **Cu\u00e1ndo es OK**: Filtros selectivos (<5% filas)\n",
    "   - **Malo**: Range queries grandes (muchos random I/O)\n",
    "\n",
    "3. **Index Only Scan:**\n",
    "   ```\n",
    "   Index Only Scan using idx_covering on ventas  (cost=0.42..4.44 rows=1)\n",
    "   ```\n",
    "   - Lee **solo** \u00edndice (covering index con INCLUDE)\n",
    "   - **Mejor performance**: Sin acceso a tabla\n",
    "   - Requiere: VACUUM para visibility map\n",
    "\n",
    "4. **Bitmap Index Scan:**\n",
    "   ```\n",
    "   Bitmap Heap Scan on ventas\n",
    "     Recheck Cond: (cliente_id = 10)\n",
    "     -> Bitmap Index Scan on idx_cliente\n",
    "   ```\n",
    "   - Crea bitmap de bloques a leer \u2192 lee secuencialmente\n",
    "   - **Cu\u00e1ndo**: Multiple index merge, rangos medianos\n",
    "\n",
    "**Tipos de JOINs:**\n",
    "\n",
    "1. **Nested Loop (NL):**\n",
    "   ```\n",
    "   Nested Loop  (cost=0.00..100.00 rows=10)\n",
    "     -> Seq Scan on peque\u00f1a  (rows=10)\n",
    "     -> Index Scan on grande  (rows=1)\n",
    "   ```\n",
    "   - Para cada fila de outer \u2192 busca en inner\n",
    "   - **\u00d3ptimo**: Outer peque\u00f1o (<100 filas) + \u00edndice en inner\n",
    "   - **Malo**: Outer grande sin \u00edndice en inner\n",
    "\n",
    "2. **Hash Join:**\n",
    "   ```\n",
    "   Hash Join  (cost=120.00..500.00 rows=1000)\n",
    "     Hash Cond: (t1.id = t2.id)\n",
    "     -> Seq Scan on t1\n",
    "     -> Hash\n",
    "       -> Seq Scan on t2\n",
    "   ```\n",
    "   - Construye hash table de tabla peque\u00f1a en memoria\n",
    "   - **\u00d3ptimo**: Joins equi (=) con tablas medianas\n",
    "   - **Malo**: Hash table no cabe en work_mem \u2192 spill to disk\n",
    "\n",
    "3. **Merge Join:**\n",
    "   ```\n",
    "   Merge Join  (cost=200.00..400.00 rows=1000)\n",
    "     Merge Cond: (t1.id = t2.id)\n",
    "     -> Sort ...\n",
    "     -> Sort ...\n",
    "   ```\n",
    "   - Requiere tablas ordenadas\n",
    "   - **\u00d3ptimo**: Ambas tablas con \u00edndice en join key\n",
    "   - **Uso**: Joins en data warehouse con datos pre-ordenados\n",
    "\n",
    "**Interpreting Costs:**\n",
    "```sql\n",
    "Nested Loop  (cost=0.42..123.45 rows=100)\n",
    "              ^^^^^^^^^^^^^^^^\n",
    "              0.42 = startup cost (setup)\n",
    "              123.45 = total cost (incluye startup)\n",
    "```\n",
    "\n",
    "**PostgreSQL Specific:**\n",
    "```sql\n",
    "EXPLAIN (ANALYZE, BUFFERS, VERBOSE) SELECT ...\n",
    "\n",
    "Buffers: shared hit=8 read=120 dirtied=0 written=0\n",
    "         ^^^^^^^^^^    ^^^^^^^^\n",
    "         Cache hits    Disk reads (malo si alto)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df160075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "CREATE TABLE ventas (\n",
    " id INTEGER PRIMARY KEY, fecha TEXT, cliente_id INT, producto_id INT, cantidad INT, precio REAL, total REAL\n",
    ");\n",
    "CREATE INDEX idx_ventas_cliente_fecha ON ventas (cliente_id, fecha);\n",
    "''')\n",
    "import random, datetime as dt\n",
    "rows = [ (i, str(dt.date(2025,10,(i%28)+1)), random.randint(1,200), random.randint(100,110), random.randint(1,5), round(random.uniform(5,200),2)) for i in range(1,5001) ]\n",
    "rows = [ (*r, r[4]*r[5]) for r in rows ]\n",
    "cur.executemany('INSERT INTO ventas (id, fecha, cliente_id, producto_id, cantidad, precio, total) VALUES (?,?,?,?,?,?,?)', rows)\n",
    "conn.commit()\n",
    "list(cur.execute('EXPLAIN QUERY PLAN SELECT * FROM ventas WHERE cliente_id=10 AND fecha>=\"2025-10-10\"').fetchall())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a8519",
   "metadata": {},
   "source": [
    "### 1.1 Impacto de \u00edndices compuestos y selectividad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689afd0",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **\u00cdndices Compuestos: Selectividad y Orden**\n",
    "\n",
    "**Selectividad:**  \n",
    "Proporci\u00f3n de filas \u00fanicas en una columna.\n",
    "\n",
    "```\n",
    "Alta selectividad:   email (99% \u00fanico)      \u2192 Excelente para \u00edndice\n",
    "Media selectividad:  ciudad (50 valores)    \u2192 \u00datil combinado\n",
    "Baja selectividad:   g\u00e9nero (M/F/O)         \u2192 In\u00fatil solo\n",
    "```\n",
    "\n",
    "**Orden en \u00cdndices Compuestos:**\n",
    "\n",
    "```sql\n",
    "CREATE INDEX idx_bad ON ventas (genero, email);\n",
    "CREATE INDEX idx_good ON ventas (email, genero);\n",
    "\n",
    "-- Query: WHERE email = 'user@example.com' AND genero = 'F'\n",
    "\n",
    "idx_bad:  Escanea todos M/F/O, luego filtra email \u274c\n",
    "idx_good: Busca email directo (selectivo), luego filtra genero \u2705\n",
    "```\n",
    "\n",
    "**Regla: Columnas M\u00c1S selectivas primero**\n",
    "\n",
    "**Left-to-Right Usage:**\n",
    "```sql\n",
    "CREATE INDEX idx ON ventas (cliente_id, fecha, producto_id);\n",
    "\n",
    "\u2705 WHERE cliente_id = 10\n",
    "\u2705 WHERE cliente_id = 10 AND fecha > '2025-01-01'\n",
    "\u2705 WHERE cliente_id = 10 AND fecha > '2025-01-01' AND producto_id = 5\n",
    "\u274c WHERE fecha > '2025-01-01'  -- No usa \u00edndice (falta cliente_id)\n",
    "\u274c WHERE producto_id = 5        -- No usa \u00edndice (faltan primeras columnas)\n",
    "```\n",
    "\n",
    "**Funciones Rompen \u00cdndices:**\n",
    "```sql\n",
    "-- \u274c No usa \u00edndice\n",
    "SELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n",
    "\n",
    "-- \u2705 \u00cdndice funcional (PostgreSQL)\n",
    "CREATE INDEX idx_email_lower ON users (LOWER(email));\n",
    "\n",
    "-- \u2705 O mejor: normalizar dato al insertar\n",
    "INSERT INTO users (email) VALUES (LOWER('User@Example.com'));\n",
    "```\n",
    "\n",
    "**LIKE Patterns:**\n",
    "```sql\n",
    "-- \u2705 Usa \u00edndice B-Tree\n",
    "WHERE email LIKE 'user@%'\n",
    "\n",
    "-- \u274c No usa \u00edndice B-Tree (leading wildcard)\n",
    "WHERE email LIKE '%@example.com'\n",
    "\n",
    "-- \u2705 Para leading wildcard: trigram index (PostgreSQL)\n",
    "CREATE INDEX idx_email_trgm ON users USING gin (email gin_trgm_ops);\n",
    "```\n",
    "\n",
    "**\u00cdndice Parcial (Filtered Index):**\n",
    "```sql\n",
    "-- Solo indexar registros activos (ahorra espacio)\n",
    "CREATE INDEX idx_active_users ON users (email) WHERE status = 'active';\n",
    "\n",
    "-- Query DEBE incluir predicado\n",
    "SELECT * FROM users WHERE email = 'x' AND status = 'active';  -- Usa \u00edndice\n",
    "SELECT * FROM users WHERE email = 'x';                         -- No usa\n",
    "```\n",
    "\n",
    "**Maintenance:**\n",
    "```sql\n",
    "-- PostgreSQL: Bloat en \u00edndices tras muchos UPDATE/DELETE\n",
    "REINDEX INDEX idx_ventas_cliente_fecha;\n",
    "\n",
    "-- An\u00e1lisis de bloat\n",
    "SELECT schemaname, tablename, \n",
    "       pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "FROM pg_tables\n",
    "WHERE schemaname NOT IN ('pg_catalog', 'information_schema')\n",
    "ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n",
    "```\n",
    "\n",
    "**Trade-offs:**\n",
    "- \u2705 \u00cdndices: Aceleran SELECT/WHERE/JOIN\n",
    "- \u274c \u00cdndices: Ralentizan INSERT/UPDATE/DELETE (mantenimiento)\n",
    "- \u274c \u00cdndices: Ocupan espacio en disco\n",
    "- **Regla**: Max 5-7 \u00edndices por tabla OLTP, m\u00e1s en OLAP\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12352c7",
   "metadata": {},
   "source": [
    "- Use \u00edndices compuestos en el orden de los predicados m\u00e1s selectivos.\n",
    "- Evite funciones sobre columnas indexadas en filtros (rompe el \u00edndice).\n",
    "- Mantenga estad\u00edsticas actualizadas (ANALYZE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d90c6",
   "metadata": {},
   "source": [
    "## 2. Particionado en PostgreSQL (DDL de referencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a936273",
   "metadata": {},
   "source": [
    "### \ud83d\uddc2\ufe0f **Particionado en PostgreSQL: Estrategias**\n",
    "\n",
    "**\u00bfPor qu\u00e9 Particionar?**\n",
    "\n",
    "1. **Query Performance**: Partition pruning reduce datos escaneados\n",
    "2. **Maintenance**: DROP partici\u00f3n antigua vs DELETE millones de filas\n",
    "3. **Parallelism**: Queries sobre particiones diferentes en paralelo\n",
    "4. **Archival**: Mover particiones antiguas a tablespaces baratos\n",
    "\n",
    "**Tipos de Particionado:**\n",
    "\n",
    "1. **RANGE (M\u00e1s com\u00fan):**\n",
    "   ```sql\n",
    "   CREATE TABLE events (\n",
    "     id BIGSERIAL,\n",
    "     timestamp TIMESTAMPTZ NOT NULL,\n",
    "     data JSONB\n",
    "   ) PARTITION BY RANGE (timestamp);\n",
    "   \n",
    "   -- Particiones mensuales\n",
    "   CREATE TABLE events_2025_10 PARTITION OF events \n",
    "     FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');\n",
    "   \n",
    "   CREATE TABLE events_2025_11 PARTITION OF events \n",
    "     FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');\n",
    "   ```\n",
    "   \n",
    "   **Uso**: Fechas, IDs secuenciales\n",
    "\n",
    "2. **LIST:**\n",
    "   ```sql\n",
    "   CREATE TABLE orders (\n",
    "     id BIGSERIAL,\n",
    "     country VARCHAR(2),\n",
    "     amount NUMERIC\n",
    "   ) PARTITION BY LIST (country);\n",
    "   \n",
    "   CREATE TABLE orders_us PARTITION OF orders FOR VALUES IN ('US');\n",
    "   CREATE TABLE orders_eu PARTITION OF orders FOR VALUES IN ('DE', 'FR', 'ES');\n",
    "   ```\n",
    "   \n",
    "   **Uso**: Categor\u00edas discretas (pa\u00eds, regi\u00f3n, status)\n",
    "\n",
    "3. **HASH:**\n",
    "   ```sql\n",
    "   CREATE TABLE users (\n",
    "     id BIGSERIAL,\n",
    "     email VARCHAR\n",
    "   ) PARTITION BY HASH (id);\n",
    "   \n",
    "   CREATE TABLE users_0 PARTITION OF users FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n",
    "   CREATE TABLE users_1 PARTITION OF users FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n",
    "   ```\n",
    "   \n",
    "   **Uso**: Distribuci\u00f3n uniforme cuando no hay columna obvia\n",
    "\n",
    "**Partition Pruning:**\n",
    "```sql\n",
    "-- Query escanea SOLO partici\u00f3n 2025_10\n",
    "EXPLAIN SELECT * FROM events WHERE timestamp >= '2025-10-15';\n",
    "\n",
    "Result:\n",
    "  -> Seq Scan on events_2025_10  -- \u2705 Solo 1 partici\u00f3n\n",
    "     Filter: (timestamp >= '2025-10-15')\n",
    "```\n",
    "\n",
    "**Sin pruning:**\n",
    "```sql\n",
    "-- \u274c Escanea TODAS las particiones (funci\u00f3n rompe pruning)\n",
    "SELECT * FROM events WHERE EXTRACT(YEAR FROM timestamp) = 2025;\n",
    "\n",
    "-- \u2705 Reescribir para habilitar pruning\n",
    "SELECT * FROM events WHERE timestamp >= '2025-01-01' AND timestamp < '2026-01-01';\n",
    "```\n",
    "\n",
    "**\u00cdndices en Particiones:**\n",
    "```sql\n",
    "-- Opci\u00f3n 1: \u00cdndice por partici\u00f3n (manual)\n",
    "CREATE INDEX ON events_2025_10 (timestamp);\n",
    "CREATE INDEX ON events_2025_11 (timestamp);\n",
    "\n",
    "-- Opci\u00f3n 2: \u00cdndice global (PostgreSQL 11+, autom\u00e1tico)\n",
    "CREATE INDEX ON events (timestamp);\n",
    "-- Crea \u00edndices en todas las particiones actuales y futuras\n",
    "```\n",
    "\n",
    "**Maintenance:**\n",
    "```sql\n",
    "-- Agregar partici\u00f3n nueva (automatizar con cron)\n",
    "CREATE TABLE events_2025_12 PARTITION OF events \n",
    "  FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');\n",
    "\n",
    "-- Archivar partici\u00f3n antigua (inst\u00e1ntaneo)\n",
    "ALTER TABLE events DETACH PARTITION events_2024_01;\n",
    "-- Ahora events_2024_01 es tabla independiente\n",
    "ALTER TABLE events_2024_01 RENAME TO events_2024_01_archive;\n",
    "\n",
    "-- Eliminar partici\u00f3n (cuidado!)\n",
    "DROP TABLE events_2024_01;  -- \u26a0\ufe0f Datos perdidos\n",
    "```\n",
    "\n",
    "**Limitaciones:**\n",
    "- Primary key DEBE incluir partition key\n",
    "- Foreign keys complicados (solo dentro de partici\u00f3n)\n",
    "- UNIQUE constraints deben incluir partition key\n",
    "\n",
    "**Partici\u00f3n Autom\u00e1tica (pg_partman):**\n",
    "```sql\n",
    "-- Extensi\u00f3n para crear/eliminar particiones autom\u00e1ticamente\n",
    "CREATE EXTENSION pg_partman;\n",
    "\n",
    "SELECT create_parent('public.events', 'timestamp', 'native', 'monthly');\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efa2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = '''\n",
    "-- Tabla particionada por rango de fecha\n",
    "CREATE TABLE ventas_part (\n",
    "  id BIGSERIAL PRIMARY KEY,\n",
    "  fecha DATE NOT NULL,\n",
    "  cliente_id INT,\n",
    "  producto_id INT,\n",
    "  cantidad INT,\n",
    "  total NUMERIC(12,2)\n",
    ") PARTITION BY RANGE (fecha);\n",
    "-- Particiones mensuales\n",
    "CREATE TABLE ventas_2025_10 PARTITION OF ventas_part FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');\n",
    "CREATE INDEX ON ventas_2025_10 (fecha, cliente_id);\n",
    "ANALYZE ventas_2025_10;\n",
    "-- Consulta con pruning de particiones\n",
    "EXPLAIN ANALYZE SELECT SUM(total) FROM ventas_part WHERE fecha >= '2025-10-10' AND fecha < '2025-10-20';\n",
    "'''\n",
    "print(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c89973",
   "metadata": {},
   "source": [
    "## 3. Data Lakes: particionado y pruning (Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5956eda",
   "metadata": {},
   "source": [
    "### \ud83c\udfd4\ufe0f **Data Lake Partitioning: Hive-style y Pruning**\n",
    "\n",
    "**Hive-style Partitioning:**\n",
    "```\n",
    "s3://datalake/events/\n",
    "\u251c\u2500\u2500 year=2025/\n",
    "\u2502   \u251c\u2500\u2500 month=10/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 day=01/\n",
    "\u2502   \u2502   \u2502   \u251c\u2500\u2500 part-00000.parquet\n",
    "\u2502   \u2502   \u2502   \u2514\u2500\u2500 part-00001.parquet\n",
    "\u2502   \u2502   \u2514\u2500\u2500 day=02/\n",
    "\u2502   \u2514\u2500\u2500 month=11/\n",
    "\u2514\u2500\u2500 year=2024/\n",
    "```\n",
    "\n",
    "**Nomenclatura Est\u00e1ndar:**\n",
    "```\n",
    "{table}/key1=value1/key2=value2/.../file.parquet\n",
    "\n",
    "\u2705 events/year=2025/month=10/day=01/data.parquet\n",
    "\u274c events/2025/10/01/data.parquet  -- No es Hive-style (sin key=value)\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "\n",
    "1. **Partition Pruning Autom\u00e1tico:**\n",
    "   ```sql\n",
    "   -- Athena/Spark/Trino detectan particiones sin metadata\n",
    "   SELECT * FROM events WHERE year = 2025 AND month = 10;\n",
    "   -- Escanea solo s3://datalake/events/year=2025/month=10/\n",
    "   ```\n",
    "\n",
    "2. **Ahorro de Costos:**\n",
    "   ```\n",
    "   Sin particionado: 1 TB escaneado = $5 (Athena)\n",
    "   Con particionado:  10 GB escaneado = $0.05 (99% ahorro)\n",
    "   ```\n",
    "\n",
    "3. **Query Performance:**\n",
    "   - Menos datos le\u00eddos = Menos tiempo\n",
    "   - Paralelizaci\u00f3n por partici\u00f3n\n",
    "\n",
    "**Columnas de Particionado (Orden importa):**\n",
    "\n",
    "```python\n",
    "# \u2705 Bueno: Alta a baja cardinalidad\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\", \"hour\").parquet(\"s3://...\")\n",
    "# Genera: year=2025/month=10/day=30/hour=14/\n",
    "\n",
    "# \u274c Malo: Baja cardinalidad primero\n",
    "df.write.partitionBy(\"country\", \"year\", \"month\").parquet(\"...\")\n",
    "# country solo tiene ~200 valores \u2192 menos paralelizaci\u00f3n\n",
    "\n",
    "# \u274c Muy Malo: Alta cardinalidad extrema\n",
    "df.write.partitionBy(\"user_id\").parquet(\"...\")\n",
    "# Millones de particiones \u2192 metadata overhead\n",
    "```\n",
    "\n",
    "**Small Files Problem:**\n",
    "```\n",
    "\u274c Malo: 10,000 archivos de 1 MB = 10 GB\n",
    "\u2705 Bueno: 80 archivos de 128 MB = 10 GB\n",
    "\n",
    "Por qu\u00e9:\n",
    "- Overhead de listar S3 (1 API call por archivo)\n",
    "- Spark tasks ineficientes (1 task por archivo)\n",
    "- Metadata explosion en Glue Catalog\n",
    "```\n",
    "\n",
    "**Soluci\u00f3n: Compaction**\n",
    "```python\n",
    "# Spark repartition antes de escribir\n",
    "df.repartition(80).write.parquet(\"s3://...\")\n",
    "\n",
    "# O COALESCE (reduce particiones sin shuffle)\n",
    "df.coalesce(80).write.parquet(\"s3://...\")\n",
    "\n",
    "# Target: 128-512 MB por archivo\n",
    "```\n",
    "\n",
    "**Formato Columnar (Parquet):**\n",
    "```\n",
    "Row-oriented (CSV):\n",
    "[id, name, age, country]\n",
    "[1, \"Alice\", 30, \"US\"]\n",
    "[2, \"Bob\", 25, \"UK\"]\n",
    "\u2193 Query: SELECT age FROM users\n",
    "\u2193 Lee TODO (3 columnas innecesarias)\n",
    "\n",
    "Column-oriented (Parquet):\n",
    "[id]:      [1, 2]\n",
    "[name]:    [\"Alice\", \"Bob\"]\n",
    "[age]:     [30, 25]  \u2190 Lee SOLO esta columna\n",
    "[country]: [\"US\", \"UK\"]\n",
    "```\n",
    "\n",
    "**Ventajas Parquet:**\n",
    "- \u2705 Compresi\u00f3n columnar (10x+ vs CSV)\n",
    "- \u2705 Predicate pushdown (skip row groups)\n",
    "- \u2705 Schema evolution (agregar columnas)\n",
    "- \u2705 Tipos de datos nativos (no strings)\n",
    "\n",
    "**Compresi\u00f3n:**\n",
    "```python\n",
    "df.write.option(\"compression\", \"snappy\").parquet(\"...\")\n",
    "\n",
    "Snappy:  R\u00e1pido, baja compresi\u00f3n (~2x)\n",
    "GZIP:    Lento, alta compresi\u00f3n (~4x)\n",
    "ZSTD:    Balance (3x, speed OK)\n",
    "```\n",
    "\n",
    "**Z-Ordering (Databricks/Delta Lake):**\n",
    "```sql\n",
    "OPTIMIZE events ZORDER BY (user_id, event_type)\n",
    "-- Co-localiza datos frecuentemente filtrados juntos\n",
    "-- Mejora data skipping (skip m\u00e1s archivos)\n",
    "```\n",
    "\n",
    "**Partitioning Anti-patterns:**\n",
    "```\n",
    "\u274c Particionar por columna con alta cardinalidad (user_id)\n",
    "\u274c M\u00e1s de 4 niveles de particionado (overhead)\n",
    "\u274c Particiones con <10 MB (too small)\n",
    "\u274c Particiones con >10 GB (too large, no paralelismo)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b058c",
   "metadata": {},
   "source": [
    "- Particionar por columnas de alta cardinalidad temporal (por ejemplo, a\u00f1o/mes/d\u00eda).\n",
    "- Ajustar tama\u00f1o de archivos (128\u2013512 MB) para evitar small files problem.\n",
    "- Pruning: motores como Spark/Athena/Trino escanean solo particiones necesarias.\n",
    "- Usa formato columnar (Parquet) y compresi\u00f3n (Snappy/ZSTD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8658b44",
   "metadata": {},
   "source": [
    "## 4. Window functions y agregaciones eficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271a83b",
   "metadata": {},
   "source": [
    "### \ud83d\udcc8 **Window Functions: Optimizaci\u00f3n de An\u00e1lisis Temporal**\n",
    "\n",
    "**Window Functions vs GROUP BY:**\n",
    "\n",
    "```sql\n",
    "-- GROUP BY: Agrega y colapsa filas\n",
    "SELECT cliente_id, SUM(total) \n",
    "FROM ventas \n",
    "GROUP BY cliente_id;\n",
    "-- Resultado: 1 fila por cliente\n",
    "\n",
    "-- Window Function: Mantiene todas las filas + columna calculada\n",
    "SELECT cliente_id, fecha, total,\n",
    "       SUM(total) OVER (PARTITION BY cliente_id) as total_cliente\n",
    "FROM ventas;\n",
    "-- Resultado: Todas las filas originales + total_cliente\n",
    "```\n",
    "\n",
    "**Sintaxis:**\n",
    "```sql\n",
    "<funci\u00f3n_ventana> OVER (\n",
    "  PARTITION BY <columnas>    -- Opcional: grupos\n",
    "  ORDER BY <columnas>        -- Opcional: orden dentro de grupo\n",
    "  ROWS/RANGE BETWEEN ...     -- Opcional: frame de ventana\n",
    ")\n",
    "```\n",
    "\n",
    "**Funciones Comunes:**\n",
    "\n",
    "1. **Ranking:**\n",
    "   ```sql\n",
    "   SELECT producto_id, ventas,\n",
    "          ROW_NUMBER() OVER (ORDER BY ventas DESC) as rn,\n",
    "          RANK() OVER (ORDER BY ventas DESC) as rank,\n",
    "          DENSE_RANK() OVER (ORDER BY ventas DESC) as dense_rank\n",
    "   FROM productos;\n",
    "   \n",
    "   -- ROW_NUMBER: 1,2,3,4,5 (sin empates)\n",
    "   -- RANK:       1,2,2,4,5 (con gaps)\n",
    "   -- DENSE_RANK: 1,2,2,3,4 (sin gaps)\n",
    "   ```\n",
    "\n",
    "2. **Agregaciones Acumulativas:**\n",
    "   ```sql\n",
    "   SELECT fecha, ventas,\n",
    "          SUM(ventas) OVER (ORDER BY fecha \n",
    "                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as acumulado\n",
    "   FROM ventas_diarias;\n",
    "   ```\n",
    "\n",
    "3. **Moving Averages:**\n",
    "   ```sql\n",
    "   -- Promedio m\u00f3vil \u00faltimos 7 d\u00edas\n",
    "   SELECT fecha, temperatura,\n",
    "          AVG(temperatura) OVER (ORDER BY fecha \n",
    "                                 ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as avg_7d\n",
    "   FROM clima;\n",
    "   ```\n",
    "\n",
    "4. **LAG/LEAD (Comparar con fila anterior/siguiente):**\n",
    "   ```sql\n",
    "   SELECT fecha, precio,\n",
    "          LAG(precio, 1) OVER (ORDER BY fecha) as precio_anterior,\n",
    "          precio - LAG(precio, 1) OVER (ORDER BY fecha) as cambio\n",
    "   FROM stock_prices;\n",
    "   ```\n",
    "\n",
    "**Frame Specifications:**\n",
    "\n",
    "```sql\n",
    "ROWS BETWEEN:  -- F\u00edsico (conteo de filas)\n",
    "  UNBOUNDED PRECEDING  -- Desde inicio\n",
    "  3 PRECEDING          -- 3 filas atr\u00e1s\n",
    "  CURRENT ROW          -- Fila actual\n",
    "  2 FOLLOWING          -- 2 filas adelante\n",
    "  UNBOUNDED FOLLOWING  -- Hasta final\n",
    "\n",
    "RANGE BETWEEN: -- L\u00f3gico (valores)\n",
    "  -- Ejemplo: RANGE BETWEEN INTERVAL '1 day' PRECEDING AND CURRENT ROW\n",
    "```\n",
    "\n",
    "**Optimizaci\u00f3n:**\n",
    "\n",
    "1. **\u00cdndice en ORDER BY:**\n",
    "   ```sql\n",
    "   CREATE INDEX idx_fecha ON ventas (fecha);\n",
    "   \n",
    "   -- Window function se beneficia del orden\n",
    "   SELECT *, SUM(total) OVER (ORDER BY fecha) FROM ventas;\n",
    "   ```\n",
    "\n",
    "2. **Multiple Windows (share window):**\n",
    "   ```sql\n",
    "   -- \u274c Malo: Define ventana 3 veces\n",
    "   SELECT \n",
    "     SUM(x) OVER (PARTITION BY y ORDER BY z),\n",
    "     AVG(x) OVER (PARTITION BY y ORDER BY z),\n",
    "     MAX(x) OVER (PARTITION BY y ORDER BY z)\n",
    "   FROM t;\n",
    "   \n",
    "   -- \u2705 Bueno: Define 1 vez, reutiliza\n",
    "   SELECT \n",
    "     SUM(x) OVER w,\n",
    "     AVG(x) OVER w,\n",
    "     MAX(x) OVER w\n",
    "   FROM t\n",
    "   WINDOW w AS (PARTITION BY y ORDER BY z);\n",
    "   ```\n",
    "\n",
    "3. **Evitar Window en WHERE:**\n",
    "   ```sql\n",
    "   -- \u274c No funciona (window functions post-WHERE)\n",
    "   SELECT * FROM ventas \n",
    "   WHERE SUM(total) OVER (PARTITION BY cliente_id) > 1000;\n",
    "   \n",
    "   -- \u2705 Usar CTE o subquery\n",
    "   WITH ranked AS (\n",
    "     SELECT *, SUM(total) OVER (PARTITION BY cliente_id) as total_cliente\n",
    "     FROM ventas\n",
    "   )\n",
    "   SELECT * FROM ranked WHERE total_cliente > 1000;\n",
    "   ```\n",
    "\n",
    "**Use Cases:**\n",
    "- Top N por grupo (ROW_NUMBER + WHERE rn <= N)\n",
    "- Running totals (cumulative sums)\n",
    "- Year-over-year comparisons (LAG con PARTITION BY mes)\n",
    "- Percentiles (PERCENTILE_CONT, NTILE)\n",
    "\n",
    "**Performance:**\n",
    "- Window functions requieren sort (costoso en tablas grandes)\n",
    "- Considerar materializar resultados si se reutilizan frecuentemente\n",
    "- Spark/Presto: Optimizan windows compartidos autom\u00e1ticamente\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql_query('SELECT fecha, cliente_id, total FROM ventas', conn)\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "df = df.sort_values(['cliente_id','fecha'])\n",
    "df['acumulado_cliente'] = df.groupby('cliente_id')['total'].cumsum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15028d",
   "metadata": {},
   "source": [
    "## 5. Buenas pr\u00e1cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27ca25",
   "metadata": {},
   "source": [
    "- Evitar SELECT * en producci\u00f3n; proyectar solo columnas necesarias.\n",
    "- Usar l\u00edmites de tiempo y paginaci\u00f3n en consultas intensivas.\n",
    "- Mantener \u00edndices y particiones seg\u00fan patrones de acceso actuales.\n",
    "- Monitorear planes y tiempos con muestras peri\u00f3dicas; automatizar regresiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83d\ude80 Servicios de Datos con FastAPI \u2192](08_fastapi_servicios_datos.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
