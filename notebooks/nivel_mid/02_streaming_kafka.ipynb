{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f514a2e2",
   "metadata": {},
   "source": [
    "# Streaming con Apache Kafka: Fundamentos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender arquitectura de streaming con Apache Kafka\n",
    "- Configurar productores y consumidores de Kafka\n",
    "- Procesar streams de datos en tiempo real\n",
    "- Implementar patrones de procesamiento de eventos\n",
    "- Integrar Kafka con Python y Pandas\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- kafka-python\n",
    "- pandas\n",
    "- Docker (para ejecutar Kafka localmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install kafka-python pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Librer√≠as cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f8a3e",
   "metadata": {},
   "source": [
    "### üìö **Apache Kafka: Arquitectura Event-Driven**\n",
    "\n",
    "**Definici√≥n:**  \n",
    "Apache Kafka es una plataforma distribuida de streaming de eventos que permite publicar, almacenar y procesar flujos de datos en tiempo real con alta disponibilidad y escalabilidad horizontal.\n",
    "\n",
    "**Componentes Core:**\n",
    "- **Broker**: Servidor que almacena y replica mensajes (logs inmutables)\n",
    "- **Topic**: Canal l√≥gico con categor√≠as de mensajes (ej: `user-events`, `transactions`)\n",
    "- **Partition**: Divisi√≥n f√≠sica de un topic para paralelizaci√≥n (0...N)\n",
    "- **Producer**: Cliente que publica mensajes a topics\n",
    "- **Consumer**: Cliente que lee mensajes desde topics\n",
    "- **Consumer Group**: Conjunto de consumers que procesan un topic en paralelo (cada partici√≥n ‚Üí 1 consumer)\n",
    "\n",
    "**Caracter√≠sticas Cr√≠ticas:**\n",
    "```\n",
    "üìä Throughput: Millones de mensajes/segundo\n",
    "üíæ Durabilidad: Persistencia en disco con replicaci√≥n\n",
    "üîÑ Ordenamiento: Garantizado por partici√≥n (no global)\n",
    "‚ö° Latencia: <10ms en configuraciones optimizadas\n",
    "```\n",
    "\n",
    "**Diferencia vs Message Queues tradicionales:**\n",
    "- RabbitMQ/SQS: Eliminan mensaje tras consumo (fire-and-forget)\n",
    "- Kafka: Retiene mensajes seg√∫n retention policy (permite replay)\n",
    "\n",
    "**Caso de Uso:**  \n",
    "Sistema de e-commerce donde cada evento (view, cart, purchase) se publica en Kafka. M√∫ltiples consumers procesan el mismo stream: uno para analytics en tiempo real, otro para recomendaciones ML, otro para inventario.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a8fe4",
   "metadata": {},
   "source": [
    "## 1. Conceptos Fundamentales de Kafka\n",
    "\n",
    "### Arquitectura:\n",
    "- **Topics**: Categor√≠as de mensajes\n",
    "- **Producers**: Publican mensajes en topics\n",
    "- **Consumers**: Leen mensajes de topics\n",
    "- **Brokers**: Servidores que almacenan mensajes\n",
    "- **Partitions**: Divisi√≥n de topics para escalabilidad\n",
    "- **Consumer Groups**: Grupo de consumidores que procesan mensajes en paralelo\n",
    "\n",
    "### Comandos Docker para Kafka:\n",
    "\n",
    "```bash\n",
    "# docker-compose.yml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Iniciar Kafka\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991610f",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de Producer (Productor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf763b",
   "metadata": {},
   "source": [
    "### üì§ **Kafka Producer: Publicaci√≥n de Eventos**\n",
    "\n",
    "**Configuraciones Cr√≠ticas del Producer:**\n",
    "\n",
    "1. **`acks` (Acknowledgment):**\n",
    "   - `acks=0`: No espera confirmaci√≥n (m√°ximo throughput, sin durabilidad)\n",
    "   - `acks=1`: Espera confirmaci√≥n del l√≠der (balance)\n",
    "   - `acks='all'`: Espera confirmaci√≥n de todos los replicas (m√°xima durabilidad)\n",
    "\n",
    "2. **`retries` y `max.in.flight.requests.per.connection`:**\n",
    "   ```python\n",
    "   retries=3  # Reintentos autom√°ticos ante fallos\n",
    "   max_in_flight_requests_per_connection=1  # Garantiza orden con retries\n",
    "   ```\n",
    "   - Si `max_in_flight > 1` con retries ‚Üí puede alterar orden de mensajes\n",
    "\n",
    "3. **Serializaci√≥n:**\n",
    "   - JSON: Legible pero mayor tama√±o\n",
    "   - Avro/Protobuf: Compacto y con schema registry (recomendado producci√≥n)\n",
    "\n",
    "**Flujo de Env√≠o:**\n",
    "```\n",
    "Producer ‚Üí [Partitioner] ‚Üí Buffer interno ‚Üí Batch ‚Üí Broker ‚Üí [Replication] ‚Üí ACK\n",
    "```\n",
    "\n",
    "**Particionamiento:**\n",
    "- Con `key`: hash(key) % num_partitions (mensajes con misma key ‚Üí misma partici√≥n)\n",
    "- Sin `key`: round-robin entre particiones\n",
    "\n",
    "**Uso en el C√≥digo:**\n",
    "- `send_event()`: Env√≠o as√≠ncrono que retorna Future\n",
    "- `send_batch()`: Optimizaci√≥n para m√∫ltiples eventos (reduce RTT)\n",
    "- `flush()`: Forzar env√≠o de buffer antes de cerrar\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10215ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventProducer:\n",
    "    \"\"\"Productor de eventos para Kafka\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers=['localhost:9092']):\n",
    "        \"\"\"\n",
    "        Inicializar productor\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Lista de brokers de Kafka\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.producer = KafkaProducer(\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "                acks='all',  # Esperar confirmaci√≥n de todos los brokers\n",
    "                retries=3,\n",
    "                max_in_flight_requests_per_connection=1\n",
    "            )\n",
    "            logger.info(\"Producer inicializado correctamente\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar producer: {e}\")\n",
    "            self.producer = None\n",
    "    \n",
    "    def send_event(self, topic, key, value):\n",
    "        \"\"\"\n",
    "        Enviar evento a Kafka\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            key: Clave del mensaje\n",
    "            value: Valor del mensaje (dict)\n",
    "        \"\"\"\n",
    "        if not self.producer:\n",
    "            logger.error(\"Producer no est√° inicializado\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            future = self.producer.send(topic, key=key, value=value)\n",
    "            # Esperar confirmaci√≥n\n",
    "            record_metadata = future.get(timeout=10)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Mensaje enviado - Topic: {record_metadata.topic}, \"\n",
    "                f\"Partition: {record_metadata.partition}, \"\n",
    "                f\"Offset: {record_metadata.offset}\"\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al enviar mensaje: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_batch(self, topic, events):\n",
    "        \"\"\"\n",
    "        Enviar lote de eventos\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            events: Lista de tuplas (key, value)\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        \n",
    "        for key, value in events:\n",
    "            if self.send_event(topic, key, value):\n",
    "                success_count += 1\n",
    "        \n",
    "        logger.info(f\"Enviados {success_count}/{len(events)} eventos\")\n",
    "        return success_count\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar producer\"\"\"\n",
    "        if self.producer:\n",
    "            self.producer.flush()\n",
    "            self.producer.close()\n",
    "            logger.info(\"Producer cerrado\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - requiere Kafka corriendo)\n",
    "print(\"Clase KafkaEventProducer definida\")\n",
    "print(\"\\nPara usar:\")\n",
    "print(\"producer = KafkaEventProducer()\")\n",
    "print(\"producer.send_event('my-topic', 'key1', {'data': 'value'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4287d",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n de Consumer (Consumidor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e15f76",
   "metadata": {},
   "source": [
    "### üì• **Kafka Consumer: Consumo de Eventos**\n",
    "\n",
    "**Configuraciones Esenciales:**\n",
    "\n",
    "1. **`group_id` (Consumer Group):**\n",
    "   - Consumers con mismo `group_id` ‚Üí distribuyen particiones entre ellos\n",
    "   - Cada partici√≥n ‚Üí asignada a un solo consumer del grupo\n",
    "   - Ejemplo: 3 partitions + 3 consumers ‚Üí 1 partition/consumer (ideal)\n",
    "   - Ejemplo: 3 partitions + 6 consumers ‚Üí 3 ociosos (over-provisioning)\n",
    "\n",
    "2. **`auto_offset_reset`:**\n",
    "   - `earliest`: Leer desde el primer mensaje disponible (√∫til para reprocesamiento)\n",
    "   - `latest`: Leer solo mensajes nuevos (default para streaming en vivo)\n",
    "   - `none`: Error si no existe offset previo\n",
    "\n",
    "3. **Commit de Offsets:**\n",
    "   - `enable_auto_commit=True`: Auto-commit cada `auto_commit_interval_ms` (m√°s simple)\n",
    "   - `enable_auto_commit=False`: Manual commit tras procesamiento exitoso (m√°s seguro)\n",
    "   ```python\n",
    "   consumer.commit()  # Commit manual expl√≠cito\n",
    "   ```\n",
    "\n",
    "**Gesti√≥n de Offsets:**\n",
    "```\n",
    "Topic: user-events\n",
    "Partition 0: [msg0, msg1, msg2, msg3, msg4] ‚Üí offset actual: 2 (ley√≥ hasta msg2)\n",
    "                                           ‚Üë\n",
    "                                      consumer commit\n",
    "```\n",
    "\n",
    "**Patrones de Consumo:**\n",
    "- **At-most-once**: Auto-commit antes de procesar (puede perder datos)\n",
    "- **At-least-once**: Commit despu√©s de procesar (puede duplicar)\n",
    "- **Exactly-once**: Transacciones Kafka + idempotencia (requiere Kafka 0.11+)\n",
    "\n",
    "**Rebalancing:**\n",
    "Cuando un consumer se une/sale del grupo ‚Üí Kafka redistribuye particiones (puede causar latencia temporal)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventConsumer:\n",
    "    \"\"\"Consumidor de eventos de Kafka\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        topics,\n",
    "        group_id='default-group',\n",
    "        bootstrap_servers=['localhost:9092']\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializar consumidor\n",
    "        \n",
    "        Args:\n",
    "            topics: Lista de topics a consumir\n",
    "            group_id: ID del consumer group\n",
    "            bootstrap_servers: Lista de brokers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.consumer = KafkaConsumer(\n",
    "                *topics,\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                group_id=group_id,\n",
    "                value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "                key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "                auto_offset_reset='earliest',  # Leer desde el inicio\n",
    "                enable_auto_commit=True,\n",
    "                auto_commit_interval_ms=1000\n",
    "            )\n",
    "            logger.info(f\"Consumer inicializado para topics: {topics}\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar consumer: {e}\")\n",
    "            self.consumer = None\n",
    "    \n",
    "    def consume_messages(self, max_messages=10, timeout_ms=5000):\n",
    "        \"\"\"\n",
    "        Consumir mensajes\n",
    "        \n",
    "        Args:\n",
    "            max_messages: M√°ximo de mensajes a consumir\n",
    "            timeout_ms: Timeout en milisegundos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de mensajes consumidos\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            logger.error(\"Consumer no est√° inicializado\")\n",
    "            return []\n",
    "        \n",
    "        messages = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                msg_data = {\n",
    "                    'topic': message.topic,\n",
    "                    'partition': message.partition,\n",
    "                    'offset': message.offset,\n",
    "                    'key': message.key,\n",
    "                    'value': message.value,\n",
    "                    'timestamp': message.timestamp\n",
    "                }\n",
    "                \n",
    "                messages.append(msg_data)\n",
    "                logger.info(f\"Mensaje consumido: {msg_data}\")\n",
    "                \n",
    "                if len(messages) >= max_messages:\n",
    "                    break\n",
    "            \n",
    "            return messages\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al consumir mensajes: {e}\")\n",
    "            return messages\n",
    "    \n",
    "    def consume_and_process(self, process_func, batch_size=100):\n",
    "        \"\"\"\n",
    "        Consumir y procesar mensajes en batch\n",
    "        \n",
    "        Args:\n",
    "            process_func: Funci√≥n para procesar batch\n",
    "            batch_size: Tama√±o del batch\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            return\n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                batch.append(message.value)\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    # Procesar batch\n",
    "                    process_func(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Consumo interrumpido por usuario\")\n",
    "            if batch:\n",
    "                process_func(batch)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar consumer\"\"\"\n",
    "        if self.consumer:\n",
    "            self.consumer.close()\n",
    "            logger.info(\"Consumer cerrado\")\n",
    "\n",
    "\n",
    "print(\"Clase KafkaEventConsumer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fe4cf",
   "metadata": {},
   "source": [
    "## 4. Simulaci√≥n de Streaming (Sin Kafka Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5c405",
   "metadata": {},
   "source": [
    "### üé≠ **Simulaci√≥n de Eventos: Testing Sin Infraestructura**\n",
    "\n",
    "**¬øPor qu√© Simular?**\n",
    "1. Kafka requiere infraestructura (Zookeeper + Broker) que puede no estar disponible en notebooks\n",
    "2. Permite desarrollar l√≥gica de procesamiento sin dependencias externas\n",
    "3. √ötil para testing unitario y desarrollo offline\n",
    "\n",
    "**Patr√≥n Event Sourcing:**\n",
    "Los eventos capturan *cambios de estado* en vez de estado actual:\n",
    "```\n",
    "Estado actual:        cart = {items: 3, total: 150}\n",
    "Event Sourcing:       [ProductAdded, ProductAdded, ProductAdded, CouponApplied]\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- Auditor√≠a completa (replay de eventos)\n",
    "- Debugging: Reproducir bugs desde secuencia de eventos\n",
    "- M√∫ltiples vistas (projections) del mismo stream\n",
    "\n",
    "**Caracter√≠sticas del Simulador:**\n",
    "- `generate_event()`: Crea eventos realistas con distribuciones aleatorias\n",
    "- `generate_stream()`: Simula flujo continuo con delays configurables\n",
    "- Delay peque√±o (0.01s) para testing r√°pido, mayor (1s) para simular real-time\n",
    "\n",
    "**Eventos de E-commerce:**\n",
    "- `view`: Usuario visualiza producto (top funnel)\n",
    "- `add_to_cart`: Agrega al carrito (middle funnel)\n",
    "- `purchase`: Completa transacci√≥n (conversion)\n",
    "- `remove_from_cart`: Abandono (negative signal)\n",
    "\n",
    "**Aplicaci√≥n Real:**  \n",
    "Este simulador representa un sistema de tracking de eventos similar a Google Analytics, Segment, o Amplitude.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulador de eventos de e-commerce\n",
    "class EcommerceEventSimulator:\n",
    "    \"\"\"Simulador de eventos de e-commerce en tiempo real\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.event_types = ['view', 'add_to_cart', 'purchase', 'remove_from_cart']\n",
    "        self.products = ['laptop', 'mouse', 'keyboard', 'monitor', 'headphones']\n",
    "        self.users = [f'user_{i}' for i in range(1, 101)]\n",
    "    \n",
    "    def generate_event(self):\n",
    "        \"\"\"Generar un evento aleatorio\"\"\"\n",
    "        return {\n",
    "            'event_id': np.random.randint(1000000, 9999999),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_id': np.random.choice(self.users),\n",
    "            'event_type': np.random.choice(self.event_types),\n",
    "            'product': np.random.choice(self.products),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "    \n",
    "    def generate_stream(self, n_events=100, delay=0.1):\n",
    "        \"\"\"Generar stream de eventos\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        for _ in range(n_events):\n",
    "            event = self.generate_event()\n",
    "            events.append(event)\n",
    "            print(f\"Evento generado: {event['event_type']} - {event['product']}\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "# Generar eventos de ejemplo\n",
    "simulator = EcommerceEventSimulator()\n",
    "print(\"\\nGenerando 10 eventos de ejemplo...\\n\")\n",
    "sample_events = simulator.generate_stream(n_events=10, delay=0.01)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_events = pd.DataFrame(sample_events)\n",
    "print(\"\\nDataFrame de eventos:\")\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca91b9",
   "metadata": {},
   "source": [
    "## 5. Procesamiento de Stream en Tiempo Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4246fb7",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è **Stream Processing: An√°lisis en Tiempo Real**\n",
    "\n",
    "**Window-Based Processing:**\n",
    "\n",
    "1. **Tumbling Window (Ventana Fija):**\n",
    "   ```\n",
    "   [0...10] [10...20] [20...30] ‚Üê No solapamiento\n",
    "   ```\n",
    "   - Cada evento pertenece a una √∫nica ventana\n",
    "   - √ötil para m√©tricas peri√≥dicas: \"ventas por minuto\"\n",
    "\n",
    "2. **Sliding Window (Ventana Deslizante):**\n",
    "   ```\n",
    "   [0...10]\n",
    "       [5...15]\n",
    "           [10...20] ‚Üê Solapamiento\n",
    "   ```\n",
    "   - Ventanas con overlap (implementada en c√≥digo)\n",
    "   - √ötil para tendencias suaves: \"promedio m√≥vil √∫ltimos 5 min\"\n",
    "\n",
    "3. **Session Window:**\n",
    "   - Agrupa eventos por inactividad\n",
    "   - Ejemplo: \"sesi√≥n de usuario termina tras 30 min sin actividad\"\n",
    "\n",
    "**M√©tricas de Streaming:**\n",
    "- **Throughput**: Eventos procesados por segundo\n",
    "- **Latency**: Tiempo desde evento ‚Üí procesamiento\n",
    "- **Watermarks**: Manejo de eventos fuera de orden (late arrivals)\n",
    "\n",
    "**Estado del Procesador:**\n",
    "```python\n",
    "self.stats = {\n",
    "    'total_events': 0,        # Contador acumulativo\n",
    "    'events_by_type': {},     # Agregaci√≥n por dimensi√≥n\n",
    "    'total_revenue': 0        # M√©trica de negocio cr√≠tica\n",
    "}\n",
    "```\n",
    "\n",
    "**Procesamiento Stateful vs Stateless:**\n",
    "- **Stateless**: Cada evento se procesa independientemente (map, filter)\n",
    "- **Stateful**: Mantiene estado entre eventos (aggregations, joins) ‚Üê Este c√≥digo\n",
    "\n",
    "**Aplicaci√≥n Real:**  \n",
    "Similar a Apache Flink/Spark Streaming para dashboards en vivo mostrando KPIs actualizados continuamente.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbe261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamProcessor:\n",
    "    \"\"\"Procesador de streams de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.events_buffer = []\n",
    "        self.stats = {\n",
    "            'total_events': 0,\n",
    "            'events_by_type': {},\n",
    "            'total_revenue': 0\n",
    "        }\n",
    "    \n",
    "    def process_event(self, event):\n",
    "        \"\"\"Procesar un evento individual\"\"\"\n",
    "        self.events_buffer.append(event)\n",
    "        self.stats['total_events'] += 1\n",
    "        \n",
    "        # Contar por tipo\n",
    "        event_type = event['event_type']\n",
    "        self.stats['events_by_type'][event_type] = \\\n",
    "            self.stats['events_by_type'].get(event_type, 0) + 1\n",
    "        \n",
    "        # Calcular revenue para purchases\n",
    "        if event_type == 'purchase':\n",
    "            self.stats['total_revenue'] += event['price'] * event['quantity']\n",
    "        \n",
    "        # Procesar ventana si est√° llena\n",
    "        if len(self.events_buffer) >= self.window_size:\n",
    "            self.process_window()\n",
    "    \n",
    "    def process_window(self):\n",
    "        \"\"\"Procesar ventana de eventos\"\"\"\n",
    "        df_window = pd.DataFrame(self.events_buffer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PROCESANDO VENTANA DE {len(df_window)} EVENTOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # An√°lisis de la ventana\n",
    "        print(\"\\nEventos por tipo:\")\n",
    "        print(df_window['event_type'].value_counts())\n",
    "        \n",
    "        print(\"\\nProductos m√°s populares:\")\n",
    "        print(df_window['product'].value_counts().head())\n",
    "        \n",
    "        # Tasa de conversi√≥n\n",
    "        views = len(df_window[df_window['event_type'] == 'view'])\n",
    "        purchases = len(df_window[df_window['event_type'] == 'purchase'])\n",
    "        conversion_rate = (purchases / views * 100) if views > 0 else 0\n",
    "        \n",
    "        print(f\"\\nTasa de conversi√≥n: {conversion_rate:.2f}%\")\n",
    "        \n",
    "        # Revenue de la ventana\n",
    "        window_revenue = df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "            lambda row: row['price'] * row['quantity'], axis=1\n",
    "        ).sum()\n",
    "        \n",
    "        print(f\"Revenue de la ventana: ${window_revenue:,.2f}\")\n",
    "        \n",
    "        # Limpiar buffer\n",
    "        self.events_buffer = []\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Obtener estad√≠sticas globales\"\"\"\n",
    "        return self.stats\n",
    "\n",
    "\n",
    "# Procesar eventos simulados\n",
    "processor = StreamProcessor(window_size=20)\n",
    "\n",
    "print(\"Generando y procesando 50 eventos...\")\n",
    "events = simulator.generate_stream(n_events=50, delay=0.01)\n",
    "\n",
    "for event in events:\n",
    "    processor.process_event(event)\n",
    "\n",
    "# Mostrar estad√≠sticas finales\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTAD√çSTICAS FINALES\")\n",
    "print(\"=\"*50)\n",
    "stats = processor.get_stats()\n",
    "print(f\"\\nTotal de eventos: {stats['total_events']}\")\n",
    "print(f\"\\nEventos por tipo:\")\n",
    "for event_type, count in stats['events_by_type'].items():\n",
    "    print(f\"  {event_type}: {count}\")\n",
    "print(f\"\\nRevenue total: ${stats['total_revenue']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c3157",
   "metadata": {},
   "source": [
    "## 6. Patrones de Procesamiento de Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327e766",
   "metadata": {},
   "source": [
    "### üîÑ **Patrones Avanzados: Sliding Windows y Agregaciones**\n",
    "\n",
    "**Implementaci√≥n de Sliding Window:**\n",
    "```python\n",
    "window_size = 10  # Tama√±o de ventana\n",
    "slide = 5         # Paso de deslizamiento\n",
    "```\n",
    "- `slide < window_size`: Ventanas solapadas (m√°s suave, m√°s c√≥mputo)\n",
    "- `slide = window_size`: Equivalente a tumbling window\n",
    "- `slide > window_size`: Gap entre ventanas (puede perder eventos)\n",
    "\n",
    "**Ventana Deslizante en Acci√≥n:**\n",
    "```\n",
    "Eventos:  [e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, e10...]\n",
    "Window 1: [e0..................e9]\n",
    "Window 2:          [e5..................e14]\n",
    "Window 3:                   [e10..................e19]\n",
    "```\n",
    "\n",
    "**M√©tricas Calculadas:**\n",
    "1. **Total Events**: Conteo simple por ventana\n",
    "2. **Unique Users**: Cardinalidad (distinct count) para medir alcance\n",
    "3. **Total Revenue**: Suma condicional solo de eventos `purchase`\n",
    "\n",
    "**Optimizaci√≥n para Producci√≥n:**\n",
    "- **Incremental Aggregation**: No recalcular todo, solo a√±adir/remover eventos del borde\n",
    "- **Late Data Handling**: Watermarks para eventos que llegan con delay\n",
    "- **State Backends**: Almacenar estado en RocksDB para ventanas grandes\n",
    "\n",
    "**Comparaci√≥n con Batch:**\n",
    "```\n",
    "Batch:     [esperar 1 hora] ‚Üí procesar todos ‚Üí resultado\n",
    "Streaming: evento ‚Üí [ventana] ‚Üí resultado parcial cada 5 min\n",
    "```\n",
    "\n",
    "**Casos de Uso:**\n",
    "- Detecci√≥n de anomal√≠as: Spike en errores en ventana de 5 min\n",
    "- Trending topics: Productos m√°s vistos en √∫ltimos 10 min\n",
    "- Conversion rate: Views vs purchases en ventana deslizante\n",
    "\n",
    "**Limitaciones de la Implementaci√≥n Actual:**\n",
    "- Usa memoria (en producci√≥n ‚Üí state backend distribuido)\n",
    "- No maneja eventos out-of-order (en producci√≥n ‚Üí event-time processing)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4683df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patr√≥n 1: Agregaciones por ventana deslizante\n",
    "def sliding_window_aggregation(events, window_size=10, slide=5):\n",
    "    \"\"\"\n",
    "    Agregaciones con ventana deslizante\n",
    "    \n",
    "    Args:\n",
    "        events: Lista de eventos\n",
    "        window_size: Tama√±o de la ventana\n",
    "        slide: Cu√°ntos eventos deslizar\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(events) - window_size + 1, slide):\n",
    "        window = events[i:i + window_size]\n",
    "        df_window = pd.DataFrame(window)\n",
    "        \n",
    "        agg_result = {\n",
    "            'window_start': i,\n",
    "            'window_end': i + window_size,\n",
    "            'total_events': len(df_window),\n",
    "            'unique_users': df_window['user_id'].nunique(),\n",
    "            'total_revenue': df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "                lambda row: row['price'] * row['quantity'], axis=1\n",
    "            ).sum()\n",
    "        }\n",
    "        \n",
    "        results.append(agg_result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Aplicar ventana deslizante\n",
    "events_for_window = simulator.generate_stream(n_events=100, delay=0)\n",
    "df_sliding = sliding_window_aggregation(events_for_window, window_size=20, slide=10)\n",
    "\n",
    "print(\"Agregaciones por ventana deslizante:\")\n",
    "print(df_sliding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c1a05",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Pr√°cticas\n",
    "\n",
    "### Conceptos Clave de Kafka:\n",
    "1. **Durabilidad**: Los mensajes se persisten en disco\n",
    "2. **Escalabilidad**: Particiones para procesamiento paralelo\n",
    "3. **Alto throughput**: Optimizado para grandes vol√∫menes\n",
    "4. **Ordenamiento**: Garantizado dentro de una partici√≥n\n",
    "5. **Consumer Groups**: Procesamiento distribuido\n",
    "\n",
    "### Mejores Pr√°cticas:\n",
    "- Usar claves de mensaje para particionamiento consistente\n",
    "- Configurar replicaci√≥n para alta disponibilidad\n",
    "- Monitorear lag de consumers\n",
    "- Implementar idempotencia en consumers\n",
    "- Usar compression para reducir tama√±o de mensajes\n",
    "- Definir retention policies apropiadas\n",
    "\n",
    "### Patrones de Streaming:\n",
    "- **Event Sourcing**: Almacenar cambios como eventos\n",
    "- **CQRS**: Separar lecturas y escrituras\n",
    "- **Windowing**: Procesar en ventanas de tiempo\n",
    "- **Aggregations**: Sumar, contar, promediar en ventanas\n",
    "- **Joins**: Combinar streams relacionados\n",
    "\n",
    "### Casos de Uso:\n",
    "- An√°lisis en tiempo real\n",
    "- Monitoreo de sistemas\n",
    "- Detecci√≥n de fraude\n",
    "- Recomendaciones personalizadas\n",
    "- IoT y telemetr√≠a\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Kafka Python Client](https://kafka-python.readthedocs.io/)\n",
    "- [Stream Processing Patterns](https://www.confluent.io/blog/streaming-data-patterns/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
