{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f514a2e2",
   "metadata": {},
   "source": [
    "# Streaming con Apache Kafka: Fundamentos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender arquitectura de streaming con Apache Kafka\n",
    "- Configurar productores y consumidores de Kafka\n",
    "- Procesar streams de datos en tiempo real\n",
    "- Implementar patrones de procesamiento de eventos\n",
    "- Integrar Kafka con Python y Pandas\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- kafka-python\n",
    "- pandas\n",
    "- Docker (para ejecutar Kafka localmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci\u00f3n de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install kafka-python pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Librer\u00edas cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f8a3e",
   "metadata": {},
   "source": [
    "### \ud83d\udcda **Apache Kafka: Arquitectura Event-Driven**\n",
    "\n",
    "**Definici\u00f3n:**  \n",
    "Apache Kafka es una plataforma distribuida de streaming de eventos que permite publicar, almacenar y procesar flujos de datos en tiempo real con alta disponibilidad y escalabilidad horizontal.\n",
    "\n",
    "**Componentes Core:**\n",
    "- **Broker**: Servidor que almacena y replica mensajes (logs inmutables)\n",
    "- **Topic**: Canal l\u00f3gico con categor\u00edas de mensajes (ej: `user-events`, `transactions`)\n",
    "- **Partition**: Divisi\u00f3n f\u00edsica de un topic para paralelizaci\u00f3n (0...N)\n",
    "- **Producer**: Cliente que publica mensajes a topics\n",
    "- **Consumer**: Cliente que lee mensajes desde topics\n",
    "- **Consumer Group**: Conjunto de consumers que procesan un topic en paralelo (cada partici\u00f3n \u2192 1 consumer)\n",
    "\n",
    "**Caracter\u00edsticas Cr\u00edticas:**\n",
    "```\n",
    "\ud83d\udcca Throughput: Millones de mensajes/segundo\n",
    "\ud83d\udcbe Durabilidad: Persistencia en disco con replicaci\u00f3n\n",
    "\ud83d\udd04 Ordenamiento: Garantizado por partici\u00f3n (no global)\n",
    "\u26a1 Latencia: <10ms en configuraciones optimizadas\n",
    "```\n",
    "\n",
    "**Diferencia vs Message Queues tradicionales:**\n",
    "- RabbitMQ/SQS: Eliminan mensaje tras consumo (fire-and-forget)\n",
    "- Kafka: Retiene mensajes seg\u00fan retention policy (permite replay)\n",
    "\n",
    "**Caso de Uso:**  \n",
    "Sistema de e-commerce donde cada evento (view, cart, purchase) se publica en Kafka. M\u00faltiples consumers procesan el mismo stream: uno para analytics en tiempo real, otro para recomendaciones ML, otro para inventario.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a8fe4",
   "metadata": {},
   "source": [
    "## 1. Conceptos Fundamentales de Kafka\n",
    "\n",
    "### Arquitectura:\n",
    "- **Topics**: Categor\u00edas de mensajes\n",
    "- **Producers**: Publican mensajes en topics\n",
    "- **Consumers**: Leen mensajes de topics\n",
    "- **Brokers**: Servidores que almacenan mensajes\n",
    "- **Partitions**: Divisi\u00f3n de topics para escalabilidad\n",
    "- **Consumer Groups**: Grupo de consumidores que procesan mensajes en paralelo\n",
    "\n",
    "### Comandos Docker para Kafka:\n",
    "\n",
    "```bash\n",
    "# docker-compose.yml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Iniciar Kafka\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991610f",
   "metadata": {},
   "source": [
    "## 2. Configuraci\u00f3n de Producer (Productor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf763b",
   "metadata": {},
   "source": [
    "### \ud83d\udce4 **Kafka Producer: Publicaci\u00f3n de Eventos**\n",
    "\n",
    "**Configuraciones Cr\u00edticas del Producer:**\n",
    "\n",
    "1. **`acks` (Acknowledgment):**\n",
    "   - `acks=0`: No espera confirmaci\u00f3n (m\u00e1ximo throughput, sin durabilidad)\n",
    "   - `acks=1`: Espera confirmaci\u00f3n del l\u00edder (balance)\n",
    "   - `acks='all'`: Espera confirmaci\u00f3n de todos los replicas (m\u00e1xima durabilidad)\n",
    "\n",
    "2. **`retries` y `max.in.flight.requests.per.connection`:**\n",
    "   ```python\n",
    "   retries=3  # Reintentos autom\u00e1ticos ante fallos\n",
    "   max_in_flight_requests_per_connection=1  # Garantiza orden con retries\n",
    "   ```\n",
    "   - Si `max_in_flight > 1` con retries \u2192 puede alterar orden de mensajes\n",
    "\n",
    "3. **Serializaci\u00f3n:**\n",
    "   - JSON: Legible pero mayor tama\u00f1o\n",
    "   - Avro/Protobuf: Compacto y con schema registry (recomendado producci\u00f3n)\n",
    "\n",
    "**Flujo de Env\u00edo:**\n",
    "```\n",
    "Producer \u2192 [Partitioner] \u2192 Buffer interno \u2192 Batch \u2192 Broker \u2192 [Replication] \u2192 ACK\n",
    "```\n",
    "\n",
    "**Particionamiento:**\n",
    "- Con `key`: hash(key) % num_partitions (mensajes con misma key \u2192 misma partici\u00f3n)\n",
    "- Sin `key`: round-robin entre particiones\n",
    "\n",
    "**Uso en el C\u00f3digo:**\n",
    "- `send_event()`: Env\u00edo as\u00edncrono que retorna Future\n",
    "- `send_batch()`: Optimizaci\u00f3n para m\u00faltiples eventos (reduce RTT)\n",
    "- `flush()`: Forzar env\u00edo de buffer antes de cerrar\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10215ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventProducer:\n",
    "    \"\"\"Productor de eventos para Kafka\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers=['localhost:9092']):\n",
    "        \"\"\"\n",
    "        Inicializar productor\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Lista de brokers de Kafka\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.producer = KafkaProducer(\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "                acks='all',  # Esperar confirmaci\u00f3n de todos los brokers\n",
    "                retries=3,\n",
    "                max_in_flight_requests_per_connection=1\n",
    "            )\n",
    "            logger.info(\"Producer inicializado correctamente\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar producer: {e}\")\n",
    "            self.producer = None\n",
    "    \n",
    "    def send_event(self, topic, key, value):\n",
    "        \"\"\"\n",
    "        Enviar evento a Kafka\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            key: Clave del mensaje\n",
    "            value: Valor del mensaje (dict)\n",
    "        \"\"\"\n",
    "        if not self.producer:\n",
    "            logger.error(\"Producer no est\u00e1 inicializado\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            future = self.producer.send(topic, key=key, value=value)\n",
    "            # Esperar confirmaci\u00f3n\n",
    "            record_metadata = future.get(timeout=10)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Mensaje enviado - Topic: {record_metadata.topic}, \"\n",
    "                f\"Partition: {record_metadata.partition}, \"\n",
    "                f\"Offset: {record_metadata.offset}\"\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al enviar mensaje: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_batch(self, topic, events):\n",
    "        \"\"\"\n",
    "        Enviar lote de eventos\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            events: Lista de tuplas (key, value)\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        \n",
    "        for key, value in events:\n",
    "            if self.send_event(topic, key, value):\n",
    "                success_count += 1\n",
    "        \n",
    "        logger.info(f\"Enviados {success_count}/{len(events)} eventos\")\n",
    "        return success_count\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar producer\"\"\"\n",
    "        if self.producer:\n",
    "            self.producer.flush()\n",
    "            self.producer.close()\n",
    "            logger.info(\"Producer cerrado\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - requiere Kafka corriendo)\n",
    "print(\"Clase KafkaEventProducer definida\")\n",
    "print(\"\\nPara usar:\")\n",
    "print(\"producer = KafkaEventProducer()\")\n",
    "print(\"producer.send_event('my-topic', 'key1', {'data': 'value'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4287d",
   "metadata": {},
   "source": [
    "## 3. Configuraci\u00f3n de Consumer (Consumidor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e15f76",
   "metadata": {},
   "source": [
    "### \ud83d\udce5 **Kafka Consumer: Consumo de Eventos**\n",
    "\n",
    "**Configuraciones Esenciales:**\n",
    "\n",
    "1. **`group_id` (Consumer Group):**\n",
    "   - Consumers con mismo `group_id` \u2192 distribuyen particiones entre ellos\n",
    "   - Cada partici\u00f3n \u2192 asignada a un solo consumer del grupo\n",
    "   - Ejemplo: 3 partitions + 3 consumers \u2192 1 partition/consumer (ideal)\n",
    "   - Ejemplo: 3 partitions + 6 consumers \u2192 3 ociosos (over-provisioning)\n",
    "\n",
    "2. **`auto_offset_reset`:**\n",
    "   - `earliest`: Leer desde el primer mensaje disponible (\u00fatil para reprocesamiento)\n",
    "   - `latest`: Leer solo mensajes nuevos (default para streaming en vivo)\n",
    "   - `none`: Error si no existe offset previo\n",
    "\n",
    "3. **Commit de Offsets:**\n",
    "   - `enable_auto_commit=True`: Auto-commit cada `auto_commit_interval_ms` (m\u00e1s simple)\n",
    "   - `enable_auto_commit=False`: Manual commit tras procesamiento exitoso (m\u00e1s seguro)\n",
    "   ```python\n",
    "   consumer.commit()  # Commit manual expl\u00edcito\n",
    "   ```\n",
    "\n",
    "**Gesti\u00f3n de Offsets:**\n",
    "```\n",
    "Topic: user-events\n",
    "Partition 0: [msg0, msg1, msg2, msg3, msg4] \u2192 offset actual: 2 (ley\u00f3 hasta msg2)\n",
    "                                           \u2191\n",
    "                                      consumer commit\n",
    "```\n",
    "\n",
    "**Patrones de Consumo:**\n",
    "- **At-most-once**: Auto-commit antes de procesar (puede perder datos)\n",
    "- **At-least-once**: Commit despu\u00e9s de procesar (puede duplicar)\n",
    "- **Exactly-once**: Transacciones Kafka + idempotencia (requiere Kafka 0.11+)\n",
    "\n",
    "**Rebalancing:**\n",
    "Cuando un consumer se une/sale del grupo \u2192 Kafka redistribuye particiones (puede causar latencia temporal)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventConsumer:\n",
    "    \"\"\"Consumidor de eventos de Kafka\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        topics,\n",
    "        group_id='default-group',\n",
    "        bootstrap_servers=['localhost:9092']\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializar consumidor\n",
    "        \n",
    "        Args:\n",
    "            topics: Lista de topics a consumir\n",
    "            group_id: ID del consumer group\n",
    "            bootstrap_servers: Lista de brokers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.consumer = KafkaConsumer(\n",
    "                *topics,\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                group_id=group_id,\n",
    "                value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "                key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "                auto_offset_reset='earliest',  # Leer desde el inicio\n",
    "                enable_auto_commit=True,\n",
    "                auto_commit_interval_ms=1000\n",
    "            )\n",
    "            logger.info(f\"Consumer inicializado para topics: {topics}\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar consumer: {e}\")\n",
    "            self.consumer = None\n",
    "    \n",
    "    def consume_messages(self, max_messages=10, timeout_ms=5000):\n",
    "        \"\"\"\n",
    "        Consumir mensajes\n",
    "        \n",
    "        Args:\n",
    "            max_messages: M\u00e1ximo de mensajes a consumir\n",
    "            timeout_ms: Timeout en milisegundos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de mensajes consumidos\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            logger.error(\"Consumer no est\u00e1 inicializado\")\n",
    "            return []\n",
    "        \n",
    "        messages = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                msg_data = {\n",
    "                    'topic': message.topic,\n",
    "                    'partition': message.partition,\n",
    "                    'offset': message.offset,\n",
    "                    'key': message.key,\n",
    "                    'value': message.value,\n",
    "                    'timestamp': message.timestamp\n",
    "                }\n",
    "                \n",
    "                messages.append(msg_data)\n",
    "                logger.info(f\"Mensaje consumido: {msg_data}\")\n",
    "                \n",
    "                if len(messages) >= max_messages:\n",
    "                    break\n",
    "            \n",
    "            return messages\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al consumir mensajes: {e}\")\n",
    "            return messages\n",
    "    \n",
    "    def consume_and_process(self, process_func, batch_size=100):\n",
    "        \"\"\"\n",
    "        Consumir y procesar mensajes en batch\n",
    "        \n",
    "        Args:\n",
    "            process_func: Funci\u00f3n para procesar batch\n",
    "            batch_size: Tama\u00f1o del batch\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            return\n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                batch.append(message.value)\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    # Procesar batch\n",
    "                    process_func(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Consumo interrumpido por usuario\")\n",
    "            if batch:\n",
    "                process_func(batch)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar consumer\"\"\"\n",
    "        if self.consumer:\n",
    "            self.consumer.close()\n",
    "            logger.info(\"Consumer cerrado\")\n",
    "\n",
    "\n",
    "print(\"Clase KafkaEventConsumer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fe4cf",
   "metadata": {},
   "source": [
    "## 4. Simulaci\u00f3n de Streaming (Sin Kafka Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5c405",
   "metadata": {},
   "source": [
    "### \ud83c\udfad **Simulaci\u00f3n de Eventos: Testing Sin Infraestructura**\n",
    "\n",
    "**\u00bfPor qu\u00e9 Simular?**\n",
    "1. Kafka requiere infraestructura (Zookeeper + Broker) que puede no estar disponible en notebooks\n",
    "2. Permite desarrollar l\u00f3gica de procesamiento sin dependencias externas\n",
    "3. \u00datil para testing unitario y desarrollo offline\n",
    "\n",
    "**Patr\u00f3n Event Sourcing:**\n",
    "Los eventos capturan *cambios de estado* en vez de estado actual:\n",
    "```\n",
    "Estado actual:        cart = {items: 3, total: 150}\n",
    "Event Sourcing:       [ProductAdded, ProductAdded, ProductAdded, CouponApplied]\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- Auditor\u00eda completa (replay de eventos)\n",
    "- Debugging: Reproducir bugs desde secuencia de eventos\n",
    "- M\u00faltiples vistas (projections) del mismo stream\n",
    "\n",
    "**Caracter\u00edsticas del Simulador:**\n",
    "- `generate_event()`: Crea eventos realistas con distribuciones aleatorias\n",
    "- `generate_stream()`: Simula flujo continuo con delays configurables\n",
    "- Delay peque\u00f1o (0.01s) para testing r\u00e1pido, mayor (1s) para simular real-time\n",
    "\n",
    "**Eventos de E-commerce:**\n",
    "- `view`: Usuario visualiza producto (top funnel)\n",
    "- `add_to_cart`: Agrega al carrito (middle funnel)\n",
    "- `purchase`: Completa transacci\u00f3n (conversion)\n",
    "- `remove_from_cart`: Abandono (negative signal)\n",
    "\n",
    "**Aplicaci\u00f3n Real:**  \n",
    "Este simulador representa un sistema de tracking de eventos similar a Google Analytics, Segment, o Amplitude.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulador de eventos de e-commerce\n",
    "class EcommerceEventSimulator:\n",
    "    \"\"\"Simulador de eventos de e-commerce en tiempo real\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.event_types = ['view', 'add_to_cart', 'purchase', 'remove_from_cart']\n",
    "        self.products = ['laptop', 'mouse', 'keyboard', 'monitor', 'headphones']\n",
    "        self.users = [f'user_{i}' for i in range(1, 101)]\n",
    "    \n",
    "    def generate_event(self):\n",
    "        \"\"\"Generar un evento aleatorio\"\"\"\n",
    "        return {\n",
    "            'event_id': np.random.randint(1000000, 9999999),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_id': np.random.choice(self.users),\n",
    "            'event_type': np.random.choice(self.event_types),\n",
    "            'product': np.random.choice(self.products),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "    \n",
    "    def generate_stream(self, n_events=100, delay=0.1):\n",
    "        \"\"\"Generar stream de eventos\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        for _ in range(n_events):\n",
    "            event = self.generate_event()\n",
    "            events.append(event)\n",
    "            print(f\"Evento generado: {event['event_type']} - {event['product']}\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "# Generar eventos de ejemplo\n",
    "simulator = EcommerceEventSimulator()\n",
    "print(\"\\nGenerando 10 eventos de ejemplo...\\n\")\n",
    "sample_events = simulator.generate_stream(n_events=10, delay=0.01)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_events = pd.DataFrame(sample_events)\n",
    "print(\"\\nDataFrame de eventos:\")\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca91b9",
   "metadata": {},
   "source": [
    "## 5. Procesamiento de Stream en Tiempo Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4246fb7",
   "metadata": {},
   "source": [
    "### \u2699\ufe0f **Stream Processing: An\u00e1lisis en Tiempo Real**\n",
    "\n",
    "**Window-Based Processing:**\n",
    "\n",
    "1. **Tumbling Window (Ventana Fija):**\n",
    "   ```\n",
    "   [0...10] [10...20] [20...30] \u2190 No solapamiento\n",
    "   ```\n",
    "   - Cada evento pertenece a una \u00fanica ventana\n",
    "   - \u00datil para m\u00e9tricas peri\u00f3dicas: \"ventas por minuto\"\n",
    "\n",
    "2. **Sliding Window (Ventana Deslizante):**\n",
    "   ```\n",
    "   [0...10]\n",
    "       [5...15]\n",
    "           [10...20] \u2190 Solapamiento\n",
    "   ```\n",
    "   - Ventanas con overlap (implementada en c\u00f3digo)\n",
    "   - \u00datil para tendencias suaves: \"promedio m\u00f3vil \u00faltimos 5 min\"\n",
    "\n",
    "3. **Session Window:**\n",
    "   - Agrupa eventos por inactividad\n",
    "   - Ejemplo: \"sesi\u00f3n de usuario termina tras 30 min sin actividad\"\n",
    "\n",
    "**M\u00e9tricas de Streaming:**\n",
    "- **Throughput**: Eventos procesados por segundo\n",
    "- **Latency**: Tiempo desde evento \u2192 procesamiento\n",
    "- **Watermarks**: Manejo de eventos fuera de orden (late arrivals)\n",
    "\n",
    "**Estado del Procesador:**\n",
    "```python\n",
    "self.stats = {\n",
    "    'total_events': 0,        # Contador acumulativo\n",
    "    'events_by_type': {},     # Agregaci\u00f3n por dimensi\u00f3n\n",
    "    'total_revenue': 0        # M\u00e9trica de negocio cr\u00edtica\n",
    "}\n",
    "```\n",
    "\n",
    "**Procesamiento Stateful vs Stateless:**\n",
    "- **Stateless**: Cada evento se procesa independientemente (map, filter)\n",
    "- **Stateful**: Mantiene estado entre eventos (aggregations, joins) \u2190 Este c\u00f3digo\n",
    "\n",
    "**Aplicaci\u00f3n Real:**  \n",
    "Similar a Apache Flink/Spark Streaming para dashboards en vivo mostrando KPIs actualizados continuamente.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbe261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamProcessor:\n",
    "    \"\"\"Procesador de streams de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.events_buffer = []\n",
    "        self.stats = {\n",
    "            'total_events': 0,\n",
    "            'events_by_type': {},\n",
    "            'total_revenue': 0\n",
    "        }\n",
    "    \n",
    "    def process_event(self, event):\n",
    "        \"\"\"Procesar un evento individual\"\"\"\n",
    "        self.events_buffer.append(event)\n",
    "        self.stats['total_events'] += 1\n",
    "        \n",
    "        # Contar por tipo\n",
    "        event_type = event['event_type']\n",
    "        self.stats['events_by_type'][event_type] = \\\n",
    "            self.stats['events_by_type'].get(event_type, 0) + 1\n",
    "        \n",
    "        # Calcular revenue para purchases\n",
    "        if event_type == 'purchase':\n",
    "            self.stats['total_revenue'] += event['price'] * event['quantity']\n",
    "        \n",
    "        # Procesar ventana si est\u00e1 llena\n",
    "        if len(self.events_buffer) >= self.window_size:\n",
    "            self.process_window()\n",
    "    \n",
    "    def process_window(self):\n",
    "        \"\"\"Procesar ventana de eventos\"\"\"\n",
    "        df_window = pd.DataFrame(self.events_buffer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PROCESANDO VENTANA DE {len(df_window)} EVENTOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # An\u00e1lisis de la ventana\n",
    "        print(\"\\nEventos por tipo:\")\n",
    "        print(df_window['event_type'].value_counts())\n",
    "        \n",
    "        print(\"\\nProductos m\u00e1s populares:\")\n",
    "        print(df_window['product'].value_counts().head())\n",
    "        \n",
    "        # Tasa de conversi\u00f3n\n",
    "        views = len(df_window[df_window['event_type'] == 'view'])\n",
    "        purchases = len(df_window[df_window['event_type'] == 'purchase'])\n",
    "        conversion_rate = (purchases / views * 100) if views > 0 else 0\n",
    "        \n",
    "        print(f\"\\nTasa de conversi\u00f3n: {conversion_rate:.2f}%\")\n",
    "        \n",
    "        # Revenue de la ventana\n",
    "        window_revenue = df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "            lambda row: row['price'] * row['quantity'], axis=1\n",
    "        ).sum()\n",
    "        \n",
    "        print(f\"Revenue de la ventana: ${window_revenue:,.2f}\")\n",
    "        \n",
    "        # Limpiar buffer\n",
    "        self.events_buffer = []\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Obtener estad\u00edsticas globales\"\"\"\n",
    "        return self.stats\n",
    "\n",
    "\n",
    "# Procesar eventos simulados\n",
    "processor = StreamProcessor(window_size=20)\n",
    "\n",
    "print(\"Generando y procesando 50 eventos...\")\n",
    "events = simulator.generate_stream(n_events=50, delay=0.01)\n",
    "\n",
    "for event in events:\n",
    "    processor.process_event(event)\n",
    "\n",
    "# Mostrar estad\u00edsticas finales\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTAD\u00cdSTICAS FINALES\")\n",
    "print(\"=\"*50)\n",
    "stats = processor.get_stats()\n",
    "print(f\"\\nTotal de eventos: {stats['total_events']}\")\n",
    "print(f\"\\nEventos por tipo:\")\n",
    "for event_type, count in stats['events_by_type'].items():\n",
    "    print(f\"  {event_type}: {count}\")\n",
    "print(f\"\\nRevenue total: ${stats['total_revenue']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c3157",
   "metadata": {},
   "source": [
    "## 6. Patrones de Procesamiento de Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327e766",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Patrones Avanzados: Sliding Windows y Agregaciones**\n",
    "\n",
    "**Implementaci\u00f3n de Sliding Window:**\n",
    "```python\n",
    "window_size = 10  # Tama\u00f1o de ventana\n",
    "slide = 5         # Paso de deslizamiento\n",
    "```\n",
    "- `slide < window_size`: Ventanas solapadas (m\u00e1s suave, m\u00e1s c\u00f3mputo)\n",
    "- `slide = window_size`: Equivalente a tumbling window\n",
    "- `slide > window_size`: Gap entre ventanas (puede perder eventos)\n",
    "\n",
    "**Ventana Deslizante en Acci\u00f3n:**\n",
    "```\n",
    "Eventos:  [e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, e10...]\n",
    "Window 1: [e0..................e9]\n",
    "Window 2:          [e5..................e14]\n",
    "Window 3:                   [e10..................e19]\n",
    "```\n",
    "\n",
    "**M\u00e9tricas Calculadas:**\n",
    "1. **Total Events**: Conteo simple por ventana\n",
    "2. **Unique Users**: Cardinalidad (distinct count) para medir alcance\n",
    "3. **Total Revenue**: Suma condicional solo de eventos `purchase`\n",
    "\n",
    "**Optimizaci\u00f3n para Producci\u00f3n:**\n",
    "- **Incremental Aggregation**: No recalcular todo, solo a\u00f1adir/remover eventos del borde\n",
    "- **Late Data Handling**: Watermarks para eventos que llegan con delay\n",
    "- **State Backends**: Almacenar estado en RocksDB para ventanas grandes\n",
    "\n",
    "**Comparaci\u00f3n con Batch:**\n",
    "```\n",
    "Batch:     [esperar 1 hora] \u2192 procesar todos \u2192 resultado\n",
    "Streaming: evento \u2192 [ventana] \u2192 resultado parcial cada 5 min\n",
    "```\n",
    "\n",
    "**Casos de Uso:**\n",
    "- Detecci\u00f3n de anomal\u00edas: Spike en errores en ventana de 5 min\n",
    "- Trending topics: Productos m\u00e1s vistos en \u00faltimos 10 min\n",
    "- Conversion rate: Views vs purchases en ventana deslizante\n",
    "\n",
    "**Limitaciones de la Implementaci\u00f3n Actual:**\n",
    "- Usa memoria (en producci\u00f3n \u2192 state backend distribuido)\n",
    "- No maneja eventos out-of-order (en producci\u00f3n \u2192 event-time processing)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4683df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patr\u00f3n 1: Agregaciones por ventana deslizante\n",
    "def sliding_window_aggregation(events, window_size=10, slide=5):\n",
    "    \"\"\"\n",
    "    Agregaciones con ventana deslizante\n",
    "    \n",
    "    Args:\n",
    "        events: Lista de eventos\n",
    "        window_size: Tama\u00f1o de la ventana\n",
    "        slide: Cu\u00e1ntos eventos deslizar\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(events) - window_size + 1, slide):\n",
    "        window = events[i:i + window_size]\n",
    "        df_window = pd.DataFrame(window)\n",
    "        \n",
    "        agg_result = {\n",
    "            'window_start': i,\n",
    "            'window_end': i + window_size,\n",
    "            'total_events': len(df_window),\n",
    "            'unique_users': df_window['user_id'].nunique(),\n",
    "            'total_revenue': df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "                lambda row: row['price'] * row['quantity'], axis=1\n",
    "            ).sum()\n",
    "        }\n",
    "        \n",
    "        results.append(agg_result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Aplicar ventana deslizante\n",
    "events_for_window = simulator.generate_stream(n_events=100, delay=0)\n",
    "df_sliding = sliding_window_aggregation(events_for_window, window_size=20, slide=10)\n",
    "\n",
    "print(\"Agregaciones por ventana deslizante:\")\n",
    "print(df_sliding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c1a05",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Pr\u00e1cticas\n",
    "\n",
    "### Conceptos Clave de Kafka:\n",
    "1. **Durabilidad**: Los mensajes se persisten en disco\n",
    "2. **Escalabilidad**: Particiones para procesamiento paralelo\n",
    "3. **Alto throughput**: Optimizado para grandes vol\u00famenes\n",
    "4. **Ordenamiento**: Garantizado dentro de una partici\u00f3n\n",
    "5. **Consumer Groups**: Procesamiento distribuido\n",
    "\n",
    "### Mejores Pr\u00e1cticas:\n",
    "- Usar claves de mensaje para particionamiento consistente\n",
    "- Configurar replicaci\u00f3n para alta disponibilidad\n",
    "- Monitorear lag de consumers\n",
    "- Implementar idempotencia en consumers\n",
    "- Usar compression para reducir tama\u00f1o de mensajes\n",
    "- Definir retention policies apropiadas\n",
    "\n",
    "### Patrones de Streaming:\n",
    "- **Event Sourcing**: Almacenar cambios como eventos\n",
    "- **CQRS**: Separar lecturas y escrituras\n",
    "- **Windowing**: Procesar en ventanas de tiempo\n",
    "- **Aggregations**: Sumar, contar, promediar en ventanas\n",
    "- **Joins**: Combinar streams relacionados\n",
    "\n",
    "### Casos de Uso:\n",
    "- An\u00e1lisis en tiempo real\n",
    "- Monitoreo de sistemas\n",
    "- Detecci\u00f3n de fraude\n",
    "- Recomendaciones personalizadas\n",
    "- IoT y telemetr\u00eda\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Kafka Python Client](https://kafka-python.readthedocs.io/)\n",
    "- [Stream Processing Patterns](https://www.confluent.io/blog/streaming-data-patterns/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda \u2192](03_cloud_aws.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
