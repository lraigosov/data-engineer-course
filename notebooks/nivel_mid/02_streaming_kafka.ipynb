{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f514a2e2",
   "metadata": {},
   "source": [
    "# Streaming con Apache Kafka: Fundamentos\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender arquitectura de streaming con Apache Kafka\n",
    "- Configurar productores y consumidores de Kafka\n",
    "- Procesar streams de datos en tiempo real\n",
    "- Implementar patrones de procesamiento de eventos\n",
    "- Integrar Kafka con Python y Pandas\n",
    "\n",
    "## Requisitos\n",
    "- Python 3.8+\n",
    "- kafka-python\n",
    "- pandas\n",
    "- Docker (para ejecutar Kafka localmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install kafka-python pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a8fe4",
   "metadata": {},
   "source": [
    "## 1. Conceptos Fundamentales de Kafka\n",
    "\n",
    "### Arquitectura:\n",
    "- **Topics**: Categorías de mensajes\n",
    "- **Producers**: Publican mensajes en topics\n",
    "- **Consumers**: Leen mensajes de topics\n",
    "- **Brokers**: Servidores que almacenan mensajes\n",
    "- **Partitions**: División de topics para escalabilidad\n",
    "- **Consumer Groups**: Grupo de consumidores que procesan mensajes en paralelo\n",
    "\n",
    "### Comandos Docker para Kafka:\n",
    "\n",
    "```bash\n",
    "# docker-compose.yml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Iniciar Kafka\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991610f",
   "metadata": {},
   "source": [
    "## 2. Configuración de Producer (Productor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10215ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventProducer:\n",
    "    \"\"\"Productor de eventos para Kafka\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers=['localhost:9092']):\n",
    "        \"\"\"\n",
    "        Inicializar productor\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Lista de brokers de Kafka\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.producer = KafkaProducer(\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "                acks='all',  # Esperar confirmación de todos los brokers\n",
    "                retries=3,\n",
    "                max_in_flight_requests_per_connection=1\n",
    "            )\n",
    "            logger.info(\"Producer inicializado correctamente\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar producer: {e}\")\n",
    "            self.producer = None\n",
    "    \n",
    "    def send_event(self, topic, key, value):\n",
    "        \"\"\"\n",
    "        Enviar evento a Kafka\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            key: Clave del mensaje\n",
    "            value: Valor del mensaje (dict)\n",
    "        \"\"\"\n",
    "        if not self.producer:\n",
    "            logger.error(\"Producer no está inicializado\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            future = self.producer.send(topic, key=key, value=value)\n",
    "            # Esperar confirmación\n",
    "            record_metadata = future.get(timeout=10)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Mensaje enviado - Topic: {record_metadata.topic}, \"\n",
    "                f\"Partition: {record_metadata.partition}, \"\n",
    "                f\"Offset: {record_metadata.offset}\"\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al enviar mensaje: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_batch(self, topic, events):\n",
    "        \"\"\"\n",
    "        Enviar lote de eventos\n",
    "        \n",
    "        Args:\n",
    "            topic: Nombre del topic\n",
    "            events: Lista de tuplas (key, value)\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        \n",
    "        for key, value in events:\n",
    "            if self.send_event(topic, key, value):\n",
    "                success_count += 1\n",
    "        \n",
    "        logger.info(f\"Enviados {success_count}/{len(events)} eventos\")\n",
    "        return success_count\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar producer\"\"\"\n",
    "        if self.producer:\n",
    "            self.producer.flush()\n",
    "            self.producer.close()\n",
    "            logger.info(\"Producer cerrado\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - requiere Kafka corriendo)\n",
    "print(\"Clase KafkaEventProducer definida\")\n",
    "print(\"\\nPara usar:\")\n",
    "print(\"producer = KafkaEventProducer()\")\n",
    "print(\"producer.send_event('my-topic', 'key1', {'data': 'value'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4287d",
   "metadata": {},
   "source": [
    "## 3. Configuración de Consumer (Consumidor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaEventConsumer:\n",
    "    \"\"\"Consumidor de eventos de Kafka\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        topics,\n",
    "        group_id='default-group',\n",
    "        bootstrap_servers=['localhost:9092']\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializar consumidor\n",
    "        \n",
    "        Args:\n",
    "            topics: Lista de topics a consumir\n",
    "            group_id: ID del consumer group\n",
    "            bootstrap_servers: Lista de brokers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.consumer = KafkaConsumer(\n",
    "                *topics,\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                group_id=group_id,\n",
    "                value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "                key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "                auto_offset_reset='earliest',  # Leer desde el inicio\n",
    "                enable_auto_commit=True,\n",
    "                auto_commit_interval_ms=1000\n",
    "            )\n",
    "            logger.info(f\"Consumer inicializado para topics: {topics}\")\n",
    "        except KafkaError as e:\n",
    "            logger.error(f\"Error al inicializar consumer: {e}\")\n",
    "            self.consumer = None\n",
    "    \n",
    "    def consume_messages(self, max_messages=10, timeout_ms=5000):\n",
    "        \"\"\"\n",
    "        Consumir mensajes\n",
    "        \n",
    "        Args:\n",
    "            max_messages: Máximo de mensajes a consumir\n",
    "            timeout_ms: Timeout en milisegundos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de mensajes consumidos\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            logger.error(\"Consumer no está inicializado\")\n",
    "            return []\n",
    "        \n",
    "        messages = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                msg_data = {\n",
    "                    'topic': message.topic,\n",
    "                    'partition': message.partition,\n",
    "                    'offset': message.offset,\n",
    "                    'key': message.key,\n",
    "                    'value': message.value,\n",
    "                    'timestamp': message.timestamp\n",
    "                }\n",
    "                \n",
    "                messages.append(msg_data)\n",
    "                logger.info(f\"Mensaje consumido: {msg_data}\")\n",
    "                \n",
    "                if len(messages) >= max_messages:\n",
    "                    break\n",
    "            \n",
    "            return messages\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al consumir mensajes: {e}\")\n",
    "            return messages\n",
    "    \n",
    "    def consume_and_process(self, process_func, batch_size=100):\n",
    "        \"\"\"\n",
    "        Consumir y procesar mensajes en batch\n",
    "        \n",
    "        Args:\n",
    "            process_func: Función para procesar batch\n",
    "            batch_size: Tamaño del batch\n",
    "        \"\"\"\n",
    "        if not self.consumer:\n",
    "            return\n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                batch.append(message.value)\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    # Procesar batch\n",
    "                    process_func(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Consumo interrumpido por usuario\")\n",
    "            if batch:\n",
    "                process_func(batch)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cerrar consumer\"\"\"\n",
    "        if self.consumer:\n",
    "            self.consumer.close()\n",
    "            logger.info(\"Consumer cerrado\")\n",
    "\n",
    "\n",
    "print(\"Clase KafkaEventConsumer definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fe4cf",
   "metadata": {},
   "source": [
    "## 4. Simulación de Streaming (Sin Kafka Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulador de eventos de e-commerce\n",
    "class EcommerceEventSimulator:\n",
    "    \"\"\"Simulador de eventos de e-commerce en tiempo real\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.event_types = ['view', 'add_to_cart', 'purchase', 'remove_from_cart']\n",
    "        self.products = ['laptop', 'mouse', 'keyboard', 'monitor', 'headphones']\n",
    "        self.users = [f'user_{i}' for i in range(1, 101)]\n",
    "    \n",
    "    def generate_event(self):\n",
    "        \"\"\"Generar un evento aleatorio\"\"\"\n",
    "        return {\n",
    "            'event_id': np.random.randint(1000000, 9999999),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_id': np.random.choice(self.users),\n",
    "            'event_type': np.random.choice(self.event_types),\n",
    "            'product': np.random.choice(self.products),\n",
    "            'price': round(np.random.uniform(10, 2000), 2),\n",
    "            'quantity': np.random.randint(1, 5)\n",
    "        }\n",
    "    \n",
    "    def generate_stream(self, n_events=100, delay=0.1):\n",
    "        \"\"\"Generar stream de eventos\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        for _ in range(n_events):\n",
    "            event = self.generate_event()\n",
    "            events.append(event)\n",
    "            print(f\"Evento generado: {event['event_type']} - {event['product']}\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "# Generar eventos de ejemplo\n",
    "simulator = EcommerceEventSimulator()\n",
    "print(\"\\nGenerando 10 eventos de ejemplo...\\n\")\n",
    "sample_events = simulator.generate_stream(n_events=10, delay=0.01)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_events = pd.DataFrame(sample_events)\n",
    "print(\"\\nDataFrame de eventos:\")\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca91b9",
   "metadata": {},
   "source": [
    "## 5. Procesamiento de Stream en Tiempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbe261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamProcessor:\n",
    "    \"\"\"Procesador de streams de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.events_buffer = []\n",
    "        self.stats = {\n",
    "            'total_events': 0,\n",
    "            'events_by_type': {},\n",
    "            'total_revenue': 0\n",
    "        }\n",
    "    \n",
    "    def process_event(self, event):\n",
    "        \"\"\"Procesar un evento individual\"\"\"\n",
    "        self.events_buffer.append(event)\n",
    "        self.stats['total_events'] += 1\n",
    "        \n",
    "        # Contar por tipo\n",
    "        event_type = event['event_type']\n",
    "        self.stats['events_by_type'][event_type] = \\\n",
    "            self.stats['events_by_type'].get(event_type, 0) + 1\n",
    "        \n",
    "        # Calcular revenue para purchases\n",
    "        if event_type == 'purchase':\n",
    "            self.stats['total_revenue'] += event['price'] * event['quantity']\n",
    "        \n",
    "        # Procesar ventana si está llena\n",
    "        if len(self.events_buffer) >= self.window_size:\n",
    "            self.process_window()\n",
    "    \n",
    "    def process_window(self):\n",
    "        \"\"\"Procesar ventana de eventos\"\"\"\n",
    "        df_window = pd.DataFrame(self.events_buffer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PROCESANDO VENTANA DE {len(df_window)} EVENTOS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Análisis de la ventana\n",
    "        print(\"\\nEventos por tipo:\")\n",
    "        print(df_window['event_type'].value_counts())\n",
    "        \n",
    "        print(\"\\nProductos más populares:\")\n",
    "        print(df_window['product'].value_counts().head())\n",
    "        \n",
    "        # Tasa de conversión\n",
    "        views = len(df_window[df_window['event_type'] == 'view'])\n",
    "        purchases = len(df_window[df_window['event_type'] == 'purchase'])\n",
    "        conversion_rate = (purchases / views * 100) if views > 0 else 0\n",
    "        \n",
    "        print(f\"\\nTasa de conversión: {conversion_rate:.2f}%\")\n",
    "        \n",
    "        # Revenue de la ventana\n",
    "        window_revenue = df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "            lambda row: row['price'] * row['quantity'], axis=1\n",
    "        ).sum()\n",
    "        \n",
    "        print(f\"Revenue de la ventana: ${window_revenue:,.2f}\")\n",
    "        \n",
    "        # Limpiar buffer\n",
    "        self.events_buffer = []\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Obtener estadísticas globales\"\"\"\n",
    "        return self.stats\n",
    "\n",
    "\n",
    "# Procesar eventos simulados\n",
    "processor = StreamProcessor(window_size=20)\n",
    "\n",
    "print(\"Generando y procesando 50 eventos...\")\n",
    "events = simulator.generate_stream(n_events=50, delay=0.01)\n",
    "\n",
    "for event in events:\n",
    "    processor.process_event(event)\n",
    "\n",
    "# Mostrar estadísticas finales\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTADÍSTICAS FINALES\")\n",
    "print(\"=\"*50)\n",
    "stats = processor.get_stats()\n",
    "print(f\"\\nTotal de eventos: {stats['total_events']}\")\n",
    "print(f\"\\nEventos por tipo:\")\n",
    "for event_type, count in stats['events_by_type'].items():\n",
    "    print(f\"  {event_type}: {count}\")\n",
    "print(f\"\\nRevenue total: ${stats['total_revenue']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c3157",
   "metadata": {},
   "source": [
    "## 6. Patrones de Procesamiento de Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4683df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patrón 1: Agregaciones por ventana deslizante\n",
    "def sliding_window_aggregation(events, window_size=10, slide=5):\n",
    "    \"\"\"\n",
    "    Agregaciones con ventana deslizante\n",
    "    \n",
    "    Args:\n",
    "        events: Lista de eventos\n",
    "        window_size: Tamaño de la ventana\n",
    "        slide: Cuántos eventos deslizar\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(events) - window_size + 1, slide):\n",
    "        window = events[i:i + window_size]\n",
    "        df_window = pd.DataFrame(window)\n",
    "        \n",
    "        agg_result = {\n",
    "            'window_start': i,\n",
    "            'window_end': i + window_size,\n",
    "            'total_events': len(df_window),\n",
    "            'unique_users': df_window['user_id'].nunique(),\n",
    "            'total_revenue': df_window[df_window['event_type'] == 'purchase'].apply(\n",
    "                lambda row: row['price'] * row['quantity'], axis=1\n",
    "            ).sum()\n",
    "        }\n",
    "        \n",
    "        results.append(agg_result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Aplicar ventana deslizante\n",
    "events_for_window = simulator.generate_stream(n_events=100, delay=0)\n",
    "df_sliding = sliding_window_aggregation(events_for_window, window_size=20, slide=10)\n",
    "\n",
    "print(\"Agregaciones por ventana deslizante:\")\n",
    "print(df_sliding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c1a05",
   "metadata": {},
   "source": [
    "## Resumen y Mejores Prácticas\n",
    "\n",
    "### Conceptos Clave de Kafka:\n",
    "1. **Durabilidad**: Los mensajes se persisten en disco\n",
    "2. **Escalabilidad**: Particiones para procesamiento paralelo\n",
    "3. **Alto throughput**: Optimizado para grandes volúmenes\n",
    "4. **Ordenamiento**: Garantizado dentro de una partición\n",
    "5. **Consumer Groups**: Procesamiento distribuido\n",
    "\n",
    "### Mejores Prácticas:\n",
    "- Usar claves de mensaje para particionamiento consistente\n",
    "- Configurar replicación para alta disponibilidad\n",
    "- Monitorear lag de consumers\n",
    "- Implementar idempotencia en consumers\n",
    "- Usar compression para reducir tamaño de mensajes\n",
    "- Definir retention policies apropiadas\n",
    "\n",
    "### Patrones de Streaming:\n",
    "- **Event Sourcing**: Almacenar cambios como eventos\n",
    "- **CQRS**: Separar lecturas y escrituras\n",
    "- **Windowing**: Procesar en ventanas de tiempo\n",
    "- **Aggregations**: Sumar, contar, promediar en ventanas\n",
    "- **Joins**: Combinar streams relacionados\n",
    "\n",
    "### Casos de Uso:\n",
    "- Análisis en tiempo real\n",
    "- Monitoreo de sistemas\n",
    "- Detección de fraude\n",
    "- Recomendaciones personalizadas\n",
    "- IoT y telemetría\n",
    "\n",
    "### Recursos Adicionales:\n",
    "- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Kafka Python Client](https://kafka-python.readthedocs.io/)\n",
    "- [Stream Processing Patterns](https://www.confluent.io/blog/streaming-data-patterns/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
