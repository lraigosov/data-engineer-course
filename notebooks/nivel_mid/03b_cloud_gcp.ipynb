{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca158fb0",
   "metadata": {},
   "source": [
    "# \u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer\n",
    "\n",
    "Este notebook introduce el ecosistema de Google Cloud Platform (GCP) para Data Engineering, cubriendo almacenamiento en Cloud Storage, transformaciones con BigQuery y Dataflow, y orquestaci\u00f3n con Cloud Composer.\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.)  \n",
    "**Nivel:** Mid  \n",
    "**Duraci\u00f3n:** 90-120 minutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679bb9b",
   "metadata": {},
   "source": [
    "## \u26a0\ufe0f RECORDATORIO IMPORTANTE\n",
    "\n",
    "### \ud83d\udea8 NOTEBOOKS vs PRODUCCI\u00d3N\n",
    "\n",
    "Este curso usa notebooks para **ense\u00f1anza**, pero en tu trabajo real:\n",
    "\n",
    "**\u274c NO uses notebooks para pipelines en producci\u00f3n**\n",
    "\n",
    "**\u2705 USA:**\n",
    "- Scripts Python modulares en `src/`\n",
    "- Cloud Composer DAGs (Airflow managed)\n",
    "- Cloud Functions para event-driven processing\n",
    "- Cloud Build para CI/CD\n",
    "- Terraform para Infrastructure as Code\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | \u00a9 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956216b",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecuci\u00f3n\n",
    "\n",
    "- Para ejecutar c\u00f3digo real de GCP necesitas:\n",
    "  - Proyecto GCP creado\n",
    "  - Credenciales configuradas (`gcloud auth login` o service account JSON)\n",
    "  - Billing habilitado\n",
    "  - APIs habilitadas: BigQuery, Cloud Storage, Dataflow, Composer\n",
    "- **Nunca subas credenciales al repositorio**\n",
    "- Usa variables de entorno o Google Cloud SDK\n",
    "- Este notebook muestra ejemplos ejecutables si tienes un proyecto GCP configurado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a05799",
   "metadata": {},
   "source": [
    "### \u2601\ufe0f **Google Cloud Platform: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de GCP para Datos:**\n",
    "\n",
    "1. **Cloud Storage (GCS)**: Object storage para Data Lakes\n",
    "   - Similar a S3, pero con modelo de consistencia fuerte desde el inicio\n",
    "   - Clases: Standard, Nearline (30d), Coldline (90d), Archive (365d)\n",
    "   - Pricing: ~$0.020/GB/mes (Standard en multi-region)\n",
    "\n",
    "2. **BigQuery**: Data Warehouse serverless con SQL\n",
    "   - Almacenamiento columnar comprimido\n",
    "   - Separaci\u00f3n compute/storage (paga solo por queries ejecutadas)\n",
    "   - Pricing: $5/TB escaneado (on-demand) o flat-rate mensual\n",
    "   - Streaming inserts: $0.01 per 200MB\n",
    "\n",
    "3. **Dataflow**: Procesamiento stream/batch con Apache Beam\n",
    "   - Serverless, auto-scaling\n",
    "   - Unified model: mismo c\u00f3digo para batch y streaming\n",
    "   - Pricing: por vCPU-hora + GB-hora (workers)\n",
    "\n",
    "4. **Cloud Composer**: Airflow totalmente administrado\n",
    "   - DAGs en Python, GKE-based\n",
    "   - Integraci\u00f3n nativa con servicios GCP\n",
    "   - Pricing: por tama\u00f1o de environment + compute\n",
    "\n",
    "5. **Cloud Functions**: Funciones serverless event-driven\n",
    "   - Triggers: HTTP, Cloud Storage, Pub/Sub, Firestore\n",
    "   - Runtime: Python 3.7-3.11, Node.js, Go, Java\n",
    "   - Pricing: por invocaciones + compute time\n",
    "\n",
    "**Arquitectura de Referencia:**\n",
    "```\n",
    "Fuentes \u2192 [Pub/Sub] \u2192 Cloud Storage (raw) \u2192 [Dataflow/Cloud Functions] \n",
    "                           \u2193\n",
    "                    Cloud Storage (curated) \u2192 BigQuery (anal\u00edtica)\n",
    "                           \u2193\n",
    "                    Cloud Composer (orquestaci\u00f3n)\n",
    "```\n",
    "\n",
    "**Ventajas de GCP para Datos:**\n",
    "- **BigQuery**: Queries extremadamente r\u00e1pidas (MPP distribuido)\n",
    "- **Integraci\u00f3n ML**: BigQuery ML, Vertex AI\n",
    "- **Consistencia fuerte**: No eventual consistency issues\n",
    "- **Kubernetes nativo**: GKE para workloads custom\n",
    "\n",
    "**Comparaci\u00f3n con AWS:**\n",
    "\n",
    "| Servicio | GCP | AWS |\n",
    "|----------|-----|-----|\n",
    "| Object Storage | Cloud Storage | S3 |\n",
    "| Data Warehouse | BigQuery | Redshift |\n",
    "| ETL Serverless | Dataflow (Beam) | Glue (PySpark) |\n",
    "| Streaming | Dataflow + Pub/Sub | Kinesis + Lambda |\n",
    "| Orchestration | Cloud Composer | MWAA (Airflow) |\n",
    "| Functions | Cloud Functions | Lambda |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8443f3",
   "metadata": {},
   "source": [
    "## 1. Cloud Storage: Data Lake B\u00e1sico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba432dd6",
   "metadata": {},
   "source": [
    "### \ud83d\uddc4\ufe0f **Cloud Storage: Fundamentos**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global \u00fanico\n",
    "  - Nombre: `mi-data-lake-gcp` (min\u00fasculas, n\u00fameros, guiones)\n",
    "  - Location: `us-central1`, `europe-west1`, `us` (multi-region)\n",
    "  - Storage class: Standard, Nearline, Coldline, Archive\n",
    "\n",
    "- **Object**: Archivo con metadata\n",
    "  - Key: `gs://bucket/path/to/file.csv`\n",
    "  - Metadata: Content-Type, custom headers\n",
    "  - Versionamiento: Object Versioning (similar a S3)\n",
    "\n",
    "**Organizacion Recomendada:**\n",
    "\n",
    "```\n",
    "gs://my-datalake/\n",
    "\u251c\u2500\u2500 raw/                    \u2190 Datos crudos\n",
    "\u2502   \u251c\u2500\u2500 sales/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 2025/10/30/\n",
    "\u2502   \u2514\u2500\u2500 customers/\n",
    "\u251c\u2500\u2500 staging/                \u2190 Datos en proceso\n",
    "\u2514\u2500\u2500 curated/                \u2190 Datos procesados\n",
    "    \u251c\u2500\u2500 sales_aggregated/\n",
    "    \u2514\u2500\u2500 customer_metrics/\n",
    "```\n",
    "\n",
    "**Lifecycle Management:**\n",
    "```json\n",
    "{\n",
    "  \"lifecycle\": {\n",
    "    \"rule\": [\n",
    "      {\n",
    "        \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n",
    "        \"condition\": {\"age\": 30}\n",
    "      },\n",
    "      {\n",
    "        \"action\": {\"type\": \"Delete\"},\n",
    "        \"condition\": {\"age\": 365}\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Operaciones con Python Client:**\n",
    "```python\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "# Crear bucket\n",
    "bucket = client.create_bucket('my-bucket', location='us-central1')\n",
    "\n",
    "# Subir archivo\n",
    "blob = bucket.blob('raw/ventas.csv')\n",
    "blob.upload_from_filename('ventas.csv')\n",
    "\n",
    "# Listar objetos\n",
    "for blob in bucket.list_blobs(prefix='raw/'):\n",
    "    print(blob.name)\n",
    "\n",
    "# Descargar archivo\n",
    "blob.download_to_filename('downloaded.csv')\n",
    "```\n",
    "\n",
    "**Costos:**\n",
    "- Storage: $0.020/GB/mes (Standard)\n",
    "- Class A operations (write): $0.05 per 10,000 ops\n",
    "- Class B operations (read): $0.004 per 10,000 ops\n",
    "- Network egress: $0.12/GB (fuera de GCP)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be43d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n (sin ejecutar sin proyecto GCP real)\n",
    "PROJECT_ID = 'mi-proyecto-gcp'\n",
    "BUCKET_NAME = f'{PROJECT_ID}-datalake'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f'\ud83d\udccb Configuraci\u00f3n: {PROJECT_ID} / {BUCKET_NAME}')\n",
    "print('\u26a0\ufe0f Requiere: gcloud auth login o service account JSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47267bf",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket y subir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ea755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c\u00f3digo (descomentar con proyecto real)\n",
    "'''\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializar cliente\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Crear bucket\n",
    "try:\n",
    "    bucket = client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
    "    print(f'\u2705 Bucket {BUCKET_NAME} creado')\n",
    "except Exception as e:\n",
    "    print(f'\u2139\ufe0f Bucket ya existe o error: {e}')\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Subir CSV\n",
    "df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "csv_string = df.to_csv(index=False)\n",
    "\n",
    "blob = bucket.blob('raw/ventas/ventas_2025_10.csv')\n",
    "blob.upload_from_string(csv_string, content_type='text/csv')\n",
    "print('\ud83d\udce4 Archivo subido a Cloud Storage')\n",
    "'''\n",
    "print('C\u00f3digo de ejemplo listo para ejecutar con proyecto GCP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37644226",
   "metadata": {},
   "source": [
    "## 2. BigQuery: Data Warehouse Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db8274",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **BigQuery: SQL Analytics a Escala**\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "BigQuery separa **storage** y **compute**:\n",
    "- Storage: Columnar format (Capacitor), compresi\u00f3n autom\u00e1tica\n",
    "- Compute: Dremel engine (MPP distribuido con miles de workers)\n",
    "\n",
    "**Beneficios:**\n",
    "- Queries sobre TB/PB en segundos\n",
    "- No administrar clusters (100% serverless)\n",
    "- Standard SQL (ANSI SQL 2011 compatible)\n",
    "- Integraci\u00f3n con herramientas BI (Looker, Tableau, Data Studio)\n",
    "\n",
    "**Conceptos:**\n",
    "\n",
    "1. **Dataset**: Contenedor l\u00f3gico de tablas\n",
    "   - Similar a \"database\" en SQL tradicional\n",
    "   - Permisos a nivel dataset\n",
    "\n",
    "2. **Table**: Datos estructurados\n",
    "   - Native tables: Datos en BigQuery storage\n",
    "   - External tables: Data en GCS (federated queries)\n",
    "   - Partitioned tables: Por fecha/rango (reduce scan)\n",
    "   - Clustered tables: Por columnas espec\u00edficas (mejor performance)\n",
    "\n",
    "3. **View**: Query guardada\n",
    "   - Authorized views: Control de acceso granular\n",
    "\n",
    "**Particionamiento:**\n",
    "```sql\n",
    "-- Tabla particionada por fecha\n",
    "CREATE TABLE `project.dataset.sales_partitioned`\n",
    "PARTITION BY DATE(order_date)\n",
    "AS SELECT * FROM `project.dataset.sales_raw`;\n",
    "\n",
    "-- Query optimizada (scan solo 1 d\u00eda)\n",
    "SELECT * FROM `project.dataset.sales_partitioned`\n",
    "WHERE order_date = '2025-10-30';\n",
    "```\n",
    "\n",
    "**Clustering:**\n",
    "```sql\n",
    "-- Tabla clusterizada por customer_id, product_id\n",
    "CREATE TABLE `project.dataset.sales_clustered`\n",
    "PARTITION BY DATE(order_date)\n",
    "CLUSTER BY customer_id, product_id\n",
    "AS SELECT * FROM `project.dataset.sales_raw`;\n",
    "\n",
    "-- Query beneficiada (pre-sorted data)\n",
    "SELECT * FROM `project.dataset.sales_clustered`\n",
    "WHERE customer_id = 12345;\n",
    "```\n",
    "\n",
    "**BigQuery ML (Machine Learning integrado):**\n",
    "```sql\n",
    "-- Crear modelo de regresi\u00f3n log\u00edstica\n",
    "CREATE OR REPLACE MODEL `project.dataset.churn_model`\n",
    "OPTIONS(model_type='logistic_reg') AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  age,\n",
    "  total_purchases,\n",
    "  churned as label\n",
    "FROM `project.dataset.customers`;\n",
    "\n",
    "-- Predecir\n",
    "SELECT * FROM ML.PREDICT(\n",
    "  MODEL `project.dataset.churn_model`,\n",
    "  (SELECT * FROM `project.dataset.new_customers`)\n",
    ");\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Particionar tablas grandes (>1GB)\n",
    "- Evitar `SELECT *` (especifica columnas)\n",
    "- Usar `_TABLE_SUFFIX` para tablas wildcard\n",
    "- Aprovechar result caching (24h gratuito)\n",
    "- Usar slots reservation para cargas predecibles\n",
    "\n",
    "**Pricing:**\n",
    "- On-demand: $5/TB escaneado\n",
    "- Flat-rate: desde $2,000/mes (100 slots)\n",
    "- Storage: $0.020/GB active, $0.010/GB long-term (90d+)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n BigQuery\n",
    "DATASET_ID = 'data_engineering_course'\n",
    "TABLE_ID = 'ventas'\n",
    "\n",
    "print(f'\ud83d\udcca Dataset: {PROJECT_ID}.{DATASET_ID}')\n",
    "print(f'\ud83d\udccb Tabla: {TABLE_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7446df5",
   "metadata": {},
   "source": [
    "### 2.1 Crear dataset y tabla desde Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06140090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo BigQuery\n",
    "'''\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Crear dataset\n",
    "dataset_ref = client.dataset(DATASET_ID)\n",
    "try:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = LOCATION\n",
    "    dataset = client.create_dataset(dataset)\n",
    "    print(f'\u2705 Dataset {DATASET_ID} creado')\n",
    "except Exception as e:\n",
    "    print(f'\u2139\ufe0f Dataset ya existe: {e}')\n",
    "\n",
    "# Cargar CSV desde GCS a BigQuery\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField('venta_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('cliente_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('producto_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('cantidad', 'INTEGER'),\n",
    "        bigquery.SchemaField('total', 'FLOAT'),\n",
    "        bigquery.SchemaField('fecha', 'DATE'),\n",
    "    ],\n",
    "    skip_leading_rows=1,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    ")\n",
    "\n",
    "uri = f'gs://{BUCKET_NAME}/raw/ventas/ventas_2025_10.csv'\n",
    "table_ref = dataset_ref.table(TABLE_ID)\n",
    "\n",
    "load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
    "load_job.result()  # Wait for job to complete\n",
    "\n",
    "print(f'\u2705 Cargados {load_job.output_rows} registros en {TABLE_ID}')\n",
    "'''\n",
    "print('C\u00f3digo BigQuery listo para ejecutar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb51a66",
   "metadata": {},
   "source": [
    "### 2.2 Queries SQL en BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d58f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de queries\n",
    "'''\n",
    "# Query simple\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "  cliente_id,\n",
    "  SUM(total) as total_ventas,\n",
    "  COUNT(*) as num_ventas\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "GROUP BY cliente_id\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "for row in results:\n",
    "    print(f'Cliente {row.cliente_id}: ${row.total_ventas:.2f} ({row.num_ventas} ventas)')\n",
    "\n",
    "# Metadata del job\n",
    "print(f'\\\\n\ud83d\udcca Query Stats:')\n",
    "print(f'  Bytes processed: {query_job.total_bytes_processed / 1e9:.2f} GB')\n",
    "print(f'  Bytes billed: {query_job.total_bytes_billed / 1e9:.2f} GB')\n",
    "print(f'  Cost estimate: ${(query_job.total_bytes_billed / 1e12) * 5:.4f}')\n",
    "'''\n",
    "print('Queries BigQuery con estimaci\u00f3n de costos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aaac5d",
   "metadata": {},
   "source": [
    "## 3. Dataflow: Procesamiento con Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8b57f",
   "metadata": {},
   "source": [
    "### \ud83c\udf0a **Dataflow: Unified Stream/Batch Processing**\n",
    "\n",
    "**Apache Beam Concepts:**\n",
    "\n",
    "Beam es un modelo de programaci\u00f3n unificado para batch y streaming:\n",
    "\n",
    "```\n",
    "Pipeline \u2192 PCollection \u2192 Transform \u2192 PCollection \u2192 ...\n",
    "```\n",
    "\n",
    "- **Pipeline**: Grafo de transformaciones\n",
    "- **PCollection**: Conjunto distribuido de datos (immutable)\n",
    "- **Transform**: Operaci\u00f3n sobre PCollection (Map, Filter, GroupByKey, etc.)\n",
    "\n",
    "**Runners:**\n",
    "- DirectRunner: Local (testing)\n",
    "- DataflowRunner: GCP managed service\n",
    "- FlinkRunner: Apache Flink\n",
    "- SparkRunner: Apache Spark\n",
    "\n",
    "**Patr\u00f3n B\u00e1sico:**\n",
    "```python\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions([\n",
    "    '--project=my-project',\n",
    "    '--region=us-central1',\n",
    "    '--runner=DataflowRunner',\n",
    "    '--temp_location=gs://my-bucket/temp',\n",
    "])\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (pipeline\n",
    "     | 'Read' >> beam.io.ReadFromText('gs://input/*.csv')\n",
    "     | 'Parse' >> beam.Map(lambda line: line.split(','))\n",
    "     | 'Filter' >> beam.Filter(lambda row: float(row[2]) > 100)\n",
    "     | 'Format' >> beam.Map(lambda row: f'{row[0]},{row[1]}')\n",
    "     | 'Write' >> beam.io.WriteToText('gs://output/result'))\n",
    "```\n",
    "\n",
    "**Windowing (Streaming):**\n",
    "```python\n",
    "from apache_beam import window\n",
    "\n",
    "(events\n",
    " | 'Window' >> beam.WindowInto(window.FixedWindows(60))  # 1-min windows\n",
    " | 'Sum' >> beam.CombinePerKey(sum)\n",
    " | 'Write' >> beam.io.WriteToBigQuery('project:dataset.table'))\n",
    "```\n",
    "\n",
    "**Transforms Comunes:**\n",
    "\n",
    "1. **ParDo** (Parallel Do):\n",
    "   ```python\n",
    "   class ExtractFields(beam.DoFn):\n",
    "       def process(self, element):\n",
    "           parts = element.split(',')\n",
    "           yield {'id': int(parts[0]), 'value': float(parts[1])}\n",
    "   \n",
    "   data | beam.ParDo(ExtractFields())\n",
    "   ```\n",
    "\n",
    "2. **GroupByKey**:\n",
    "   ```python\n",
    "   # (key, value) pairs \u2192 (key, [values])\n",
    "   pairs | beam.GroupByKey()\n",
    "   ```\n",
    "\n",
    "3. **CombinePerKey**:\n",
    "   ```python\n",
    "   # M\u00e1s eficiente que GroupByKey + Map\n",
    "   pairs | beam.CombinePerKey(sum)\n",
    "   ```\n",
    "\n",
    "4. **Flatten**:\n",
    "   ```python\n",
    "   # Merge m\u00faltiples PCollections\n",
    "   (pcoll1, pcoll2, pcoll3) | beam.Flatten()\n",
    "   ```\n",
    "\n",
    "**Side Inputs (Broadcasting):**\n",
    "```python\n",
    "lookup_table = (pipeline\n",
    "                | 'Read Lookup' >> beam.io.ReadFromText('gs://lookup.csv')\n",
    "                | 'Parse' >> beam.Map(lambda x: x.split(',')))\n",
    "\n",
    "main_data | beam.Map(\n",
    "    lambda x, table: enrich(x, table),\n",
    "    beam.pvalue.AsDict(lookup_table)\n",
    ")\n",
    "```\n",
    "\n",
    "**Dataflow vs Alternatives:**\n",
    "\n",
    "| Aspecto | Dataflow | Spark | Flink |\n",
    "|---------|----------|-------|-------|\n",
    "| **Model** | Beam (unified) | RDD/DataFrame | DataStream |\n",
    "| **Serverless** | \u2705 S\u00ed | \u274c No (EMR/Databricks) | \u274c No |\n",
    "| **Exactly-once** | \u2705 S\u00ed | \u26a0\ufe0f Dif\u00edcil | \u2705 S\u00ed |\n",
    "| **Late Data** | \u2705 Excellent | \u26a0\ufe0f Manual | \u2705 Good |\n",
    "| **Learning Curve** | Media | Baja | Alta |\n",
    "\n",
    "**Use Cases:**\n",
    "- ETL masivos (TB \u2192 PB)\n",
    "- Real-time analytics (con Pub/Sub)\n",
    "- ML feature engineering\n",
    "- Data quality validation\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eba46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de pipeline Dataflow\n",
    "dataflow_example = '''\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class ParseCSV(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        parts = element.split(',')\n",
    "        yield {\n",
    "            'venta_id': int(parts[0]),\n",
    "            'cliente_id': int(parts[1]),\n",
    "            'total': float(parts[4])\n",
    "        }\n",
    "\n",
    "class FilterHighValue(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element['total'] > 100:\n",
    "            yield element\n",
    "\n",
    "options = PipelineOptions([\n",
    "    '--project=PROJECT_ID',\n",
    "    '--region=us-central1',\n",
    "    '--runner=DataflowRunner',\n",
    "    '--temp_location=gs://BUCKET/temp',\n",
    "    '--staging_location=gs://BUCKET/staging',\n",
    "])\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (pipeline\n",
    "     | 'Read GCS' >> beam.io.ReadFromText('gs://BUCKET/raw/ventas/*.csv')\n",
    "     | 'Skip Header' >> beam.Filter(lambda line: not line.startswith('venta_id'))\n",
    "     | 'Parse' >> beam.ParDo(ParseCSV())\n",
    "     | 'Filter' >> beam.ParDo(FilterHighValue())\n",
    "     | 'Aggregate' >> beam.Map(lambda x: (x['cliente_id'], x['total']))\n",
    "     | 'Group' >> beam.CombinePerKey(sum)\n",
    "     | 'Format' >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\")\n",
    "     | 'Write' >> beam.io.WriteToText('gs://BUCKET/curated/ventas_summary'))\n",
    "'''\n",
    "\n",
    "print(dataflow_example)\n",
    "print('\\\\n\ud83d\udca1 Para ejecutar: python dataflow_job.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ffe76",
   "metadata": {},
   "source": [
    "## 4. Cloud Composer: Airflow Administrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ed071",
   "metadata": {},
   "source": [
    "### \ud83c\udfbc **Cloud Composer: Airflow en GCP**\n",
    "\n",
    "**\u00bfQu\u00e9 es Cloud Composer?**\n",
    "\n",
    "Servicio totalmente administrado de Apache Airflow en GCP:\n",
    "- Basado en GKE (Google Kubernetes Engine)\n",
    "- Auto-scaling de workers\n",
    "- Integraci\u00f3n nativa con servicios GCP\n",
    "- Monitoreo con Cloud Logging/Monitoring\n",
    "\n",
    "**Componentes:**\n",
    "\n",
    "1. **Environment**: Cluster Airflow dedicado\n",
    "   - Web server (UI)\n",
    "   - Scheduler\n",
    "   - Workers (Celery o Kubernetes)\n",
    "   - Database (Cloud SQL PostgreSQL)\n",
    "\n",
    "2. **DAGs Folder**: GCS bucket autom\u00e1tico\n",
    "   - `gs://[bucket]/dags/`\n",
    "   - Sync autom\u00e1tico al subir DAGs\n",
    "\n",
    "**Operators GCP:**\n",
    "\n",
    "```python\n",
    "from airflow.providers.google.cloud.operators.bigquery import (\n",
    "    BigQueryCreateEmptyDatasetOperator,\n",
    "    BigQueryInsertJobOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.operators.gcs import (\n",
    "    GCSCreateBucketOperator,\n",
    "    GCSDeleteBucketOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import (\n",
    "    GCSToBigQueryOperator,\n",
    ")\n",
    "\n",
    "with DAG('gcp_pipeline', schedule_interval='@daily') as dag:\n",
    "    \n",
    "    create_dataset = BigQueryCreateEmptyDatasetOperator(\n",
    "        task_id='create_dataset',\n",
    "        dataset_id='my_dataset',\n",
    "        project_id=PROJECT_ID,\n",
    "    )\n",
    "    \n",
    "    load_to_bq = GCSToBigQueryOperator(\n",
    "        task_id='load_csv_to_bq',\n",
    "        bucket='my-bucket',\n",
    "        source_objects=['raw/ventas/*.csv'],\n",
    "        destination_project_dataset_table=f'{PROJECT_ID}.my_dataset.ventas',\n",
    "        schema_fields=[\n",
    "            {'name': 'venta_id', 'type': 'INTEGER'},\n",
    "            {'name': 'cliente_id', 'type': 'INTEGER'},\n",
    "            {'name': 'total', 'type': 'FLOAT'},\n",
    "        ],\n",
    "        write_disposition='WRITE_TRUNCATE',\n",
    "    )\n",
    "    \n",
    "    run_query = BigQueryInsertJobOperator(\n",
    "        task_id='aggregate_sales',\n",
    "        configuration={\n",
    "            'query': {\n",
    "                'query': '''\n",
    "                    CREATE OR REPLACE TABLE `{}.my_dataset.sales_summary` AS\n",
    "                    SELECT cliente_id, SUM(total) as total\n",
    "                    FROM `{}.my_dataset.ventas`\n",
    "                    GROUP BY cliente_id\n",
    "                '''.format(PROJECT_ID, PROJECT_ID),\n",
    "                'useLegacySql': False,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    create_dataset >> load_to_bq >> run_query\n",
    "```\n",
    "\n",
    "**Dataflow Operator:**\n",
    "```python\n",
    "from airflow.providers.google.cloud.operators.dataflow import (\n",
    "    DataflowCreatePythonJobOperator,\n",
    ")\n",
    "\n",
    "run_dataflow = DataflowCreatePythonJobOperator(\n",
    "    task_id='dataflow_etl',\n",
    "    py_file='gs://my-bucket/dataflow/pipeline.py',\n",
    "    job_name='ventas-etl',\n",
    "    options={\n",
    "        'project': PROJECT_ID,\n",
    "        'region': 'us-central1',\n",
    "        'tempLocation': 'gs://my-bucket/temp',\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "**Environment Variables & Connections:**\n",
    "\n",
    "```python\n",
    "from airflow.models import Variable\n",
    "\n",
    "# Airflow Variables (en UI o CLI)\n",
    "PROJECT_ID = Variable.get('gcp_project_id')\n",
    "BUCKET = Variable.get('data_bucket')\n",
    "\n",
    "# Connections (GCP \u2192 Airflow Connection)\n",
    "# ID: google_cloud_default\n",
    "# Type: Google Cloud\n",
    "# Keyfile JSON: [service account JSON]\n",
    "```\n",
    "\n",
    "**Pricing:**\n",
    "- Environment: ~$300-500/mes (peque\u00f1o)\n",
    "- Compute: Workers + scheduler\n",
    "- Storage: GCS para DAGs + logs\n",
    "- Database: Cloud SQL (managed)\n",
    "\n",
    "**Alternatives:**\n",
    "- Managed Airflow (MWAA en AWS)\n",
    "- Self-hosted Airflow (Kubernetes)\n",
    "- Cloud Workflows (simple orchestration)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af701db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG para Composer\n",
    "composer_dag = '''\n",
    "from airflow import DAG\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'ventas_daily_etl',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline diario de ventas',\n",
    "    schedule_interval='0 2 * * *',  # 2 AM daily\n",
    "    catchup=False,\n",
    "    tags=['ventas', 'etl', 'gcp'],\n",
    ") as dag:\n",
    "    \n",
    "    load_raw = GCSToBigQueryOperator(\n",
    "        task_id='load_raw_data',\n",
    "        bucket='my-datalake',\n",
    "        source_objects=['raw/ventas/{{ ds }}/*.csv'],\n",
    "        destination_project_dataset_table='project.dataset.ventas_raw',\n",
    "        write_disposition='WRITE_APPEND',\n",
    "        skip_leading_rows=1,\n",
    "    )\n",
    "    \n",
    "    transform = BigQueryInsertJobOperator(\n",
    "        task_id='transform_data',\n",
    "        configuration={\n",
    "            'query': {\n",
    "                'query': \"\"\"\n",
    "                    CREATE OR REPLACE TABLE `project.dataset.ventas_clean` AS\n",
    "                    SELECT \n",
    "                        venta_id,\n",
    "                        cliente_id,\n",
    "                        CAST(total AS FLOAT64) as total,\n",
    "                        DATE(fecha) as fecha\n",
    "                    FROM `project.dataset.ventas_raw`\n",
    "                    WHERE fecha = '{{ ds }}'\n",
    "                      AND total > 0\n",
    "                \"\"\",\n",
    "                'useLegacySql': False,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    load_raw >> transform\n",
    "\n",
    "# Subir a: gs://[composer-bucket]/dags/ventas_etl.py\n",
    "'''\n",
    "\n",
    "print(composer_dag)\n",
    "print('\\\\n\ud83d\udca1 Subir a Cloud Composer DAGs folder en GCS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b13f4",
   "metadata": {},
   "source": [
    "## 5. Cloud Run: Containerized Serverless Workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b1a26",
   "metadata": {},
   "source": [
    "### \u26a1 **Cloud Functions: Serverless para Datos**\n",
    "\n",
    "**Triggers Disponibles:**\n",
    "\n",
    "1. **HTTP**: API endpoints\n",
    "2. **Cloud Storage**: Object created/deleted/updated\n",
    "3. **Pub/Sub**: Message queue events\n",
    "4. **Firestore**: Document changes\n",
    "5. **Cloud Scheduler**: Cron jobs\n",
    "\n",
    "**Ejemplo: Procesar CSV al subir a GCS**\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by Cloud Storage when CSV uploaded.\n",
    "    \n",
    "    Args:\n",
    "        event (dict): Event payload (file metadata)\n",
    "        context (google.cloud.functions.Context): Event context\n",
    "    \"\"\"\n",
    "    file_name = event['name']\n",
    "    bucket_name = event['bucket']\n",
    "    \n",
    "    print(f'Processing file: gs://{bucket_name}/{file_name}')\n",
    "    \n",
    "    # Skip if not in raw/ prefix\n",
    "    if not file_name.startswith('raw/'):\n",
    "        print('Skipping non-raw file')\n",
    "        return\n",
    "    \n",
    "    # Read CSV from GCS\n",
    "    gcs_uri = f'gs://{bucket_name}/{file_name}'\n",
    "    df = pd.read_csv(gcs_uri)\n",
    "    \n",
    "    # Transform\n",
    "    df_clean = df.dropna()\n",
    "    df_clean['total'] = df_clean['total'].astype(float)\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    client = bigquery.Client()\n",
    "    table_id = 'project.dataset.ventas_processed'\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition='WRITE_APPEND',\n",
    "    )\n",
    "    \n",
    "    job = client.load_table_from_dataframe(df_clean, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    print(f'Loaded {len(df_clean)} rows to {table_id}')\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "google-cloud-bigquery\n",
    "google-cloud-storage\n",
    "pandas\n",
    "```\n",
    "\n",
    "**Deploy:**\n",
    "```bash\n",
    "gcloud functions deploy process_csv \\\\\n",
    "  --runtime python310 \\\\\n",
    "  --trigger-resource my-bucket \\\\\n",
    "  --trigger-event google.storage.object.finalize \\\\\n",
    "  --entry-point process_csv \\\\\n",
    "  --region us-central1 \\\\\n",
    "  --memory 512MB \\\\\n",
    "  --timeout 300s\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Idempotent functions (puede ejecutarse m\u00faltiples veces)\n",
    "- Timeout < 9 min (max 540s)\n",
    "- Lightweight dependencies (cold start impact)\n",
    "- Use Pub/Sub para retry logic\n",
    "- Cloud Run para workloads >9 min\n",
    "\n",
    "**Pub/Sub + Cloud Functions:**\n",
    "```python\n",
    "import base64\n",
    "import json\n",
    "\n",
    "def process_message(event, context):\n",
    "    \"\"\"Triggered from Pub/Sub topic\"\"\"\n",
    "    pubsub_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "    data = json.loads(pubsub_message)\n",
    "    \n",
    "    print(f'Processing message: {data}')\n",
    "    # ETL logic here\n",
    "```\n",
    "\n",
    "**Publish to Pub/Sub:**\n",
    "```bash\n",
    "gcloud pubsub topics publish data-events \\\\\n",
    "  --message '{\"file\": \"gs://bucket/data.csv\", \"type\": \"sales\"}'\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b987ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de Cloud Function completo\n",
    "cloud_function_example = '''\n",
    "# main.py - Cloud Function para validar y cargar datos\n",
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def validate_and_load(event, context):\n",
    "    \"\"\"\n",
    "    Cloud Function triggered por Cloud Storage.\n",
    "    Valida CSV y carga a BigQuery.\n",
    "    \"\"\"\n",
    "    file_name = event['name']\n",
    "    bucket_name = event['bucket']\n",
    "    \n",
    "    logging.info(f'File: gs://{bucket_name}/{file_name}')\n",
    "    \n",
    "    # Read from GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    content = blob.download_as_text()\n",
    "    \n",
    "    # Parse CSV\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "    \n",
    "    # Validations\n",
    "    required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f'Missing columns: {required_cols}')\n",
    "    \n",
    "    if df['total'].isnull().sum() > 0:\n",
    "        raise ValueError('Null values found in total column')\n",
    "    \n",
    "    # Clean\n",
    "    df['total'] = df['total'].astype(float)\n",
    "    df = df[df['total'] > 0]\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    bq_client = bigquery.Client()\n",
    "    table_id = 'project.dataset.ventas'\n",
    "    \n",
    "    job = bq_client.load_table_from_dataframe(\n",
    "        df, \n",
    "        table_id,\n",
    "        job_config=bigquery.LoadJobConfig(\n",
    "            write_disposition='WRITE_APPEND'\n",
    "        )\n",
    "    )\n",
    "    job.result()\n",
    "    \n",
    "    logging.info(f'\u2705 Loaded {len(df)} rows')\n",
    "    return f'Success: {len(df)} rows'\n",
    "\n",
    "# Deploy:\n",
    "# gcloud functions deploy validate_and_load \\\\\n",
    "#   --runtime python310 \\\\\n",
    "#   --trigger-resource my-bucket \\\\\n",
    "#   --trigger-event google.storage.object.finalize \\\\\n",
    "#   --entry-point validate_and_load\n",
    "'''\n",
    "\n",
    "print(cloud_function_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b6922",
   "metadata": {},
   "source": [
    "## 5. Cloud Run: Containerized Workloads Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe4b9c",
   "metadata": {},
   "source": [
    "### \ud83d\udc33 **Cloud Run: Contenedores Serverless para Cargas Personalizadas**\n",
    "\n",
    "**\u00bfQu\u00e9 es Cloud Run?**\n",
    "\n",
    "Cloud Run es una plataforma serverless completamente administrada que ejecuta contenedores Docker sin l\u00edmite de 9 minutos (diferencia clave vs Cloud Functions).\n",
    "\n",
    "**Caracter\u00edsticas Principales:**\n",
    "\n",
    "- **Basado en Kubernetes**: Compatible con est\u00e1ndar Knative\n",
    "- **Sin l\u00edmite de tiempo**: Soporta procesos largos (hasta horas)\n",
    "- **Request-based scaling**: Escala de 0 a N instancias seg\u00fan tr\u00e1fico\n",
    "- **Portable**: C\u00f3digo se puede mover entre Cloud Run, GKE, on-prem\n",
    "- **Pricing**: Paga por CPU/memoria durante tiempo de ejecuci\u00f3n + requests\n",
    "\n",
    "**Cu\u00e1ndo usar Cloud Run vs Cloud Functions:**\n",
    "\n",
    "| Aspecto | Cloud Run | Cloud Functions |\n",
    "|---------|-----------|-----------------|\n",
    "| **Tiempo l\u00edmite** | Sin l\u00edmite pr\u00e1ctico | 9 min max |\n",
    "| **Lenguajes** | Cualquiera (Docker) | Python, Node, Go, Java |\n",
    "| **Dependencias** | Control total | Limitado por entorno |\n",
    "| **Custom binaries** | \u2705 S\u00ed | \u274c No |\n",
    "| **Cold start** | ~1-3s | ~0.5-1s |\n",
    "| **HTTP/gRPC** | \u2705 Ambos | Solo HTTP |\n",
    "| **Use case** | ETL largo, APIs, ML inference | Event handling, triggers r\u00e1pidos |\n",
    "\n",
    "**Arquitectura para Data Engineering:**\n",
    "\n",
    "```\n",
    "1. ETL LARGO (>9 min)\n",
    "   Cloud Scheduler \u2192 Cloud Run \u2192 BigQuery\n",
    "                       \u2193\n",
    "                  Cloud Storage (resultados)\n",
    "\n",
    "2. API DE DATOS\n",
    "   Load Balancer \u2192 Cloud Run Service\n",
    "                       \u2193\n",
    "                  BigQuery / Cloud SQL\n",
    "\n",
    "3. ML INFERENCE\n",
    "   Pub/Sub \u2192 Cloud Run \u2192 Modelo \u2192 BigQuery\n",
    "```\n",
    "\n",
    "**Ejemplo: ETL Python en Cloud Run**\n",
    "\n",
    "**Dockerfile:**\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Instalar dependencias\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copiar c\u00f3digo\n",
    "COPY main.py .\n",
    "\n",
    "# Variable de entorno para puerto\n",
    "ENV PORT=8080\n",
    "\n",
    "# Comando de inicio\n",
    "CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 main:app\n",
    "```\n",
    "\n",
    "**main.py:**\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "@app.route('/process-sales', methods=['POST'])\n",
    "def process_sales():\n",
    "    \"\"\"\n",
    "    Endpoint para procesar ventas desde GCS a BigQuery.\n",
    "    Puede tomar >9 minutos sin problemas.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    bucket_name = data.get('bucket')\n",
    "    prefix = data.get('prefix', 'raw/sales/')\n",
    "    \n",
    "    # Leer todos los CSV desde GCS (proceso largo)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    \n",
    "    dfs = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.csv'):\n",
    "            content = blob.download_as_bytes()\n",
    "            df = pd.read_csv(pd.io.common.BytesIO(content))\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Combinar y transformar\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Limpieza\n",
    "    df_clean = df_combined.dropna()\n",
    "    df_clean['total'] = df_clean['total'].astype(float)\n",
    "    \n",
    "    # Cargar a BigQuery (proceso largo)\n",
    "    table_id = 'project.dataset.sales'\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition='WRITE_APPEND',\n",
    "    )\n",
    "    \n",
    "    job = bq_client.load_table_from_dataframe(\n",
    "        df_clean, \n",
    "        table_id, \n",
    "        job_config=job_config\n",
    "    )\n",
    "    job.result()  # Esperar a que termine\n",
    "    \n",
    "    return jsonify({\n",
    "        'status': 'success',\n",
    "        'rows_processed': len(df_clean),\n",
    "        'table': table_id\n",
    "    })\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return 'OK', 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "flask\n",
    "gunicorn\n",
    "google-cloud-bigquery\n",
    "google-cloud-storage\n",
    "pandas\n",
    "```\n",
    "\n",
    "**Deploy a Cloud Run:**\n",
    "```bash\n",
    "# Build container\n",
    "gcloud builds submit --tag gcr.io/PROJECT_ID/sales-etl\n",
    "\n",
    "# Deploy a Cloud Run\n",
    "gcloud run deploy sales-etl \\\\\n",
    "    --image gcr.io/PROJECT_ID/sales-etl \\\\\n",
    "    --platform managed \\\\\n",
    "    --region us-central1 \\\\\n",
    "    --allow-unauthenticated \\\\\n",
    "    --memory 2Gi \\\\\n",
    "    --cpu 2 \\\\\n",
    "    --timeout 3600 \\\\\n",
    "    --max-instances 10 \\\\\n",
    "    --set-env-vars PROJECT_ID=my-project\n",
    "\n",
    "# Obtener URL\n",
    "SERVICE_URL=$(gcloud run services describe sales-etl --region us-central1 --format 'value(status.url)')\n",
    "\n",
    "# Invocar desde Cloud Scheduler (cron diario)\n",
    "gcloud scheduler jobs create http daily-sales-etl \\\\\n",
    "    --schedule=\"0 2 * * *\" \\\\\n",
    "    --uri=\"${SERVICE_URL}/process-sales\" \\\\\n",
    "    --http-method=POST \\\\\n",
    "    --message-body='{\"bucket\":\"my-bucket\",\"prefix\":\"raw/sales/\"}'\n",
    "```\n",
    "\n",
    "**Autoscaling y Concurrency:**\n",
    "\n",
    "```bash\n",
    "# Configurar autoscaling\n",
    "gcloud run services update sales-etl \\\\\n",
    "    --min-instances 0 \\\\\n",
    "    --max-instances 100 \\\\\n",
    "    --concurrency 10 \\\\\n",
    "    --cpu-throttling \\\\\n",
    "    --region us-central1\n",
    "\n",
    "# 0 instancias cuando no hay tr\u00e1fico (scale-to-zero)\n",
    "# Hasta 100 instancias bajo carga\n",
    "# 10 requests concurrentes por instancia\n",
    "```\n",
    "\n",
    "**VPC Connector (acceso a recursos privados):**\n",
    "\n",
    "```bash\n",
    "# Crear VPC connector\n",
    "gcloud compute networks vpc-access connectors create my-connector \\\\\n",
    "    --region us-central1 \\\\\n",
    "    --range 10.8.0.0/28\n",
    "\n",
    "# Configurar Cloud Run para usar VPC\n",
    "gcloud run services update sales-etl \\\\\n",
    "    --vpc-connector my-connector \\\\\n",
    "    --region us-central1\n",
    "\n",
    "# Ahora puede acceder a Cloud SQL, Memorystore (Redis), etc.\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "\n",
    "```python\n",
    "# Agregar logging estructurado\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Log en formato JSON para Cloud Logging\n",
    "log_entry = {\n",
    "    'severity': 'INFO',\n",
    "    'message': 'Processing batch',\n",
    "    'rows': len(df),\n",
    "    'duration_seconds': elapsed_time\n",
    "}\n",
    "print(json.dumps(log_entry))\n",
    "\n",
    "# M\u00e9tricas custom\n",
    "from opencensus.ext.stackdriver import stats_exporter\n",
    "exporter = stats_exporter.new_stats_exporter()\n",
    "```\n",
    "\n",
    "**Pricing:**\n",
    "- CPU: $0.00002400 por vCPU-segundo\n",
    "- Memory: $0.00000250 por GB-segundo\n",
    "- Requests: $0.40 por 1M requests\n",
    "- Networking: Egress est\u00e1ndar\n",
    "\n",
    "**Ejemplo de costo:**\n",
    "- 1,000 requests/d\u00eda\n",
    "- 2 vCPU, 2GB RAM\n",
    "- 60s por request promedio\n",
    "- **Costo mensual: ~$9**\n",
    "\n",
    "**Ventajas de Cloud Run:**\n",
    "- \u2705 Sin l\u00edmite de tiempo (vs 9 min Functions)\n",
    "- \u2705 Portabilidad (Docker est\u00e1ndar)\n",
    "- \u2705 Control total de dependencias\n",
    "- \u2705 gRPC nativo (ideal para ML serving)\n",
    "- \u2705 Blue/green deployments built-in\n",
    "- \u2705 Traffic splitting (canary releases)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba83d9f",
   "metadata": {},
   "source": [
    "## 7. Comparaci\u00f3n: GCP vs AWS vs Azure\n",
    "\n",
    "### \ud83d\udd04 **Multi-Cloud Comparison**\n",
    "\n",
    "| Servicio | GCP | AWS | Azure |\n",
    "|----------|-----|-----|-------|\n",
    "| **Object Storage** | Cloud Storage | S3 | Blob Storage |\n",
    "| **Data Warehouse** | BigQuery | Redshift | Synapse Analytics |\n",
    "| **ETL Serverless** | Dataflow (Beam) | Glue (PySpark) | Data Factory |\n",
    "| **Streaming** | Pub/Sub + Dataflow | Kinesis + Lambda | Event Hubs + Stream Analytics |\n",
    "| **Orchestration** | Cloud Composer | MWAA (Airflow) | Data Factory |\n",
    "| **Serverless Compute** | Cloud Functions | Lambda | Functions |\n",
    "| **Notebooks** | Vertex AI Workbench | SageMaker | Machine Learning Studio |\n",
    "| **ML Platform** | Vertex AI | SageMaker | Azure ML |\n",
    "\n",
    "**Cu\u00e1ndo elegir cada cloud:**\n",
    "\n",
    "**GCP:**\n",
    "- \u2705 BigQuery (mejor Data Warehouse serverless)\n",
    "- \u2705 Kubernetes-first (GKE es l\u00edder)\n",
    "- \u2705 ML/AI (TensorFlow, Vertex AI)\n",
    "- \u2705 Pricing transparente\n",
    "- \u274c Menos servicios que AWS\n",
    "- \u274c Menor presencia enterprise\n",
    "\n",
    "**AWS:**\n",
    "- \u2705 M\u00e1s servicios (200+)\n",
    "- \u2705 Mayor adoption (33% market share)\n",
    "- \u2705 Mejor documentaci\u00f3n/comunidad\n",
    "- \u2705 Compliance certifications m\u00e1s amplio\n",
    "- \u274c Pricing complejo\n",
    "- \u274c Muchos servicios legacy\n",
    "\n",
    "**Azure:**\n",
    "- \u2705 Integraci\u00f3n Microsoft (Active Directory, Office 365)\n",
    "- \u2705 H\u00edbrido (on-prem + cloud con Azure Arc)\n",
    "- \u2705 Windows workloads\n",
    "- \u274c Curva de aprendizaje pronunciada\n",
    "- \u274c Documentaci\u00f3n inconsistente\n",
    "\n",
    "**Arquitectura Multi-Cloud:**\n",
    "```\n",
    "On-Premise \u2192 Azure (ingesta + AD)\n",
    "              \u2193\n",
    "          Cloud Storage (staging)\n",
    "              \u2193\n",
    "       BigQuery (analytics) \u2190 Looker (BI)\n",
    "              \u2193\n",
    "      S3 (archival) \u2192 Glacier\n",
    "```\n",
    "\n",
    "**Portabilidad:**\n",
    "- Terraform para IaC multi-cloud\n",
    "- Apache Beam (portable Dataflow/Flink/Spark)\n",
    "- Kubernetes para compute portable\n",
    "- Parquet/Avro para data formats\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8ca08",
   "metadata": {},
   "source": [
    "## 8. Ejercicios Pr\u00e1cticos\n",
    "\n",
    "### \ud83d\udcdd **Ejercicios Hands-On**\n",
    "\n",
    "**Ejercicio 1: Cloud Storage + BigQuery Pipeline**\n",
    "- Objetivo: Crear un pipeline completo desde GCS a BigQuery\n",
    "- Tareas:\n",
    "  1. Crear bucket con lifecycle policy (Nearline despu\u00e9s de 30 d\u00edas)\n",
    "  2. Subir archivo CSV particionado por fecha (`raw/ventas/2025/10/30/`)\n",
    "  3. Crear dataset BigQuery\n",
    "  4. Cargar CSV a tabla particionada por `DATE(fecha)`\n",
    "  5. Ejecutar query agregada con costo < $0.01\n",
    "  6. Validar con `INFORMATION_SCHEMA.JOBS_BY_PROJECT`\n",
    "\n",
    "**Ejercicio 2: Dataflow ETL Batch**\n",
    "- Objetivo: Procesar archivos CSV con Apache Beam\n",
    "- Tareas:\n",
    "  1. Crear script Beam que lea m\u00faltiples CSVs desde GCS\n",
    "  2. Implementar transformaciones: filtrar nulos, validar rangos\n",
    "  3. Agregar por `cliente_id` (total ventas, cantidad)\n",
    "  4. Escribir resultados a BigQuery\n",
    "  5. Ejecutar con DirectRunner (local) y DataflowRunner (cloud)\n",
    "  6. Comparar tiempos y costos\n",
    "\n",
    "**Ejercicio 3: Cloud Composer DAG Completo**\n",
    "- Objetivo: Orquestar pipeline end-to-end\n",
    "- Tareas:\n",
    "  1. Task 1: `GCSObjectExistenceSensor` - validar archivos\n",
    "  2. Task 2: `DataflowCreatePythonJobOperator` - ejecutar ETL\n",
    "  3. Task 3: `BigQueryInsertJobOperator` - crear tabla agregada\n",
    "  4. Task 4: `BigQueryCheckOperator` - validar calidad datos\n",
    "  5. Task 5 (condicional): `EmailOperator` si hay errores\n",
    "  6. Programar con `schedule_interval='@daily'`\n",
    "\n",
    "**Ejercicio 4: Cloud Function Event-Driven**\n",
    "- Objetivo: Procesar archivos autom\u00e1ticamente al subir\n",
    "- Tareas:\n",
    "  1. Crear function con trigger `google.storage.object.finalize`\n",
    "  2. Validar schema CSV (columnas requeridas, tipos)\n",
    "  3. Si v\u00e1lido \u2192 Publicar mensaje Pub/Sub topic `valid-data`\n",
    "  4. Si inv\u00e1lido \u2192 Mover a `gs://bucket/errors/` y loguear\n",
    "  5. Deploy y testear con `gsutil cp`\n",
    "  6. Monitorear con Cloud Logging\n",
    "\n",
    "**Ejercicio 5: Cloud Run ETL Largo**\n",
    "- Objetivo: Procesar workload >9 minutos\n",
    "- Tareas:\n",
    "  1. Crear Dockerfile con Python + pandas + google-cloud-bigquery\n",
    "  2. Implementar Flask app con endpoint `/process-sales`\n",
    "  3. Leer 100+ archivos CSV desde GCS (simular carga pesada)\n",
    "  4. Agregar y escribir a BigQuery\n",
    "  5. Deploy a Cloud Run con 2GB RAM, timeout 3600s\n",
    "  6. Configurar Cloud Scheduler para ejecuci\u00f3n diaria 2 AM\n",
    "  7. Implementar logging JSON estructurado\n",
    "\n",
    "**Ejercicio 6: BigQuery Optimization Challenge**\n",
    "- Objetivo: Reducir costos de queries\n",
    "- Tareas:\n",
    "  1. Crear tabla `ventas_raw` sin particionar (10M registros)\n",
    "  2. Ejecutar query agregada y medir bytes escaneados\n",
    "  3. Crear `ventas_partitioned` (por fecha)\n",
    "  4. Crear `ventas_clustered` (partici\u00f3n + cluster por cliente)\n",
    "  5. Comparar costos: raw vs partitioned vs clustered\n",
    "  6. Implementar materialized view para agregaciones frecuentes\n",
    "  7. Usar `APPROX_COUNT_DISTINCT` para estimaciones r\u00e1pidas\n",
    "\n",
    "**Ejercicio 7: BigQuery ML Predicci\u00f3n**\n",
    "- Objetivo: Entrenar modelo sin salir de BigQuery\n",
    "- Tareas:\n",
    "  1. Crear modelo de clasificaci\u00f3n: `churned` (0/1)\n",
    "  2. Features: `total_compras`, `dias_ultima_compra`, `edad`\n",
    "  3. Split train/test con `ML.SPLIT`\n",
    "  4. Evaluar con `ML.EVALUATE`\n",
    "  5. Predecir nuevos clientes con `ML.PREDICT`\n",
    "  6. Exportar modelo para serving externo\n",
    "\n",
    "**Recursos de Aprendizaje:**\n",
    "- [GCP Free Tier](https://cloud.google.com/free) - $300 cr\u00e9dito por 90 d\u00edas\n",
    "- [BigQuery Sandbox](https://cloud.google.com/bigquery/docs/sandbox) - Sin billing requerido\n",
    "- [Qwiklabs: Data Engineering](https://www.qwiklabs.com/quests/132)\n",
    "- [Coursera: Data Engineering on GCP](https://www.coursera.org/specializations/gcp-data-machine-learning)\n",
    "- [Cloud Run Quickstarts](https://cloud.google.com/run/docs/quickstarts)\n",
    "- [Apache Beam Katas](https://github.com/apache/beam/tree/master/learning/katas)\n",
    "\n",
    "**Tips para Practicar:**\n",
    "- Usa datasets p\u00fablicos de BigQuery para experimentar gratis\n",
    "- Limita queries con `LIMIT` para reducir costos\n",
    "- Prueba Dataflow con DirectRunner antes de cloud\n",
    "- Usa `--dry-run` en gcloud para validar comandos\n",
    "- Monitorea costos en [Cloud Billing](https://console.cloud.google.com/billing)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8cbd5",
   "metadata": {},
   "source": [
    "## 9. Conclusi\u00f3n\n",
    "\n",
    "### \ud83c\udfaf **Key Takeaways de GCP para Data Engineering**\n",
    "\n",
    "**Fortalezas de GCP:**\n",
    "\n",
    "1. **BigQuery es excepcional:**\n",
    "   - Queries extremadamente r\u00e1pidas sobre TB/PB\n",
    "   - Serverless (no tuning de clusters)\n",
    "   - BigQuery ML integrado (modelos sin salir de SQL)\n",
    "   - Flat-rate pricing predecible para cargas pesadas\n",
    "   - Result caching autom\u00e1tico (48h gratis)\n",
    "\n",
    "2. **Cloud Run ofrece flexibilidad \u00fanica:**\n",
    "   - Sin l\u00edmite de tiempo (vs 9 min Functions)\n",
    "   - Contenedores Docker est\u00e1ndar (portabilidad)\n",
    "   - Auto-scaling de 0 a miles de instancias\n",
    "   - Ideal para ETL largo, APIs, ML serving\n",
    "\n",
    "3. **Dataflow con Apache Beam:**\n",
    "   - Unified model (mismo c\u00f3digo batch + streaming)\n",
    "   - Portable a otros runners (Flink, Spark)\n",
    "   - Auto-scaling inteligente\n",
    "   - Windowing avanzado para streaming\n",
    "\n",
    "4. **Integraci\u00f3n cohesiva:**\n",
    "   - IAM unificado (service accounts)\n",
    "   - Cloud Logging/Monitoring centralizado\n",
    "   - Stackdriver para observabilidad\n",
    "   - Vertex AI para ML lifecycle completo\n",
    "\n",
    "**Limitaciones:**\n",
    "\n",
    "- Menos servicios que AWS (pero m\u00e1s enfocados)\n",
    "- Lock-in en BigQuery (aunque usa Standard SQL)\n",
    "- Menor comunidad/recursos que AWS\n",
    "- Algunas features enterprise menos maduras\n",
    "\n",
    "**Comparaci\u00f3n Final:**\n",
    "\n",
    "| Aspecto | GCP | AWS | Azure |\n",
    "|---------|-----|-----|-------|\n",
    "| **DW Serverless** | \u2b50\u2b50\u2b50\u2b50\u2b50 BigQuery | \u2b50\u2b50\u2b50\u2b50 Athena | \u2b50\u2b50\u2b50 Synapse Serverless |\n",
    "| **Serverless Compute** | \u2b50\u2b50\u2b50\u2b50\u2b50 Cloud Run | \u2b50\u2b50\u2b50\u2b50 Lambda | \u2b50\u2b50\u2b50 Functions |\n",
    "| **Pricing Clarity** | \u2b50\u2b50\u2b50\u2b50\u2b50 Muy claro | \u2b50\u2b50\u2b50 Complejo | \u2b50\u2b50\u2b50 Medio |\n",
    "| **ML/AI** | \u2b50\u2b50\u2b50\u2b50\u2b50 Vertex AI | \u2b50\u2b50\u2b50\u2b50 SageMaker | \u2b50\u2b50\u2b50 Azure ML |\n",
    "| **Kubernetes** | \u2b50\u2b50\u2b50\u2b50\u2b50 GKE (creadores) | \u2b50\u2b50\u2b50\u2b50 EKS | \u2b50\u2b50\u2b50\u2b50 AKS |\n",
    "| **Enterprise** | \u2b50\u2b50\u2b50 Menor adoption | \u2b50\u2b50\u2b50\u2b50\u2b50 L\u00edder | \u2b50\u2b50\u2b50\u2b50 Microsoft stack |\n",
    "\n",
    "**Cu\u00e1ndo elegir GCP:**\n",
    "\n",
    "\u2705 **Elige GCP si:**\n",
    "- BigQuery es tu DW principal (mejor del mercado)\n",
    "- ML/AI es cr\u00edtico (TensorFlow nativo, TPUs)\n",
    "- Kubernetes-heavy (GKE es el mejor)\n",
    "- Quieres pricing simple y transparente\n",
    "- Startup/tech company (cultura cloud-native)\n",
    "\n",
    "\u274c **Evita GCP si:**\n",
    "- Necesitas 200+ servicios especializados (AWS tiene m\u00e1s)\n",
    "- Microsoft stack es mandatorio (Active Directory, etc.)\n",
    "- Compliance muy espec\u00edfico (AWS tiene m\u00e1s certifications)\n",
    "- Equipo ya experto en AWS/Azure\n",
    "\n",
    "**Pr\u00f3ximos Pasos:**\n",
    "\n",
    "1. \u2705 **Crear cuenta GCP** ([free tier $300](https://cloud.google.com/free))\n",
    "2. \u2705 **Completar Qwiklab:** [Data Engineering on GCP](https://www.qwiklabs.com/quests/132)\n",
    "3. \u2705 **Certificaci\u00f3n:** [Professional Data Engineer](https://cloud.google.com/certification/data-engineer)\n",
    "4. \u2705 **Explorar:**\n",
    "   - Vertex AI Pipelines (ML workflow)\n",
    "   - Dataplex (data governance)\n",
    "   - BigQuery Omni (multi-cloud)\n",
    "   - Cloud Spanner (global SQL)\n",
    "\n",
    "**Stack Recomendado GCP:**\n",
    "\n",
    "```\n",
    "Ingesta: Pub/Sub + Dataflow\n",
    "Storage: Cloud Storage (raw) + BigQuery (curated)\n",
    "Compute: Cloud Run (ETL) + Dataflow (batch/stream)\n",
    "Orchestration: Cloud Composer (Airflow)\n",
    "Serving: BigQuery + Looker (BI)\n",
    "ML: Vertex AI + BigQuery ML\n",
    "Governance: Dataplex + Data Catalog\n",
    "```\n",
    "\n",
    "**Happy data engineering en GCP! \ud83d\ude80**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "\u00a9 2024-2025 - Data Engineering Modular Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks \u2192](03c_cloud_azure.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
