{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca158fb0",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è GCP para Ingenier√≠a de Datos: BigQuery, Cloud Storage, Dataflow y Composer\n",
    "\n",
    "Este notebook introduce el ecosistema de Google Cloud Platform (GCP) para Data Engineering, cubriendo almacenamiento en Cloud Storage, transformaciones con BigQuery y Dataflow, y orquestaci√≥n con Cloud Composer.\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.)  \n",
    "**Nivel:** Mid  \n",
    "**Duraci√≥n:** 90-120 minutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679bb9b",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è RECORDATORIO IMPORTANTE\n",
    "\n",
    "### üö® NOTEBOOKS vs PRODUCCI√ìN\n",
    "\n",
    "Este curso usa notebooks para **ense√±anza**, pero en tu trabajo real:\n",
    "\n",
    "**‚ùå NO uses notebooks para pipelines en producci√≥n**\n",
    "\n",
    "**‚úÖ USA:**\n",
    "- Scripts Python modulares en `src/`\n",
    "- Cloud Composer DAGs (Airflow managed)\n",
    "- Cloud Functions para event-driven processing\n",
    "- Cloud Build para CI/CD\n",
    "- Terraform para Infrastructure as Code\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | ¬© 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956216b",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecuci√≥n\n",
    "\n",
    "- Para ejecutar c√≥digo real de GCP necesitas:\n",
    "  - Proyecto GCP creado\n",
    "  - Credenciales configuradas (`gcloud auth login` o service account JSON)\n",
    "  - Billing habilitado\n",
    "  - APIs habilitadas: BigQuery, Cloud Storage, Dataflow, Composer\n",
    "- **Nunca subas credenciales al repositorio**\n",
    "- Usa variables de entorno o Google Cloud SDK\n",
    "- Este notebook muestra ejemplos ejecutables si tienes un proyecto GCP configurado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a05799",
   "metadata": {},
   "source": [
    "### ‚òÅÔ∏è **Google Cloud Platform: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de GCP para Datos:**\n",
    "\n",
    "1. **Cloud Storage (GCS)**: Object storage para Data Lakes\n",
    "   - Similar a S3, pero con modelo de consistencia fuerte desde el inicio\n",
    "   - Clases: Standard, Nearline (30d), Coldline (90d), Archive (365d)\n",
    "   - Pricing: ~$0.020/GB/mes (Standard en multi-region)\n",
    "\n",
    "2. **BigQuery**: Data Warehouse serverless con SQL\n",
    "   - Almacenamiento columnar comprimido\n",
    "   - Separaci√≥n compute/storage (paga solo por queries ejecutadas)\n",
    "   - Pricing: $5/TB escaneado (on-demand) o flat-rate mensual\n",
    "   - Streaming inserts: $0.01 per 200MB\n",
    "\n",
    "3. **Dataflow**: Procesamiento stream/batch con Apache Beam\n",
    "   - Serverless, auto-scaling\n",
    "   - Unified model: mismo c√≥digo para batch y streaming\n",
    "   - Pricing: por vCPU-hora + GB-hora (workers)\n",
    "\n",
    "4. **Cloud Composer**: Airflow totalmente administrado\n",
    "   - DAGs en Python, GKE-based\n",
    "   - Integraci√≥n nativa con servicios GCP\n",
    "   - Pricing: por tama√±o de environment + compute\n",
    "\n",
    "5. **Cloud Functions**: Funciones serverless event-driven\n",
    "   - Triggers: HTTP, Cloud Storage, Pub/Sub, Firestore\n",
    "   - Runtime: Python 3.7-3.11, Node.js, Go, Java\n",
    "   - Pricing: por invocaciones + compute time\n",
    "\n",
    "**Arquitectura de Referencia:**\n",
    "```\n",
    "Fuentes ‚Üí [Pub/Sub] ‚Üí Cloud Storage (raw) ‚Üí [Dataflow/Cloud Functions] \n",
    "                           ‚Üì\n",
    "                    Cloud Storage (curated) ‚Üí BigQuery (anal√≠tica)\n",
    "                           ‚Üì\n",
    "                    Cloud Composer (orquestaci√≥n)\n",
    "```\n",
    "\n",
    "**Ventajas de GCP para Datos:**\n",
    "- **BigQuery**: Queries extremadamente r√°pidas (MPP distribuido)\n",
    "- **Integraci√≥n ML**: BigQuery ML, Vertex AI\n",
    "- **Consistencia fuerte**: No eventual consistency issues\n",
    "- **Kubernetes nativo**: GKE para workloads custom\n",
    "\n",
    "**Comparaci√≥n con AWS:**\n",
    "\n",
    "| Servicio | GCP | AWS |\n",
    "|----------|-----|-----|\n",
    "| Object Storage | Cloud Storage | S3 |\n",
    "| Data Warehouse | BigQuery | Redshift |\n",
    "| ETL Serverless | Dataflow (Beam) | Glue (PySpark) |\n",
    "| Streaming | Dataflow + Pub/Sub | Kinesis + Lambda |\n",
    "| Orchestration | Cloud Composer | MWAA (Airflow) |\n",
    "| Functions | Cloud Functions | Lambda |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8443f3",
   "metadata": {},
   "source": [
    "## 1. Cloud Storage: Data Lake B√°sico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba432dd6",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è **Cloud Storage: Fundamentos**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global √∫nico\n",
    "  - Nombre: `mi-data-lake-gcp` (min√∫sculas, n√∫meros, guiones)\n",
    "  - Location: `us-central1`, `europe-west1`, `us` (multi-region)\n",
    "  - Storage class: Standard, Nearline, Coldline, Archive\n",
    "\n",
    "- **Object**: Archivo con metadata\n",
    "  - Key: `gs://bucket/path/to/file.csv`\n",
    "  - Metadata: Content-Type, custom headers\n",
    "  - Versionamiento: Object Versioning (similar a S3)\n",
    "\n",
    "**Organizacion Recomendada:**\n",
    "\n",
    "```\n",
    "gs://my-datalake/\n",
    "‚îú‚îÄ‚îÄ raw/                    ‚Üê Datos crudos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sales/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025/10/30/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ customers/\n",
    "‚îú‚îÄ‚îÄ staging/                ‚Üê Datos en proceso\n",
    "‚îî‚îÄ‚îÄ curated/                ‚Üê Datos procesados\n",
    "    ‚îú‚îÄ‚îÄ sales_aggregated/\n",
    "    ‚îî‚îÄ‚îÄ customer_metrics/\n",
    "```\n",
    "\n",
    "**Lifecycle Management:**\n",
    "```json\n",
    "{\n",
    "  \"lifecycle\": {\n",
    "    \"rule\": [\n",
    "      {\n",
    "        \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n",
    "        \"condition\": {\"age\": 30}\n",
    "      },\n",
    "      {\n",
    "        \"action\": {\"type\": \"Delete\"},\n",
    "        \"condition\": {\"age\": 365}\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Operaciones con Python Client:**\n",
    "```python\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "# Crear bucket\n",
    "bucket = client.create_bucket('my-bucket', location='us-central1')\n",
    "\n",
    "# Subir archivo\n",
    "blob = bucket.blob('raw/ventas.csv')\n",
    "blob.upload_from_filename('ventas.csv')\n",
    "\n",
    "# Listar objetos\n",
    "for blob in bucket.list_blobs(prefix='raw/'):\n",
    "    print(blob.name)\n",
    "\n",
    "# Descargar archivo\n",
    "blob.download_to_filename('downloaded.csv')\n",
    "```\n",
    "\n",
    "**Costos:**\n",
    "- Storage: $0.020/GB/mes (Standard)\n",
    "- Class A operations (write): $0.05 per 10,000 ops\n",
    "- Class B operations (read): $0.004 per 10,000 ops\n",
    "- Network egress: $0.12/GB (fuera de GCP)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be43d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n (sin ejecutar sin proyecto GCP real)\n",
    "PROJECT_ID = 'mi-proyecto-gcp'\n",
    "BUCKET_NAME = f'{PROJECT_ID}-datalake'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f'üìã Configuraci√≥n: {PROJECT_ID} / {BUCKET_NAME}')\n",
    "print('‚ö†Ô∏è Requiere: gcloud auth login o service account JSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47267bf",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket y subir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ea755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥digo (descomentar con proyecto real)\n",
    "'''\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializar cliente\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Crear bucket\n",
    "try:\n",
    "    bucket = client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
    "    print(f'‚úÖ Bucket {BUCKET_NAME} creado')\n",
    "except Exception as e:\n",
    "    print(f'‚ÑπÔ∏è Bucket ya existe o error: {e}')\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Subir CSV\n",
    "df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "csv_string = df.to_csv(index=False)\n",
    "\n",
    "blob = bucket.blob('raw/ventas/ventas_2025_10.csv')\n",
    "blob.upload_from_string(csv_string, content_type='text/csv')\n",
    "print('üì§ Archivo subido a Cloud Storage')\n",
    "'''\n",
    "print('C√≥digo de ejemplo listo para ejecutar con proyecto GCP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37644226",
   "metadata": {},
   "source": [
    "## 2. BigQuery: Data Warehouse Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db8274",
   "metadata": {},
   "source": [
    "### üìä **BigQuery: SQL Analytics a Escala**\n",
    "\n",
    "**Arquitectura:**\n",
    "\n",
    "BigQuery separa **storage** y **compute**:\n",
    "- Storage: Columnar format (Capacitor), compresi√≥n autom√°tica\n",
    "- Compute: Dremel engine (MPP distribuido con miles de workers)\n",
    "\n",
    "**Beneficios:**\n",
    "- Queries sobre TB/PB en segundos\n",
    "- No administrar clusters (100% serverless)\n",
    "- Standard SQL (ANSI SQL 2011 compatible)\n",
    "- Integraci√≥n con herramientas BI (Looker, Tableau, Data Studio)\n",
    "\n",
    "**Conceptos:**\n",
    "\n",
    "1. **Dataset**: Contenedor l√≥gico de tablas\n",
    "   - Similar a \"database\" en SQL tradicional\n",
    "   - Permisos a nivel dataset\n",
    "\n",
    "2. **Table**: Datos estructurados\n",
    "   - Native tables: Datos en BigQuery storage\n",
    "   - External tables: Data en GCS (federated queries)\n",
    "   - Partitioned tables: Por fecha/rango (reduce scan)\n",
    "   - Clustered tables: Por columnas espec√≠ficas (mejor performance)\n",
    "\n",
    "3. **View**: Query guardada\n",
    "   - Authorized views: Control de acceso granular\n",
    "\n",
    "**Particionamiento:**\n",
    "```sql\n",
    "-- Tabla particionada por fecha\n",
    "CREATE TABLE `project.dataset.sales_partitioned`\n",
    "PARTITION BY DATE(order_date)\n",
    "AS SELECT * FROM `project.dataset.sales_raw`;\n",
    "\n",
    "-- Query optimizada (scan solo 1 d√≠a)\n",
    "SELECT * FROM `project.dataset.sales_partitioned`\n",
    "WHERE order_date = '2025-10-30';\n",
    "```\n",
    "\n",
    "**Clustering:**\n",
    "```sql\n",
    "-- Tabla clusterizada por customer_id, product_id\n",
    "CREATE TABLE `project.dataset.sales_clustered`\n",
    "PARTITION BY DATE(order_date)\n",
    "CLUSTER BY customer_id, product_id\n",
    "AS SELECT * FROM `project.dataset.sales_raw`;\n",
    "\n",
    "-- Query beneficiada (pre-sorted data)\n",
    "SELECT * FROM `project.dataset.sales_clustered`\n",
    "WHERE customer_id = 12345;\n",
    "```\n",
    "\n",
    "**BigQuery ML (Machine Learning integrado):**\n",
    "```sql\n",
    "-- Crear modelo de regresi√≥n log√≠stica\n",
    "CREATE OR REPLACE MODEL `project.dataset.churn_model`\n",
    "OPTIONS(model_type='logistic_reg') AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  age,\n",
    "  total_purchases,\n",
    "  churned as label\n",
    "FROM `project.dataset.customers`;\n",
    "\n",
    "-- Predecir\n",
    "SELECT * FROM ML.PREDICT(\n",
    "  MODEL `project.dataset.churn_model`,\n",
    "  (SELECT * FROM `project.dataset.new_customers`)\n",
    ");\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Particionar tablas grandes (>1GB)\n",
    "- Evitar `SELECT *` (especifica columnas)\n",
    "- Usar `_TABLE_SUFFIX` para tablas wildcard\n",
    "- Aprovechar result caching (24h gratuito)\n",
    "- Usar slots reservation para cargas predecibles\n",
    "\n",
    "**Pricing:**\n",
    "- On-demand: $5/TB escaneado\n",
    "- Flat-rate: desde $2,000/mes (100 slots)\n",
    "- Storage: $0.020/GB active, $0.010/GB long-term (90d+)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n BigQuery\n",
    "DATASET_ID = 'data_engineering_course'\n",
    "TABLE_ID = 'ventas'\n",
    "\n",
    "print(f'üìä Dataset: {PROJECT_ID}.{DATASET_ID}')\n",
    "print(f'üìã Tabla: {TABLE_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7446df5",
   "metadata": {},
   "source": [
    "### 2.1 Crear dataset y tabla desde Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06140090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo BigQuery\n",
    "'''\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Crear dataset\n",
    "dataset_ref = client.dataset(DATASET_ID)\n",
    "try:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = LOCATION\n",
    "    dataset = client.create_dataset(dataset)\n",
    "    print(f'‚úÖ Dataset {DATASET_ID} creado')\n",
    "except Exception as e:\n",
    "    print(f'‚ÑπÔ∏è Dataset ya existe: {e}')\n",
    "\n",
    "# Cargar CSV desde GCS a BigQuery\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField('venta_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('cliente_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('producto_id', 'INTEGER'),\n",
    "        bigquery.SchemaField('cantidad', 'INTEGER'),\n",
    "        bigquery.SchemaField('total', 'FLOAT'),\n",
    "        bigquery.SchemaField('fecha', 'DATE'),\n",
    "    ],\n",
    "    skip_leading_rows=1,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    ")\n",
    "\n",
    "uri = f'gs://{BUCKET_NAME}/raw/ventas/ventas_2025_10.csv'\n",
    "table_ref = dataset_ref.table(TABLE_ID)\n",
    "\n",
    "load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
    "load_job.result()  # Wait for job to complete\n",
    "\n",
    "print(f'‚úÖ Cargados {load_job.output_rows} registros en {TABLE_ID}')\n",
    "'''\n",
    "print('C√≥digo BigQuery listo para ejecutar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb51a66",
   "metadata": {},
   "source": [
    "### 2.2 Queries SQL en BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d58f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de queries\n",
    "'''\n",
    "# Query simple\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "  cliente_id,\n",
    "  SUM(total) as total_ventas,\n",
    "  COUNT(*) as num_ventas\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "GROUP BY cliente_id\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "for row in results:\n",
    "    print(f'Cliente {row.cliente_id}: ${row.total_ventas:.2f} ({row.num_ventas} ventas)')\n",
    "\n",
    "# Metadata del job\n",
    "print(f'\\\\nüìä Query Stats:')\n",
    "print(f'  Bytes processed: {query_job.total_bytes_processed / 1e9:.2f} GB')\n",
    "print(f'  Bytes billed: {query_job.total_bytes_billed / 1e9:.2f} GB')\n",
    "print(f'  Cost estimate: ${(query_job.total_bytes_billed / 1e12) * 5:.4f}')\n",
    "'''\n",
    "print('Queries BigQuery con estimaci√≥n de costos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aaac5d",
   "metadata": {},
   "source": [
    "## 3. Dataflow: Procesamiento con Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8b57f",
   "metadata": {},
   "source": [
    "### üåä **Dataflow: Unified Stream/Batch Processing**\n",
    "\n",
    "**Apache Beam Concepts:**\n",
    "\n",
    "Beam es un modelo de programaci√≥n unificado para batch y streaming:\n",
    "\n",
    "```\n",
    "Pipeline ‚Üí PCollection ‚Üí Transform ‚Üí PCollection ‚Üí ...\n",
    "```\n",
    "\n",
    "- **Pipeline**: Grafo de transformaciones\n",
    "- **PCollection**: Conjunto distribuido de datos (immutable)\n",
    "- **Transform**: Operaci√≥n sobre PCollection (Map, Filter, GroupByKey, etc.)\n",
    "\n",
    "**Runners:**\n",
    "- DirectRunner: Local (testing)\n",
    "- DataflowRunner: GCP managed service\n",
    "- FlinkRunner: Apache Flink\n",
    "- SparkRunner: Apache Spark\n",
    "\n",
    "**Patr√≥n B√°sico:**\n",
    "```python\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions([\n",
    "    '--project=my-project',\n",
    "    '--region=us-central1',\n",
    "    '--runner=DataflowRunner',\n",
    "    '--temp_location=gs://my-bucket/temp',\n",
    "])\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (pipeline\n",
    "     | 'Read' >> beam.io.ReadFromText('gs://input/*.csv')\n",
    "     | 'Parse' >> beam.Map(lambda line: line.split(','))\n",
    "     | 'Filter' >> beam.Filter(lambda row: float(row[2]) > 100)\n",
    "     | 'Format' >> beam.Map(lambda row: f'{row[0]},{row[1]}')\n",
    "     | 'Write' >> beam.io.WriteToText('gs://output/result'))\n",
    "```\n",
    "\n",
    "**Windowing (Streaming):**\n",
    "```python\n",
    "from apache_beam import window\n",
    "\n",
    "(events\n",
    " | 'Window' >> beam.WindowInto(window.FixedWindows(60))  # 1-min windows\n",
    " | 'Sum' >> beam.CombinePerKey(sum)\n",
    " | 'Write' >> beam.io.WriteToBigQuery('project:dataset.table'))\n",
    "```\n",
    "\n",
    "**Transforms Comunes:**\n",
    "\n",
    "1. **ParDo** (Parallel Do):\n",
    "   ```python\n",
    "   class ExtractFields(beam.DoFn):\n",
    "       def process(self, element):\n",
    "           parts = element.split(',')\n",
    "           yield {'id': int(parts[0]), 'value': float(parts[1])}\n",
    "   \n",
    "   data | beam.ParDo(ExtractFields())\n",
    "   ```\n",
    "\n",
    "2. **GroupByKey**:\n",
    "   ```python\n",
    "   # (key, value) pairs ‚Üí (key, [values])\n",
    "   pairs | beam.GroupByKey()\n",
    "   ```\n",
    "\n",
    "3. **CombinePerKey**:\n",
    "   ```python\n",
    "   # M√°s eficiente que GroupByKey + Map\n",
    "   pairs | beam.CombinePerKey(sum)\n",
    "   ```\n",
    "\n",
    "4. **Flatten**:\n",
    "   ```python\n",
    "   # Merge m√∫ltiples PCollections\n",
    "   (pcoll1, pcoll2, pcoll3) | beam.Flatten()\n",
    "   ```\n",
    "\n",
    "**Side Inputs (Broadcasting):**\n",
    "```python\n",
    "lookup_table = (pipeline\n",
    "                | 'Read Lookup' >> beam.io.ReadFromText('gs://lookup.csv')\n",
    "                | 'Parse' >> beam.Map(lambda x: x.split(',')))\n",
    "\n",
    "main_data | beam.Map(\n",
    "    lambda x, table: enrich(x, table),\n",
    "    beam.pvalue.AsDict(lookup_table)\n",
    ")\n",
    "```\n",
    "\n",
    "**Dataflow vs Alternatives:**\n",
    "\n",
    "| Aspecto | Dataflow | Spark | Flink |\n",
    "|---------|----------|-------|-------|\n",
    "| **Model** | Beam (unified) | RDD/DataFrame | DataStream |\n",
    "| **Serverless** | ‚úÖ S√≠ | ‚ùå No (EMR/Databricks) | ‚ùå No |\n",
    "| **Exactly-once** | ‚úÖ S√≠ | ‚ö†Ô∏è Dif√≠cil | ‚úÖ S√≠ |\n",
    "| **Late Data** | ‚úÖ Excellent | ‚ö†Ô∏è Manual | ‚úÖ Good |\n",
    "| **Learning Curve** | Media | Baja | Alta |\n",
    "\n",
    "**Use Cases:**\n",
    "- ETL masivos (TB ‚Üí PB)\n",
    "- Real-time analytics (con Pub/Sub)\n",
    "- ML feature engineering\n",
    "- Data quality validation\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eba46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de pipeline Dataflow\n",
    "dataflow_example = '''\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class ParseCSV(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        parts = element.split(',')\n",
    "        yield {\n",
    "            'venta_id': int(parts[0]),\n",
    "            'cliente_id': int(parts[1]),\n",
    "            'total': float(parts[4])\n",
    "        }\n",
    "\n",
    "class FilterHighValue(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element['total'] > 100:\n",
    "            yield element\n",
    "\n",
    "options = PipelineOptions([\n",
    "    '--project=PROJECT_ID',\n",
    "    '--region=us-central1',\n",
    "    '--runner=DataflowRunner',\n",
    "    '--temp_location=gs://BUCKET/temp',\n",
    "    '--staging_location=gs://BUCKET/staging',\n",
    "])\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (pipeline\n",
    "     | 'Read GCS' >> beam.io.ReadFromText('gs://BUCKET/raw/ventas/*.csv')\n",
    "     | 'Skip Header' >> beam.Filter(lambda line: not line.startswith('venta_id'))\n",
    "     | 'Parse' >> beam.ParDo(ParseCSV())\n",
    "     | 'Filter' >> beam.ParDo(FilterHighValue())\n",
    "     | 'Aggregate' >> beam.Map(lambda x: (x['cliente_id'], x['total']))\n",
    "     | 'Group' >> beam.CombinePerKey(sum)\n",
    "     | 'Format' >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\")\n",
    "     | 'Write' >> beam.io.WriteToText('gs://BUCKET/curated/ventas_summary'))\n",
    "'''\n",
    "\n",
    "print(dataflow_example)\n",
    "print('\\\\nüí° Para ejecutar: python dataflow_job.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ffe76",
   "metadata": {},
   "source": [
    "## 4. Cloud Composer: Airflow Administrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ed071",
   "metadata": {},
   "source": [
    "### üéº **Cloud Composer: Airflow en GCP**\n",
    "\n",
    "**¬øQu√© es Cloud Composer?**\n",
    "\n",
    "Servicio totalmente administrado de Apache Airflow en GCP:\n",
    "- Basado en GKE (Google Kubernetes Engine)\n",
    "- Auto-scaling de workers\n",
    "- Integraci√≥n nativa con servicios GCP\n",
    "- Monitoreo con Cloud Logging/Monitoring\n",
    "\n",
    "**Componentes:**\n",
    "\n",
    "1. **Environment**: Cluster Airflow dedicado\n",
    "   - Web server (UI)\n",
    "   - Scheduler\n",
    "   - Workers (Celery o Kubernetes)\n",
    "   - Database (Cloud SQL PostgreSQL)\n",
    "\n",
    "2. **DAGs Folder**: GCS bucket autom√°tico\n",
    "   - `gs://[bucket]/dags/`\n",
    "   - Sync autom√°tico al subir DAGs\n",
    "\n",
    "**Operators GCP:**\n",
    "\n",
    "```python\n",
    "from airflow.providers.google.cloud.operators.bigquery import (\n",
    "    BigQueryCreateEmptyDatasetOperator,\n",
    "    BigQueryInsertJobOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.operators.gcs import (\n",
    "    GCSCreateBucketOperator,\n",
    "    GCSDeleteBucketOperator,\n",
    ")\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import (\n",
    "    GCSToBigQueryOperator,\n",
    ")\n",
    "\n",
    "with DAG('gcp_pipeline', schedule_interval='@daily') as dag:\n",
    "    \n",
    "    create_dataset = BigQueryCreateEmptyDatasetOperator(\n",
    "        task_id='create_dataset',\n",
    "        dataset_id='my_dataset',\n",
    "        project_id=PROJECT_ID,\n",
    "    )\n",
    "    \n",
    "    load_to_bq = GCSToBigQueryOperator(\n",
    "        task_id='load_csv_to_bq',\n",
    "        bucket='my-bucket',\n",
    "        source_objects=['raw/ventas/*.csv'],\n",
    "        destination_project_dataset_table=f'{PROJECT_ID}.my_dataset.ventas',\n",
    "        schema_fields=[\n",
    "            {'name': 'venta_id', 'type': 'INTEGER'},\n",
    "            {'name': 'cliente_id', 'type': 'INTEGER'},\n",
    "            {'name': 'total', 'type': 'FLOAT'},\n",
    "        ],\n",
    "        write_disposition='WRITE_TRUNCATE',\n",
    "    )\n",
    "    \n",
    "    run_query = BigQueryInsertJobOperator(\n",
    "        task_id='aggregate_sales',\n",
    "        configuration={\n",
    "            'query': {\n",
    "                'query': '''\n",
    "                    CREATE OR REPLACE TABLE `{}.my_dataset.sales_summary` AS\n",
    "                    SELECT cliente_id, SUM(total) as total\n",
    "                    FROM `{}.my_dataset.ventas`\n",
    "                    GROUP BY cliente_id\n",
    "                '''.format(PROJECT_ID, PROJECT_ID),\n",
    "                'useLegacySql': False,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    create_dataset >> load_to_bq >> run_query\n",
    "```\n",
    "\n",
    "**Dataflow Operator:**\n",
    "```python\n",
    "from airflow.providers.google.cloud.operators.dataflow import (\n",
    "    DataflowCreatePythonJobOperator,\n",
    ")\n",
    "\n",
    "run_dataflow = DataflowCreatePythonJobOperator(\n",
    "    task_id='dataflow_etl',\n",
    "    py_file='gs://my-bucket/dataflow/pipeline.py',\n",
    "    job_name='ventas-etl',\n",
    "    options={\n",
    "        'project': PROJECT_ID,\n",
    "        'region': 'us-central1',\n",
    "        'tempLocation': 'gs://my-bucket/temp',\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "**Environment Variables & Connections:**\n",
    "\n",
    "```python\n",
    "from airflow.models import Variable\n",
    "\n",
    "# Airflow Variables (en UI o CLI)\n",
    "PROJECT_ID = Variable.get('gcp_project_id')\n",
    "BUCKET = Variable.get('data_bucket')\n",
    "\n",
    "# Connections (GCP ‚Üí Airflow Connection)\n",
    "# ID: google_cloud_default\n",
    "# Type: Google Cloud\n",
    "# Keyfile JSON: [service account JSON]\n",
    "```\n",
    "\n",
    "**Pricing:**\n",
    "- Environment: ~$300-500/mes (peque√±o)\n",
    "- Compute: Workers + scheduler\n",
    "- Storage: GCS para DAGs + logs\n",
    "- Database: Cloud SQL (managed)\n",
    "\n",
    "**Alternatives:**\n",
    "- Managed Airflow (MWAA en AWS)\n",
    "- Self-hosted Airflow (Kubernetes)\n",
    "- Cloud Workflows (simple orchestration)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af701db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de DAG para Composer\n",
    "composer_dag = '''\n",
    "from airflow import DAG\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'ventas_daily_etl',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline diario de ventas',\n",
    "    schedule_interval='0 2 * * *',  # 2 AM daily\n",
    "    catchup=False,\n",
    "    tags=['ventas', 'etl', 'gcp'],\n",
    ") as dag:\n",
    "    \n",
    "    load_raw = GCSToBigQueryOperator(\n",
    "        task_id='load_raw_data',\n",
    "        bucket='my-datalake',\n",
    "        source_objects=['raw/ventas/{{ ds }}/*.csv'],\n",
    "        destination_project_dataset_table='project.dataset.ventas_raw',\n",
    "        write_disposition='WRITE_APPEND',\n",
    "        skip_leading_rows=1,\n",
    "    )\n",
    "    \n",
    "    transform = BigQueryInsertJobOperator(\n",
    "        task_id='transform_data',\n",
    "        configuration={\n",
    "            'query': {\n",
    "                'query': \"\"\"\n",
    "                    CREATE OR REPLACE TABLE `project.dataset.ventas_clean` AS\n",
    "                    SELECT \n",
    "                        venta_id,\n",
    "                        cliente_id,\n",
    "                        CAST(total AS FLOAT64) as total,\n",
    "                        DATE(fecha) as fecha\n",
    "                    FROM `project.dataset.ventas_raw`\n",
    "                    WHERE fecha = '{{ ds }}'\n",
    "                      AND total > 0\n",
    "                \"\"\",\n",
    "                'useLegacySql': False,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    load_raw >> transform\n",
    "\n",
    "# Subir a: gs://[composer-bucket]/dags/ventas_etl.py\n",
    "'''\n",
    "\n",
    "print(composer_dag)\n",
    "print('\\\\nüí° Subir a Cloud Composer DAGs folder en GCS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b13f4",
   "metadata": {},
   "source": [
    "## 5. Cloud Functions: Event-Driven Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b1a26",
   "metadata": {},
   "source": [
    "### ‚ö° **Cloud Functions: Serverless para Datos**\n",
    "\n",
    "**Triggers Disponibles:**\n",
    "\n",
    "1. **HTTP**: API endpoints\n",
    "2. **Cloud Storage**: Object created/deleted/updated\n",
    "3. **Pub/Sub**: Message queue events\n",
    "4. **Firestore**: Document changes\n",
    "5. **Cloud Scheduler**: Cron jobs\n",
    "\n",
    "**Ejemplo: Procesar CSV al subir a GCS**\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by Cloud Storage when CSV uploaded.\n",
    "    \n",
    "    Args:\n",
    "        event (dict): Event payload (file metadata)\n",
    "        context (google.cloud.functions.Context): Event context\n",
    "    \"\"\"\n",
    "    file_name = event['name']\n",
    "    bucket_name = event['bucket']\n",
    "    \n",
    "    print(f'Processing file: gs://{bucket_name}/{file_name}')\n",
    "    \n",
    "    # Skip if not in raw/ prefix\n",
    "    if not file_name.startswith('raw/'):\n",
    "        print('Skipping non-raw file')\n",
    "        return\n",
    "    \n",
    "    # Read CSV from GCS\n",
    "    gcs_uri = f'gs://{bucket_name}/{file_name}'\n",
    "    df = pd.read_csv(gcs_uri)\n",
    "    \n",
    "    # Transform\n",
    "    df_clean = df.dropna()\n",
    "    df_clean['total'] = df_clean['total'].astype(float)\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    client = bigquery.Client()\n",
    "    table_id = 'project.dataset.ventas_processed'\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition='WRITE_APPEND',\n",
    "    )\n",
    "    \n",
    "    job = client.load_table_from_dataframe(df_clean, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    print(f'Loaded {len(df_clean)} rows to {table_id}')\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "google-cloud-bigquery\n",
    "google-cloud-storage\n",
    "pandas\n",
    "```\n",
    "\n",
    "**Deploy:**\n",
    "```bash\n",
    "gcloud functions deploy process_csv \\\\\n",
    "  --runtime python310 \\\\\n",
    "  --trigger-resource my-bucket \\\\\n",
    "  --trigger-event google.storage.object.finalize \\\\\n",
    "  --entry-point process_csv \\\\\n",
    "  --region us-central1 \\\\\n",
    "  --memory 512MB \\\\\n",
    "  --timeout 300s\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Idempotent functions (puede ejecutarse m√∫ltiples veces)\n",
    "- Timeout < 9 min (max 540s)\n",
    "- Lightweight dependencies (cold start impact)\n",
    "- Use Pub/Sub para retry logic\n",
    "- Cloud Run para workloads >9 min\n",
    "\n",
    "**Pub/Sub + Cloud Functions:**\n",
    "```python\n",
    "import base64\n",
    "import json\n",
    "\n",
    "def process_message(event, context):\n",
    "    \"\"\"Triggered from Pub/Sub topic\"\"\"\n",
    "    pubsub_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "    data = json.loads(pubsub_message)\n",
    "    \n",
    "    print(f'Processing message: {data}')\n",
    "    # ETL logic here\n",
    "```\n",
    "\n",
    "**Publish to Pub/Sub:**\n",
    "```bash\n",
    "gcloud pubsub topics publish data-events \\\\\n",
    "  --message '{\"file\": \"gs://bucket/data.csv\", \"type\": \"sales\"}'\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b987ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de Cloud Function completo\n",
    "cloud_function_example = '''\n",
    "# main.py - Cloud Function para validar y cargar datos\n",
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def validate_and_load(event, context):\n",
    "    \"\"\"\n",
    "    Cloud Function triggered por Cloud Storage.\n",
    "    Valida CSV y carga a BigQuery.\n",
    "    \"\"\"\n",
    "    file_name = event['name']\n",
    "    bucket_name = event['bucket']\n",
    "    \n",
    "    logging.info(f'File: gs://{bucket_name}/{file_name}')\n",
    "    \n",
    "    # Read from GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    content = blob.download_as_text()\n",
    "    \n",
    "    # Parse CSV\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "    \n",
    "    # Validations\n",
    "    required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f'Missing columns: {required_cols}')\n",
    "    \n",
    "    if df['total'].isnull().sum() > 0:\n",
    "        raise ValueError('Null values found in total column')\n",
    "    \n",
    "    # Clean\n",
    "    df['total'] = df['total'].astype(float)\n",
    "    df = df[df['total'] > 0]\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    bq_client = bigquery.Client()\n",
    "    table_id = 'project.dataset.ventas'\n",
    "    \n",
    "    job = bq_client.load_table_from_dataframe(\n",
    "        df, \n",
    "        table_id,\n",
    "        job_config=bigquery.LoadJobConfig(\n",
    "            write_disposition='WRITE_APPEND'\n",
    "        )\n",
    "    )\n",
    "    job.result()\n",
    "    \n",
    "    logging.info(f'‚úÖ Loaded {len(df)} rows')\n",
    "    return f'Success: {len(df)} rows'\n",
    "\n",
    "# Deploy:\n",
    "# gcloud functions deploy validate_and_load \\\\\n",
    "#   --runtime python310 \\\\\n",
    "#   --trigger-resource my-bucket \\\\\n",
    "#   --trigger-event google.storage.object.finalize \\\\\n",
    "#   --entry-point validate_and_load\n",
    "'''\n",
    "\n",
    "print(cloud_function_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1dfa4",
   "metadata": {},
   "source": [
    "## 6. Comparaci√≥n: GCP vs AWS vs Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea38c28",
   "metadata": {},
   "source": [
    "### üîÑ **Multi-Cloud Comparison**\n",
    "\n",
    "| Servicio | GCP | AWS | Azure |\n",
    "|----------|-----|-----|-------|\n",
    "| **Object Storage** | Cloud Storage | S3 | Blob Storage |\n",
    "| **Data Warehouse** | BigQuery | Redshift | Synapse Analytics |\n",
    "| **ETL Serverless** | Dataflow (Beam) | Glue (PySpark) | Data Factory |\n",
    "| **Streaming** | Pub/Sub + Dataflow | Kinesis + Lambda | Event Hubs + Stream Analytics |\n",
    "| **Orchestration** | Cloud Composer | MWAA (Airflow) | Data Factory |\n",
    "| **Serverless Compute** | Cloud Functions | Lambda | Functions |\n",
    "| **Notebooks** | Vertex AI Workbench | SageMaker | Machine Learning Studio |\n",
    "| **ML Platform** | Vertex AI | SageMaker | Azure ML |\n",
    "\n",
    "**Cu√°ndo elegir cada cloud:**\n",
    "\n",
    "**GCP:**\n",
    "- ‚úÖ BigQuery (mejor Data Warehouse serverless)\n",
    "- ‚úÖ Kubernetes-first (GKE es l√≠der)\n",
    "- ‚úÖ ML/AI (TensorFlow, Vertex AI)\n",
    "- ‚úÖ Pricing transparente\n",
    "- ‚ùå Menos servicios que AWS\n",
    "- ‚ùå Menor presencia enterprise\n",
    "\n",
    "**AWS:**\n",
    "- ‚úÖ M√°s servicios (200+)\n",
    "- ‚úÖ Mayor adoption (33% market share)\n",
    "- ‚úÖ Mejor documentaci√≥n/comunidad\n",
    "- ‚úÖ Compliance certifications m√°s amplio\n",
    "- ‚ùå Pricing complejo\n",
    "- ‚ùå Muchos servicios legacy\n",
    "\n",
    "**Azure:**\n",
    "- ‚úÖ Integraci√≥n Microsoft (Active Directory, Office 365)\n",
    "- ‚úÖ H√≠brido (on-prem + cloud con Azure Arc)\n",
    "- ‚úÖ Windows workloads\n",
    "- ‚ùå Curva de aprendizaje pronunciada\n",
    "- ‚ùå Documentaci√≥n inconsistente\n",
    "\n",
    "**Arquitectura Multi-Cloud:**\n",
    "```\n",
    "On-Premise ‚Üí Azure (ingesta + AD)\n",
    "              ‚Üì\n",
    "          Cloud Storage (staging)\n",
    "              ‚Üì\n",
    "       BigQuery (analytics) ‚Üê Looker (BI)\n",
    "              ‚Üì\n",
    "      S3 (archival) ‚Üí Glacier\n",
    "```\n",
    "\n",
    "**Portabilidad:**\n",
    "- Terraform para IaC multi-cloud\n",
    "- Apache Beam (portable Dataflow/Flink/Spark)\n",
    "- Kubernetes para compute portable\n",
    "- Parquet/Avro para data formats\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba83d9f",
   "metadata": {},
   "source": [
    "## 7. Ejercicios Pr√°cticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea414ea3",
   "metadata": {},
   "source": [
    "### üìù **Ejercicios**\n",
    "\n",
    "1. **Cloud Storage + BigQuery:**\n",
    "   - Crear bucket con lifecycle policy (Nearline a 30 d√≠as)\n",
    "   - Subir CSV particionado por fecha\n",
    "   - Cargar a BigQuery table particionada\n",
    "   - Query con costo < $0.01\n",
    "\n",
    "2. **Dataflow Pipeline:**\n",
    "   - Leer CSVs de GCS\n",
    "   - Filtrar registros inv√°lidos\n",
    "   - Agregar por cliente\n",
    "   - Escribir a BigQuery\n",
    "\n",
    "3. **Cloud Composer DAG:**\n",
    "   - Task 1: Validar archivos en GCS\n",
    "   - Task 2: Run Dataflow job\n",
    "   - Task 3: Ejecutar query BigQuery\n",
    "   - Task 4: Enviar alerta a Pub/Sub\n",
    "\n",
    "4. **Cloud Function:**\n",
    "   - Trigger: Object created en GCS\n",
    "   - Validar schema del CSV\n",
    "   - Si v√°lido ‚Üí Pub/Sub topic \"valid-data\"\n",
    "   - Si inv√°lido ‚Üí Pub/Sub topic \"invalid-data\"\n",
    "\n",
    "5. **BigQuery Optimization:**\n",
    "   - Crear tabla particionada + clustered\n",
    "   - Comparar costo query con/sin optimizaci√≥n\n",
    "   - Implementar result caching\n",
    "\n",
    "**Recursos:**\n",
    "- [GCP Free Tier](https://cloud.google.com/free)\n",
    "- [BigQuery Sandbox](https://cloud.google.com/bigquery/docs/sandbox) (sin billing)\n",
    "- [Qwiklabs GCP](https://www.qwiklabs.com/catalog?keywords=data%20engineering&cloud%5B%5D=GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8ca08",
   "metadata": {},
   "source": [
    "## 8. Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8cbd5",
   "metadata": {},
   "source": [
    "### üéØ **Key Takeaways**\n",
    "\n",
    "**GCP Strengths para Data Engineering:**\n",
    "\n",
    "1. **BigQuery es excepcional:**\n",
    "   - Queries extremadamente r√°pidas\n",
    "   - Serverless (no tuning de clusters)\n",
    "   - BigQuery ML integrado\n",
    "   - Flat-rate predictable pricing\n",
    "\n",
    "2. **Dataflow ofrece flexibilidad:**\n",
    "   - Unified model (batch + streaming)\n",
    "   - Apache Beam portable\n",
    "   - Auto-scaling inteligente\n",
    "\n",
    "3. **Integraci√≥n cohesiva:**\n",
    "   - IAM unified\n",
    "   - Cloud Logging/Monitoring centralized\n",
    "   - Stackdriver para observabilidad\n",
    "\n",
    "**Limitaciones:**\n",
    "\n",
    "- Menos servicios que AWS\n",
    "- Lock-in en BigQuery (no standard SQL 100%)\n",
    "- Menor comunidad/recursos que AWS\n",
    "\n",
    "**Pr√≥ximos Pasos:**\n",
    "\n",
    "1. Crear cuenta GCP (free tier $300 cr√©dito)\n",
    "2. Completar [Data Engineering Qwiklab](https://www.qwiklabs.com/quests/132)\n",
    "3. Certificaci√≥n: [Professional Data Engineer](https://cloud.google.com/certification/data-engineer)\n",
    "4. Explorar Vertex AI para ML Pipelines\n",
    "\n",
    "**Happy data engineering en GCP! üöÄ**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "¬© 2024-2025 - Data Engineering Modular Course"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
