{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefa2d37",
   "metadata": {},
   "source": [
    "# 🔄 Proyecto Integrador Mid 2: Kafka → Streaming → Data Lake y Monitoreo\n",
    "\n",
    "Objetivo: implementar un pipeline near-real-time que ingiere eventos en Kafka, los valida y transforma, escribe salidas particionadas en Parquet y expone métricas básicas de procesamiento.\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Notebooks Mid 02 (Kafka), 05 (DataOps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235874a",
   "metadata": {},
   "source": [
    "### 🎯 **Streaming Pipeline: Real-Time vs Near-Real-Time**\n",
    "\n",
    "**Objetivo del Proyecto:**  \n",
    "Construir un pipeline de **near-real-time** que procesa eventos desde Kafka hasta Data Lake (Parquet particionado), aplicando patrones de streaming, idempotencia y observabilidad.\n",
    "\n",
    "**Arquitectura del Sistema:**\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                        EVENT SOURCES                          │\n",
    "│          (Web Apps, Mobile Apps, IoT Devices)                 │\n",
    "└─────────────────────┬────────────────────────────────────────┘\n",
    "                      │ Events/sec\n",
    "                      ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      APACHE KAFKA                            │\n",
    "│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │\n",
    "│  │Partition│  │Partition│  │Partition│  │Partition│        │\n",
    "│  │    0    │  │    1    │  │    2    │  │    3    │        │\n",
    "│  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │\n",
    "│  Topic: pi2_events (Retention: 7 days)                      │\n",
    "└─────────────────────┬───────────────────────────────────────┘\n",
    "                      │ Poll batches (50-200 msgs)\n",
    "                      ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   CONSUMER/PROCESSOR                         │\n",
    "│                                                               │\n",
    "│  ┌──────────────────────────────────────────────────┐       │\n",
    "│  │  1. VALIDATE    → Cerberus schema enforcement    │       │\n",
    "│  │  2. DEDUP       → Checkpoint DB (SQLite/Redis)   │       │\n",
    "│  │  3. ENRICH      → Add processing_ts, normalize   │       │\n",
    "│  │  4. TRANSFORM   → Business logic                 │       │\n",
    "│  └──────────────────────────────────────────────────┘       │\n",
    "│                                                               │\n",
    "└─────────────────────┬───────────────────────────────────────┘\n",
    "                      │ Micro-batches\n",
    "       ┌──────────────┼──────────────┐\n",
    "       ▼              ▼               ▼\n",
    "┌──────────┐   ┌──────────┐   ┌──────────┐\n",
    "│  SINK 1  │   │  SINK 2  │   │  SINK 3  │\n",
    "│  Parquet │   │  Metrics │   │   DLQ    │\n",
    "│  (Lake)  │   │  (Prom.) │   │ (Errors) │\n",
    "└──────────┘   └──────────┘   └──────────┘\n",
    "      │              │               │\n",
    "      ▼              ▼               ▼\n",
    "  Data Lake      Dashboard     Error Queue\n",
    " (Partitioned)   (Grafana)    (Reprocessing)\n",
    "```\n",
    "\n",
    "**Near-Real-Time vs Real-Time:**\n",
    "\n",
    "| Característica | **Real-Time** | **Near-Real-Time** |\n",
    "|----------------|---------------|---------------------|\n",
    "| **Latencia** | < 100 ms | 1-60 segundos |\n",
    "| **Procesamiento** | Event-by-event | Micro-batches (50-200 events) |\n",
    "| **Throughput** | Menor | Mayor (batch efficiency) |\n",
    "| **Complejidad** | Alta (state management) | Media (simpler checkpointing) |\n",
    "| **Casos de uso** | Trading, fraud detection | Analytics, monitoring, ETL |\n",
    "\n",
    "**¿Por qué Near-Real-Time para este proyecto?**\n",
    "\n",
    "✅ **Balance latencia/throughput**: Batches de 50-200 eventos optimizan I/O  \n",
    "✅ **Idempotencia simplificada**: Checkpoint por batch vs por evento  \n",
    "✅ **Costo-efectivo**: Menos recursos que streaming puro  \n",
    "✅ **Suficiente para analytics**: 30-60s lag aceptable para dashboards\n",
    "\n",
    "**Tecnologías Integradas:**\n",
    "\n",
    "1. **Kafka** (Notebook 02): Event streaming, consumer groups, offset management\n",
    "2. **Cerberus** (Notebook 06): Schema validation para eventos\n",
    "3. **Idempotencia**: Checkpoint DB para evitar duplicados (at-least-once → exactly-once semántico)\n",
    "4. **Parquet** (Notebook 07): Particionamiento por fecha (Hive-style)\n",
    "5. **Logging** (Notebook 05): Loguru structured logging para observabilidad\n",
    "6. **DataOps**: Métricas, alerting, DLQ (Dead Letter Queue)\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "- **E-commerce**: Tracking de clicks/views/purchases para recomendaciones en tiempo real\n",
    "- **FinTech**: Detección de fraude (validación de transacciones sospechosas)\n",
    "- **Gaming**: Analytics de comportamiento de jugadores (sessions, achievements)\n",
    "- **IoT**: Ingestión de telemetría de sensores industriales\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697542d",
   "metadata": {},
   "source": [
    "## 0) Requisitos y ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4295e",
   "metadata": {},
   "source": [
    "- Necesitas un clúster Kafka en local (Docker Compose) o remoto.\n",
    "- Dependencias opcionales: `kafka-python` o `confluent-kafka`. No están activas por defecto en `requirements.txt`.\n",
    "- Este notebook incluye un modo de simulación sin Kafka para que puedas practicar la lógica de validación/transformación/sink.\n",
    "- Variables de entorno: `KAFKA_BOOTSTRAP_SERVERS`, `KAFKA_TOPIC`, `OUT_DIR` (por defecto `datasets/processed/pi2/`).\n",
    "\n",
    "Ejemplo Docker Compose (para referencia):\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.4.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.4.0\n",
    "    depends_on: [zookeeper]\n",
    "    ports: ['9092:9092']\n",
    "    environment:\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad1acd",
   "metadata": {},
   "source": [
    "## 1) Esquema de eventos y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb1103",
   "metadata": {},
   "source": [
    "### 📝 **Schema Validation: Event-Driven Architecture**\n",
    "\n",
    "**¿Por qué validar schemas en streaming?**\n",
    "\n",
    "En arquitecturas event-driven, **productores y consumidores están desacoplados**:\n",
    "\n",
    "```\n",
    "Producer A (v1.0) ───┐\n",
    "Producer B (v1.2) ───┼──> Kafka Topic ───> Consumer C (v1.1)\n",
    "Producer C (v2.0) ───┘                       ↓\n",
    "                                      ¿Qué schema esperar?\n",
    "```\n",
    "\n",
    "**Problemas sin validación:**\n",
    "\n",
    "❌ **Schema drift**: Producer envía campo nuevo `discount` → Consumer crashea  \n",
    "❌ **Type mismatches**: Producer envía `\"123\"` (string) → Consumer espera `123` (int)  \n",
    "❌ **Missing fields**: Producer olvida campo obligatorio `usuario_id`  \n",
    "❌ **Poison pills**: Evento malformado bloquea consumer indefinidamente\n",
    "\n",
    "**Solución: Schema Validation con Cerberus**\n",
    "\n",
    "```python\n",
    "event_schema = {\n",
    "    'event_id': {\n",
    "        'type': 'string',\n",
    "        'required': True,\n",
    "        'regex': r'^e\\d+$'  # Format: e1, e123, e99999\n",
    "    },\n",
    "    'ts': {\n",
    "        'type': 'string',\n",
    "        'required': True,\n",
    "        'regex': r'^\\d{4}-\\d{2}-\\d{2}T'  # ISO 8601\n",
    "    },\n",
    "    'usuario_id': {\n",
    "        'type': 'integer',\n",
    "        'required': True,\n",
    "        'min': 1,\n",
    "        'max': 1_000_000\n",
    "    },\n",
    "    'accion': {\n",
    "        'type': 'string',\n",
    "        'allowed': ['click', 'view', 'purchase'],\n",
    "        'required': True\n",
    "    },\n",
    "    'monto': {\n",
    "        'type': 'float',\n",
    "        'nullable': True,  # Solo presente en 'purchase'\n",
    "        'min': 0,\n",
    "        'max': 100_000\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Ventajas de Cerberus:**\n",
    "\n",
    "✅ **Dict-based**: Fácil serialización (guardar schema en JSON/YAML)  \n",
    "✅ **Composable**: `allow_unknown=True` permite backward compatibility  \n",
    "✅ **Custom validators**: Extendible con lógica de negocio\n",
    "\n",
    "**Alternativas consideradas:**\n",
    "\n",
    "| Tool | Pros | Contras |\n",
    "|------|------|---------|\n",
    "| **Cerberus** | Lightweight, flexible | No type hints |\n",
    "| **Pydantic** | Type safety, IDE support | Más verboso |\n",
    "| **JSON Schema** | Standard, language-agnostic | Menos ergonómico en Python |\n",
    "| **Avro** | Schema evolution, compact | Requiere Schema Registry |\n",
    "\n",
    "**Estrategia de Evolución de Schema:**\n",
    "\n",
    "```python\n",
    "# v1.0 (inicial)\n",
    "{'event_id', 'ts', 'usuario_id', 'accion', 'monto'}\n",
    "\n",
    "# v1.1 (backward compatible)\n",
    "{'event_id', 'ts', 'usuario_id', 'accion', 'monto', 'session_id'} \n",
    "# allow_unknown=True → consumer v1.0 ignora 'session_id'\n",
    "\n",
    "# v2.0 (breaking change)\n",
    "{'event_id', 'ts', 'user_id', 'action', 'amount'}\n",
    "# Renombrado de campos → Requiere consumer v2.0\n",
    "\n",
    "# Estrategia: Topic versioning\n",
    "topics: pi2_events_v1, pi2_events_v2\n",
    "```\n",
    "\n",
    "**Dead Letter Queue (DLQ) Pattern:**\n",
    "\n",
    "```python\n",
    "def process_with_dlq(events):\n",
    "    good, bad = [], []\n",
    "    for evt in events:\n",
    "        if not validator.validate(evt):\n",
    "            bad.append({\n",
    "                'event': evt,\n",
    "                'errors': validator.errors,\n",
    "                'timestamp': datetime.utcnow().isoformat()\n",
    "            })\n",
    "            continue\n",
    "        good.append(evt)\n",
    "    \n",
    "    # Enviar errores a DLQ (topic separado)\n",
    "    if bad:\n",
    "        producer.send('pi2_events_dlq', bad)\n",
    "    \n",
    "    return good\n",
    "```\n",
    "\n",
    "**Beneficios del DLQ:**\n",
    "\n",
    "✅ **No se pierden eventos**: Errores se guardan para análisis  \n",
    "✅ **No bloquea pipeline**: Consumer continúa procesando eventos válidos  \n",
    "✅ **Trazabilidad**: Logs de errores para debugging  \n",
    "✅ **Reprocessing**: Equipo puede corregir y reingerir desde DLQ\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904aa140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerberus import Validator\n",
    "event_schema = {\n",
    "  'event_id': {'type':'string', 'required': True},\n",
    "  'ts': {'type':'string', 'required': True},\n",
    "  'usuario_id': {'type':'integer', 'required': True},\n",
    "  'accion': {'type':'string', 'allowed':['click','view','purchase']},\n",
    "  'monto': {'type':'float', 'nullable': True, 'min': 0}\n",
    "}\n",
    "validator = Validator(event_schema, allow_unknown=True)\n",
    "def is_valid_event(evt):\n",
    "    return validator.validate(evt)\n",
    "\n",
    "# Ejemplo\n",
    "is_valid_event({'event_id':'e1','ts':'2025-10-30T12:00:00Z','usuario_id':1,'accion':'click','monto':None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99efdc",
   "metadata": {},
   "source": [
    "## 2) Productor (Kafka) y modo simulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, random\n",
    "from datetime import datetime, timezone\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS','localhost:9092')\n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC','pi2_events')\n",
    "\n",
    "def gen_event(i: int):\n",
    "    return {\n",
    "        'event_id': f'e{i}',\n",
    "        'ts': datetime.now(timezone.utc).isoformat(),\n",
    "        'usuario_id': random.randint(1,1000),\n",
    "        'accion': random.choice(['click','view','purchase']),\n",
    "        'monto': round(random.uniform(1,500),2) if random.random()>0.8 else None\n",
    "    }\n",
    "\n",
    "def produce_simulation(n=50):\n",
    "    return [gen_event(i) for i in range(n)]\n",
    "\n",
    "# Productor Kafka (opcional)\n",
    "def produce_kafka(n=50):\n",
    "    try:\n",
    "        from kafka import KafkaProducer\n",
    "        producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        for i in range(n):\n",
    "            evt = gen_event(i)\n",
    "            producer.send(KAFKA_TOPIC, evt)\n",
    "        producer.flush()\n",
    "        return n\n",
    "    except Exception as e:\n",
    "        print('Kafka no disponible, usa modo simulación:', e)\n",
    "        return None\n",
    "\n",
    "simulated = produce_simulation(100)\n",
    "len(simulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e20bb",
   "metadata": {},
   "source": [
    "## 3) Consumidor/Procesador: validación, enriquecimiento e idempotencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf463656",
   "metadata": {},
   "source": [
    "### 🔄 **Idempotencia: Exactly-Once Semantics**\n",
    "\n",
    "**El Problema del At-Least-Once Delivery:**\n",
    "\n",
    "Kafka garantiza **at-least-once** por defecto:\n",
    "\n",
    "```\n",
    "Consumer Poll Batch:\n",
    "  Event e1 → Process ✅ → Commit offset ✅\n",
    "  Event e2 → Process ✅ → Commit offset ❌ (crash)\n",
    "  \n",
    "Consumer Restart:\n",
    "  Event e2 → Process ✅ (duplicated!)\n",
    "  Event e3 → Process ✅\n",
    "```\n",
    "\n",
    "**Consecuencias:**\n",
    "\n",
    "❌ **Duplicados en DB**: `INSERT e2` dos veces → métricas incorrectas  \n",
    "❌ **Aggregations erróneas**: `SUM(monto)` cuenta `e2` doble  \n",
    "❌ **Inconsistencias**: Dashboard muestra 150 eventos pero solo 100 únicos\n",
    "\n",
    "**Solución 1: Checkpoint Database (Implementación en el Notebook)**\n",
    "\n",
    "```python\n",
    "# SQLite como checkpoint store\n",
    "def ensure_ckpt():\n",
    "    conn = sqlite3.connect('checkpoint.sqlite')\n",
    "    conn.execute('CREATE TABLE IF NOT EXISTS seen (event_id TEXT PRIMARY KEY)')\n",
    "    conn.commit()\n",
    "\n",
    "def is_seen(event_id: str) -> bool:\n",
    "    cur.execute('SELECT 1 FROM seen WHERE event_id=?', (event_id,))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "def mark_seen(event_id: str):\n",
    "    cur.execute('INSERT OR IGNORE INTO seen (event_id) VALUES (?)', (event_id,))\n",
    "```\n",
    "\n",
    "**Cómo funciona:**\n",
    "\n",
    "1. Consumer lee batch de Kafka\n",
    "2. Para cada evento, verifica `is_seen(event_id)`\n",
    "3. Si duplicado → skip\n",
    "4. Si nuevo → procesa + `mark_seen(event_id)`\n",
    "5. Commit Kafka offset al final del batch\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "✅ **Pros:**\n",
    "- Exactly-once semántico (deduplicación garantizada)\n",
    "- Simple de implementar\n",
    "- Funciona con cualquier backend (SQLite/Redis/PostgreSQL)\n",
    "\n",
    "❌ **Contras:**\n",
    "- Overhead de DB lookup por evento (~1-5ms por query)\n",
    "- Checkpoint DB crece indefinidamente (requiere TTL/cleanup)\n",
    "- No funciona si `event_id` no es único globalmente\n",
    "\n",
    "**Solución 2: Kafka Transactions (Alternative)**\n",
    "\n",
    "```python\n",
    "# Producer transaccional\n",
    "producer = KafkaProducer(\n",
    "    transactional_id='pi2-producer',\n",
    "    enable_idempotence=True\n",
    ")\n",
    "producer.begin_transaction()\n",
    "producer.send('pi2_events', event)\n",
    "producer.commit_transaction()\n",
    "\n",
    "# Consumer transaccional\n",
    "consumer = KafkaConsumer(\n",
    "    isolation_level='read_committed',  # Solo lee transacciones completas\n",
    "    enable_auto_commit=False\n",
    ")\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "✅ Exactly-once nativo de Kafka  \n",
    "✅ No requiere checkpoint externo  \n",
    "✅ Menor latencia\n",
    "\n",
    "**Desventajas:**\n",
    "❌ Requiere Kafka 0.11+ y configuración compleja  \n",
    "❌ No protege contra duplicados entre diferentes topics/pipelines\n",
    "\n",
    "**Solución 3: Idempotent Writes (DB-Level)**\n",
    "\n",
    "```python\n",
    "# PostgreSQL UPSERT\n",
    "INSERT INTO events (event_id, ts, monto)\n",
    "VALUES ('e1', '2025-10-30', 100.0)\n",
    "ON CONFLICT (event_id) DO UPDATE SET\n",
    "  ts = EXCLUDED.ts,\n",
    "  monto = EXCLUDED.monto;\n",
    "\n",
    "# MongoDB upsert\n",
    "db.events.update_one(\n",
    "    {'event_id': 'e1'},\n",
    "    {'$set': {'ts': '2025-10-30', 'monto': 100.0}},\n",
    "    upsert=True\n",
    ")\n",
    "```\n",
    "\n",
    "**TTL (Time-To-Live) para Checkpoint Cleanup:**\n",
    "\n",
    "```python\n",
    "# Redis con TTL automático (7 días)\n",
    "redis.setex(f'seen:{event_id}', 7*86400, '1')\n",
    "\n",
    "# PostgreSQL con expiration timestamp\n",
    "CREATE TABLE seen (\n",
    "  event_id TEXT PRIMARY KEY,\n",
    "  seen_at TIMESTAMPTZ DEFAULT NOW()\n",
    ");\n",
    "CREATE INDEX idx_seen_at ON seen (seen_at);\n",
    "\n",
    "-- Cleanup job (diario)\n",
    "DELETE FROM seen WHERE seen_at < NOW() - INTERVAL '7 days';\n",
    "```\n",
    "\n",
    "**¿Por qué 7 días?**\n",
    "\n",
    "- Kafka retention policy: 7 días (por defecto)\n",
    "- Si evento re-aparece después de 7 días → fuera del window de Kafka\n",
    "- Balance entre storage y safety window\n",
    "\n",
    "**Metrics para Monitorear:**\n",
    "\n",
    "```python\n",
    "duplicates_detected = Counter('duplicates_total', 'Total duplicate events')\n",
    "checkpoint_db_size = Gauge('checkpoint_db_bytes', 'Checkpoint DB size')\n",
    "\n",
    "def process_events(events):\n",
    "    for evt in events:\n",
    "        if is_seen(evt['event_id']):\n",
    "            duplicates_detected.inc()\n",
    "            continue\n",
    "        # Process...\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sqlite3\n",
    "from typing import Iterable, Dict, Any\n",
    "\n",
    "OUT_DIR = os.getenv('OUT_DIR','datasets/processed/pi2/')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "CKPT_DB = os.path.join(OUT_DIR, 'checkpoint.sqlite')\n",
    "\n",
    "def ensure_ckpt():\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('CREATE TABLE IF NOT EXISTS seen (event_id TEXT PRIMARY KEY)')\n",
    "    conn.commit(); conn.close()\n",
    "\n",
    "def is_seen(event_id: str) -> bool:\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT 1 FROM seen WHERE event_id=?', (event_id,))\n",
    "    row = cur.fetchone()\n",
    "    conn.close()\n",
    "    return row is not None\n",
    "\n",
    "def mark_seen(event_id: str):\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('INSERT OR IGNORE INTO seen(event_id) VALUES (?)', (event_id,))\n",
    "    conn.commit(); conn.close()\n",
    "\n",
    "def enrich(evt: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    evt = dict(evt)\n",
    "    evt['processing_ts'] = datetime.now(timezone.utc).isoformat()\n",
    "    evt['monto'] = float(evt['monto']) if evt.get('monto') is not None else 0.0\n",
    "    return evt\n",
    "\n",
    "def process_events(events: Iterable[Dict[str,Any]]):\n",
    "    ensure_ckpt()\n",
    "    good, bad = [], []\n",
    "    for evt in events:\n",
    "        if not is_valid_event(evt):\n",
    "            bad.append({'evt':evt, 'err':'schema'})\n",
    "            continue\n",
    "        if is_seen(evt['event_id']):\n",
    "            logger.info(f\n",
    ")\n",
    "            continue\n",
    "        evt2 = enrich(evt)\n",
    "        good.append(evt2)\n",
    "        mark_seen(evt['event_id'])\n",
    "    return good, bad\n",
    "\n",
    "ok, ko = process_events(simulated)\n",
    "len(ok), len(ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae89ec",
   "metadata": {},
   "source": [
    "## 4) Sink: escribir Parquet particionado por fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d85e04",
   "metadata": {},
   "source": [
    "### 💾 **Parquet Sink: Partitioning Strategy**\n",
    "\n",
    "**¿Por qué Parquet para Streaming?**\n",
    "\n",
    "**Comparativa de formatos:**\n",
    "\n",
    "| Formato | Compresión | Query Speed | Streaming Friendly | Tooling |\n",
    "|---------|------------|-------------|---------------------|---------|\n",
    "| **CSV** | 1x | Slow (full scan) | ✅ Sí | Universal |\n",
    "| **JSON** | 2x | Slow | ✅ Sí | Universal |\n",
    "| **Avro** | 5x | Medium | ✅ Sí | Schema Registry |\n",
    "| **Parquet** | 10x | Fast (columnar) | ⚠️ Micro-batches | Spark/Athena/Presto |\n",
    "| **ORC** | 12x | Fast | ⚠️ Micro-batches | Hive-centric |\n",
    "\n",
    "**Decisión: Parquet**\n",
    "- ✅ Mejor compresión (10x vs CSV)\n",
    "- ✅ Query performance (columnar scan)\n",
    "- ✅ Compatible con Athena/Spark/pandas\n",
    "- ⚠️ Requiere buffering (no event-by-event)\n",
    "\n",
    "**Estrategia de Particionamiento:**\n",
    "\n",
    "```\n",
    "datasets/processed/pi2/\n",
    "├── date=2025-10-30/\n",
    "│   ├── events_1730246400.parquet  (timestamp: 12:00 PM)\n",
    "│   ├── events_1730250000.parquet  (timestamp: 01:00 PM)\n",
    "│   └── events_1730253600.parquet  (timestamp: 02:00 PM)\n",
    "├── date=2025-10-31/\n",
    "│   ├── events_1730332800.parquet\n",
    "│   └── events_1730336400.parquet\n",
    "└── date=2025-11-01/\n",
    "    └── events_1730419200.parquet\n",
    "```\n",
    "\n",
    "**Ventajas del Hive-style Partitioning:**\n",
    "\n",
    "1. **Partition Pruning (Athena/Spark):**\n",
    "   ```sql\n",
    "   SELECT SUM(monto) FROM events\n",
    "   WHERE date = '2025-10-30'  -- Solo lee 1 partición (3 archivos)\n",
    "   -- vs full scan de 100 GB → scan de 300 MB\n",
    "   ```\n",
    "\n",
    "2. **Lifecycle Management:**\n",
    "   ```bash\n",
    "   # Eliminar datos antiguos (GDPR compliance)\n",
    "   rm -rf datasets/processed/pi2/date=2025-01-*\n",
    "   ```\n",
    "\n",
    "3. **Incremental Processing:**\n",
    "   ```python\n",
    "   # Solo procesar particiones nuevas\n",
    "   new_partitions = [d for d in os.listdir('pi2') \n",
    "                     if d > f'date={last_processed_date}']\n",
    "   ```\n",
    "\n",
    "**Small Files Problem:**\n",
    "\n",
    "**Problema:**\n",
    "```\n",
    "date=2025-10-30/\n",
    "├── events_1730246401.parquet  (12 KB)   ← Ineficiente\n",
    "├── events_1730246402.parquet  (8 KB)    ← Overhead de metadata\n",
    "├── events_1730246403.parquet  (15 KB)   ← Muchos archivos pequeños\n",
    "└── ... (1000 archivos)\n",
    "```\n",
    "\n",
    "**Impacto:**\n",
    "- Athena cobra por # de archivos escaneados ($5/TB + $0.002/archivo)\n",
    "- Spark: Overhead de open/close file handles\n",
    "- Target: **128-512 MB por archivo**\n",
    "\n",
    "**Solución 1: Buffering en memoria**\n",
    "```python\n",
    "buffer = []\n",
    "BUFFER_SIZE = 1000  # Eventos\n",
    "\n",
    "def write_buffered(event):\n",
    "    buffer.append(event)\n",
    "    if len(buffer) >= BUFFER_SIZE:\n",
    "        flush_to_parquet(buffer)\n",
    "        buffer.clear()\n",
    "```\n",
    "\n",
    "**Solución 2: Compaction Job (Airflow)**\n",
    "```python\n",
    "# DAG diario de compactación\n",
    "@task\n",
    "def compact_partition(date):\n",
    "    small_files = glob(f'date={date}/*.parquet')\n",
    "    if len(small_files) > 100:\n",
    "        df = pd.concat([pd.read_parquet(f) for f in small_files])\n",
    "        df.to_parquet(f'date={date}/compacted.parquet')\n",
    "        for f in small_files:\n",
    "            os.remove(f)\n",
    "```\n",
    "\n",
    "**Escritura Atómica (Evitar Archivos Corruptos):**\n",
    "\n",
    "```python\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def atomic_write_parquet(df, final_path):\n",
    "    # 1. Escribir a archivo temporal\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.parquet') as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        df.to_parquet(tmp_path)\n",
    "    \n",
    "    # 2. Mover a destino final (operación atómica en filesystems POSIX)\n",
    "    shutil.move(tmp_path, final_path)\n",
    "```\n",
    "\n",
    "**Por qué es importante:**\n",
    "- Si consumer crashea durante `to_parquet()` → archivo parcialmente escrito\n",
    "- Athena/Spark leen archivo corrupto → query fails\n",
    "- Atomic move garantiza: archivo completo o no existe\n",
    "\n",
    "**Compression Codecs:**\n",
    "\n",
    "```python\n",
    "df.to_parquet(\n",
    "    'events.parquet',\n",
    "    compression='snappy',  # Default: Balance speed/ratio\n",
    "    # compression='gzip',   # Better ratio, slower\n",
    "    # compression='zstd',   # Modern: Fast + good ratio\n",
    "    engine='pyarrow'\n",
    ")\n",
    "```\n",
    "\n",
    "**Benchmark:**\n",
    "\n",
    "| Codec | Compression Ratio | Write Speed | Read Speed |\n",
    "|-------|-------------------|-------------|------------|\n",
    "| None | 1x | 100 MB/s | 200 MB/s |\n",
    "| Snappy | 3x | 80 MB/s | 180 MB/s |\n",
    "| Gzip | 5x | 30 MB/s | 100 MB/s |\n",
    "| Zstd | 4.5x | 70 MB/s | 160 MB/s |\n",
    "\n",
    "**Recomendación:** `snappy` para streaming (balance latencia/compresión)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da70547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def write_parquet(events):\n",
    "    if not events:\n",
    "        return None\n",
    "    df = pd.DataFrame(events)\n",
    "    df['date'] = pd.to_datetime(df['ts']).dt.date.astype(str)\n",
    "    for d, part in df.groupby('date'):\n",
    "        part_dir = Path(OUT_DIR) / f'date={d}'\n",
    "        part_dir.mkdir(parents=True, exist_ok=True)\n",
    "        fp = part_dir / f'events_{int(time.time())}.parquet'\n",
    "        part.drop(columns=['date']).to_parquet(fp, index=False)\n",
    "    return True\n",
    "\n",
    "write_parquet(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2f805",
   "metadata": {},
   "source": [
    "## 5) Métricas y logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total = len(simulated)\n",
    "validos = len(ok)\n",
    "invalidos = len(ko)\n",
    "metricas = f'total {total}\n",
    "validos {validos}\n",
    "invalidos {invalidos}\n",
    "'\n",
    "with open(os.path.join(OUT_DIR, 'metrics.txt'), 'w') as f:\n",
    "    f.write(metricas)\n",
    "logger.info(f'metrics total={total} validos={validos} invalidos={invalidos}')\n",
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b531b3",
   "metadata": {},
   "source": [
    "## 6) Consumidor Kafka (opcional en vivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_kafka(max_messages=100):\n",
    "    try:\n",
    "        from kafka import KafkaConsumer\n",
    "        consumer = KafkaConsumer(\n",
    "            KAFKA_TOPIC,\n",
    "            bootstrap_servers=KAFKA_BOOTSTRAP,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=False,\n",
    "            value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    "        )\n",
    "        batch = []\n",
    "        for i, msg in enumerate(consumer):\n",
    "            batch.append(msg.value)\n",
    "            if len(batch) >= 50 or i+1 >= max_messages:\n",
    "                ok, ko = process_events(batch)\n",
    "                write_parquet(ok)\n",
    "                # commit offsets al final del batch\n",
    "                consumer.commit()\n",
    "                logger.info(f'batch size={len(batch)} ok={len(ok)} ko={len(ko)}')\n",
    "                batch = []\n",
    "                if i+1 >= max_messages:\n",
    "                    break\n",
    "        consumer.close()\n",
    "    except Exception as e:\n",
    "        print('Kafka no disponible, salta esta sección:', e)\n",
    "\n",
    "# consume_kafka(200)  # Descomentar para una prueba en vivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c8605",
   "metadata": {},
   "source": [
    "## 7) Buenas prácticas y extensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a977c2e",
   "metadata": {},
   "source": [
    "### 📈 **Observability: Metrics, Logging & Alerting**\n",
    "\n",
    "**Three Pillars of Observability:**\n",
    "\n",
    "1. **Metrics**: What's happening? (aggregations, rates, gauges)\n",
    "2. **Logs**: Why did it happen? (detailed context, debugging)\n",
    "3. **Traces**: How did it flow? (distributed tracing, latency breakdown)\n",
    "\n",
    "**Streaming Pipeline Metrics (RED Method):**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge, Summary\n",
    "\n",
    "# Rate: Events/sec processed\n",
    "events_processed = Counter(\n",
    "    'kafka_events_processed_total',\n",
    "    'Total events processed',\n",
    "    ['status']  # Labels: success, validation_error, duplicate\n",
    ")\n",
    "\n",
    "# Errors: Validation failures, exceptions\n",
    "validation_errors = Counter(\n",
    "    'kafka_validation_errors_total',\n",
    "    'Schema validation errors',\n",
    "    ['error_type']\n",
    ")\n",
    "\n",
    "# Duration: Latency distribution\n",
    "processing_latency = Histogram(\n",
    "    'kafka_processing_seconds',\n",
    "    'Event processing latency',\n",
    "    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    ")\n",
    "\n",
    "# Additional metrics\n",
    "kafka_lag = Gauge(\n",
    "    'kafka_consumer_lag',\n",
    "    'Consumer lag (uncommitted offsets)',\n",
    "    ['topic', 'partition']\n",
    ")\n",
    "\n",
    "checkpoint_db_size = Gauge(\n",
    "    'checkpoint_db_size_bytes',\n",
    "    'Checkpoint database size'\n",
    ")\n",
    "\n",
    "batch_size = Summary(\n",
    "    'kafka_batch_size',\n",
    "    'Batch size distribution'\n",
    ")\n",
    "```\n",
    "\n",
    "**Usage en el Pipeline:**\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def process_batch(events):\n",
    "    start = time.time()\n",
    "    batch_size.observe(len(events))\n",
    "    \n",
    "    good, bad = [], []\n",
    "    for evt in events:\n",
    "        if not validator.validate(evt):\n",
    "            events_processed.labels(status='validation_error').inc()\n",
    "            validation_errors.labels(error_type='schema').inc()\n",
    "            bad.append(evt)\n",
    "            continue\n",
    "        \n",
    "        if is_seen(evt['event_id']):\n",
    "            events_processed.labels(status='duplicate').inc()\n",
    "            continue\n",
    "        \n",
    "        good.append(evt)\n",
    "        events_processed.labels(status='success').inc()\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    processing_latency.observe(duration)\n",
    "    \n",
    "    return good, bad\n",
    "```\n",
    "\n",
    "**Structured Logging con Loguru:**\n",
    "\n",
    "```python\n",
    "from loguru import logger\n",
    "import sys\n",
    "\n",
    "# Configuración\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(\n",
    "    sys.stderr,\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {extra[request_id]} | {message}\",\n",
    "    level=\"INFO\",\n",
    "    serialize=True  # JSON output\n",
    ")\n",
    "\n",
    "# Contexto enriquecido\n",
    "logger = logger.bind(request_id=\"batch-123\", consumer_group=\"pi2-cg\")\n",
    "\n",
    "# Logging en pipeline\n",
    "logger.info(f\"Processing batch\", batch_size=len(events), topic=\"pi2_events\")\n",
    "logger.warning(f\"High validation errors\", error_rate=0.15, threshold=0.10)\n",
    "logger.error(f\"Kafka consumer lagging\", lag_seconds=300, max_lag=60)\n",
    "\n",
    "# Output (JSON):\n",
    "{\n",
    "  \"timestamp\": \"2025-10-30 14:23:45\",\n",
    "  \"level\": \"INFO\",\n",
    "  \"message\": \"Processing batch\",\n",
    "  \"request_id\": \"batch-123\",\n",
    "  \"consumer_group\": \"pi2-cg\",\n",
    "  \"batch_size\": 150,\n",
    "  \"topic\": \"pi2_events\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Alerting Rules (Prometheus/Alertmanager):**\n",
    "\n",
    "```yaml\n",
    "groups:\n",
    "  - name: kafka_pipeline\n",
    "    interval: 30s\n",
    "    rules:\n",
    "      # Alta latencia de procesamiento\n",
    "      - alert: HighProcessingLatency\n",
    "        expr: histogram_quantile(0.95, kafka_processing_seconds) > 1.0\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"P95 latency > 1s\"\n",
    "          description: \"{{ $value }}s processing time\"\n",
    "      \n",
    "      # Consumer lag alto\n",
    "      - alert: ConsumerLagging\n",
    "        expr: kafka_consumer_lag > 10000\n",
    "        for: 10m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"Consumer lag > 10k events\"\n",
    "          description: \"Topic {{ $labels.topic }} lag: {{ $value }}\"\n",
    "      \n",
    "      # Tasa de errores alta\n",
    "      - alert: HighValidationErrors\n",
    "        expr: rate(kafka_validation_errors_total[5m]) > 10\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Validation errors > 10/min\"\n",
    "```\n",
    "\n",
    "**Backpressure Handling:**\n",
    "\n",
    "```python\n",
    "MAX_BATCH_SIZE = 500\n",
    "MAX_PROCESSING_TIME = 30  # seconds\n",
    "\n",
    "def consume_with_backpressure():\n",
    "    consumer = KafkaConsumer(\n",
    "        max_poll_records=MAX_BATCH_SIZE,  # Limit batch size\n",
    "        max_poll_interval_ms=300000       # 5 min timeout\n",
    "    )\n",
    "    \n",
    "    for msg in consumer:\n",
    "        start = time.time()\n",
    "        batch = [msg.value]\n",
    "        \n",
    "        # Consumir batch completo\n",
    "        while time.time() - start < MAX_PROCESSING_TIME:\n",
    "            try:\n",
    "                msg = consumer.poll(timeout_ms=100)\n",
    "                if not msg:\n",
    "                    break\n",
    "                batch.extend([m.value for m in msg.values()])\n",
    "                if len(batch) >= MAX_BATCH_SIZE:\n",
    "                    break\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        # Procesar batch\n",
    "        process_batch(batch)\n",
    "        \n",
    "        # Pausar si backpressure (opcional)\n",
    "        if len(batch) >= MAX_BATCH_SIZE * 0.9:\n",
    "            logger.warning(\"Backpressure detected, pausing 5s\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "        consumer.commit()\n",
    "```\n",
    "\n",
    "**Dashboard (Grafana Queries):**\n",
    "\n",
    "```promql\n",
    "# Throughput (events/sec)\n",
    "rate(kafka_events_processed_total[1m])\n",
    "\n",
    "# Error rate\n",
    "sum(rate(kafka_events_processed_total{status!=\"success\"}[5m])) \n",
    "/ \n",
    "sum(rate(kafka_events_processed_total[5m]))\n",
    "\n",
    "# P95 latency\n",
    "histogram_quantile(0.95, kafka_processing_seconds_bucket)\n",
    "\n",
    "# Consumer lag trend\n",
    "kafka_consumer_lag\n",
    "```\n",
    "\n",
    "**SLI/SLO Example:**\n",
    "\n",
    "```python\n",
    "# Service Level Indicator: % successful events\n",
    "SLI = (success_events / total_events) * 100\n",
    "\n",
    "# Service Level Objective: 99.5% success rate\n",
    "SLO = 99.5\n",
    "\n",
    "# Error Budget: 0.5% = 432 failed events/day (@ 10k events/hour)\n",
    "error_budget = (100 - SLO) / 100 * total_events\n",
    "\n",
    "if SLI < SLO:\n",
    "    alert(\"SLO breach: pausing non-critical deployments\")\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50413a",
   "metadata": {},
   "source": [
    "- Reintentos con DLQ (cola de mensajes de errores) y trazabilidad por `event_id`.\n",
    "- Idempotencia con checkpoint durable (SQLite/Redis/DB) y caducidad.\n",
    "- Backpressure: controlar tamaño de batch y límites de latencia.\n",
    "- Observabilidad: exportar métricas a Prometheus y logs estructurados.\n",
    "- Seguridad: evitar PII en logs; cifrado en tránsito y at-rest donde aplique."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
