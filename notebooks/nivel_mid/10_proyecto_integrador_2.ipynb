{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefa2d37",
   "metadata": {},
   "source": [
    "# ðŸ”„ Proyecto Integrador Mid 2: Kafka â†’ Streaming â†’ Data Lake y Monitoreo\n",
    "\n",
    "Objetivo: implementar un pipeline near-real-time que ingiere eventos en Kafka, los valida y transforma, escribe salidas particionadas en Parquet y expone mÃ©tricas bÃ¡sicas de procesamiento.\n",
    "\n",
    "- DuraciÃ³n: 120â€“150 min\n",
    "- Dificultad: Media/Alta\n",
    "- Prerrequisitos: Notebooks Mid 02 (Kafka), 05 (DataOps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697542d",
   "metadata": {},
   "source": [
    "## 0) Requisitos y ejecuciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4295e",
   "metadata": {},
   "source": [
    "- Necesitas un clÃºster Kafka en local (Docker Compose) o remoto.\n",
    "- Dependencias opcionales: `kafka-python` o `confluent-kafka`. No estÃ¡n activas por defecto en `requirements.txt`.\n",
    "- Este notebook incluye un modo de simulaciÃ³n sin Kafka para que puedas practicar la lÃ³gica de validaciÃ³n/transformaciÃ³n/sink.\n",
    "- Variables de entorno: `KAFKA_BOOTSTRAP_SERVERS`, `KAFKA_TOPIC`, `OUT_DIR` (por defecto `datasets/processed/pi2/`).\n",
    "\n",
    "Ejemplo Docker Compose (para referencia):\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.4.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.4.0\n",
    "    depends_on: [zookeeper]\n",
    "    ports: ['9092:9092']\n",
    "    environment:\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad1acd",
   "metadata": {},
   "source": [
    "## 1) Esquema de eventos y validaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904aa140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerberus import Validator\n",
    "event_schema = {\n",
    "  'event_id': {'type':'string', 'required': True},\n",
    "  'ts': {'type':'string', 'required': True},\n",
    "  'usuario_id': {'type':'integer', 'required': True},\n",
    "  'accion': {'type':'string', 'allowed':['click','view','purchase']},\n",
    "  'monto': {'type':'float', 'nullable': True, 'min': 0}\n",
    "}\n",
    "validator = Validator(event_schema, allow_unknown=True)\n",
    "def is_valid_event(evt):\n",
    "    return validator.validate(evt)\n",
    "\n",
    "# Ejemplo\n",
    "is_valid_event({'event_id':'e1','ts':'2025-10-30T12:00:00Z','usuario_id':1,'accion':'click','monto':None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99efdc",
   "metadata": {},
   "source": [
    "## 2) Productor (Kafka) y modo simulaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, random\n",
    "from datetime import datetime, timezone\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS','localhost:9092')\n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC','pi2_events')\n",
    "\n",
    "def gen_event(i: int):\n",
    "    return {\n",
    "        'event_id': f'e{i}',\n",
    "        'ts': datetime.now(timezone.utc).isoformat(),\n",
    "        'usuario_id': random.randint(1,1000),\n",
    "        'accion': random.choice(['click','view','purchase']),\n",
    "        'monto': round(random.uniform(1,500),2) if random.random()>0.8 else None\n",
    "    }\n",
    "\n",
    "def produce_simulation(n=50):\n",
    "    return [gen_event(i) for i in range(n)]\n",
    "\n",
    "# Productor Kafka (opcional)\n",
    "def produce_kafka(n=50):\n",
    "    try:\n",
    "        from kafka import KafkaProducer\n",
    "        producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        for i in range(n):\n",
    "            evt = gen_event(i)\n",
    "            producer.send(KAFKA_TOPIC, evt)\n",
    "        producer.flush()\n",
    "        return n\n",
    "    except Exception as e:\n",
    "        print('Kafka no disponible, usa modo simulaciÃ³n:', e)\n",
    "        return None\n",
    "\n",
    "simulated = produce_simulation(100)\n",
    "len(simulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e20bb",
   "metadata": {},
   "source": [
    "## 3) Consumidor/Procesador: validaciÃ³n, enriquecimiento e idempotencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sqlite3\n",
    "from typing import Iterable, Dict, Any\n",
    "\n",
    "OUT_DIR = os.getenv('OUT_DIR','datasets/processed/pi2/')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "CKPT_DB = os.path.join(OUT_DIR, 'checkpoint.sqlite')\n",
    "\n",
    "def ensure_ckpt():\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('CREATE TABLE IF NOT EXISTS seen (event_id TEXT PRIMARY KEY)')\n",
    "    conn.commit(); conn.close()\n",
    "\n",
    "def is_seen(event_id: str) -> bool:\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT 1 FROM seen WHERE event_id=?', (event_id,))\n",
    "    row = cur.fetchone()\n",
    "    conn.close()\n",
    "    return row is not None\n",
    "\n",
    "def mark_seen(event_id: str):\n",
    "    conn = sqlite3.connect(CKPT_DB)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('INSERT OR IGNORE INTO seen(event_id) VALUES (?)', (event_id,))\n",
    "    conn.commit(); conn.close()\n",
    "\n",
    "def enrich(evt: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    evt = dict(evt)\n",
    "    evt['processing_ts'] = datetime.now(timezone.utc).isoformat()\n",
    "    evt['monto'] = float(evt['monto']) if evt.get('monto') is not None else 0.0\n",
    "    return evt\n",
    "\n",
    "def process_events(events: Iterable[Dict[str,Any]]):\n",
    "    ensure_ckpt()\n",
    "    good, bad = [], []\n",
    "    for evt in events:\n",
    "        if not is_valid_event(evt):\n",
    "            bad.append({'evt':evt, 'err':'schema'})\n",
    "            continue\n",
    "        if is_seen(evt['event_id']):\n",
    "            logger.info(f\n",
    ")\n",
    "            continue\n",
    "        evt2 = enrich(evt)\n",
    "        good.append(evt2)\n",
    "        mark_seen(evt['event_id'])\n",
    "    return good, bad\n",
    "\n",
    "ok, ko = process_events(simulated)\n",
    "len(ok), len(ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae89ec",
   "metadata": {},
   "source": [
    "## 4) Sink: escribir Parquet particionado por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da70547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def write_parquet(events):\n",
    "    if not events:\n",
    "        return None\n",
    "    df = pd.DataFrame(events)\n",
    "    df['date'] = pd.to_datetime(df['ts']).dt.date.astype(str)\n",
    "    for d, part in df.groupby('date'):\n",
    "        part_dir = Path(OUT_DIR) / f'date={d}'\n",
    "        part_dir.mkdir(parents=True, exist_ok=True)\n",
    "        fp = part_dir / f'events_{int(time.time())}.parquet'\n",
    "        part.drop(columns=['date']).to_parquet(fp, index=False)\n",
    "    return True\n",
    "\n",
    "write_parquet(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2f805",
   "metadata": {},
   "source": [
    "## 5) MÃ©tricas y logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total = len(simulated)\n",
    "validos = len(ok)\n",
    "invalidos = len(ko)\n",
    "metricas = f'total {total}\n",
    "validos {validos}\n",
    "invalidos {invalidos}\n",
    "'\n",
    "with open(os.path.join(OUT_DIR, 'metrics.txt'), 'w') as f:\n",
    "    f.write(metricas)\n",
    "logger.info(f'metrics total={total} validos={validos} invalidos={invalidos}')\n",
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b531b3",
   "metadata": {},
   "source": [
    "## 6) Consumidor Kafka (opcional en vivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_kafka(max_messages=100):\n",
    "    try:\n",
    "        from kafka import KafkaConsumer\n",
    "        consumer = KafkaConsumer(\n",
    "            KAFKA_TOPIC,\n",
    "            bootstrap_servers=KAFKA_BOOTSTRAP,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=False,\n",
    "            value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    "        )\n",
    "        batch = []\n",
    "        for i, msg in enumerate(consumer):\n",
    "            batch.append(msg.value)\n",
    "            if len(batch) >= 50 or i+1 >= max_messages:\n",
    "                ok, ko = process_events(batch)\n",
    "                write_parquet(ok)\n",
    "                # commit offsets al final del batch\n",
    "                consumer.commit()\n",
    "                logger.info(f'batch size={len(batch)} ok={len(ok)} ko={len(ko)}')\n",
    "                batch = []\n",
    "                if i+1 >= max_messages:\n",
    "                    break\n",
    "        consumer.close()\n",
    "    except Exception as e:\n",
    "        print('Kafka no disponible, salta esta secciÃ³n:', e)\n",
    "\n",
    "# consume_kafka(200)  # Descomentar para una prueba en vivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c8605",
   "metadata": {},
   "source": [
    "## 7) Buenas prÃ¡cticas y extensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50413a",
   "metadata": {},
   "source": [
    "- Reintentos con DLQ (cola de mensajes de errores) y trazabilidad por `event_id`.\n",
    "- Idempotencia con checkpoint durable (SQLite/Redis/DB) y caducidad.\n",
    "- Backpressure: controlar tamaÃ±o de batch y lÃ­mites de latencia.\n",
    "- Observabilidad: exportar mÃ©tricas a Prometheus y logs estructurados.\n",
    "- Seguridad: evitar PII en logs; cifrado en trÃ¡nsito y at-rest donde aplique."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
