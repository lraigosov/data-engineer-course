{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e55eee6",
   "metadata": {},
   "source": [
    "# ‚ôªÔ∏è DataOps y CI/CD para Pipelines de Datos\n",
    "\n",
    "Objetivo: implementar pr√°cticas de DataOps con control de calidad, pruebas automatizadas, hooks de Git y pipelines de CI/CD (GitHub Actions) para flujos de datos.\n",
    "\n",
    "- Duraci√≥n: 90 min\n",
    "- Dificultad: Media\n",
    "- Prerrequisitos: Pytest b√°sico, Git y GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28db7f",
   "metadata": {},
   "source": [
    "### ‚ôªÔ∏è **DataOps: DevOps para Datos**\n",
    "\n",
    "**Definici√≥n:**  \n",
    "DataOps es la aplicaci√≥n de pr√°cticas DevOps (CI/CD, IaC, monitoreo) a pipelines de datos para aumentar velocidad, calidad y confiabilidad del desarrollo de analytics.\n",
    "\n",
    "**Principios Core:**\n",
    "\n",
    "1. **Automation First:**\n",
    "   - Tests autom√°ticos de calidad de datos\n",
    "   - Deployment autom√°tico de pipelines\n",
    "   - Alertas autom√°ticas ante anomal√≠as\n",
    "\n",
    "2. **Version Control Everything:**\n",
    "   - C√≥digo (Python, SQL)\n",
    "   - Configuraci√≥n (YAML, JSON)\n",
    "   - Schemas (Avro, Protobuf)\n",
    "   - Infraestructura (Terraform, CloudFormation)\n",
    "\n",
    "3. **Observability:**\n",
    "   - Logging estructurado (JSON logs)\n",
    "   - M√©tricas: latencia, throughput, error rate\n",
    "   - Lineage: Trazabilidad origen ‚Üí destino\n",
    "\n",
    "4. **Testing Pyramid para Datos:**\n",
    "\n",
    "```\n",
    "       ‚ñ≤\n",
    "      /E2E\\           ‚Üê End-to-End (pocos, lentos)\n",
    "     /‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\\n",
    "    /Integr.\\        ‚Üê Integration tests (medianos)\n",
    "   /‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\\n",
    "  /Unit Tests \\      ‚Üê Unit tests (muchos, r√°pidos)\n",
    " /‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\\n",
    "  Data Quality       ‚Üê Schema validation, nulls, ranges\n",
    "```\n",
    "\n",
    "**Comparaci√≥n con Software Engineering:**\n",
    "\n",
    "| Software DevOps | DataOps |\n",
    "|-----------------|---------|\n",
    "| Unit tests | Schema validation |\n",
    "| Integration tests | Pipeline tests |\n",
    "| Code review | Data quality review |\n",
    "| Blue/Green deploy | Dual-write patterns |\n",
    "| Monitoring (APM) | Data observability |\n",
    "\n",
    "**Herramientas:**\n",
    "\n",
    "- **Testing**: Great Expectations, Pandera, dbt tests\n",
    "- **CI/CD**: GitHub Actions, GitLab CI, Jenkins\n",
    "- **Orchestration**: Airflow, Prefect, Dagster\n",
    "- **Observability**: DataDog, Monte Carlo, Bigeye\n",
    "- **Version Control**: Git + DVC (Data Version Control)\n",
    "\n",
    "**Impacto:**\n",
    "\n",
    "- ‚¨áÔ∏è 80% reducci√≥n en incidentes de datos\n",
    "- ‚¨ÜÔ∏è 10x velocidad de deployment\n",
    "- ‚¨ÜÔ∏è 95%+ confianza en datos para decisiones\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda789a7",
   "metadata": {},
   "source": [
    "## 1. Pruebas de datos con Great Expectations y Pandera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0b146",
   "metadata": {},
   "source": [
    "### üõ°Ô∏è **Data Quality Testing: Great Expectations vs Pandera**\n",
    "\n",
    "**Great Expectations:**\n",
    "\n",
    "```python\n",
    "# Approach declarativo con expectations\n",
    "df.expect_column_values_to_not_be_null('user_id')\n",
    "df.expect_column_values_to_be_between('age', 0, 120)\n",
    "df.expect_column_values_to_be_in_set('status', ['active', 'inactive'])\n",
    "df.expect_column_values_to_match_regex('email', r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 300+ expectations predefinidas\n",
    "- Data Docs: HTML reports autom√°ticos\n",
    "- Checkpoints: Validaci√≥n en pipeline stages\n",
    "- Profiling: Auto-genera expectations desde data samples\n",
    "\n",
    "**Pandera (Type Hints para DataFrames):**\n",
    "\n",
    "```python\n",
    "from pandera import DataFrameSchema, Column, Check\n",
    "\n",
    "schema = DataFrameSchema({\n",
    "    'user_id': Column(int, Check.gt(0), nullable=False),\n",
    "    'age': Column(int, Check.in_range(0, 120)),\n",
    "    'email': Column(str, Check.str_matches(r'^[\\w\\.-]+@')),\n",
    "    'created_at': Column('datetime64[ns]')\n",
    "})\n",
    "\n",
    "@pa.check_types\n",
    "def process_users(df: DataFrame[schema]) -> DataFrame:\n",
    "    # Validaci√≥n autom√°tica en runtime\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "- Integraci√≥n nativa con type hints (mypy compatible)\n",
    "- Lightweight (sin dependencias pesadas)\n",
    "- Hip√≥tesis testing estad√≠stico integrado\n",
    "\n",
    "**Comparaci√≥n:**\n",
    "\n",
    "| Aspecto | Great Expectations | Pandera |\n",
    "|---------|-------------------|---------|\n",
    "| **Curva aprendizaje** | Media-alta | Baja |\n",
    "| **Reporting** | Excelente (Data Docs) | B√°sico |\n",
    "| **Performance** | M√°s lento (overhead) | M√°s r√°pido |\n",
    "| **Type Safety** | No | S√≠ (mypy) |\n",
    "| **Use Case** | Enterprise, governance | Fast prototyping |\n",
    "\n",
    "**Estrategia H√≠brida:**\n",
    "- Pandera: Desarrollo local + CI tests\n",
    "- Great Expectations: Producci√≥n + auditor√≠a\n",
    "\n",
    "**Niveles de Validaci√≥n:**\n",
    "1. **Schema**: Columnas, tipos, nullability\n",
    "2. **Rango**: Min, max, percentiles\n",
    "3. **Relaciones**: Foreign keys, duplicates\n",
    "4. **Distribuci√≥n**: Mean, std, outliers\n",
    "5. **Negocio**: Reglas custom (ej: revenue >= costs)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    import great_expectations as ge\n",
    "    from pandera import DataFrameSchema, Column, Check\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id':[1,2,3],\n",
    "        'total':[100.0, 50.0, 25.5],\n",
    "        'metodo_pago':['tarjeta','cash','tarjeta']\n",
    "    })\n",
    "    # Great Expectations estilo r√°pido\n",
    "    gdf = ge.from_pandas(df)\n",
    "    gdf.expect_column_values_to_not_be_null('venta_id')\n",
    "    gdf.expect_column_values_to_be_between('total', 0, 10000)\n",
    "    print('GE checks:', gdf.validate().to_json_dict()['statistics'])\n",
    "    # Pandera schema\n",
    "    schema = DataFrameSchema({\n",
    "        'venta_id': Column(int, Check.gt(0)),\n",
    "        'total': Column(float, Check.ge(0)),\n",
    "        'metodo_pago': Column(str)\n",
    "    })\n",
    "    schema.validate(df)\n",
    "    print('Pandera OK')\n",
    "except Exception as e:\n",
    "    print('Instala great-expectations y pandera si deseas ejecutar este bloque:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331107e6",
   "metadata": {},
   "source": [
    "## 2. Pytest: estructura m√≠nima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa37420",
   "metadata": {},
   "source": [
    "### üß™ **Pytest: Testing Framework para Data Pipelines**\n",
    "\n",
    "**Estructura de Proyecto:**\n",
    "```\n",
    "proyecto/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ extract.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transform.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ load.py\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ conftest.py         ‚Üê Fixtures compartidos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_transform.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_load.py\n",
    "‚îú‚îÄ‚îÄ pytest.ini              ‚Üê Configuraci√≥n\n",
    "‚îî‚îÄ‚îÄ requirements-dev.txt\n",
    "```\n",
    "\n",
    "**Fixtures (Setup/Teardown):**\n",
    "```python\n",
    "# conftest.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_df():\n",
    "    \"\"\"Fixture reutilizable para tests\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'id': [1, 2, 3],\n",
    "        'value': [10, 20, 30]\n",
    "    })\n",
    "\n",
    "@pytest.fixture\n",
    "def db_connection():\n",
    "    conn = create_connection()\n",
    "    yield conn  # Test ejecuta aqu√≠\n",
    "    conn.close()  # Cleanup autom√°tico\n",
    "```\n",
    "\n",
    "**Parametrizaci√≥n (Data-Driven Tests):**\n",
    "```python\n",
    "@pytest.mark.parametrize(\"input,expected\", [\n",
    "    (10, 10.0),\n",
    "    (-5, 0.0),\n",
    "    ('invalid', 0.0),\n",
    "    (None, 0.0)\n",
    "])\n",
    "def test_clean_total(input, expected):\n",
    "    assert clean_total(input) == expected\n",
    "```\n",
    "\n",
    "**Mocking (Aislamiento de dependencias):**\n",
    "```python\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "@patch('requests.get')\n",
    "def test_api_extractor(mock_get):\n",
    "    mock_get.return_value.json.return_value = {'data': [1, 2, 3]}\n",
    "    result = extract_from_api('https://api.example.com')\n",
    "    assert len(result) == 3\n",
    "    mock_get.assert_called_once()\n",
    "```\n",
    "\n",
    "**Coverage (Cobertura de C√≥digo):**\n",
    "```bash\n",
    "pytest --cov=src --cov-report=html\n",
    "# Genera htmlcov/index.html con l√≠neas cubiertas/no cubiertas\n",
    "```\n",
    "\n",
    "**Markers (Categorizaci√≥n de Tests):**\n",
    "```python\n",
    "@pytest.mark.slow\n",
    "def test_large_dataset():\n",
    "    # Test que tarda >10s\n",
    "    pass\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_db_connection():\n",
    "    # Test que requiere DB real\n",
    "    pass\n",
    "\n",
    "# Ejecutar solo tests r√°pidos\n",
    "pytest -m \"not slow\"\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Tests independientes (sin orden)\n",
    "- Nombres descriptivos: `test_transform_handles_null_values`\n",
    "- Arrange-Act-Assert pattern\n",
    "- Un assert por test (o conceptos relacionados)\n",
    "- Tests < 1s (unit), < 10s (integration)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e608f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_transform.py\n",
    "sample_code = r'''\n",
    "# archivo: src/transform.py\n",
    "def clean_total(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        return max(v, 0.0)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# archivo: tests/test_transform.py\n",
    "from src.transform import clean_total\n",
    "def test_clean_total():\n",
    "    assert clean_total(10) == 10.0\n",
    "    assert clean_total(-5) == 0.0\n",
    "    assert clean_total('oops') == 0.0\n",
    "'''\n",
    "print(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e1144",
   "metadata": {},
   "source": [
    "## 3. Pre-commit hooks (lint, format, tests r√°pidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23eaf0",
   "metadata": {},
   "source": [
    "### ü™ù **Pre-commit Hooks: Quality Gates Locales**\n",
    "\n",
    "**¬øQu√© son los Pre-commit Hooks?**  \n",
    "Scripts que se ejecutan autom√°ticamente **antes de cada commit** para validar c√≥digo, formato, seguridad, etc. Bloquean el commit si fallan.\n",
    "\n",
    "**Instalaci√≥n:**\n",
    "```bash\n",
    "pip install pre-commit\n",
    "# Crear .pre-commit-config.yaml\n",
    "pre-commit install  # Activa hooks en .git/hooks/\n",
    "```\n",
    "\n",
    "**Hooks Esenciales para Data Engineering:**\n",
    "\n",
    "1. **Black (Code Formatter):**\n",
    "   - Auto-formatea c√≥digo Python (PEP 8)\n",
    "   - \"The uncompromising formatter\"\n",
    "   ```python\n",
    "   # Antes\n",
    "   x=[1,2,3];y={'a':1,'b':2}\n",
    "   \n",
    "   # Despu√©s\n",
    "   x = [1, 2, 3]\n",
    "   y = {\"a\": 1, \"b\": 2}\n",
    "   ```\n",
    "\n",
    "2. **isort (Import Organizer):**\n",
    "   - Ordena imports alfab√©ticamente\n",
    "   ```python\n",
    "   # Antes\n",
    "   import sys\n",
    "   from myproject import utils\n",
    "   import os\n",
    "   \n",
    "   # Despu√©s\n",
    "   import os\n",
    "   import sys\n",
    "   from myproject import utils\n",
    "   ```\n",
    "\n",
    "3. **flake8 (Linter):**\n",
    "   - Detecta errores sint√°cticos, variables no usadas, imports redundantes\n",
    "   - Warnings: E501 (l√≠nea >79 chars), F841 (variable asignada pero no usada)\n",
    "\n",
    "4. **mypy (Type Checker):**\n",
    "   - Valida type hints\n",
    "   ```python\n",
    "   def add(x: int, y: int) -> int:\n",
    "       return x + y\n",
    "   \n",
    "   add(\"1\", \"2\")  # ‚ùå mypy detecta error\n",
    "   ```\n",
    "\n",
    "5. **detect-secrets:**\n",
    "   - Busca API keys, passwords hardcodeados\n",
    "   - Bloquea commits con `AWS_SECRET_ACCESS_KEY = \"xxx\"`\n",
    "\n",
    "**Hooks Custom para Datos:**\n",
    "```yaml\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: check-sql-syntax\n",
    "      name: Validate SQL files\n",
    "      entry: sqlfluff lint\n",
    "      language: system\n",
    "      files: \\.sql$\n",
    "    \n",
    "    - id: validate-schemas\n",
    "      name: Check Avro schemas\n",
    "      entry: python scripts/validate_schemas.py\n",
    "      language: python\n",
    "      files: schemas/.*\\.avsc$\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "```bash\n",
    "git add transform.py\n",
    "git commit -m \"Fix bug\"\n",
    "  ‚Üì\n",
    "[pre-commit] black........................Passed\n",
    "[pre-commit] isort.......................Passed\n",
    "[pre-commit] flake8......................Failed\n",
    "  - src/transform.py:10:1: F401 'pandas' imported but unused\n",
    "  ‚Üì\n",
    "[Commit bloqueado - fix errores y reintenta]\n",
    "```\n",
    "\n",
    "**Skip Hooks (Emergencia):**\n",
    "```bash\n",
    "git commit --no-verify -m \"Hotfix cr√≠tico\"\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- ‚¨ÜÔ∏è Calidad de c√≥digo consistente en equipo\n",
    "- ‚¨áÔ∏è Menos issues en code review\n",
    "- ‚ö° Feedback instant√°neo vs esperar CI\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_commit_cfg = r'''\n",
    "repos:\n",
    "  - repo: https://github.com/psf/black\n",
    "    rev: 22.6.0\n",
    "    hooks:\n",
    "      - id: black\n",
    "  - repo: https://github.com/PyCQA/isort\n",
    "    rev: 5.10.1\n",
    "    hooks:\n",
    "      - id: isort\n",
    "  - repo: https://github.com/pycqa/flake8\n",
    "    rev: 5.0.4\n",
    "    hooks:\n",
    "      - id: flake8\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: pytest-quick\n",
    "        name: pytest quick\n",
    "        entry: pytest -q\n",
    "        language: system\n",
    "        types: [python]\n",
    "'''\n",
    "print(pre_commit_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de1d7b",
   "metadata": {},
   "source": [
    "## 4. GitHub Actions: CI para validar el repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185194da",
   "metadata": {},
   "source": [
    "### üîÑ **GitHub Actions: CI/CD Autom√°tico**\n",
    "\n",
    "**Concepto:**  \n",
    "GitHub Actions ejecuta workflows autom√°ticamente en eventos (push, PR, schedule) en runners de GitHub (Ubuntu/Windows/macOS).\n",
    "\n",
    "**Anatom√≠a de un Workflow:**\n",
    "\n",
    "```yaml\n",
    "name: ci                          # Nombre del workflow\n",
    "on: [push, pull_request]          # Triggers\n",
    "\n",
    "jobs:\n",
    "  test:                           # Job ID\n",
    "    runs-on: ubuntu-latest        # Runner environment\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3 # Action oficial (git clone)\n",
    "      \n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          cache: 'pip'            # Cache de dependencias\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: pytest --cov=src --cov-report=xml\n",
    "      \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          files: ./coverage.xml\n",
    "```\n",
    "\n",
    "**Triggers Avanzados:**\n",
    "\n",
    "1. **Schedule (Cron):**\n",
    "   ```yaml\n",
    "   on:\n",
    "     schedule:\n",
    "       - cron: '0 2 * * *'  # Diario a las 2 AM UTC\n",
    "   ```\n",
    "\n",
    "2. **Manual Dispatch:**\n",
    "   ```yaml\n",
    "   on:\n",
    "     workflow_dispatch:\n",
    "       inputs:\n",
    "         environment:\n",
    "           description: 'Target environment'\n",
    "           required: true\n",
    "           default: 'staging'\n",
    "   ```\n",
    "\n",
    "3. **Path Filters:**\n",
    "   ```yaml\n",
    "   on:\n",
    "     push:\n",
    "       paths:\n",
    "         - 'src/**'\n",
    "         - 'tests/**'\n",
    "   ```\n",
    "\n",
    "**Parallel Jobs (Matrix Strategy):**\n",
    "```yaml\n",
    "jobs:\n",
    "  test:\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: ['3.8', '3.9', '3.10', '3.11']\n",
    "        os: [ubuntu-latest, windows-latest]\n",
    "    runs-on: ${{ matrix.os }}\n",
    "    steps:\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ matrix.python-version }}\n",
    "```\n",
    "\n",
    "**Secrets Management:**\n",
    "```yaml\n",
    "- name: Deploy to AWS\n",
    "  env:\n",
    "    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "  run: aws s3 sync dist/ s3://my-bucket/\n",
    "```\n",
    "\n",
    "**Pipeline Completo para Data:**\n",
    "```yaml\n",
    "jobs:\n",
    "  lint:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: flake8 src/\n",
    "  \n",
    "  test:\n",
    "    needs: lint\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: pytest -v\n",
    "  \n",
    "  data-quality:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: great_expectations checkpoint run validation_suite\n",
    "  \n",
    "  deploy:\n",
    "    needs: [test, data-quality]\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: docker build -t pipeline:${{ github.sha }} .\n",
    "      - run: docker push pipeline:${{ github.sha }}\n",
    "```\n",
    "\n",
    "**Artifacts (Compartir entre jobs):**\n",
    "```yaml\n",
    "- name: Generate report\n",
    "  run: python generate_report.py\n",
    "\n",
    "- uses: actions/upload-artifact@v3\n",
    "  with:\n",
    "    name: data-report\n",
    "    path: reports/*.html\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Public repos: Gratis ilimitado\n",
    "- Private repos: 2,000 min/mes gratis (luego $0.008/min)\n",
    "- Self-hosted runners para reducir costos\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gha_yaml = r'''\n",
    "name: ci\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          pip install -r curso_ingenieria_datos/requirements.txt\n",
    "      - name: Lint & Test\n",
    "        run: |\n",
    "          pip install pytest flake8 black\n",
    "          flake8 .\n",
    "          pytest -q\n",
    "'''\n",
    "print(gha_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb9d12",
   "metadata": {},
   "source": [
    "## 5. Observabilidad: logs y m√©tricas m√≠nimas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43f070",
   "metadata": {},
   "source": [
    "### üìä **Observability: Logs, M√©tricas y Alertas**\n",
    "\n",
    "**Structured Logging con Loguru:**\n",
    "\n",
    "```python\n",
    "from loguru import logger\n",
    "\n",
    "# Configuraci√≥n\n",
    "logger.add(\n",
    "    \"logs/pipeline_{time}.log\",\n",
    "    rotation=\"500 MB\",\n",
    "    retention=\"10 days\",\n",
    "    level=\"INFO\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\"\n",
    ")\n",
    "\n",
    "# Contexto estructurado\n",
    "logger.bind(pipeline=\"etl\", dataset=\"ventas\").info(\n",
    "    \"Processed batch\",\n",
    "    records=1000,\n",
    "    latency_ms=150.5,\n",
    "    status=\"success\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Output JSON (Parse con herramientas):**\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"2025-10-30T14:23:45.123Z\",\n",
    "  \"level\": \"INFO\",\n",
    "  \"pipeline\": \"etl\",\n",
    "  \"dataset\": \"ventas\",\n",
    "  \"message\": \"Processed batch\",\n",
    "  \"records\": 1000,\n",
    "  \"latency_ms\": 150.5,\n",
    "  \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "\n",
    "**M√©tricas Esenciales (RED Pattern):**\n",
    "\n",
    "1. **Rate (Throughput):**\n",
    "   ```python\n",
    "   records_processed = 10000\n",
    "   duration_seconds = 60\n",
    "   throughput = records_processed / duration_seconds\n",
    "   logger.info(f\"throughput={throughput:.2f} records/sec\")\n",
    "   ```\n",
    "\n",
    "2. **Errors (Error Rate):**\n",
    "   ```python\n",
    "   total_records = 10000\n",
    "   failed_records = 50\n",
    "   error_rate = (failed_records / total_records) * 100\n",
    "   logger.warning(f\"error_rate={error_rate:.2f}%\")\n",
    "   \n",
    "   if error_rate > 5.0:\n",
    "       raise AlertException(\"High error rate detected\")\n",
    "   ```\n",
    "\n",
    "3. **Duration (Latency):**\n",
    "   ```python\n",
    "   import time\n",
    "   start = time.time()\n",
    "   process_batch()\n",
    "   latency = time.time() - start\n",
    "   logger.info(f\"latency={latency:.3f}s\")\n",
    "   ```\n",
    "\n",
    "**Prometheus Metrics (Production):**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "records_processed = Counter('records_processed_total', 'Total records')\n",
    "pipeline_duration = Histogram('pipeline_duration_seconds', 'Duration')\n",
    "active_pipelines = Gauge('active_pipelines', 'Currently running')\n",
    "\n",
    "@pipeline_duration.time()\n",
    "def run_pipeline():\n",
    "    records_processed.inc(1000)\n",
    "    # L√≥gica del pipeline\n",
    "```\n",
    "\n",
    "**Data Observability Platforms:**\n",
    "\n",
    "1. **Monte Carlo / Bigeye:**\n",
    "   - Anomaly detection autom√°tico (distribuci√≥n, volumen, freshness)\n",
    "   - Lineage visual (upstream/downstream dependencies)\n",
    "   - Alertas: \"Tabla X no actualizada en 4 horas\"\n",
    "\n",
    "2. **DataDog / New Relic:**\n",
    "   - APM para pipelines\n",
    "   - Dashboards customizables\n",
    "   - Distributed tracing\n",
    "\n",
    "3. **dbt Cloud:**\n",
    "   - Test results hist√≥ricos\n",
    "   - Model timing trends\n",
    "   - Exposures: Qu√© dashboards dependen de qu√© modelos\n",
    "\n",
    "**Alerting Strategy:**\n",
    "```python\n",
    "class DataQualityAlert:\n",
    "    def __init__(self):\n",
    "        self.thresholds = {\n",
    "            'null_rate': 0.05,      # Max 5% nulls\n",
    "            'duplicate_rate': 0.01, # Max 1% duplicates\n",
    "            'freshness_hours': 4    # Data < 4h old\n",
    "        }\n",
    "    \n",
    "    def check_and_alert(self, metrics):\n",
    "        if metrics['null_rate'] > self.thresholds['null_rate']:\n",
    "            send_slack_alert(f\"‚ö†Ô∏è High null rate: {metrics['null_rate']:.2%}\")\n",
    "            send_pagerduty(severity='high')\n",
    "```\n",
    "\n",
    "**Golden Signals para Data:**\n",
    "- **Freshness**: ¬øCu√°ndo lleg√≥ el √∫ltimo dato?\n",
    "- **Volume**: ¬øN√∫mero de registros esperado?\n",
    "- **Schema**: ¬øColumnas/tipos cambiaron?\n",
    "- **Distribution**: ¬øValores fuera de rango?\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import time, random\n",
    "\n",
    "def process_batch(n=5):\n",
    "    for i in range(n):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if random.random() < 0.1:\n",
    "                raise ValueError('Fallo aleatorio')\n",
    "            time.sleep(0.05)\n",
    "            latency = time.time() - start\n",
    "            logger.info(f'item={i} status=ok latency={latency:.3f}s')\n",
    "        except Exception as e:\n",
    "            logger.error(f'item={i} status=error err={e}')\n",
    "process_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910500e",
   "metadata": {},
   "source": [
    "## 6. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffabc4",
   "metadata": {},
   "source": [
    "1. Agrega una regla de pre-commit que bloquee archivos > 2 MB.\n",
    "2. Crea un workflow adicional que ejecute validaciones de datos con Great Expectations.\n",
    "3. A√±ade cobertura de pruebas y publica un badge en el README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Conectores Avanzados: REST, GraphQL, SFTP ‚Üí](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "",
    "**üìö √çndice de Nivel Mid:**",
    "- [Apache Airflow: Fundamentos](01_apache_airflow_fundamentos.ipynb)",
    "- [Streaming con Kafka](02_streaming_kafka.ipynb)",
    "- [Cloud AWS: S3, Glue, Athena, Lambda](03_cloud_aws.ipynb)",
    "- [Cloud GCP: BigQuery, Dataflow, Cloud Run](03b_cloud_gcp.ipynb)",
    "- [Cloud Azure: ADLS, Synapse, ADF, Databricks](03c_cloud_azure.ipynb)",
    "- [Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "- [DataOps y CI/CD](05_dataops_cicd.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "- [Conectores Avanzados: REST, GraphQL, SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "- [Optimizaci√≥n SQL y Particionado](07_optimizacion_sql_particionado.ipynb)",
    "- [FastAPI y Servicios de Datos](08_fastapi_servicios_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Conectores Avanzados: REST, GraphQL, SFTP ‚Üí](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "",
    "**üìö √çndice de Nivel Mid:**",
    "- [Apache Airflow: Fundamentos](01_apache_airflow_fundamentos.ipynb)",
    "- [Streaming con Kafka](02_streaming_kafka.ipynb)",
    "- [Cloud AWS: S3, Glue, Athena, Lambda](03_cloud_aws.ipynb)",
    "- [Cloud GCP: BigQuery, Dataflow, Cloud Run](03b_cloud_gcp.ipynb)",
    "- [Cloud Azure: ADLS, Synapse, ADF, Databricks](03c_cloud_azure.ipynb)",
    "- [Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "- [DataOps y CI/CD](05_dataops_cicd.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "- [Conectores Avanzados: REST, GraphQL, SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "- [Optimizaci√≥n SQL y Particionado](07_optimizacion_sql_particionado.ipynb)",
    "- [FastAPI y Servicios de Datos](08_fastapi_servicios_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üß≠ Navegaci√≥n",
    "",
    "**‚Üê Anterior:** [‚Üê Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "",
    "**Siguiente ‚Üí:** [Conectores Avanzados: REST, GraphQL, SFTP ‚Üí](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "",
    "**üìö √çndice de Nivel Mid:**",
    "- [Apache Airflow: Fundamentos](01_apache_airflow_fundamentos.ipynb)",
    "- [Streaming con Kafka](02_streaming_kafka.ipynb)",
    "- [Cloud AWS: S3, Glue, Athena, Lambda](03_cloud_aws.ipynb)",
    "- [Cloud GCP: BigQuery, Dataflow, Cloud Run](03b_cloud_gcp.ipynb)",
    "- [Cloud Azure: ADLS, Synapse, ADF, Databricks](03c_cloud_azure.ipynb)",
    "- [Bases de Datos: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)",
    "- [DataOps y CI/CD](05_dataops_cicd.ipynb) ‚Üê üîµ Est√°s aqu√≠",
    "- [Conectores Avanzados: REST, GraphQL, SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)",
    "- [Optimizaci√≥n SQL y Particionado](07_optimizacion_sql_particionado.ipynb)",
    "- [FastAPI y Servicios de Datos](08_fastapi_servicios_datos.ipynb)",
    "- [Proyecto Integrador 1](09_proyecto_integrador_1.ipynb)",
    "- [Proyecto Integrador 2](10_proyecto_integrador_2.ipynb)",
    "",
    "**üéì Otros Niveles:**",
    "- [Nivel Junior](../nivel_junior/README.md)",
    "- [Nivel Mid](../nivel_mid/README.md)",
    "- [Nivel Senior](../nivel_senior/README.md)",
    "- [Nivel GenAI](../nivel_genai/README.md)",
    "- [Negocio LATAM](../negocios_latam/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}