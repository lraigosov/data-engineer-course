{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e55eee6",
   "metadata": {},
   "source": [
    "# \u267b\ufe0f DataOps y CI/CD para Pipelines de Datos\n",
    "\n",
    "Objetivo: implementar pr\u00e1cticas de DataOps con control de calidad, pruebas automatizadas, hooks de Git y pipelines de CI/CD (GitHub Actions) para flujos de datos.\n",
    "\n",
    "- Duraci\u00f3n: 90 min\n",
    "- Dificultad: Media\n",
    "- Prerrequisitos: Pytest b\u00e1sico, Git y GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28db7f",
   "metadata": {},
   "source": [
    "### \u267b\ufe0f **DataOps: DevOps para Datos**\n",
    "\n",
    "**Definici\u00f3n:**  \n",
    "DataOps es la aplicaci\u00f3n de pr\u00e1cticas DevOps (CI/CD, IaC, monitoreo) a pipelines de datos para aumentar velocidad, calidad y confiabilidad del desarrollo de analytics.\n",
    "\n",
    "**Principios Core:**\n",
    "\n",
    "1. **Automation First:**\n",
    "   - Tests autom\u00e1ticos de calidad de datos\n",
    "   - Deployment autom\u00e1tico de pipelines\n",
    "   - Alertas autom\u00e1ticas ante anomal\u00edas\n",
    "\n",
    "2. **Version Control Everything:**\n",
    "   - C\u00f3digo (Python, SQL)\n",
    "   - Configuraci\u00f3n (YAML, JSON)\n",
    "   - Schemas (Avro, Protobuf)\n",
    "   - Infraestructura (Terraform, CloudFormation)\n",
    "\n",
    "3. **Observability:**\n",
    "   - Logging estructurado (JSON logs)\n",
    "   - M\u00e9tricas: latencia, throughput, error rate\n",
    "   - Lineage: Trazabilidad origen \u2192 destino\n",
    "\n",
    "4. **Testing Pyramid para Datos:**\n",
    "\n",
    "```\n",
    "       \u25b2\n",
    "      /E2E\\           \u2190 End-to-End (pocos, lentos)\n",
    "     /\u2500\u2500\u2500\u2500\u2500\\\n",
    "    /Integr.\\        \u2190 Integration tests (medianos)\n",
    "   /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n",
    "  /Unit Tests \\      \u2190 Unit tests (muchos, r\u00e1pidos)\n",
    " /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n",
    "  Data Quality       \u2190 Schema validation, nulls, ranges\n",
    "```\n",
    "\n",
    "**Comparaci\u00f3n con Software Engineering:**\n",
    "\n",
    "| Software DevOps | DataOps |\n",
    "|-----------------|---------|\n",
    "| Unit tests | Schema validation |\n",
    "| Integration tests | Pipeline tests |\n",
    "| Code review | Data quality review |\n",
    "| Blue/Green deploy | Dual-write patterns |\n",
    "| Monitoring (APM) | Data observability |\n",
    "\n",
    "**Herramientas:**\n",
    "\n",
    "- **Testing**: Great Expectations, Pandera, dbt tests\n",
    "- **CI/CD**: GitHub Actions, GitLab CI, Jenkins\n",
    "- **Orchestration**: Airflow, Prefect, Dagster\n",
    "- **Observability**: DataDog, Monte Carlo, Bigeye\n",
    "- **Version Control**: Git + DVC (Data Version Control)\n",
    "\n",
    "**Impacto:**\n",
    "\n",
    "- \u2b07\ufe0f 80% reducci\u00f3n en incidentes de datos\n",
    "- \u2b06\ufe0f 10x velocidad de deployment\n",
    "- \u2b06\ufe0f 95%+ confianza en datos para decisiones\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda789a7",
   "metadata": {},
   "source": [
    "## 1. Pruebas de datos con Great Expectations y Pandera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0b146",
   "metadata": {},
   "source": [
    "### \ud83d\udee1\ufe0f **Data Quality Testing: Great Expectations vs Pandera**\n",
    "\n",
    "**Great Expectations:**\n",
    "\n",
    "```python\n",
    "# Approach declarativo con expectations\n",
    "df.expect_column_values_to_not_be_null('user_id')\n",
    "df.expect_column_values_to_be_between('age', 0, 120)\n",
    "df.expect_column_values_to_be_in_set('status', ['active', 'inactive'])\n",
    "df.expect_column_values_to_match_regex('email', r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n",
    "```\n",
    "\n",
    "**Caracter\u00edsticas:**\n",
    "- 300+ expectations predefinidas\n",
    "- Data Docs: HTML reports autom\u00e1ticos\n",
    "- Checkpoints: Validaci\u00f3n en pipeline stages\n",
    "- Profiling: Auto-genera expectations desde data samples\n",
    "\n",
    "**Pandera (Type Hints para DataFrames):**\n",
    "\n",
    "```python\n",
    "from pandera import DataFrameSchema, Column, Check\n",
    "\n",
    "schema = DataFrameSchema({\n",
    "    'user_id': Column(int, Check.gt(0), nullable=False),\n",
    "    'age': Column(int, Check.in_range(0, 120)),\n",
    "    'email': Column(str, Check.str_matches(r'^[\\w\\.-]+@')),\n",
    "    'created_at': Column('datetime64[ns]')\n",
    "})\n",
    "\n",
    "@pa.check_types\n",
    "def process_users(df: DataFrame[schema]) -> DataFrame:\n",
    "    # Validaci\u00f3n autom\u00e1tica en runtime\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "- Integraci\u00f3n nativa con type hints (mypy compatible)\n",
    "- Lightweight (sin dependencias pesadas)\n",
    "- Hip\u00f3tesis testing estad\u00edstico integrado\n",
    "\n",
    "**Comparaci\u00f3n:**\n",
    "\n",
    "| Aspecto | Great Expectations | Pandera |\n",
    "|---------|-------------------|---------|\n",
    "| **Curva aprendizaje** | Media-alta | Baja |\n",
    "| **Reporting** | Excelente (Data Docs) | B\u00e1sico |\n",
    "| **Performance** | M\u00e1s lento (overhead) | M\u00e1s r\u00e1pido |\n",
    "| **Type Safety** | No | S\u00ed (mypy) |\n",
    "| **Use Case** | Enterprise, governance | Fast prototyping |\n",
    "\n",
    "**Estrategia H\u00edbrida:**\n",
    "- Pandera: Desarrollo local + CI tests\n",
    "- Great Expectations: Producci\u00f3n + auditor\u00eda\n",
    "\n",
    "**Niveles de Validaci\u00f3n:**\n",
    "1. **Schema**: Columnas, tipos, nullability\n",
    "2. **Rango**: Min, max, percentiles\n",
    "3. **Relaciones**: Foreign keys, duplicates\n",
    "4. **Distribuci\u00f3n**: Mean, std, outliers\n",
    "5. **Negocio**: Reglas custom (ej: revenue >= costs)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    import great_expectations as ge\n",
    "    from pandera import DataFrameSchema, Column, Check\n",
    "    df = pd.DataFrame({\n",
    "        'venta_id':[1,2,3],\n",
    "        'total':[100.0, 50.0, 25.5],\n",
    "        'metodo_pago':['tarjeta','cash','tarjeta']\n",
    "    })\n",
    "    # Great Expectations estilo r\u00e1pido\n",
    "    gdf = ge.from_pandas(df)\n",
    "    gdf.expect_column_values_to_not_be_null('venta_id')\n",
    "    gdf.expect_column_values_to_be_between('total', 0, 10000)\n",
    "    print('GE checks:', gdf.validate().to_json_dict()['statistics'])\n",
    "    # Pandera schema\n",
    "    schema = DataFrameSchema({\n",
    "        'venta_id': Column(int, Check.gt(0)),\n",
    "        'total': Column(float, Check.ge(0)),\n",
    "        'metodo_pago': Column(str)\n",
    "    })\n",
    "    schema.validate(df)\n",
    "    print('Pandera OK')\n",
    "except Exception as e:\n",
    "    print('Instala great-expectations y pandera si deseas ejecutar este bloque:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331107e6",
   "metadata": {},
   "source": [
    "## 2. Pytest: estructura m\u00ednima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa37420",
   "metadata": {},
   "source": [
    "### \ud83e\uddea **Pytest: Testing Framework para Data Pipelines**\n",
    "\n",
    "**Estructura de Proyecto:**\n",
    "```\n",
    "proyecto/\n",
    "\u251c\u2500\u2500 src/\n",
    "\u2502   \u251c\u2500\u2500 extract.py\n",
    "\u2502   \u251c\u2500\u2500 transform.py\n",
    "\u2502   \u2514\u2500\u2500 load.py\n",
    "\u251c\u2500\u2500 tests/\n",
    "\u2502   \u251c\u2500\u2500 conftest.py         \u2190 Fixtures compartidos\n",
    "\u2502   \u251c\u2500\u2500 test_extract.py\n",
    "\u2502   \u251c\u2500\u2500 test_transform.py\n",
    "\u2502   \u2514\u2500\u2500 test_load.py\n",
    "\u251c\u2500\u2500 pytest.ini              \u2190 Configuraci\u00f3n\n",
    "\u2514\u2500\u2500 requirements-dev.txt\n",
    "```\n",
    "\n",
    "**Fixtures (Setup/Teardown):**\n",
    "```python\n",
    "# conftest.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_df():\n",
    "    \"\"\"Fixture reutilizable para tests\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'id': [1, 2, 3],\n",
    "        'value': [10, 20, 30]\n",
    "    })\n",
    "\n",
    "@pytest.fixture\n",
    "def db_connection():\n",
    "    conn = create_connection()\n",
    "    yield conn  # Test ejecuta aqu\u00ed\n",
    "    conn.close()  # Cleanup autom\u00e1tico\n",
    "```\n",
    "\n",
    "**Parametrizaci\u00f3n (Data-Driven Tests):**\n",
    "```python\n",
    "@pytest.mark.parametrize(\"input,expected\", [\n",
    "    (10, 10.0),\n",
    "    (-5, 0.0),\n",
    "    ('invalid', 0.0),\n",
    "    (None, 0.0)\n",
    "])\n",
    "def test_clean_total(input, expected):\n",
    "    assert clean_total(input) == expected\n",
    "```\n",
    "\n",
    "**Mocking (Aislamiento de dependencias):**\n",
    "```python\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "@patch('requests.get')\n",
    "def test_api_extractor(mock_get):\n",
    "    mock_get.return_value.json.return_value = {'data': [1, 2, 3]}\n",
    "    result = extract_from_api('https://api.example.com')\n",
    "    assert len(result) == 3\n",
    "    mock_get.assert_called_once()\n",
    "```\n",
    "\n",
    "**Coverage (Cobertura de C\u00f3digo):**\n",
    "```bash\n",
    "pytest --cov=src --cov-report=html\n",
    "# Genera htmlcov/index.html con l\u00edneas cubiertas/no cubiertas\n",
    "```\n",
    "\n",
    "**Markers (Categorizaci\u00f3n de Tests):**\n",
    "```python\n",
    "@pytest.mark.slow\n",
    "def test_large_dataset():\n",
    "    # Test que tarda >10s\n",
    "    pass\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_db_connection():\n",
    "    # Test que requiere DB real\n",
    "    pass\n",
    "\n",
    "# Ejecutar solo tests r\u00e1pidos\n",
    "pytest -m \"not slow\"\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Tests independientes (sin orden)\n",
    "- Nombres descriptivos: `test_transform_handles_null_values`\n",
    "- Arrange-Act-Assert pattern\n",
    "- Un assert por test (o conceptos relacionados)\n",
    "- Tests < 1s (unit), < 10s (integration)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e608f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_transform.py\n",
    "sample_code = r'''\n",
    "# archivo: src/transform.py\n",
    "def clean_total(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        return max(v, 0.0)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# archivo: tests/test_transform.py\n",
    "from src.transform import clean_total\n",
    "def test_clean_total():\n",
    "    assert clean_total(10) == 10.0\n",
    "    assert clean_total(-5) == 0.0\n",
    "    assert clean_total('oops') == 0.0\n",
    "'''\n",
    "print(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e1144",
   "metadata": {},
   "source": [
    "## 3. Pre-commit hooks (lint, format, tests r\u00e1pidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23eaf0",
   "metadata": {},
   "source": [
    "### \ud83e\ude9d **Pre-commit Hooks: Quality Gates Locales**\n",
    "\n",
    "**\u00bfQu\u00e9 son los Pre-commit Hooks?**  \n",
    "Scripts que se ejecutan autom\u00e1ticamente **antes de cada commit** para validar c\u00f3digo, formato, seguridad, etc. Bloquean el commit si fallan.\n",
    "\n",
    "**Instalaci\u00f3n:**\n",
    "```bash\n",
    "pip install pre-commit\n",
    "# Crear .pre-commit-config.yaml\n",
    "pre-commit install  # Activa hooks en .git/hooks/\n",
    "```\n",
    "\n",
    "**Hooks Esenciales para Data Engineering:**\n",
    "\n",
    "1. **Black (Code Formatter):**\n",
    "   - Auto-formatea c\u00f3digo Python (PEP 8)\n",
    "   - \"The uncompromising formatter\"\n",
    "   ```python\n",
    "   # Antes\n",
    "   x=[1,2,3];y={'a':1,'b':2}\n",
    "   \n",
    "   # Despu\u00e9s\n",
    "   x = [1, 2, 3]\n",
    "   y = {\"a\": 1, \"b\": 2}\n",
    "   ```\n",
    "\n",
    "2. **isort (Import Organizer):**\n",
    "   - Ordena imports alfab\u00e9ticamente\n",
    "   ```python\n",
    "   # Antes\n",
    "   import sys\n",
    "   from myproject import utils\n",
    "   import os\n",
    "   \n",
    "   # Despu\u00e9s\n",
    "   import os\n",
    "   import sys\n",
    "   from myproject import utils\n",
    "   ```\n",
    "\n",
    "3. **flake8 (Linter):**\n",
    "   - Detecta errores sint\u00e1cticos, variables no usadas, imports redundantes\n",
    "   - Warnings: E501 (l\u00ednea >79 chars), F841 (variable asignada pero no usada)\n",
    "\n",
    "4. **mypy (Type Checker):**\n",
    "   - Valida type hints\n",
    "   ```python\n",
    "   def add(x: int, y: int) -> int:\n",
    "       return x + y\n",
    "   \n",
    "   add(\"1\", \"2\")  # \u274c mypy detecta error\n",
    "   ```\n",
    "\n",
    "5. **detect-secrets:**\n",
    "   - Busca API keys, passwords hardcodeados\n",
    "   - Bloquea commits con `AWS_SECRET_ACCESS_KEY = \"xxx\"`\n",
    "\n",
    "**Hooks Custom para Datos:**\n",
    "```yaml\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: check-sql-syntax\n",
    "      name: Validate SQL files\n",
    "      entry: sqlfluff lint\n",
    "      language: system\n",
    "      files: \\.sql$\n",
    "    \n",
    "    - id: validate-schemas\n",
    "      name: Check Avro schemas\n",
    "      entry: python scripts/validate_schemas.py\n",
    "      language: python\n",
    "      files: schemas/.*\\.avsc$\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "```bash\n",
    "git add transform.py\n",
    "git commit -m \"Fix bug\"\n",
    "  \u2193\n",
    "[pre-commit] black........................Passed\n",
    "[pre-commit] isort.......................Passed\n",
    "[pre-commit] flake8......................Failed\n",
    "  - src/transform.py:10:1: F401 'pandas' imported but unused\n",
    "  \u2193\n",
    "[Commit bloqueado - fix errores y reintenta]\n",
    "```\n",
    "\n",
    "**Skip Hooks (Emergencia):**\n",
    "```bash\n",
    "git commit --no-verify -m \"Hotfix cr\u00edtico\"\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- \u2b06\ufe0f Calidad de c\u00f3digo consistente en equipo\n",
    "- \u2b07\ufe0f Menos issues en code review\n",
    "- \u26a1 Feedback instant\u00e1neo vs esperar CI\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_commit_cfg = r'''\n",
    "repos:\n",
    "  - repo: https://github.com/psf/black\n",
    "    rev: 22.6.0\n",
    "    hooks:\n",
    "      - id: black\n",
    "  - repo: https://github.com/PyCQA/isort\n",
    "    rev: 5.10.1\n",
    "    hooks:\n",
    "      - id: isort\n",
    "  - repo: https://github.com/pycqa/flake8\n",
    "    rev: 5.0.4\n",
    "    hooks:\n",
    "      - id: flake8\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: pytest-quick\n",
    "        name: pytest quick\n",
    "        entry: pytest -q\n",
    "        language: system\n",
    "        types: [python]\n",
    "'''\n",
    "print(pre_commit_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de1d7b",
   "metadata": {},
   "source": [
    "## 4. GitHub Actions: CI para validar el repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185194da",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **GitHub Actions: CI/CD Autom\u00e1tico**\n",
    "\n",
    "**Concepto:**  \n",
    "GitHub Actions ejecuta workflows autom\u00e1ticamente en eventos (push, PR, schedule) en runners de GitHub (Ubuntu/Windows/macOS).\n",
    "\n",
    "**Anatom\u00eda de un Workflow:**\n",
    "\n",
    "```yaml\n",
    "name: ci                          # Nombre del workflow\n",
    "on: [push, pull_request]          # Triggers\n",
    "\n",
    "jobs:\n",
    "  test:                           # Job ID\n",
    "    runs-on: ubuntu-latest        # Runner environment\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3 # Action oficial (git clone)\n",
    "      \n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          cache: 'pip'            # Cache de dependencias\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: pytest --cov=src --cov-report=xml\n",
    "      \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          files: ./coverage.xml\n",
    "```\n",
    "\n",
    "**Triggers Avanzados:**\n",
    "\n",
    "1. **Schedule (Cron):**\n",
    "   ```yaml\n",
    "   on:\n",
    "     schedule:\n",
    "       - cron: '0 2 * * *'  # Diario a las 2 AM UTC\n",
    "   ```\n",
    "\n",
    "2. **Manual Dispatch:**\n",
    "   ```yaml\n",
    "   on:\n",
    "     workflow_dispatch:\n",
    "       inputs:\n",
    "         environment:\n",
    "           description: 'Target environment'\n",
    "           required: true\n",
    "           default: 'staging'\n",
    "   ```\n",
    "\n",
    "3. **Path Filters:**\n",
    "   ```yaml\n",
    "   on:\n",
    "     push:\n",
    "       paths:\n",
    "         - 'src/**'\n",
    "         - 'tests/**'\n",
    "   ```\n",
    "\n",
    "**Parallel Jobs (Matrix Strategy):**\n",
    "```yaml\n",
    "jobs:\n",
    "  test:\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: ['3.8', '3.9', '3.10', '3.11']\n",
    "        os: [ubuntu-latest, windows-latest]\n",
    "    runs-on: ${{ matrix.os }}\n",
    "    steps:\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ matrix.python-version }}\n",
    "```\n",
    "\n",
    "**Secrets Management:**\n",
    "```yaml\n",
    "- name: Deploy to AWS\n",
    "  env:\n",
    "    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "  run: aws s3 sync dist/ s3://my-bucket/\n",
    "```\n",
    "\n",
    "**Pipeline Completo para Data:**\n",
    "```yaml\n",
    "jobs:\n",
    "  lint:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: flake8 src/\n",
    "  \n",
    "  test:\n",
    "    needs: lint\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: pytest -v\n",
    "  \n",
    "  data-quality:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: great_expectations checkpoint run validation_suite\n",
    "  \n",
    "  deploy:\n",
    "    needs: [test, data-quality]\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: docker build -t pipeline:${{ github.sha }} .\n",
    "      - run: docker push pipeline:${{ github.sha }}\n",
    "```\n",
    "\n",
    "**Artifacts (Compartir entre jobs):**\n",
    "```yaml\n",
    "- name: Generate report\n",
    "  run: python generate_report.py\n",
    "\n",
    "- uses: actions/upload-artifact@v3\n",
    "  with:\n",
    "    name: data-report\n",
    "    path: reports/*.html\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Public repos: Gratis ilimitado\n",
    "- Private repos: 2,000 min/mes gratis (luego $0.008/min)\n",
    "- Self-hosted runners para reducir costos\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gha_yaml = r'''\n",
    "name: ci\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          pip install -r curso_ingenieria_datos/requirements.txt\n",
    "      - name: Lint & Test\n",
    "        run: |\n",
    "          pip install pytest flake8 black\n",
    "          flake8 .\n",
    "          pytest -q\n",
    "'''\n",
    "print(gha_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb9d12",
   "metadata": {},
   "source": [
    "## 5. Observabilidad: logs y m\u00e9tricas m\u00ednimas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43f070",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **Observability: Logs, M\u00e9tricas y Alertas**\n",
    "\n",
    "**Structured Logging con Loguru:**\n",
    "\n",
    "```python\n",
    "from loguru import logger\n",
    "\n",
    "# Configuraci\u00f3n\n",
    "logger.add(\n",
    "    \"logs/pipeline_{time}.log\",\n",
    "    rotation=\"500 MB\",\n",
    "    retention=\"10 days\",\n",
    "    level=\"INFO\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\"\n",
    ")\n",
    "\n",
    "# Contexto estructurado\n",
    "logger.bind(pipeline=\"etl\", dataset=\"ventas\").info(\n",
    "    \"Processed batch\",\n",
    "    records=1000,\n",
    "    latency_ms=150.5,\n",
    "    status=\"success\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Output JSON (Parse con herramientas):**\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"2025-10-30T14:23:45.123Z\",\n",
    "  \"level\": \"INFO\",\n",
    "  \"pipeline\": \"etl\",\n",
    "  \"dataset\": \"ventas\",\n",
    "  \"message\": \"Processed batch\",\n",
    "  \"records\": 1000,\n",
    "  \"latency_ms\": 150.5,\n",
    "  \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "\n",
    "**M\u00e9tricas Esenciales (RED Pattern):**\n",
    "\n",
    "1. **Rate (Throughput):**\n",
    "   ```python\n",
    "   records_processed = 10000\n",
    "   duration_seconds = 60\n",
    "   throughput = records_processed / duration_seconds\n",
    "   logger.info(f\"throughput={throughput:.2f} records/sec\")\n",
    "   ```\n",
    "\n",
    "2. **Errors (Error Rate):**\n",
    "   ```python\n",
    "   total_records = 10000\n",
    "   failed_records = 50\n",
    "   error_rate = (failed_records / total_records) * 100\n",
    "   logger.warning(f\"error_rate={error_rate:.2f}%\")\n",
    "   \n",
    "   if error_rate > 5.0:\n",
    "       raise AlertException(\"High error rate detected\")\n",
    "   ```\n",
    "\n",
    "3. **Duration (Latency):**\n",
    "   ```python\n",
    "   import time\n",
    "   start = time.time()\n",
    "   process_batch()\n",
    "   latency = time.time() - start\n",
    "   logger.info(f\"latency={latency:.3f}s\")\n",
    "   ```\n",
    "\n",
    "**Prometheus Metrics (Production):**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "records_processed = Counter('records_processed_total', 'Total records')\n",
    "pipeline_duration = Histogram('pipeline_duration_seconds', 'Duration')\n",
    "active_pipelines = Gauge('active_pipelines', 'Currently running')\n",
    "\n",
    "@pipeline_duration.time()\n",
    "def run_pipeline():\n",
    "    records_processed.inc(1000)\n",
    "    # L\u00f3gica del pipeline\n",
    "```\n",
    "\n",
    "**Data Observability Platforms:**\n",
    "\n",
    "1. **Monte Carlo / Bigeye:**\n",
    "   - Anomaly detection autom\u00e1tico (distribuci\u00f3n, volumen, freshness)\n",
    "   - Lineage visual (upstream/downstream dependencies)\n",
    "   - Alertas: \"Tabla X no actualizada en 4 horas\"\n",
    "\n",
    "2. **DataDog / New Relic:**\n",
    "   - APM para pipelines\n",
    "   - Dashboards customizables\n",
    "   - Distributed tracing\n",
    "\n",
    "3. **dbt Cloud:**\n",
    "   - Test results hist\u00f3ricos\n",
    "   - Model timing trends\n",
    "   - Exposures: Qu\u00e9 dashboards dependen de qu\u00e9 modelos\n",
    "\n",
    "**Alerting Strategy:**\n",
    "```python\n",
    "class DataQualityAlert:\n",
    "    def __init__(self):\n",
    "        self.thresholds = {\n",
    "            'null_rate': 0.05,      # Max 5% nulls\n",
    "            'duplicate_rate': 0.01, # Max 1% duplicates\n",
    "            'freshness_hours': 4    # Data < 4h old\n",
    "        }\n",
    "    \n",
    "    def check_and_alert(self, metrics):\n",
    "        if metrics['null_rate'] > self.thresholds['null_rate']:\n",
    "            send_slack_alert(f\"\u26a0\ufe0f High null rate: {metrics['null_rate']:.2%}\")\n",
    "            send_pagerduty(severity='high')\n",
    "```\n",
    "\n",
    "**Golden Signals para Data:**\n",
    "- **Freshness**: \u00bfCu\u00e1ndo lleg\u00f3 el \u00faltimo dato?\n",
    "- **Volume**: \u00bfN\u00famero de registros esperado?\n",
    "- **Schema**: \u00bfColumnas/tipos cambiaron?\n",
    "- **Distribution**: \u00bfValores fuera de rango?\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import time, random\n",
    "\n",
    "def process_batch(n=5):\n",
    "    for i in range(n):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if random.random() < 0.1:\n",
    "                raise ValueError('Fallo aleatorio')\n",
    "            time.sleep(0.05)\n",
    "            latency = time.time() - start\n",
    "            logger.info(f'item={i} status=ok latency={latency:.3f}s')\n",
    "        except Exception as e:\n",
    "            logger.error(f'item={i} status=error err={e}')\n",
    "process_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910500e",
   "metadata": {},
   "source": [
    "## 6. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffabc4",
   "metadata": {},
   "source": [
    "1. Agrega una regla de pre-commit que bloquee archivos > 2 MB.\n",
    "2. Crea un workflow adicional que ejecute validaciones de datos con Great Expectations.\n",
    "3. A\u00f1ade cobertura de pruebas y publica un badge en el README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP \u2192](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb)\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
