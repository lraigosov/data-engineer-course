{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7ebbc",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è AWS para Ingenier√≠a de Datos: S3, Glue, Athena y Lambda\n",
    "\n",
    "Este notebook introduce un flujo moderno de datos en AWS con almacenamiento en S3, transformaci√≥n con Glue (PySpark), consulta con Athena (SQL sobre S3) y orquestaci√≥n con eventos/Lambda. Incluye ejemplos de c√≥digo con `boto3` y pr√°cticas recomendadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f739",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecuci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ddce0",
   "metadata": {},
   "source": [
    "- Para ejecutar c√≥digo real de AWS necesitas credenciales configuradas en tu entorno (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`).\n",
    "- Nunca subas credenciales al repositorio. Usa variables de entorno o perfiles de AWS CLI.\n",
    "- Este notebook es auto-contenido con bloques que pueden ejecutarse si tu entorno ya tiene permisos.\n",
    "- Alternativa local: usar LocalStack para simular servicios AWS en tu m√°quina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470127",
   "metadata": {},
   "source": [
    "## 1. S3: Data Lake B√°sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, io\n",
    "import pandas as pd\n",
    "# import boto3  # Descomenta si tienes credenciales configuradas\n",
    "\n",
    "BUCKET = 'mi-data-lake-demo-123456'  # Cambia por un nombre √∫nico global\n",
    "PREFIX_RAW = 'raw/ventas/'\n",
    "PREFIX_CURATED = 'curated/ventas/'\n",
    "\n",
    "print('üëÜ Define el bucket y prefijos antes de ejecutar contra AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a155",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# try:\n",
    "#     s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': 'us-east-1'})\n",
    "#     print('‚úÖ Bucket creado')\n",
    "# except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     print('‚ÑπÔ∏è Bucket ya existe')\n",
    "# except Exception as e:\n",
    "#     print('‚ùå Error creando bucket:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13106a9",
   "metadata": {},
   "source": [
    "### 1.2 Subir dataset de ejemplo a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "# csv_bytes = df.to_csv(index=False).encode('utf-8')\n",
    "# s3.put_object(Bucket=BUCKET, Key=PREFIX_RAW + 'ventas_2025_10.csv', Body=csv_bytes)\n",
    "# print('üì§ Archivo subido a S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc890b4",
   "metadata": {},
   "source": [
    "## 2. Glue: Transformaciones con PySpark (Job Script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3c7a",
   "metadata": {},
   "source": [
    "Ejemplo de script de Glue (PySpark) para leer CSVs crudos de S3, limpiar/transformar y escribir en formato Parquet particionado por mes. Guarda este script como `glue_job.py` y s√∫belo a un Job de Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f656567",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_job_script = r'''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','BUCKET','PREFIX_RAW','PREFIX_CURATED'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "raw_path = f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
