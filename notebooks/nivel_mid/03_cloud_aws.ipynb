{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7ebbc",
   "metadata": {},
   "source": [
    "# ☁️ AWS para Ingeniería de Datos: S3, Glue, Athena y Lambda\n",
    "\n",
    "Este notebook introduce un flujo moderno de datos en AWS con almacenamiento en S3, transformación con Glue (PySpark), consulta con Athena (SQL sobre S3) y orquestación con eventos/Lambda. Incluye ejemplos de código con `boto3` y prácticas recomendadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f739",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ddce0",
   "metadata": {},
   "source": [
    "- Para ejecutar código real de AWS necesitas credenciales configuradas en tu entorno (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`).\n",
    "- Nunca subas credenciales al repositorio. Usa variables de entorno o perfiles de AWS CLI.\n",
    "- Este notebook es auto-contenido con bloques que pueden ejecutarse si tu entorno ya tiene permisos.\n",
    "- Alternativa local: usar LocalStack para simular servicios AWS en tu máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390295",
   "metadata": {},
   "source": [
    "### ☁️ **AWS Cloud: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de AWS para Datos:**\n",
    "\n",
    "1. **S3 (Simple Storage Service)**: Data Lake base\n",
    "   - Almacenamiento ilimitado con 99.999999999% durabilidad (11 nines)\n",
    "   - Pricing: Pay-per-use (~$0.023/GB/mes en Standard)\n",
    "   - Clases de almacenamiento: Standard → Infrequent Access → Glacier (archival)\n",
    "\n",
    "2. **Glue**: Servicio ETL serverless con PySpark\n",
    "   - Auto-escalado de workers (DPU - Data Processing Units)\n",
    "   - Glue Catalog: Metastore centralizado (Hive-compatible)\n",
    "   - Crawlers: Descubrimiento automático de schemas\n",
    "\n",
    "3. **Athena**: Motor SQL interactivo sobre S3\n",
    "   - Basado en Presto, consulta Parquet/ORC/CSV sin infraestructura\n",
    "   - Pricing: $5 por TB escaneado (optimizar con particiones)\n",
    "\n",
    "4. **Redshift**: Data Warehouse MPP (Massively Parallel Processing)\n",
    "   - Columnar storage con compresión\n",
    "   - COPY desde S3, UNLOAD hacia S3\n",
    "\n",
    "5. **EMR (Elastic MapReduce)**: Clusters Spark/Hadoop administrados\n",
    "   - Spot instances para reducir costos 70-90%\n",
    "   - Notebooks Jupyter integrados\n",
    "\n",
    "**Arquitectura Lambda (Batch + Streaming):**\n",
    "```\n",
    "Fuentes → [Kinesis/Kafka] → S3 Raw → [Glue/EMR] → S3 Curated → [Athena/Redshift] → BI\n",
    "                ↓\n",
    "           Lambda (Near Real-Time)\n",
    "```\n",
    "\n",
    "**IAM Best Practices:**\n",
    "- Roles > Users (principio de menor privilegio)\n",
    "- Políticas: `s3:GetObject`, `s3:PutObject` (granular por bucket/prefix)\n",
    "- No hardcodear credenciales: usar IAM Roles, AWS Secrets Manager\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470127",
   "metadata": {},
   "source": [
    "## 1. S3: Data Lake Básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6affbf",
   "metadata": {},
   "source": [
    "### 🗄️ **S3: Fundamentos del Data Lake**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global único (namespace global AWS)\n",
    "  - Nombre: `mi-data-lake-123456` (minúsculas, números, guiones)\n",
    "  - Región: `us-east-1`, `eu-west-1`, etc.\n",
    "\n",
    "- **Prefix (Key)**: Jerarquía lógica simulada (no son carpetas reales)\n",
    "  ```\n",
    "  s3://bucket/raw/ventas/2025/10/ventas_2025_10_30.csv\n",
    "           │     └─────┬─────┘ └──┬──┘ └────┬────┘\n",
    "         Bucket      Prefix    Partición   Archivo\n",
    "  ```\n",
    "\n",
    "- **Versionamiento**: Protección contra eliminación accidental\n",
    "  - Enabled: Cada PUT crea nueva versión con ID único\n",
    "  - Lifecycle policies: Migrar versiones antiguas a Glacier\n",
    "\n",
    "**Patrones de Organización:**\n",
    "\n",
    "1. **Medallion Architecture (Databricks):**\n",
    "   ```\n",
    "   /bronze/    ← Datos crudos (raw, sin transformar)\n",
    "   /silver/    ← Cleaned & validated\n",
    "   /gold/      ← Business-level aggregates\n",
    "   ```\n",
    "\n",
    "2. **Por Dominio:**\n",
    "   ```\n",
    "   /sales/raw/, /sales/curated/\n",
    "   /customers/raw/, /customers/curated/\n",
    "   ```\n",
    "\n",
    "3. **Particionamiento Hive-style:**\n",
    "   ```\n",
    "   /year=2025/month=10/day=30/data.parquet\n",
    "   ```\n",
    "   - Athena/Glue entienden automáticamente las particiones\n",
    "   - Reduce escaneo en queries: `WHERE year=2025 AND month=10`\n",
    "\n",
    "**Operaciones con boto3:**\n",
    "- `put_object()`: Upload inline (para archivos pequeños <5GB)\n",
    "- `upload_file()`: Upload desde disco local (multipart automático)\n",
    "- `list_objects_v2()`: Listar con paginación (max 1000 por request)\n",
    "\n",
    "**Costos:**\n",
    "- PUT/COPY/POST/LIST: $0.005 per 1,000 requests\n",
    "- GET/SELECT: $0.0004 per 1,000 requests\n",
    "- Data transfer OUT: $0.09/GB (dentro de AWS: gratis)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, io\n",
    "import pandas as pd\n",
    "# import boto3  # Descomenta si tienes credenciales configuradas\n",
    "\n",
    "BUCKET = 'mi-data-lake-demo-123456'  # Cambia por un nombre único global\n",
    "PREFIX_RAW = 'raw/ventas/'\n",
    "PREFIX_CURATED = 'curated/ventas/'\n",
    "\n",
    "print('👆 Define el bucket y prefijos antes de ejecutar contra AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a155",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# try:\n",
    "#     s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': 'us-east-1'})\n",
    "#     print('✅ Bucket creado')\n",
    "# except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     print('ℹ️ Bucket ya existe')\n",
    "# except Exception as e:\n",
    "#     print('❌ Error creando bucket:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13106a9",
   "metadata": {},
   "source": [
    "### 1.2 Subir dataset de ejemplo a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "# csv_bytes = df.to_csv(index=False).encode('utf-8')\n",
    "# s3.put_object(Bucket=BUCKET, Key=PREFIX_RAW + 'ventas_2025_10.csv', Body=csv_bytes)\n",
    "# print('📤 Archivo subido a S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc890b4",
   "metadata": {},
   "source": [
    "## 2. Glue: Transformaciones con PySpark (Job Script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6efd1",
   "metadata": {},
   "source": [
    "### 🔧 **AWS Glue: ETL Serverless con PySpark**\n",
    "\n",
    "**Componentes de Glue:**\n",
    "\n",
    "1. **Glue Data Catalog:**\n",
    "   - Metastore centralizado (bases de datos + tablas)\n",
    "   - Compatible con Hive Metastore (usado por Athena, EMR, Redshift Spectrum)\n",
    "   - Schema discovery con Crawlers\n",
    "\n",
    "2. **Glue Crawlers:**\n",
    "   - Escanean S3 y detectan formato/schema automáticamente\n",
    "   - Actualizan el Catalog con nuevas particiones\n",
    "   - Schedule: Cron expressions (`cron(0 0 * * ? *)` = diario a medianoche)\n",
    "\n",
    "3. **Glue Jobs (ETL Scripts):**\n",
    "   - Python Shell (para scripts ligeros)\n",
    "   - PySpark (para transformaciones distribuidas)\n",
    "   - DPU (Data Processing Unit): 1 DPU = 4 vCPU + 16GB RAM\n",
    "   - Pricing: $0.44 por DPU-hora\n",
    "\n",
    "**GlueContext vs SparkContext:**\n",
    "```python\n",
    "# SparkContext: Estándar PySpark\n",
    "sc = SparkContext()\n",
    "\n",
    "# GlueContext: Extensión AWS con métodos adicionales\n",
    "glueContext = GlueContext(sc)\n",
    "glueContext.create_dynamic_frame_from_catalog()  # Lee desde Catalog\n",
    "glueContext.write_dynamic_frame.from_options()    # Escribe con conversiones\n",
    "```\n",
    "\n",
    "**DynamicFrame vs DataFrame:**\n",
    "- **DataFrame (Spark)**: Tipado estricto, requiere schema consistente\n",
    "- **DynamicFrame (Glue)**: Schema flexible, maneja datos semi-estructurados\n",
    "  - `resolveChoice()`: Resuelve conflictos de tipos\n",
    "  - `unbox()`: Desempaqueta structs complejos\n",
    "\n",
    "**Patrón de Job de Glue:**\n",
    "```python\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PARAM1'])\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# [ETL logic aquí]\n",
    "\n",
    "job.commit()  # Marca job como exitoso en Catalog\n",
    "```\n",
    "\n",
    "**Optimizaciones:**\n",
    "- **Pushdown Predicates**: Filtrar en lectura reduce datos procesados\n",
    "- **Partition Pruning**: Leer solo particiones necesarias\n",
    "- **Columnar Formats**: Parquet/ORC reducen IO 10x vs CSV\n",
    "\n",
    "**Uso Real:**\n",
    "Script Glue orquestado por Step Functions o EventBridge (CloudWatch Events) para procesamiento automático al detectar nuevos archivos en S3.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3c7a",
   "metadata": {},
   "source": [
    "Ejemplo de script de Glue (PySpark) para leer CSVs crudos de S3, limpiar/transformar y escribir en formato Parquet particionado por mes. Guarda este script como `glue_job.py` y súbelo a un Job de Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f656567",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_job_script = r'''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','BUCKET','PREFIX_RAW','PREFIX_CURATED'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "raw_path = f\"s3://{args['BUCKET']}/{args['PREFIX_RAW']}*.csv\"\n",
    "curated_path = f\"s3://{args['BUCKET']}/{args['PREFIX_CURATED']}\"\n",
    "\n",
    "# Leer CSV desde S3\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "\n",
    "# Transformaciones: limpiar nulos, filtrar valores positivos, agregar timestamp\n",
    "df_clean = (df\n",
    "    .dropna(subset=['venta_id', 'total'])\n",
    "    .filter(F.col('total') > 0)\n",
    "    .withColumn('procesado_en', F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Escribir como Parquet particionado por mes\n",
    "df_clean.write.partitionBy('fecha').mode('overwrite').parquet(curated_path)\n",
    "\n",
    "job.commit()\n",
    "print('✅ ETL completado')\n",
    "'''\n",
    "print('📝 Script de Glue listo para deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3349e01",
   "metadata": {},
   "source": [
    "## 3. AWS Lambda: Procesamiento Serverless Event-Driven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62351c",
   "metadata": {},
   "source": [
    "### ⚡ **AWS Lambda: Compute Serverless para Datos**\n",
    "\n",
    "**¿Qué es Lambda?**\n",
    "\n",
    "AWS Lambda es un servicio de computación serverless que ejecuta código en respuesta a eventos, sin necesidad de aprovisionar o administrar servidores.\n",
    "\n",
    "**Características Principales:**\n",
    "\n",
    "- **Event-Driven**: Se activa automáticamente con triggers (S3, DynamoDB, EventBridge, API Gateway)\n",
    "- **Auto-Scaling**: Escala automáticamente de 0 a miles de invocaciones concurrentes\n",
    "- **Pay-per-Use**: Cobro solo por tiempo de ejecución (milisegundos)\n",
    "- **Límites**: Max 15 minutos por invocación, hasta 10GB de RAM\n",
    "\n",
    "**Triggers Comunes para Data Engineering:**\n",
    "\n",
    "1. **S3 Events**: Procesar archivos al subirse\n",
    "   ```python\n",
    "   # Lambda se activa cuando se crea objeto en S3\n",
    "   def lambda_handler(event, context):\n",
    "       bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "       key = event['Records'][0]['s3']['object']['key']\n",
    "       # Procesar archivo\n",
    "   ```\n",
    "\n",
    "2. **EventBridge (CloudWatch Events)**: Cron jobs programados\n",
    "   ```python\n",
    "   # Ejecutar diariamente a las 2 AM\n",
    "   # Regla EventBridge: rate(1 day) o cron(0 2 * * ? *)\n",
    "   ```\n",
    "\n",
    "3. **DynamoDB Streams**: Procesar cambios en tiempo real\n",
    "   ```python\n",
    "   # Lambda recibe cambios de DynamoDB\n",
    "   for record in event['Records']:\n",
    "       if record['eventName'] == 'INSERT':\n",
    "           # Procesar nuevo registro\n",
    "   ```\n",
    "\n",
    "4. **SQS/SNS**: Cola de mensajes para procesamiento asíncrono\n",
    "   ```python\n",
    "   # Lambda consume mensajes de SQS automáticamente\n",
    "   for record in event['Records']:\n",
    "       body = json.loads(record['body'])\n",
    "   ```\n",
    "\n",
    "**Arquitectura Lambda para Datos:**\n",
    "\n",
    "```\n",
    "S3 (new file) → Lambda → Validar → S3 (curated)\n",
    "                   ↓\n",
    "                 SQS (errores) → Lambda (retry)\n",
    "                   ↓\n",
    "              CloudWatch Logs (monitoring)\n",
    "```\n",
    "\n",
    "**Capas (Layers):**\n",
    "- Librerías compartidas (pandas, requests, etc.)\n",
    "- Reduce tamaño del deployment package\n",
    "- Reutilización entre funciones\n",
    "\n",
    "**Best Practices:**\n",
    "- **Idempotencia**: Lambda puede reintentar, asegurar que múltiples ejecuciones no causen problemas\n",
    "- **Environment Variables**: Configuración (buckets, endpoints) sin hardcodear\n",
    "- **Timeout**: Configurar según necesidad (default 3s, max 15min)\n",
    "- **Memory**: Mayor memoria = más CPU (128MB a 10GB)\n",
    "- **Dead Letter Queue (DLQ)**: SQS/SNS para errores no recuperables\n",
    "- **Async invocation**: Para procesamiento no-blocking\n",
    "\n",
    "**Limitaciones:**\n",
    "- ❌ Max 15 minutos (para ETL largo, usar Glue/EMR)\n",
    "- ❌ Max 10GB RAM\n",
    "- ❌ Max 250MB deployment package (sin layers)\n",
    "- ❌ /tmp storage max 512MB (efímero)\n",
    "- ✅ Ideal para: Validación, transformaciones ligeras, triggers\n",
    "\n",
    "**Pricing:**\n",
    "- Requests: $0.20 por 1M requests\n",
    "- Duration: $0.0000166667 por GB-segundo\n",
    "- Ejemplo: 1M requests x 1GB x 1s = ~$17/mes\n",
    "- Free tier: 1M requests + 400,000 GB-segundos/mes\n",
    "\n",
    "**Alternativas:**\n",
    "- **Glue Python Shell**: Para scripts Python no-PySpark (no límite 15min)\n",
    "- **ECS Fargate**: Para procesos largos sin administrar servidores\n",
    "- **Step Functions**: Orquestación de múltiples Lambdas\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f427134",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo: Lambda para validar CSV al subir a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_handler_code = '''\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by S3 PutObject event.\n",
    "    Validates CSV structure and moves to curated zone if valid.\n",
    "    \"\"\"\n",
    "    # Extraer información del evento\n",
    "    for record in event['Records']:\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        key = record['s3']['object']['key']\n",
    "        \n",
    "        print(f'Processing file: s3://{bucket}/{key}')\n",
    "        \n",
    "        # Skip if not CSV\n",
    "        if not key.endswith('.csv'):\n",
    "            print('Skipping non-CSV file')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Leer CSV desde S3\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            csv_content = response['Body'].read()\n",
    "            df = pd.read_csv(BytesIO(csv_content))\n",
    "            \n",
    "            # Validaciones\n",
    "            required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                raise ValueError(f'Missing columns: {missing_cols}')\n",
    "            \n",
    "            # Validar tipos y valores\n",
    "            if df['total'].isnull().sum() > 0:\n",
    "                raise ValueError(f'Null values found in total column: {df[\"total\"].isnull().sum()}')\n",
    "            \n",
    "            if (df['total'] < 0).sum() > 0:\n",
    "                raise ValueError(f'Negative values in total: {(df[\"total\"] < 0).sum()}')\n",
    "            \n",
    "            # Si válido, mover a zona curated\n",
    "            curated_key = key.replace('/raw/', '/curated/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=curated_key\n",
    "            )\n",
    "            \n",
    "            print(f'✅ File validated and copied to: s3://{bucket}/{curated_key}')\n",
    "            \n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps({\n",
    "                    'message': 'File processed successfully',\n",
    "                    'rows': len(df),\n",
    "                    'output': f's3://{bucket}/{curated_key}'\n",
    "                })\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'❌ Error processing file: {str(e)}')\n",
    "            \n",
    "            # Mover a carpeta de errores\n",
    "            error_key = key.replace('/raw/', '/errors/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=error_key\n",
    "            )\n",
    "            \n",
    "            # Enviar a DLQ o SNS para alertas\n",
    "            return {\n",
    "                'statusCode': 500,\n",
    "                'body': json.dumps({\n",
    "                    'error': str(e),\n",
    "                    'file': f's3://{bucket}/{key}'\n",
    "                })\n",
    "            }\n",
    "'''\n",
    "\n",
    "print('📝 Lambda handler para validación de CSV')\n",
    "print('\\\\nPara deployar:')\n",
    "print('1. Crear función Lambda en AWS Console')\n",
    "print('2. Agregar layer con pandas (o usar Lambda Container)')\n",
    "print('3. Configurar trigger S3 en bucket/raw/')\n",
    "print('4. Ajustar timeout (60s) y memoria (512MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f34d9",
   "metadata": {},
   "source": [
    "## 4. Athena: SQL sobre S3 sin Infraestructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82e6f3",
   "metadata": {},
   "source": [
    "### 📊 **Amazon Athena: Query Engine Serverless**\n",
    "\n",
    "Athena permite ejecutar queries SQL estándar directamente sobre datos en S3 sin necesidad de cargarlos en una base de datos.\n",
    "\n",
    "**Características:**\n",
    "- **Basado en Presto/Trino**: Engine SQL distribuido\n",
    "- **Sin servidores**: No hay clusters que administrar\n",
    "- **Standard SQL**: Compatible con ANSI SQL\n",
    "- **Formatos soportados**: CSV, JSON, Parquet, ORC, Avro\n",
    "\n",
    "**Pricing:** $5 por TB de datos escaneados\n",
    "\n",
    "**Optimizaciones clave:**\n",
    "1. **Particionar datos**: Reduce escaneo con `WHERE year=2025 AND month=10`\n",
    "2. **Formato columnar**: Parquet/ORC reducen escaneo 10x vs CSV\n",
    "3. **Comprimir**: SNAPPY, GZIP, ZSTD reducen tamaño\n",
    "4. **Proyección**: `SELECT col1, col2` en vez de `SELECT *`\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```sql\n",
    "-- Crear tabla externa apuntando a S3\n",
    "CREATE EXTERNAL TABLE ventas (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    producto_id INT,\n",
    "    cantidad INT,\n",
    "    total DECIMAL(10,2),\n",
    "    fecha DATE\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://mi-data-lake/curated/ventas/';\n",
    "\n",
    "-- Agregar particiones\n",
    "MSCK REPAIR TABLE ventas;\n",
    "\n",
    "-- Query con partition pruning (solo escanea Oct 2025)\n",
    "SELECT cliente_id, SUM(total) as total_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Integración con Python (boto3):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "query = \\\"\\\"\\\"\n",
    "SELECT cliente_id, COUNT(*) as num_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-bucket/athena-results/'}\n",
    ")\n",
    "\n",
    "query_id = response['QueryExecutionId']\n",
    "\n",
    "# Esperar a que termine\n",
    "while True:\n",
    "    status = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "    state = status['QueryExecution']['Status']['State']\n",
    "    \n",
    "    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "if state == 'SUCCEEDED':\n",
    "    results = athena.get_query_results(QueryExecutionId=query_id)\n",
    "    # Procesar resultados\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745195ec",
   "metadata": {},
   "source": [
    "## 5. Arquitectura Completa: S3 + Lambda + Glue + Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533cfa8",
   "metadata": {},
   "source": [
    "### 🏗️ **Pipeline End-to-End en AWS**\n",
    "\n",
    "**Flujo completo de datos:**\n",
    "\n",
    "```\n",
    "1. INGESTA\n",
    "   Fuentes → S3 (bucket/raw/)\n",
    "   \n",
    "2. VALIDACIÓN (Lambda)\n",
    "   S3 Event → Lambda → Validar schema\n",
    "                ↓\n",
    "           S3 (curated/) o S3 (errors/)\n",
    "   \n",
    "3. TRANSFORMACIÓN (Glue)\n",
    "   EventBridge (cron) → Glue Job (PySpark)\n",
    "                          ↓\n",
    "                    S3 (gold/) Parquet particionado\n",
    "   \n",
    "4. CATALOGACIÓN (Glue Crawler)\n",
    "   Glue Crawler → Detecta schema → Glue Catalog\n",
    "   \n",
    "5. ANÁLISIS (Athena)\n",
    "   Athena → SQL queries → S3 (resultados)\n",
    "            ↓\n",
    "       QuickSight (BI) / Python (análisis)\n",
    "```\n",
    "\n",
    "**Ejemplo de implementación:**\n",
    "\n",
    "**Paso 1: Configurar S3 Bucket**\n",
    "```bash\n",
    "aws s3 mb s3://mi-data-lake-2025\n",
    "aws s3api put-bucket-versioning \\\\\n",
    "    --bucket mi-data-lake-2025 \\\\\n",
    "    --versioning-configuration Status=Enabled\n",
    "```\n",
    "\n",
    "**Paso 2: Deploy Lambda**\n",
    "```bash\n",
    "# Empaquetar código con dependencias\n",
    "zip lambda_function.zip lambda_function.py\n",
    "aws lambda create-function \\\\\n",
    "    --function-name validate-csv \\\\\n",
    "    --runtime python3.10 \\\\\n",
    "    --role arn:aws:iam::123456:role/lambda-s3-role \\\\\n",
    "    --handler lambda_function.lambda_handler \\\\\n",
    "    --zip-file fileb://lambda_function.zip \\\\\n",
    "    --timeout 60 \\\\\n",
    "    --memory-size 512\n",
    "```\n",
    "\n",
    "**Paso 3: Configurar S3 Event Notification**\n",
    "```json\n",
    "{\n",
    "  \"LambdaFunctionConfigurations\": [\n",
    "    {\n",
    "      \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456:function:validate-csv\",\n",
    "      \"Events\": [\"s3:ObjectCreated:*\"],\n",
    "      \"Filter\": {\n",
    "        \"Key\": {\n",
    "          \"FilterRules\": [\n",
    "            {\"Name\": \"prefix\", \"Value\": \"raw/\"},\n",
    "            {\"Name\": \"suffix\", \"Value\": \".csv\"}\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Paso 4: Crear Glue Job**\n",
    "```bash\n",
    "aws glue create-job \\\\\n",
    "    --name transform-ventas \\\\\n",
    "    --role AWSGlueServiceRole \\\\\n",
    "    --command Name=glueetl,ScriptLocation=s3://scripts/glue_job.py \\\\\n",
    "    --default-arguments '{\n",
    "        \"--BUCKET\":\"mi-data-lake-2025\",\n",
    "        \"--PREFIX_RAW\":\"curated/ventas/\",\n",
    "        \"--PREFIX_CURATED\":\"gold/ventas/\"\n",
    "    }' \\\\\n",
    "    --glue-version 3.0 \\\\\n",
    "    --number-of-workers 2 \\\\\n",
    "    --worker-type G.1X\n",
    "```\n",
    "\n",
    "**Paso 5: Programar Glue Job con EventBridge**\n",
    "```bash\n",
    "aws events put-rule \\\\\n",
    "    --name daily-etl \\\\\n",
    "    --schedule-expression \"cron(0 2 * * ? *)\"\n",
    "\n",
    "aws events put-targets \\\\\n",
    "    --rule daily-etl \\\\\n",
    "    --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:glue:us-east-1:123456:job/transform-ventas\"\n",
    "```\n",
    "\n",
    "**Paso 6: Query con Athena**\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString='SELECT * FROM ventas WHERE year=2025 LIMIT 10',\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-data-lake-2025/athena-results/'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- **CloudWatch Logs**: Lambda + Glue logs\n",
    "- **CloudWatch Metrics**: Invocations, errors, duration\n",
    "- **X-Ray**: Tracing distribuido\n",
    "- **CloudTrail**: Auditoría de acciones\n",
    "\n",
    "**Costos estimados (ejemplo):**\n",
    "- S3 storage (100GB): ~$2.30/mes\n",
    "- Lambda (10M invocations, 512MB, 1s avg): ~$17/mes\n",
    "- Glue Job (diario, 2 DPU, 30 min): ~$13/mes\n",
    "- Athena (10TB escaneados/mes): ~$50/mes\n",
    "- **Total**: ~$82/mes\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660939bd",
   "metadata": {},
   "source": [
    "## 6. Conclusión y Mejores Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b557d0c",
   "metadata": {},
   "source": [
    "### 🎯 **Key Takeaways de AWS para Data Engineering**\n",
    "\n",
    "**Servicios Clave:**\n",
    "- **S3**: Foundation del data lake (durabilidad 11 nines)\n",
    "- **Lambda**: Validación y transformaciones ligeras event-driven\n",
    "- **Glue**: ETL serverless con PySpark para transformaciones pesadas\n",
    "- **Athena**: Queries SQL ad-hoc sobre S3 sin infraestructura\n",
    "- **Redshift**: Data Warehouse para analytics de alta performance\n",
    "- **EMR**: Clusters Spark/Hadoop para procesamiento masivo\n",
    "\n",
    "**Cuándo usar cada servicio:**\n",
    "\n",
    "| Caso de Uso | Servicio | Razón |\n",
    "|-------------|----------|-------|\n",
    "| Transformación <5 min, <10GB RAM | **Lambda** | Serverless, rápido, económico |\n",
    "| ETL batch, PySpark distribuido | **Glue** | Auto-scaling, sin servidores |\n",
    "| Queries exploratorias sobre S3 | **Athena** | Sin setup, paga por escaneo |\n",
    "| DW con TBs, queries complejas | **Redshift** | MPP, optimizado para analytics |\n",
    "| Custom Spark jobs, ML training | **EMR** | Control total, spot instances |\n",
    "| Real-time streaming | **Kinesis** | Managed Kafka-like service |\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **Organización de S3**:\n",
    "   - Usar particionamiento Hive-style (`year=2025/month=10/day=30/`)\n",
    "   - Formatos columnares (Parquet/ORC) para reducir costos Athena\n",
    "   - Lifecycle policies para migrar a Glacier\n",
    "\n",
    "2. **Lambda**:\n",
    "   - Idempotencia (múltiples invocaciones no dañan)\n",
    "   - DLQ para manejar errores\n",
    "   - Environment variables para configuración\n",
    "   - Layers para compartir dependencias\n",
    "\n",
    "3. **Glue**:\n",
    "   - DynamicFrame para datos semi-estructurados\n",
    "   - Glue Catalog como metastore centralizado\n",
    "   - Partition pruning para optimizar lectura\n",
    "   - Bookmark para procesamiento incremental\n",
    "\n",
    "4. **Athena**:\n",
    "   - Particionar datos agresivamente\n",
    "   - CTAS (Create Table As Select) para materializar resultados\n",
    "   - Workgroups para cost control por equipo\n",
    "\n",
    "5. **Seguridad**:\n",
    "   - IAM Roles (nunca hardcodear credenciales)\n",
    "   - S3 bucket policies (restringir por prefix)\n",
    "   - VPC endpoints para tráfico privado\n",
    "   - Encryption at-rest (S3-SSE) y in-transit (HTTPS)\n",
    "\n",
    "6. **Costos**:\n",
    "   - Spot instances para EMR (70-90% descuento)\n",
    "   - Reserved capacity para Glue/Redshift predictibles\n",
    "   - Comprimir datos (reduce storage + transfer)\n",
    "   - Monitoreo con AWS Cost Explorer\n",
    "\n",
    "**Próximos Pasos:**\n",
    "1. ✅ Completar notebooks GCP y Azure para comparación\n",
    "2. ✅ Practicar con [AWS Free Tier](https://aws.amazon.com/free/)\n",
    "3. ✅ Certificación: [AWS Certified Data Analytics - Specialty](https://aws.amazon.com/certification/certified-data-analytics-specialty/)\n",
    "4. ✅ Explorar Step Functions para orquestación compleja\n",
    "\n",
    "**Happy data engineering en AWS! 🚀**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "© 2024-2025 - Data Engineering Modular Course"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
