{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7ebbc",
   "metadata": {},
   "source": [
    "# ☁️ AWS para Ingeniería de Datos: S3, Glue, Athena y Lambda\n",
    "\n",
    "Este notebook introduce un flujo moderno de datos en AWS con almacenamiento en S3, transformación con Glue (PySpark), consulta con Athena (SQL sobre S3) y orquestación con eventos/Lambda. Incluye ejemplos de código con `boto3` y prácticas recomendadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f739",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ddce0",
   "metadata": {},
   "source": [
    "- Para ejecutar código real de AWS necesitas credenciales configuradas en tu entorno (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`).\n",
    "- Nunca subas credenciales al repositorio. Usa variables de entorno o perfiles de AWS CLI.\n",
    "- Este notebook es auto-contenido con bloques que pueden ejecutarse si tu entorno ya tiene permisos.\n",
    "- Alternativa local: usar LocalStack para simular servicios AWS en tu máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390295",
   "metadata": {},
   "source": [
    "### ☁️ **AWS Cloud: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de AWS para Datos:**\n",
    "\n",
    "1. **S3 (Simple Storage Service)**: Data Lake base\n",
    "   - Almacenamiento ilimitado con 99.999999999% durabilidad (11 nines)\n",
    "   - Pricing: Pay-per-use (~$0.023/GB/mes en Standard)\n",
    "   - Clases de almacenamiento: Standard → Infrequent Access → Glacier (archival)\n",
    "\n",
    "2. **Glue**: Servicio ETL serverless con PySpark\n",
    "   - Auto-escalado de workers (DPU - Data Processing Units)\n",
    "   - Glue Catalog: Metastore centralizado (Hive-compatible)\n",
    "   - Crawlers: Descubrimiento automático de schemas\n",
    "\n",
    "3. **Athena**: Motor SQL interactivo sobre S3\n",
    "   - Basado en Presto, consulta Parquet/ORC/CSV sin infraestructura\n",
    "   - Pricing: $5 por TB escaneado (optimizar con particiones)\n",
    "\n",
    "4. **Redshift**: Data Warehouse MPP (Massively Parallel Processing)\n",
    "   - Columnar storage con compresión\n",
    "   - COPY desde S3, UNLOAD hacia S3\n",
    "\n",
    "5. **EMR (Elastic MapReduce)**: Clusters Spark/Hadoop administrados\n",
    "   - Spot instances para reducir costos 70-90%\n",
    "   - Notebooks Jupyter integrados\n",
    "\n",
    "**Arquitectura Lambda (Batch + Streaming):**\n",
    "```\n",
    "Fuentes → [Kinesis/Kafka] → S3 Raw → [Glue/EMR] → S3 Curated → [Athena/Redshift] → BI\n",
    "                ↓\n",
    "           Lambda (Near Real-Time)\n",
    "```\n",
    "\n",
    "**IAM Best Practices:**\n",
    "- Roles > Users (principio de menor privilegio)\n",
    "- Políticas: `s3:GetObject`, `s3:PutObject` (granular por bucket/prefix)\n",
    "- No hardcodear credenciales: usar IAM Roles, AWS Secrets Manager\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470127",
   "metadata": {},
   "source": [
    "## 1. S3: Data Lake Básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6affbf",
   "metadata": {},
   "source": [
    "### 🗄️ **S3: Fundamentos del Data Lake**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global único (namespace global AWS)\n",
    "  - Nombre: `mi-data-lake-123456` (minúsculas, números, guiones)\n",
    "  - Región: `us-east-1`, `eu-west-1`, etc.\n",
    "\n",
    "- **Prefix (Key)**: Jerarquía lógica simulada (no son carpetas reales)\n",
    "  ```\n",
    "  s3://bucket/raw/ventas/2025/10/ventas_2025_10_30.csv\n",
    "           │     └─────┬─────┘ └──┬──┘ └────┬────┘\n",
    "         Bucket      Prefix    Partición   Archivo\n",
    "  ```\n",
    "\n",
    "- **Versionamiento**: Protección contra eliminación accidental\n",
    "  - Enabled: Cada PUT crea nueva versión con ID único\n",
    "  - Lifecycle policies: Migrar versiones antiguas a Glacier\n",
    "\n",
    "**Patrones de Organización:**\n",
    "\n",
    "1. **Medallion Architecture (Databricks):**\n",
    "   ```\n",
    "   /bronze/    ← Datos crudos (raw, sin transformar)\n",
    "   /silver/    ← Cleaned & validated\n",
    "   /gold/      ← Business-level aggregates\n",
    "   ```\n",
    "\n",
    "2. **Por Dominio:**\n",
    "   ```\n",
    "   /sales/raw/, /sales/curated/\n",
    "   /customers/raw/, /customers/curated/\n",
    "   ```\n",
    "\n",
    "3. **Particionamiento Hive-style:**\n",
    "   ```\n",
    "   /year=2025/month=10/day=30/data.parquet\n",
    "   ```\n",
    "   - Athena/Glue entienden automáticamente las particiones\n",
    "   - Reduce escaneo en queries: `WHERE year=2025 AND month=10`\n",
    "\n",
    "**Operaciones con boto3:**\n",
    "- `put_object()`: Upload inline (para archivos pequeños <5GB)\n",
    "- `upload_file()`: Upload desde disco local (multipart automático)\n",
    "- `list_objects_v2()`: Listar con paginación (max 1000 por request)\n",
    "\n",
    "**Costos:**\n",
    "- PUT/COPY/POST/LIST: $0.005 per 1,000 requests\n",
    "- GET/SELECT: $0.0004 per 1,000 requests\n",
    "- Data transfer OUT: $0.09/GB (dentro de AWS: gratis)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, io\n",
    "import pandas as pd\n",
    "# import boto3  # Descomenta si tienes credenciales configuradas\n",
    "\n",
    "BUCKET = 'mi-data-lake-demo-123456'  # Cambia por un nombre único global\n",
    "PREFIX_RAW = 'raw/ventas/'\n",
    "PREFIX_CURATED = 'curated/ventas/'\n",
    "\n",
    "print('👆 Define el bucket y prefijos antes de ejecutar contra AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a155",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# try:\n",
    "#     s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': 'us-east-1'})\n",
    "#     print('✅ Bucket creado')\n",
    "# except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     print('ℹ️ Bucket ya existe')\n",
    "# except Exception as e:\n",
    "#     print('❌ Error creando bucket:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13106a9",
   "metadata": {},
   "source": [
    "### 1.2 Subir dataset de ejemplo a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "# csv_bytes = df.to_csv(index=False).encode('utf-8')\n",
    "# s3.put_object(Bucket=BUCKET, Key=PREFIX_RAW + 'ventas_2025_10.csv', Body=csv_bytes)\n",
    "# print('📤 Archivo subido a S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc890b4",
   "metadata": {},
   "source": [
    "## 2. Glue: Transformaciones con PySpark (Job Script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6efd1",
   "metadata": {},
   "source": [
    "### 🔧 **AWS Glue: ETL Serverless con PySpark**\n",
    "\n",
    "**Componentes de Glue:**\n",
    "\n",
    "1. **Glue Data Catalog:**\n",
    "   - Metastore centralizado (bases de datos + tablas)\n",
    "   - Compatible con Hive Metastore (usado por Athena, EMR, Redshift Spectrum)\n",
    "   - Schema discovery con Crawlers\n",
    "\n",
    "2. **Glue Crawlers:**\n",
    "   - Escanean S3 y detectan formato/schema automáticamente\n",
    "   - Actualizan el Catalog con nuevas particiones\n",
    "   - Schedule: Cron expressions (`cron(0 0 * * ? *)` = diario a medianoche)\n",
    "\n",
    "3. **Glue Jobs (ETL Scripts):**\n",
    "   - Python Shell (para scripts ligeros)\n",
    "   - PySpark (para transformaciones distribuidas)\n",
    "   - DPU (Data Processing Unit): 1 DPU = 4 vCPU + 16GB RAM\n",
    "   - Pricing: $0.44 por DPU-hora\n",
    "\n",
    "**GlueContext vs SparkContext:**\n",
    "```python\n",
    "# SparkContext: Estándar PySpark\n",
    "sc = SparkContext()\n",
    "\n",
    "# GlueContext: Extensión AWS con métodos adicionales\n",
    "glueContext = GlueContext(sc)\n",
    "glueContext.create_dynamic_frame_from_catalog()  # Lee desde Catalog\n",
    "glueContext.write_dynamic_frame.from_options()    # Escribe con conversiones\n",
    "```\n",
    "\n",
    "**DynamicFrame vs DataFrame:**\n",
    "- **DataFrame (Spark)**: Tipado estricto, requiere schema consistente\n",
    "- **DynamicFrame (Glue)**: Schema flexible, maneja datos semi-estructurados\n",
    "  - `resolveChoice()`: Resuelve conflictos de tipos\n",
    "  - `unbox()`: Desempaqueta structs complejos\n",
    "\n",
    "**Patrón de Job de Glue:**\n",
    "```python\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PARAM1'])\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# [ETL logic aquí]\n",
    "\n",
    "job.commit()  # Marca job como exitoso en Catalog\n",
    "```\n",
    "\n",
    "**Optimizaciones:**\n",
    "- **Pushdown Predicates**: Filtrar en lectura reduce datos procesados\n",
    "- **Partition Pruning**: Leer solo particiones necesarias\n",
    "- **Columnar Formats**: Parquet/ORC reducen IO 10x vs CSV\n",
    "\n",
    "**Uso Real:**\n",
    "Script Glue orquestado por Step Functions o EventBridge (CloudWatch Events) para procesamiento automático al detectar nuevos archivos en S3.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3c7a",
   "metadata": {},
   "source": [
    "Ejemplo de script de Glue (PySpark) para leer CSVs crudos de S3, limpiar/transformar y escribir en formato Parquet particionado por mes. Guarda este script como `glue_job.py` y súbelo a un Job de Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f656567",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_job_script = r'''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','BUCKET','PREFIX_RAW','PREFIX_CURATED'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "raw_path = f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
