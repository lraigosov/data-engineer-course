{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7ebbc",
   "metadata": {},
   "source": [
    "# \u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda\n",
    "\n",
    "Este notebook introduce un flujo moderno de datos en AWS con almacenamiento en S3, transformaci\u00f3n con Glue (PySpark), consulta con Athena (SQL sobre S3) y orquestaci\u00f3n con eventos/Lambda. Incluye ejemplos de c\u00f3digo con `boto3` y pr\u00e1cticas recomendadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f739",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecuci\u00f3n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ddce0",
   "metadata": {},
   "source": [
    "- Para ejecutar c\u00f3digo real de AWS necesitas credenciales configuradas en tu entorno (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`).\n",
    "- Nunca subas credenciales al repositorio. Usa variables de entorno o perfiles de AWS CLI.\n",
    "- Este notebook es auto-contenido con bloques que pueden ejecutarse si tu entorno ya tiene permisos.\n",
    "- Alternativa local: usar LocalStack para simular servicios AWS en tu m\u00e1quina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390295",
   "metadata": {},
   "source": [
    "### \u2601\ufe0f **AWS Cloud: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de AWS para Datos:**\n",
    "\n",
    "1. **S3 (Simple Storage Service)**: Data Lake base\n",
    "   - Almacenamiento ilimitado con 99.999999999% durabilidad (11 nines)\n",
    "   - Pricing: Pay-per-use (~$0.023/GB/mes en Standard)\n",
    "   - Clases de almacenamiento: Standard \u2192 Infrequent Access \u2192 Glacier (archival)\n",
    "\n",
    "2. **Glue**: Servicio ETL serverless con PySpark\n",
    "   - Auto-escalado de workers (DPU - Data Processing Units)\n",
    "   - Glue Catalog: Metastore centralizado (Hive-compatible)\n",
    "   - Crawlers: Descubrimiento autom\u00e1tico de schemas\n",
    "\n",
    "3. **Athena**: Motor SQL interactivo sobre S3\n",
    "   - Basado en Presto, consulta Parquet/ORC/CSV sin infraestructura\n",
    "   - Pricing: $5 por TB escaneado (optimizar con particiones)\n",
    "\n",
    "4. **Redshift**: Data Warehouse MPP (Massively Parallel Processing)\n",
    "   - Columnar storage con compresi\u00f3n\n",
    "   - COPY desde S3, UNLOAD hacia S3\n",
    "\n",
    "5. **EMR (Elastic MapReduce)**: Clusters Spark/Hadoop administrados\n",
    "   - Spot instances para reducir costos 70-90%\n",
    "   - Notebooks Jupyter integrados\n",
    "\n",
    "**Arquitectura Lambda (Batch + Streaming):**\n",
    "```\n",
    "Fuentes \u2192 [Kinesis/Kafka] \u2192 S3 Raw \u2192 [Glue/EMR] \u2192 S3 Curated \u2192 [Athena/Redshift] \u2192 BI\n",
    "                \u2193\n",
    "           Lambda (Near Real-Time)\n",
    "```\n",
    "\n",
    "**IAM Best Practices:**\n",
    "- Roles > Users (principio de menor privilegio)\n",
    "- Pol\u00edticas: `s3:GetObject`, `s3:PutObject` (granular por bucket/prefix)\n",
    "- No hardcodear credenciales: usar IAM Roles, AWS Secrets Manager\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470127",
   "metadata": {},
   "source": [
    "## 1. S3: Data Lake B\u00e1sico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6affbf",
   "metadata": {},
   "source": [
    "### \ud83d\uddc4\ufe0f **S3: Fundamentos del Data Lake**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global \u00fanico (namespace global AWS)\n",
    "  - Nombre: `mi-data-lake-123456` (min\u00fasculas, n\u00fameros, guiones)\n",
    "  - Regi\u00f3n: `us-east-1`, `eu-west-1`, etc.\n",
    "\n",
    "- **Prefix (Key)**: Jerarqu\u00eda l\u00f3gica simulada (no son carpetas reales)\n",
    "  ```\n",
    "  s3://bucket/raw/ventas/2025/10/ventas_2025_10_30.csv\n",
    "           \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "         Bucket      Prefix    Partici\u00f3n   Archivo\n",
    "  ```\n",
    "\n",
    "- **Versionamiento**: Protecci\u00f3n contra eliminaci\u00f3n accidental\n",
    "  - Enabled: Cada PUT crea nueva versi\u00f3n con ID \u00fanico\n",
    "  - Lifecycle policies: Migrar versiones antiguas a Glacier\n",
    "\n",
    "**Patrones de Organizaci\u00f3n:**\n",
    "\n",
    "1. **Medallion Architecture (Databricks):**\n",
    "   ```\n",
    "   /bronze/    \u2190 Datos crudos (raw, sin transformar)\n",
    "   /silver/    \u2190 Cleaned & validated\n",
    "   /gold/      \u2190 Business-level aggregates\n",
    "   ```\n",
    "\n",
    "2. **Por Dominio:**\n",
    "   ```\n",
    "   /sales/raw/, /sales/curated/\n",
    "   /customers/raw/, /customers/curated/\n",
    "   ```\n",
    "\n",
    "3. **Particionamiento Hive-style:**\n",
    "   ```\n",
    "   /year=2025/month=10/day=30/data.parquet\n",
    "   ```\n",
    "   - Athena/Glue entienden autom\u00e1ticamente las particiones\n",
    "   - Reduce escaneo en queries: `WHERE year=2025 AND month=10`\n",
    "\n",
    "**Operaciones con boto3:**\n",
    "- `put_object()`: Upload inline (para archivos peque\u00f1os <5GB)\n",
    "- `upload_file()`: Upload desde disco local (multipart autom\u00e1tico)\n",
    "- `list_objects_v2()`: Listar con paginaci\u00f3n (max 1000 por request)\n",
    "\n",
    "**Costos:**\n",
    "- PUT/COPY/POST/LIST: $0.005 per 1,000 requests\n",
    "- GET/SELECT: $0.0004 per 1,000 requests\n",
    "- Data transfer OUT: $0.09/GB (dentro de AWS: gratis)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, io\n",
    "import pandas as pd\n",
    "# import boto3  # Descomenta si tienes credenciales configuradas\n",
    "\n",
    "BUCKET = 'mi-data-lake-demo-123456'  # Cambia por un nombre \u00fanico global\n",
    "PREFIX_RAW = 'raw/ventas/'\n",
    "PREFIX_CURATED = 'curated/ventas/'\n",
    "\n",
    "print('\ud83d\udc46 Define el bucket y prefijos antes de ejecutar contra AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a155",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# try:\n",
    "#     s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': 'us-east-1'})\n",
    "#     print('\u2705 Bucket creado')\n",
    "# except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     print('\u2139\ufe0f Bucket ya existe')\n",
    "# except Exception as e:\n",
    "#     print('\u274c Error creando bucket:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13106a9",
   "metadata": {},
   "source": [
    "### 1.2 Subir dataset de ejemplo a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "# csv_bytes = df.to_csv(index=False).encode('utf-8')\n",
    "# s3.put_object(Bucket=BUCKET, Key=PREFIX_RAW + 'ventas_2025_10.csv', Body=csv_bytes)\n",
    "# print('\ud83d\udce4 Archivo subido a S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc890b4",
   "metadata": {},
   "source": [
    "## 2. Glue: Transformaciones con PySpark (Job Script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6efd1",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 **AWS Glue: ETL Serverless con PySpark**\n",
    "\n",
    "**Componentes de Glue:**\n",
    "\n",
    "1. **Glue Data Catalog:**\n",
    "   - Metastore centralizado (bases de datos + tablas)\n",
    "   - Compatible con Hive Metastore (usado por Athena, EMR, Redshift Spectrum)\n",
    "   - Schema discovery con Crawlers\n",
    "\n",
    "2. **Glue Crawlers:**\n",
    "   - Escanean S3 y detectan formato/schema autom\u00e1ticamente\n",
    "   - Actualizan el Catalog con nuevas particiones\n",
    "   - Schedule: Cron expressions (`cron(0 0 * * ? *)` = diario a medianoche)\n",
    "\n",
    "3. **Glue Jobs (ETL Scripts):**\n",
    "   - Python Shell (para scripts ligeros)\n",
    "   - PySpark (para transformaciones distribuidas)\n",
    "   - DPU (Data Processing Unit): 1 DPU = 4 vCPU + 16GB RAM\n",
    "   - Pricing: $0.44 por DPU-hora\n",
    "\n",
    "**GlueContext vs SparkContext:**\n",
    "```python\n",
    "# SparkContext: Est\u00e1ndar PySpark\n",
    "sc = SparkContext()\n",
    "\n",
    "# GlueContext: Extensi\u00f3n AWS con m\u00e9todos adicionales\n",
    "glueContext = GlueContext(sc)\n",
    "glueContext.create_dynamic_frame_from_catalog()  # Lee desde Catalog\n",
    "glueContext.write_dynamic_frame.from_options()    # Escribe con conversiones\n",
    "```\n",
    "\n",
    "**DynamicFrame vs DataFrame:**\n",
    "- **DataFrame (Spark)**: Tipado estricto, requiere schema consistente\n",
    "- **DynamicFrame (Glue)**: Schema flexible, maneja datos semi-estructurados\n",
    "  - `resolveChoice()`: Resuelve conflictos de tipos\n",
    "  - `unbox()`: Desempaqueta structs complejos\n",
    "\n",
    "**Patr\u00f3n de Job de Glue:**\n",
    "```python\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PARAM1'])\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# [ETL logic aqu\u00ed]\n",
    "\n",
    "job.commit()  # Marca job como exitoso en Catalog\n",
    "```\n",
    "\n",
    "**Optimizaciones:**\n",
    "- **Pushdown Predicates**: Filtrar en lectura reduce datos procesados\n",
    "- **Partition Pruning**: Leer solo particiones necesarias\n",
    "- **Columnar Formats**: Parquet/ORC reducen IO 10x vs CSV\n",
    "\n",
    "**Uso Real:**\n",
    "Script Glue orquestado por Step Functions o EventBridge (CloudWatch Events) para procesamiento autom\u00e1tico al detectar nuevos archivos en S3.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3c7a",
   "metadata": {},
   "source": [
    "Ejemplo de script de Glue (PySpark) para leer CSVs crudos de S3, limpiar/transformar y escribir en formato Parquet particionado por mes. Guarda este script como `glue_job.py` y s\u00fabelo a un Job de Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f656567",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_job_script = r'''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','BUCKET','PREFIX_RAW','PREFIX_CURATED'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "raw_path = f\"s3://{args['BUCKET']}/{args['PREFIX_RAW']}*.csv\"\n",
    "curated_path = f\"s3://{args['BUCKET']}/{args['PREFIX_CURATED']}\"\n",
    "\n",
    "# Leer CSV desde S3\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "\n",
    "# Transformaciones: limpiar nulos, filtrar valores positivos, agregar timestamp\n",
    "df_clean = (df\n",
    "    .dropna(subset=['venta_id', 'total'])\n",
    "    .filter(F.col('total') > 0)\n",
    "    .withColumn('procesado_en', F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Escribir como Parquet particionado por mes\n",
    "df_clean.write.partitionBy('fecha').mode('overwrite').parquet(curated_path)\n",
    "\n",
    "job.commit()\n",
    "print('\u2705 ETL completado')\n",
    "'''\n",
    "print('\ud83d\udcdd Script de Glue listo para deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3349e01",
   "metadata": {},
   "source": [
    "## 3. AWS Lambda: Procesamiento Serverless Event-Driven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62351c",
   "metadata": {},
   "source": [
    "### \u26a1 **AWS Lambda: Compute Serverless para Datos**\n",
    "\n",
    "**\u00bfQu\u00e9 es Lambda?**\n",
    "\n",
    "AWS Lambda es un servicio de computaci\u00f3n serverless que ejecuta c\u00f3digo en respuesta a eventos, sin necesidad de aprovisionar o administrar servidores.\n",
    "\n",
    "**Caracter\u00edsticas Principales:**\n",
    "\n",
    "- **Event-Driven**: Se activa autom\u00e1ticamente con triggers (S3, DynamoDB, EventBridge, API Gateway)\n",
    "- **Auto-Scaling**: Escala autom\u00e1ticamente de 0 a miles de invocaciones concurrentes\n",
    "- **Pay-per-Use**: Cobro solo por tiempo de ejecuci\u00f3n (milisegundos)\n",
    "- **L\u00edmites**: Max 15 minutos por invocaci\u00f3n, hasta 10GB de RAM\n",
    "\n",
    "**Triggers Comunes para Data Engineering:**\n",
    "\n",
    "1. **S3 Events**: Procesar archivos al subirse\n",
    "   ```python\n",
    "   # Lambda se activa cuando se crea objeto en S3\n",
    "   def lambda_handler(event, context):\n",
    "       bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "       key = event['Records'][0]['s3']['object']['key']\n",
    "       # Procesar archivo\n",
    "   ```\n",
    "\n",
    "2. **EventBridge (CloudWatch Events)**: Cron jobs programados\n",
    "   ```python\n",
    "   # Ejecutar diariamente a las 2 AM\n",
    "   # Regla EventBridge: rate(1 day) o cron(0 2 * * ? *)\n",
    "   ```\n",
    "\n",
    "3. **DynamoDB Streams**: Procesar cambios en tiempo real\n",
    "   ```python\n",
    "   # Lambda recibe cambios de DynamoDB\n",
    "   for record in event['Records']:\n",
    "       if record['eventName'] == 'INSERT':\n",
    "           # Procesar nuevo registro\n",
    "   ```\n",
    "\n",
    "4. **SQS/SNS**: Cola de mensajes para procesamiento as\u00edncrono\n",
    "   ```python\n",
    "   # Lambda consume mensajes de SQS autom\u00e1ticamente\n",
    "   for record in event['Records']:\n",
    "       body = json.loads(record['body'])\n",
    "   ```\n",
    "\n",
    "**Arquitectura Lambda para Datos:**\n",
    "\n",
    "```\n",
    "S3 (new file) \u2192 Lambda \u2192 Validar \u2192 S3 (curated)\n",
    "                   \u2193\n",
    "                 SQS (errores) \u2192 Lambda (retry)\n",
    "                   \u2193\n",
    "              CloudWatch Logs (monitoring)\n",
    "```\n",
    "\n",
    "**Capas (Layers):**\n",
    "- Librer\u00edas compartidas (pandas, requests, etc.)\n",
    "- Reduce tama\u00f1o del deployment package\n",
    "- Reutilizaci\u00f3n entre funciones\n",
    "\n",
    "**Best Practices:**\n",
    "- **Idempotencia**: Lambda puede reintentar, asegurar que m\u00faltiples ejecuciones no causen problemas\n",
    "- **Environment Variables**: Configuraci\u00f3n (buckets, endpoints) sin hardcodear\n",
    "- **Timeout**: Configurar seg\u00fan necesidad (default 3s, max 15min)\n",
    "- **Memory**: Mayor memoria = m\u00e1s CPU (128MB a 10GB)\n",
    "- **Dead Letter Queue (DLQ)**: SQS/SNS para errores no recuperables\n",
    "- **Async invocation**: Para procesamiento no-blocking\n",
    "\n",
    "**Limitaciones:**\n",
    "- \u274c Max 15 minutos (para ETL largo, usar Glue/EMR)\n",
    "- \u274c Max 10GB RAM\n",
    "- \u274c Max 250MB deployment package (sin layers)\n",
    "- \u274c /tmp storage max 512MB (ef\u00edmero)\n",
    "- \u2705 Ideal para: Validaci\u00f3n, transformaciones ligeras, triggers\n",
    "\n",
    "**Pricing:**\n",
    "- Requests: $0.20 por 1M requests\n",
    "- Duration: $0.0000166667 por GB-segundo\n",
    "- Ejemplo: 1M requests x 1GB x 1s = ~$17/mes\n",
    "- Free tier: 1M requests + 400,000 GB-segundos/mes\n",
    "\n",
    "**Alternativas:**\n",
    "- **Glue Python Shell**: Para scripts Python no-PySpark (no l\u00edmite 15min)\n",
    "- **ECS Fargate**: Para procesos largos sin administrar servidores\n",
    "- **Step Functions**: Orquestaci\u00f3n de m\u00faltiples Lambdas\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f427134",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo: Lambda para validar CSV al subir a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_handler_code = '''\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by S3 PutObject event.\n",
    "    Validates CSV structure and moves to curated zone if valid.\n",
    "    \"\"\"\n",
    "    # Extraer informaci\u00f3n del evento\n",
    "    for record in event['Records']:\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        key = record['s3']['object']['key']\n",
    "        \n",
    "        print(f'Processing file: s3://{bucket}/{key}')\n",
    "        \n",
    "        # Skip if not CSV\n",
    "        if not key.endswith('.csv'):\n",
    "            print('Skipping non-CSV file')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Leer CSV desde S3\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            csv_content = response['Body'].read()\n",
    "            df = pd.read_csv(BytesIO(csv_content))\n",
    "            \n",
    "            # Validaciones\n",
    "            required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                raise ValueError(f'Missing columns: {missing_cols}')\n",
    "            \n",
    "            # Validar tipos y valores\n",
    "            if df['total'].isnull().sum() > 0:\n",
    "                raise ValueError(f'Null values found in total column: {df[\"total\"].isnull().sum()}')\n",
    "            \n",
    "            if (df['total'] < 0).sum() > 0:\n",
    "                raise ValueError(f'Negative values in total: {(df[\"total\"] < 0).sum()}')\n",
    "            \n",
    "            # Si v\u00e1lido, mover a zona curated\n",
    "            curated_key = key.replace('/raw/', '/curated/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=curated_key\n",
    "            )\n",
    "            \n",
    "            print(f'\u2705 File validated and copied to: s3://{bucket}/{curated_key}')\n",
    "            \n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps({\n",
    "                    'message': 'File processed successfully',\n",
    "                    'rows': len(df),\n",
    "                    'output': f's3://{bucket}/{curated_key}'\n",
    "                })\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\u274c Error processing file: {str(e)}')\n",
    "            \n",
    "            # Mover a carpeta de errores\n",
    "            error_key = key.replace('/raw/', '/errors/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=error_key\n",
    "            )\n",
    "            \n",
    "            # Enviar a DLQ o SNS para alertas\n",
    "            return {\n",
    "                'statusCode': 500,\n",
    "                'body': json.dumps({\n",
    "                    'error': str(e),\n",
    "                    'file': f's3://{bucket}/{key}'\n",
    "                })\n",
    "            }\n",
    "'''\n",
    "\n",
    "print('\ud83d\udcdd Lambda handler para validaci\u00f3n de CSV')\n",
    "print('\\\\nPara deployar:')\n",
    "print('1. Crear funci\u00f3n Lambda en AWS Console')\n",
    "print('2. Agregar layer con pandas (o usar Lambda Container)')\n",
    "print('3. Configurar trigger S3 en bucket/raw/')\n",
    "print('4. Ajustar timeout (60s) y memoria (512MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f34d9",
   "metadata": {},
   "source": [
    "## 4. Athena: SQL sobre S3 sin Infraestructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82e6f3",
   "metadata": {},
   "source": [
    "### \ud83d\udcca **Amazon Athena: Query Engine Serverless**\n",
    "\n",
    "Athena permite ejecutar queries SQL est\u00e1ndar directamente sobre datos en S3 sin necesidad de cargarlos en una base de datos.\n",
    "\n",
    "**Caracter\u00edsticas:**\n",
    "- **Basado en Presto/Trino**: Engine SQL distribuido\n",
    "- **Sin servidores**: No hay clusters que administrar\n",
    "- **Standard SQL**: Compatible con ANSI SQL\n",
    "- **Formatos soportados**: CSV, JSON, Parquet, ORC, Avro\n",
    "\n",
    "**Pricing:** $5 por TB de datos escaneados\n",
    "\n",
    "**Optimizaciones clave:**\n",
    "1. **Particionar datos**: Reduce escaneo con `WHERE year=2025 AND month=10`\n",
    "2. **Formato columnar**: Parquet/ORC reducen escaneo 10x vs CSV\n",
    "3. **Comprimir**: SNAPPY, GZIP, ZSTD reducen tama\u00f1o\n",
    "4. **Proyecci\u00f3n**: `SELECT col1, col2` en vez de `SELECT *`\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```sql\n",
    "-- Crear tabla externa apuntando a S3\n",
    "CREATE EXTERNAL TABLE ventas (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    producto_id INT,\n",
    "    cantidad INT,\n",
    "    total DECIMAL(10,2),\n",
    "    fecha DATE\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://mi-data-lake/curated/ventas/';\n",
    "\n",
    "-- Agregar particiones\n",
    "MSCK REPAIR TABLE ventas;\n",
    "\n",
    "-- Query con partition pruning (solo escanea Oct 2025)\n",
    "SELECT cliente_id, SUM(total) as total_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Integraci\u00f3n con Python (boto3):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "query = \\\"\\\"\\\"\n",
    "SELECT cliente_id, COUNT(*) as num_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-bucket/athena-results/'}\n",
    ")\n",
    "\n",
    "query_id = response['QueryExecutionId']\n",
    "\n",
    "# Esperar a que termine\n",
    "while True:\n",
    "    status = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "    state = status['QueryExecution']['Status']['State']\n",
    "    \n",
    "    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "if state == 'SUCCEEDED':\n",
    "    results = athena.get_query_results(QueryExecutionId=query_id)\n",
    "    # Procesar resultados\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745195ec",
   "metadata": {},
   "source": [
    "## 5. Arquitectura Completa: S3 + Lambda + Glue + Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533cfa8",
   "metadata": {},
   "source": [
    "### \ud83c\udfd7\ufe0f **Pipeline End-to-End en AWS**\n",
    "\n",
    "**Flujo completo de datos:**\n",
    "\n",
    "```\n",
    "1. INGESTA\n",
    "   Fuentes \u2192 S3 (bucket/raw/)\n",
    "   \n",
    "2. VALIDACI\u00d3N (Lambda)\n",
    "   S3 Event \u2192 Lambda \u2192 Validar schema\n",
    "                \u2193\n",
    "           S3 (curated/) o S3 (errors/)\n",
    "   \n",
    "3. TRANSFORMACI\u00d3N (Glue)\n",
    "   EventBridge (cron) \u2192 Glue Job (PySpark)\n",
    "                          \u2193\n",
    "                    S3 (gold/) Parquet particionado\n",
    "   \n",
    "4. CATALOGACI\u00d3N (Glue Crawler)\n",
    "   Glue Crawler \u2192 Detecta schema \u2192 Glue Catalog\n",
    "   \n",
    "5. AN\u00c1LISIS (Athena)\n",
    "   Athena \u2192 SQL queries \u2192 S3 (resultados)\n",
    "            \u2193\n",
    "       QuickSight (BI) / Python (an\u00e1lisis)\n",
    "```\n",
    "\n",
    "**Ejemplo de implementaci\u00f3n:**\n",
    "\n",
    "**Paso 1: Configurar S3 Bucket**\n",
    "```bash\n",
    "aws s3 mb s3://mi-data-lake-2025\n",
    "aws s3api put-bucket-versioning \\\\\n",
    "    --bucket mi-data-lake-2025 \\\\\n",
    "    --versioning-configuration Status=Enabled\n",
    "```\n",
    "\n",
    "**Paso 2: Deploy Lambda**\n",
    "```bash\n",
    "# Empaquetar c\u00f3digo con dependencias\n",
    "zip lambda_function.zip lambda_function.py\n",
    "aws lambda create-function \\\\\n",
    "    --function-name validate-csv \\\\\n",
    "    --runtime python3.10 \\\\\n",
    "    --role arn:aws:iam::123456:role/lambda-s3-role \\\\\n",
    "    --handler lambda_function.lambda_handler \\\\\n",
    "    --zip-file fileb://lambda_function.zip \\\\\n",
    "    --timeout 60 \\\\\n",
    "    --memory-size 512\n",
    "```\n",
    "\n",
    "**Paso 3: Configurar S3 Event Notification**\n",
    "```json\n",
    "{\n",
    "  \"LambdaFunctionConfigurations\": [\n",
    "    {\n",
    "      \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456:function:validate-csv\",\n",
    "      \"Events\": [\"s3:ObjectCreated:*\"],\n",
    "      \"Filter\": {\n",
    "        \"Key\": {\n",
    "          \"FilterRules\": [\n",
    "            {\"Name\": \"prefix\", \"Value\": \"raw/\"},\n",
    "            {\"Name\": \"suffix\", \"Value\": \".csv\"}\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Paso 4: Crear Glue Job**\n",
    "```bash\n",
    "aws glue create-job \\\\\n",
    "    --name transform-ventas \\\\\n",
    "    --role AWSGlueServiceRole \\\\\n",
    "    --command Name=glueetl,ScriptLocation=s3://scripts/glue_job.py \\\\\n",
    "    --default-arguments '{\n",
    "        \"--BUCKET\":\"mi-data-lake-2025\",\n",
    "        \"--PREFIX_RAW\":\"curated/ventas/\",\n",
    "        \"--PREFIX_CURATED\":\"gold/ventas/\"\n",
    "    }' \\\\\n",
    "    --glue-version 3.0 \\\\\n",
    "    --number-of-workers 2 \\\\\n",
    "    --worker-type G.1X\n",
    "```\n",
    "\n",
    "**Paso 5: Programar Glue Job con EventBridge**\n",
    "```bash\n",
    "aws events put-rule \\\\\n",
    "    --name daily-etl \\\\\n",
    "    --schedule-expression \"cron(0 2 * * ? *)\"\n",
    "\n",
    "aws events put-targets \\\\\n",
    "    --rule daily-etl \\\\\n",
    "    --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:glue:us-east-1:123456:job/transform-ventas\"\n",
    "```\n",
    "\n",
    "**Paso 6: Query con Athena**\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString='SELECT * FROM ventas WHERE year=2025 LIMIT 10',\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-data-lake-2025/athena-results/'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- **CloudWatch Logs**: Lambda + Glue logs\n",
    "- **CloudWatch Metrics**: Invocations, errors, duration\n",
    "- **X-Ray**: Tracing distribuido\n",
    "- **CloudTrail**: Auditor\u00eda de acciones\n",
    "\n",
    "**Costos estimados (ejemplo):**\n",
    "- S3 storage (100GB): ~$2.30/mes\n",
    "- Lambda (10M invocations, 512MB, 1s avg): ~$17/mes\n",
    "- Glue Job (diario, 2 DPU, 30 min): ~$13/mes\n",
    "- Athena (10TB escaneados/mes): ~$50/mes\n",
    "- **Total**: ~$82/mes\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660939bd",
   "metadata": {},
   "source": [
    "## 6. Conclusi\u00f3n y Mejores Pr\u00e1cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b557d0c",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf **Key Takeaways de AWS para Data Engineering**\n",
    "\n",
    "**Servicios Clave:**\n",
    "- **S3**: Foundation del data lake (durabilidad 11 nines)\n",
    "- **Lambda**: Validaci\u00f3n y transformaciones ligeras event-driven\n",
    "- **Glue**: ETL serverless con PySpark para transformaciones pesadas\n",
    "- **Athena**: Queries SQL ad-hoc sobre S3 sin infraestructura\n",
    "- **Redshift**: Data Warehouse para analytics de alta performance\n",
    "- **EMR**: Clusters Spark/Hadoop para procesamiento masivo\n",
    "\n",
    "**Cu\u00e1ndo usar cada servicio:**\n",
    "\n",
    "| Caso de Uso | Servicio | Raz\u00f3n |\n",
    "|-------------|----------|-------|\n",
    "| Transformaci\u00f3n <5 min, <10GB RAM | **Lambda** | Serverless, r\u00e1pido, econ\u00f3mico |\n",
    "| ETL batch, PySpark distribuido | **Glue** | Auto-scaling, sin servidores |\n",
    "| Queries exploratorias sobre S3 | **Athena** | Sin setup, paga por escaneo |\n",
    "| DW con TBs, queries complejas | **Redshift** | MPP, optimizado para analytics |\n",
    "| Custom Spark jobs, ML training | **EMR** | Control total, spot instances |\n",
    "| Real-time streaming | **Kinesis** | Managed Kafka-like service |\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **Organizaci\u00f3n de S3**:\n",
    "   - Usar particionamiento Hive-style (`year=2025/month=10/day=30/`)\n",
    "   - Formatos columnares (Parquet/ORC) para reducir costos Athena\n",
    "   - Lifecycle policies para migrar a Glacier\n",
    "\n",
    "2. **Lambda**:\n",
    "   - Idempotencia (m\u00faltiples invocaciones no da\u00f1an)\n",
    "   - DLQ para manejar errores\n",
    "   - Environment variables para configuraci\u00f3n\n",
    "   - Layers para compartir dependencias\n",
    "\n",
    "3. **Glue**:\n",
    "   - DynamicFrame para datos semi-estructurados\n",
    "   - Glue Catalog como metastore centralizado\n",
    "   - Partition pruning para optimizar lectura\n",
    "   - Bookmark para procesamiento incremental\n",
    "\n",
    "4. **Athena**:\n",
    "   - Particionar datos agresivamente\n",
    "   - CTAS (Create Table As Select) para materializar resultados\n",
    "   - Workgroups para cost control por equipo\n",
    "\n",
    "5. **Seguridad**:\n",
    "   - IAM Roles (nunca hardcodear credenciales)\n",
    "   - S3 bucket policies (restringir por prefix)\n",
    "   - VPC endpoints para tr\u00e1fico privado\n",
    "   - Encryption at-rest (S3-SSE) y in-transit (HTTPS)\n",
    "\n",
    "6. **Costos**:\n",
    "   - Spot instances para EMR (70-90% descuento)\n",
    "   - Reserved capacity para Glue/Redshift predictibles\n",
    "   - Comprimir datos (reduce storage + transfer)\n",
    "   - Monitoreo con AWS Cost Explorer\n",
    "\n",
    "**Pr\u00f3ximos Pasos:**\n",
    "1. \u2705 Completar notebooks GCP y Azure para comparaci\u00f3n\n",
    "2. \u2705 Practicar con [AWS Free Tier](https://aws.amazon.com/free/)\n",
    "3. \u2705 Certificaci\u00f3n: [AWS Certified Data Analytics - Specialty](https://aws.amazon.com/certification/certified-data-analytics-specialty/)\n",
    "4. \u2705 Explorar Step Functions para orquestaci\u00f3n compleja\n",
    "\n",
    "**Happy data engineering en AWS! \ud83d\ude80**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "\u00a9 2024-2025 - Data Engineering Modular Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\udded Navegaci\u00f3n\n",
    "\n",
    "**\u2190 Anterior:** [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "\n",
    "**Siguiente \u2192:** [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer \u2192](03b_cloud_gcp.ipynb)\n",
    "\n",
    "**\ud83d\udcda \u00cdndice de Nivel Mid:**\n",
    "- [\u26a1 Mid - 01. Orquestaci\u00f3n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [\u2601\ufe0f AWS para Ingenier\u00eda de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb) \u2190 \ud83d\udd35 Est\u00e1s aqu\u00ed\n",
    "- [\u2601\ufe0f GCP para Ingenier\u00eda de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [\u2601\ufe0f Azure para Ingenier\u00eda de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [\ud83d\uddc4\ufe0f Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [\u267b\ufe0f DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [\ud83c\udf10 Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [\ud83e\udde9 Optimizaci\u00f3n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [\ud83d\ude80 Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [\ud83e\uddea Proyecto Integrador Mid 1: API \u2192 DB \u2192 Parquet con Orquestaci\u00f3n](09_proyecto_integrador_1.ipynb)\n",
    "- [\ud83d\udd04 Proyecto Integrador Mid 2: Kafka \u2192 Streaming \u2192 Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**\ud83c\udf93 Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
