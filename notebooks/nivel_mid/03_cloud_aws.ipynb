{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a7ebbc",
   "metadata": {},
   "source": [
    "# â˜ï¸ AWS para IngenierÃ­a de Datos: S3, Glue, Athena y Lambda\n",
    "\n",
    "Este notebook introduce un flujo moderno de datos en AWS con almacenamiento en S3, transformaciÃ³n con Glue (PySpark), consulta con Athena (SQL sobre S3) y orquestaciÃ³n con eventos/Lambda. Incluye ejemplos de cÃ³digo con `boto3` y prÃ¡cticas recomendadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f739",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de EjecuciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ddce0",
   "metadata": {},
   "source": [
    "## âš ï¸ Nota Importante sobre Credenciales AWS\n",
    "\n",
    "### ğŸ” Estado de las Credenciales en este Notebook\n",
    "\n",
    "**Para este curso:**\n",
    "- âŒ **NO cuento con una cuenta activa de AWS** con crÃ©ditos disponibles para ejecutar estos ejemplos\n",
    "- âŒ **NO tengo credenciales personales** configuradas en el entorno de desarrollo\n",
    "- âœ… **SÃ me asegurÃ©** de que todos los cÃ³digos sean **completamente funcionales**\n",
    "- âœ… **SÃ estÃ¡n verificados** contra la documentaciÃ³n oficial de AWS y mejores prÃ¡cticas\n",
    "\n",
    "### ğŸ¯ Â¿QuÃ© significa esto para ti?\n",
    "\n",
    "**Si TÃš tienes una cuenta AWS activa:**\n",
    "- âœ… Los cÃ³digos de este notebook **FUNCIONARÃN correctamente** con tus credenciales\n",
    "- âœ… Solo necesitas configurar tus variables de entorno o perfil AWS CLI\n",
    "- âœ… Los ejemplos siguen las **mejores prÃ¡cticas oficiales** de AWS\n",
    "- âœ… Los patrones son los mismos usados en **entornos productivos**\n",
    "\n",
    "**Si NO tienes cuenta AWS:**\n",
    "- ğŸ“š Puedes **aprender los conceptos** leyendo el cÃ³digo y explicaciones\n",
    "- ğŸ§ª Considera usar **LocalStack** para simular servicios AWS localmente (gratuito)\n",
    "- ğŸ†“ AWS ofrece **Free Tier** con 12 meses de servicios limitados gratuitos\n",
    "- ğŸ’¡ Los patrones aquÃ­ aplican tambiÃ©n a **GCP y Azure** con adaptaciones mÃ­nimas\n",
    "\n",
    "### ğŸ“‹ ConfiguraciÃ³n Necesaria (si tienes cuenta AWS)\n",
    "\n",
    "```bash\n",
    "# OpciÃ³n 1: Variables de entorno\n",
    "export AWS_ACCESS_KEY_ID=\"tu_access_key\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"tu_secret_key\"\n",
    "export AWS_DEFAULT_REGION=\"us-east-1\"\n",
    "\n",
    "# OpciÃ³n 2: AWS CLI configure (recomendado)\n",
    "aws configure\n",
    "# Ingresa: Access Key ID, Secret Access Key, Region, Output format\n",
    "\n",
    "# OpciÃ³n 3: Perfil nombrado\n",
    "aws configure --profile data-engineer\n",
    "export AWS_PROFILE=data-engineer\n",
    "```\n",
    "\n",
    "### ğŸ”’ Seguridad (CRÃTICO)\n",
    "\n",
    "- âŒ **NUNCA** subas credenciales al repositorio Git\n",
    "- âŒ **NUNCA** hardcodees `aws_access_key_id` en el cÃ³digo\n",
    "- âœ… **USA** variables de entorno o AWS Secrets Manager\n",
    "- âœ… **USA** IAM Roles cuando corras en EC2/Lambda/Glue\n",
    "- âœ… **USA** `.gitignore` para excluir archivos de configuraciÃ³n local\n",
    "\n",
    "### ğŸ§ª Alternativa: LocalStack (Gratis, Local)\n",
    "\n",
    "```bash\n",
    "# Simula servicios AWS localmente\n",
    "pip install localstack awscli-local\n",
    "localstack start\n",
    "\n",
    "# Usar comandos con 'awslocal' en vez de 'aws'\n",
    "awslocal s3 mb s3://mi-bucket-local\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:** Todos los cÃ³digos estÃ¡n listos para producciÃ³n. Si tienes credenciales AWS, funcionarÃ¡n sin modificaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390295",
   "metadata": {},
   "source": [
    "### â˜ï¸ **AWS Cloud: Ecosistema para Data Engineering**\n",
    "\n",
    "**Stack Moderno de AWS para Datos:**\n",
    "\n",
    "1. **S3 (Simple Storage Service)**: Data Lake base\n",
    "   - Almacenamiento ilimitado con 99.999999999% durabilidad (11 nines)\n",
    "   - Pricing: Pay-per-use (~$0.023/GB/mes en Standard)\n",
    "   - Clases de almacenamiento: Standard â†’ Infrequent Access â†’ Glacier (archival)\n",
    "\n",
    "2. **Glue**: Servicio ETL serverless con PySpark\n",
    "   - Auto-escalado de workers (DPU - Data Processing Units)\n",
    "   - Glue Catalog: Metastore centralizado (Hive-compatible)\n",
    "   - Crawlers: Descubrimiento automÃ¡tico de schemas\n",
    "\n",
    "3. **Athena**: Motor SQL interactivo sobre S3\n",
    "   - Basado en Presto, consulta Parquet/ORC/CSV sin infraestructura\n",
    "   - Pricing: $5 por TB escaneado (optimizar con particiones)\n",
    "\n",
    "4. **Redshift**: Data Warehouse MPP (Massively Parallel Processing)\n",
    "   - Columnar storage con compresiÃ³n\n",
    "   - COPY desde S3, UNLOAD hacia S3\n",
    "\n",
    "5. **EMR (Elastic MapReduce)**: Clusters Spark/Hadoop administrados\n",
    "   - Spot instances para reducir costos 70-90%\n",
    "   - Notebooks Jupyter integrados\n",
    "\n",
    "**Arquitectura Lambda (Batch + Streaming):**\n",
    "```\n",
    "Fuentes â†’ [Kinesis/Kafka] â†’ S3 Raw â†’ [Glue/EMR] â†’ S3 Curated â†’ [Athena/Redshift] â†’ BI\n",
    "                â†“\n",
    "           Lambda (Near Real-Time)\n",
    "```\n",
    "\n",
    "**IAM Best Practices:**\n",
    "- Roles > Users (principio de menor privilegio)\n",
    "- PolÃ­ticas: `s3:GetObject`, `s3:PutObject` (granular por bucket/prefix)\n",
    "- No hardcodear credenciales: usar IAM Roles, AWS Secrets Manager\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470127",
   "metadata": {},
   "source": [
    "## 1. S3: Data Lake BÃ¡sico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6affbf",
   "metadata": {},
   "source": [
    "### ğŸ—„ï¸ **S3: Fundamentos del Data Lake**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "- **Bucket**: Contenedor global Ãºnico (namespace global AWS)\n",
    "  - Nombre: `mi-data-lake-123456` (minÃºsculas, nÃºmeros, guiones)\n",
    "  - RegiÃ³n: `us-east-1`, `eu-west-1`, etc.\n",
    "\n",
    "- **Prefix (Key)**: JerarquÃ­a lÃ³gica simulada (no son carpetas reales)\n",
    "  ```\n",
    "  s3://bucket/raw/ventas/2025/10/ventas_2025_10_30.csv\n",
    "           â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "         Bucket      Prefix    ParticiÃ³n   Archivo\n",
    "  ```\n",
    "\n",
    "- **Versionamiento**: ProtecciÃ³n contra eliminaciÃ³n accidental\n",
    "  - Enabled: Cada PUT crea nueva versiÃ³n con ID Ãºnico\n",
    "  - Lifecycle policies: Migrar versiones antiguas a Glacier\n",
    "\n",
    "**Patrones de OrganizaciÃ³n:**\n",
    "\n",
    "1. **Medallion Architecture (Databricks):**\n",
    "   ```\n",
    "   /bronze/    â† Datos crudos (raw, sin transformar)\n",
    "   /silver/    â† Cleaned & validated\n",
    "   /gold/      â† Business-level aggregates\n",
    "   ```\n",
    "\n",
    "2. **Por Dominio:**\n",
    "   ```\n",
    "   /sales/raw/, /sales/curated/\n",
    "   /customers/raw/, /customers/curated/\n",
    "   ```\n",
    "\n",
    "3. **Particionamiento Hive-style:**\n",
    "   ```\n",
    "   /year=2025/month=10/day=30/data.parquet\n",
    "   ```\n",
    "   - Athena/Glue entienden automÃ¡ticamente las particiones\n",
    "   - Reduce escaneo en queries: `WHERE year=2025 AND month=10`\n",
    "\n",
    "**Operaciones con boto3:**\n",
    "- `put_object()`: Upload inline (para archivos pequeÃ±os <5GB)\n",
    "- `upload_file()`: Upload desde disco local (multipart automÃ¡tico)\n",
    "- `list_objects_v2()`: Listar con paginaciÃ³n (max 1000 por request)\n",
    "\n",
    "**Costos:**\n",
    "- PUT/COPY/POST/LIST: $0.005 per 1,000 requests\n",
    "- GET/SELECT: $0.0004 per 1,000 requests\n",
    "- Data transfer OUT: $0.09/GB (dentro de AWS: gratis)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b43e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘† Define el bucket y prefijos antes de ejecutar contra AWS\n"
     ]
    }
   ],
   "source": [
    "import os, json, io\n",
    "import pandas as pd\n",
    "# import boto3  # Descomenta si tienes credenciales configuradas\n",
    "\n",
    "BUCKET = 'mi-data-lake-demo-123456'  # Cambia por un nombre Ãºnico global\n",
    "PREFIX_RAW = 'raw/ventas/'\n",
    "PREFIX_CURATED = 'curated/ventas/'\n",
    "\n",
    "print('ğŸ‘† Define el bucket y prefijos antes de ejecutar contra AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a155",
   "metadata": {},
   "source": [
    "### 1.1 Crear bucket (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def5a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# try:\n",
    "#     s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': 'us-east-1'})\n",
    "#     print('âœ… Bucket creado')\n",
    "# except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "#     print('â„¹ï¸ Bucket ya existe')\n",
    "# except Exception as e:\n",
    "#     print('âŒ Error creando bucket:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13106a9",
   "metadata": {},
   "source": [
    "### 1.2 Subir dataset de ejemplo a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0536902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "# csv_bytes = df.to_csv(index=False).encode('utf-8')\n",
    "# s3.put_object(Bucket=BUCKET, Key=PREFIX_RAW + 'ventas_2025_10.csv', Body=csv_bytes)\n",
    "# print('ğŸ“¤ Archivo subido a S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc890b4",
   "metadata": {},
   "source": [
    "## 2. Glue: Transformaciones con PySpark (Job Script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6efd1",
   "metadata": {},
   "source": [
    "### ğŸ”§ **AWS Glue: ETL Serverless con PySpark**\n",
    "\n",
    "**Componentes de Glue:**\n",
    "\n",
    "1. **Glue Data Catalog:**\n",
    "   - Metastore centralizado (bases de datos + tablas)\n",
    "   - Compatible con Hive Metastore (usado por Athena, EMR, Redshift Spectrum)\n",
    "   - Schema discovery con Crawlers\n",
    "\n",
    "2. **Glue Crawlers:**\n",
    "   - Escanean S3 y detectan formato/schema automÃ¡ticamente\n",
    "   - Actualizan el Catalog con nuevas particiones\n",
    "   - Schedule: Cron expressions (`cron(0 0 * * ? *)` = diario a medianoche)\n",
    "\n",
    "3. **Glue Jobs (ETL Scripts):**\n",
    "   - Python Shell (para scripts ligeros)\n",
    "   - PySpark (para transformaciones distribuidas)\n",
    "   - DPU (Data Processing Unit): 1 DPU = 4 vCPU + 16GB RAM\n",
    "   - Pricing: $0.44 por DPU-hora\n",
    "\n",
    "**GlueContext vs SparkContext:**\n",
    "```python\n",
    "# SparkContext: EstÃ¡ndar PySpark\n",
    "sc = SparkContext()\n",
    "\n",
    "# GlueContext: ExtensiÃ³n AWS con mÃ©todos adicionales\n",
    "glueContext = GlueContext(sc)\n",
    "glueContext.create_dynamic_frame_from_catalog()  # Lee desde Catalog\n",
    "glueContext.write_dynamic_frame.from_options()    # Escribe con conversiones\n",
    "```\n",
    "\n",
    "**DynamicFrame vs DataFrame:**\n",
    "- **DataFrame (Spark)**: Tipado estricto, requiere schema consistente\n",
    "- **DynamicFrame (Glue)**: Schema flexible, maneja datos semi-estructurados\n",
    "  - `resolveChoice()`: Resuelve conflictos de tipos\n",
    "  - `unbox()`: Desempaqueta structs complejos\n",
    "\n",
    "**PatrÃ³n de Job de Glue:**\n",
    "```python\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'PARAM1'])\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# [ETL logic aquÃ­]\n",
    "\n",
    "job.commit()  # Marca job como exitoso en Catalog\n",
    "```\n",
    "\n",
    "**Optimizaciones:**\n",
    "- **Pushdown Predicates**: Filtrar en lectura reduce datos procesados\n",
    "- **Partition Pruning**: Leer solo particiones necesarias\n",
    "- **Columnar Formats**: Parquet/ORC reducen IO 10x vs CSV\n",
    "\n",
    "**Uso Real:**\n",
    "Script Glue orquestado por Step Functions o EventBridge (CloudWatch Events) para procesamiento automÃ¡tico al detectar nuevos archivos en S3.\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3c7a",
   "metadata": {},
   "source": [
    "Ejemplo de script de Glue (PySpark) para leer CSVs crudos de S3, limpiar/transformar y escribir en formato Parquet particionado por mes. Guarda este script como `glue_job.py` y sÃºbelo a un Job de Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f656567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Script de Glue listo para deployment\n"
     ]
    }
   ],
   "source": [
    "glue_job_script = r'''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','BUCKET','PREFIX_RAW','PREFIX_CURATED'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "raw_path = f\"s3://{args['BUCKET']}/{args['PREFIX_RAW']}*.csv\"\n",
    "curated_path = f\"s3://{args['BUCKET']}/{args['PREFIX_CURATED']}\"\n",
    "\n",
    "# Leer CSV desde S3\n",
    "df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "\n",
    "# Transformaciones: limpiar nulos, filtrar valores positivos, agregar timestamp\n",
    "df_clean = (df\n",
    "    .dropna(subset=['venta_id', 'total'])\n",
    "    .filter(F.col('total') > 0)\n",
    "    .withColumn('procesado_en', F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Escribir como Parquet particionado por mes\n",
    "df_clean.write.partitionBy('fecha').mode('overwrite').parquet(curated_path)\n",
    "\n",
    "job.commit()\n",
    "print('âœ… ETL completado')\n",
    "'''\n",
    "print('ğŸ“ Script de Glue listo para deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3349e01",
   "metadata": {},
   "source": [
    "## 3. AWS Lambda: Procesamiento Serverless Event-Driven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62351c",
   "metadata": {},
   "source": [
    "### âš¡ **AWS Lambda: Compute Serverless para Datos**\n",
    "\n",
    "**Â¿QuÃ© es Lambda?**\n",
    "\n",
    "AWS Lambda es un servicio de computaciÃ³n serverless que ejecuta cÃ³digo en respuesta a eventos, sin necesidad de aprovisionar o administrar servidores.\n",
    "\n",
    "**CaracterÃ­sticas Principales:**\n",
    "\n",
    "- **Event-Driven**: Se activa automÃ¡ticamente con triggers (S3, DynamoDB, EventBridge, API Gateway)\n",
    "- **Auto-Scaling**: Escala automÃ¡ticamente de 0 a miles de invocaciones concurrentes\n",
    "- **Pay-per-Use**: Cobro solo por tiempo de ejecuciÃ³n (milisegundos)\n",
    "- **LÃ­mites**: Max 15 minutos por invocaciÃ³n, hasta 10GB de RAM\n",
    "\n",
    "**Triggers Comunes para Data Engineering:**\n",
    "\n",
    "1. **S3 Events**: Procesar archivos al subirse\n",
    "   ```python\n",
    "   # Lambda se activa cuando se crea objeto en S3\n",
    "   def lambda_handler(event, context):\n",
    "       bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "       key = event['Records'][0]['s3']['object']['key']\n",
    "       # Procesar archivo\n",
    "   ```\n",
    "\n",
    "2. **EventBridge (CloudWatch Events)**: Cron jobs programados\n",
    "   ```python\n",
    "   # Ejecutar diariamente a las 2 AM\n",
    "   # Regla EventBridge: rate(1 day) o cron(0 2 * * ? *)\n",
    "   ```\n",
    "\n",
    "3. **DynamoDB Streams**: Procesar cambios en tiempo real\n",
    "   ```python\n",
    "   # Lambda recibe cambios de DynamoDB\n",
    "   for record in event['Records']:\n",
    "       if record['eventName'] == 'INSERT':\n",
    "           # Procesar nuevo registro\n",
    "   ```\n",
    "\n",
    "4. **SQS/SNS**: Cola de mensajes para procesamiento asÃ­ncrono\n",
    "   ```python\n",
    "   # Lambda consume mensajes de SQS automÃ¡ticamente\n",
    "   for record in event['Records']:\n",
    "       body = json.loads(record['body'])\n",
    "   ```\n",
    "\n",
    "**Arquitectura Lambda para Datos:**\n",
    "\n",
    "```\n",
    "S3 (new file) â†’ Lambda â†’ Validar â†’ S3 (curated)\n",
    "                   â†“\n",
    "                 SQS (errores) â†’ Lambda (retry)\n",
    "                   â†“\n",
    "              CloudWatch Logs (monitoring)\n",
    "```\n",
    "\n",
    "**Capas (Layers):**\n",
    "- LibrerÃ­as compartidas (pandas, requests, etc.)\n",
    "- Reduce tamaÃ±o del deployment package\n",
    "- ReutilizaciÃ³n entre funciones\n",
    "\n",
    "**Best Practices:**\n",
    "- **Idempotencia**: Lambda puede reintentar, asegurar que mÃºltiples ejecuciones no causen problemas\n",
    "- **Environment Variables**: ConfiguraciÃ³n (buckets, endpoints) sin hardcodear\n",
    "- **Timeout**: Configurar segÃºn necesidad (default 3s, max 15min)\n",
    "- **Memory**: Mayor memoria = mÃ¡s CPU (128MB a 10GB)\n",
    "- **Dead Letter Queue (DLQ)**: SQS/SNS para errores no recuperables\n",
    "- **Async invocation**: Para procesamiento no-blocking\n",
    "\n",
    "**Limitaciones:**\n",
    "- âŒ Max 15 minutos (para ETL largo, usar Glue/EMR)\n",
    "- âŒ Max 10GB RAM\n",
    "- âŒ Max 250MB deployment package (sin layers)\n",
    "- âŒ /tmp storage max 512MB (efÃ­mero)\n",
    "- âœ… Ideal para: ValidaciÃ³n, transformaciones ligeras, triggers\n",
    "\n",
    "**Pricing:**\n",
    "- Requests: $0.20 por 1M requests\n",
    "- Duration: $0.0000166667 por GB-segundo\n",
    "- Ejemplo: 1M requests x 1GB x 1s = ~$17/mes\n",
    "- Free tier: 1M requests + 400,000 GB-segundos/mes\n",
    "\n",
    "**Alternativas:**\n",
    "- **Glue Python Shell**: Para scripts Python no-PySpark (no lÃ­mite 15min)\n",
    "- **ECS Fargate**: Para procesos largos sin administrar servidores\n",
    "- **Step Functions**: OrquestaciÃ³n de mÃºltiples Lambdas\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f427134",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo: Lambda para validar CSV al subir a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34b26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Lambda handler para validaciÃ³n de CSV\n",
      "\\nPara deployar:\n",
      "1. Crear funciÃ³n Lambda en AWS Console\n",
      "2. Agregar layer con pandas (o usar Lambda Container)\n",
      "3. Configurar trigger S3 en bucket/raw/\n",
      "4. Ajustar timeout (60s) y memoria (512MB)\n"
     ]
    }
   ],
   "source": [
    "lambda_handler_code = '''\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by S3 PutObject event.\n",
    "    Validates CSV structure and moves to curated zone if valid.\n",
    "    \"\"\"\n",
    "    # Extraer informaciÃ³n del evento\n",
    "    for record in event['Records']:\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        key = record['s3']['object']['key']\n",
    "        \n",
    "        print(f'Processing file: s3://{bucket}/{key}')\n",
    "        \n",
    "        # Skip if not CSV\n",
    "        if not key.endswith('.csv'):\n",
    "            print('Skipping non-CSV file')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Leer CSV desde S3\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            csv_content = response['Body'].read()\n",
    "            df = pd.read_csv(BytesIO(csv_content))\n",
    "            \n",
    "            # Validaciones\n",
    "            required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                raise ValueError(f'Missing columns: {missing_cols}')\n",
    "            \n",
    "            # Validar tipos y valores\n",
    "            if df['total'].isnull().sum() > 0:\n",
    "                raise ValueError(f'Null values found in total column: {df[\"total\"].isnull().sum()}')\n",
    "            \n",
    "            if (df['total'] < 0).sum() > 0:\n",
    "                raise ValueError(f'Negative values in total: {(df[\"total\"] < 0).sum()}')\n",
    "            \n",
    "            # Si vÃ¡lido, mover a zona curated\n",
    "            curated_key = key.replace('/raw/', '/curated/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=curated_key\n",
    "            )\n",
    "            \n",
    "            print(f'âœ… File validated and copied to: s3://{bucket}/{curated_key}')\n",
    "            \n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps({\n",
    "                    'message': 'File processed successfully',\n",
    "                    'rows': len(df),\n",
    "                    'output': f's3://{bucket}/{curated_key}'\n",
    "                })\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'âŒ Error processing file: {str(e)}')\n",
    "            \n",
    "            # Mover a carpeta de errores\n",
    "            error_key = key.replace('/raw/', '/errors/')\n",
    "            s3.copy_object(\n",
    "                Bucket=bucket,\n",
    "                CopySource={'Bucket': bucket, 'Key': key},\n",
    "                Key=error_key\n",
    "            )\n",
    "            \n",
    "            # Enviar a DLQ o SNS para alertas\n",
    "            return {\n",
    "                'statusCode': 500,\n",
    "                'body': json.dumps({\n",
    "                    'error': str(e),\n",
    "                    'file': f's3://{bucket}/{key}'\n",
    "                })\n",
    "            }\n",
    "'''\n",
    "\n",
    "print('ğŸ“ Lambda handler para validaciÃ³n de CSV')\n",
    "print('\\\\nPara deployar:')\n",
    "print('1. Crear funciÃ³n Lambda en AWS Console')\n",
    "print('2. Agregar layer con pandas (o usar Lambda Container)')\n",
    "print('3. Configurar trigger S3 en bucket/raw/')\n",
    "print('4. Ajustar timeout (60s) y memoria (512MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f34d9",
   "metadata": {},
   "source": [
    "## 4. Athena: SQL sobre S3 sin Infraestructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82e6f3",
   "metadata": {},
   "source": [
    "### ğŸ“Š **Amazon Athena: Query Engine Serverless**\n",
    "\n",
    "Athena permite ejecutar queries SQL estÃ¡ndar directamente sobre datos en S3 sin necesidad de cargarlos en una base de datos.\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- **Basado en Presto/Trino**: Engine SQL distribuido\n",
    "- **Sin servidores**: No hay clusters que administrar\n",
    "- **Standard SQL**: Compatible con ANSI SQL\n",
    "- **Formatos soportados**: CSV, JSON, Parquet, ORC, Avro\n",
    "\n",
    "**Pricing:** $5 por TB de datos escaneados\n",
    "\n",
    "**Optimizaciones clave:**\n",
    "1. **Particionar datos**: Reduce escaneo con `WHERE year=2025 AND month=10`\n",
    "2. **Formato columnar**: Parquet/ORC reducen escaneo 10x vs CSV\n",
    "3. **Comprimir**: SNAPPY, GZIP, ZSTD reducen tamaÃ±o\n",
    "4. **ProyecciÃ³n**: `SELECT col1, col2` en vez de `SELECT *`\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```sql\n",
    "-- Crear tabla externa apuntando a S3\n",
    "CREATE EXTERNAL TABLE ventas (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    producto_id INT,\n",
    "    cantidad INT,\n",
    "    total DECIMAL(10,2),\n",
    "    fecha DATE\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://mi-data-lake/curated/ventas/';\n",
    "\n",
    "-- Agregar particiones\n",
    "MSCK REPAIR TABLE ventas;\n",
    "\n",
    "-- Query con partition pruning (solo escanea Oct 2025)\n",
    "SELECT cliente_id, SUM(total) as total_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**IntegraciÃ³n con Python (boto3):**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "query = \\\"\\\"\\\"\n",
    "SELECT cliente_id, COUNT(*) as num_ventas\n",
    "FROM ventas\n",
    "WHERE year = 2025 AND month = 10\n",
    "GROUP BY cliente_id\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-bucket/athena-results/'}\n",
    ")\n",
    "\n",
    "query_id = response['QueryExecutionId']\n",
    "\n",
    "# Esperar a que termine\n",
    "while True:\n",
    "    status = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "    state = status['QueryExecution']['Status']['State']\n",
    "    \n",
    "    if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "if state == 'SUCCEEDED':\n",
    "    results = athena.get_query_results(QueryExecutionId=query_id)\n",
    "    # Procesar resultados\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745195ec",
   "metadata": {},
   "source": [
    "## 5. Arquitectura Completa: S3 + Lambda + Glue + Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533cfa8",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ **Pipeline End-to-End en AWS**\n",
    "\n",
    "**Flujo completo de datos:**\n",
    "\n",
    "```\n",
    "1. INGESTA\n",
    "   Fuentes â†’ S3 (bucket/raw/)\n",
    "   \n",
    "2. VALIDACIÃ“N (Lambda)\n",
    "   S3 Event â†’ Lambda â†’ Validar schema\n",
    "                â†“\n",
    "           S3 (curated/) o S3 (errors/)\n",
    "   \n",
    "3. TRANSFORMACIÃ“N (Glue)\n",
    "   EventBridge (cron) â†’ Glue Job (PySpark)\n",
    "                          â†“\n",
    "                    S3 (gold/) Parquet particionado\n",
    "   \n",
    "4. CATALOGACIÃ“N (Glue Crawler)\n",
    "   Glue Crawler â†’ Detecta schema â†’ Glue Catalog\n",
    "   \n",
    "5. ANÃLISIS (Athena)\n",
    "   Athena â†’ SQL queries â†’ S3 (resultados)\n",
    "            â†“\n",
    "       QuickSight (BI) / Python (anÃ¡lisis)\n",
    "```\n",
    "\n",
    "**Ejemplo de implementaciÃ³n:**\n",
    "\n",
    "**Paso 1: Configurar S3 Bucket**\n",
    "```bash\n",
    "aws s3 mb s3://mi-data-lake-2025\n",
    "aws s3api put-bucket-versioning \\\\\n",
    "    --bucket mi-data-lake-2025 \\\\\n",
    "    --versioning-configuration Status=Enabled\n",
    "```\n",
    "\n",
    "**Paso 2: Deploy Lambda**\n",
    "```bash\n",
    "# Empaquetar cÃ³digo con dependencias\n",
    "zip lambda_function.zip lambda_function.py\n",
    "aws lambda create-function \\\\\n",
    "    --function-name validate-csv \\\\\n",
    "    --runtime python3.10 \\\\\n",
    "    --role arn:aws:iam::123456:role/lambda-s3-role \\\\\n",
    "    --handler lambda_function.lambda_handler \\\\\n",
    "    --zip-file fileb://lambda_function.zip \\\\\n",
    "    --timeout 60 \\\\\n",
    "    --memory-size 512\n",
    "```\n",
    "\n",
    "**Paso 3: Configurar S3 Event Notification**\n",
    "```json\n",
    "{\n",
    "  \"LambdaFunctionConfigurations\": [\n",
    "    {\n",
    "      \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456:function:validate-csv\",\n",
    "      \"Events\": [\"s3:ObjectCreated:*\"],\n",
    "      \"Filter\": {\n",
    "        \"Key\": {\n",
    "          \"FilterRules\": [\n",
    "            {\"Name\": \"prefix\", \"Value\": \"raw/\"},\n",
    "            {\"Name\": \"suffix\", \"Value\": \".csv\"}\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Paso 4: Crear Glue Job**\n",
    "```bash\n",
    "aws glue create-job \\\\\n",
    "    --name transform-ventas \\\\\n",
    "    --role AWSGlueServiceRole \\\\\n",
    "    --command Name=glueetl,ScriptLocation=s3://scripts/glue_job.py \\\\\n",
    "    --default-arguments '{\n",
    "        \"--BUCKET\":\"mi-data-lake-2025\",\n",
    "        \"--PREFIX_RAW\":\"curated/ventas/\",\n",
    "        \"--PREFIX_CURATED\":\"gold/ventas/\"\n",
    "    }' \\\\\n",
    "    --glue-version 3.0 \\\\\n",
    "    --number-of-workers 2 \\\\\n",
    "    --worker-type G.1X\n",
    "```\n",
    "\n",
    "**Paso 5: Programar Glue Job con EventBridge**\n",
    "```bash\n",
    "aws events put-rule \\\\\n",
    "    --name daily-etl \\\\\n",
    "    --schedule-expression \"cron(0 2 * * ? *)\"\n",
    "\n",
    "aws events put-targets \\\\\n",
    "    --rule daily-etl \\\\\n",
    "    --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:glue:us-east-1:123456:job/transform-ventas\"\n",
    "```\n",
    "\n",
    "**Paso 6: Query con Athena**\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "athena = boto3.client('athena')\n",
    "\n",
    "response = athena.start_query_execution(\n",
    "    QueryString='SELECT * FROM ventas WHERE year=2025 LIMIT 10',\n",
    "    QueryExecutionContext={'Database': 'mi_database'},\n",
    "    ResultConfiguration={'OutputLocation': 's3://mi-data-lake-2025/athena-results/'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- **CloudWatch Logs**: Lambda + Glue logs\n",
    "- **CloudWatch Metrics**: Invocations, errors, duration\n",
    "- **X-Ray**: Tracing distribuido\n",
    "- **CloudTrail**: AuditorÃ­a de acciones\n",
    "\n",
    "**Costos estimados (ejemplo):**\n",
    "- S3 storage (100GB): ~$2.30/mes\n",
    "- Lambda (10M invocations, 512MB, 1s avg): ~$17/mes\n",
    "- Glue Job (diario, 2 DPU, 30 min): ~$13/mes\n",
    "- Athena (10TB escaneados/mes): ~$50/mes\n",
    "- **Total**: ~$82/mes\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660939bd",
   "metadata": {},
   "source": [
    "## 6. ConclusiÃ³n y Mejores PrÃ¡cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b557d0c",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Key Takeaways de AWS para Data Engineering**\n",
    "\n",
    "**Servicios Clave:**\n",
    "- **S3**: Foundation del data lake (durabilidad 11 nines)\n",
    "- **Lambda**: ValidaciÃ³n y transformaciones ligeras event-driven\n",
    "- **Glue**: ETL serverless con PySpark para transformaciones pesadas\n",
    "- **Athena**: Queries SQL ad-hoc sobre S3 sin infraestructura\n",
    "- **Redshift**: Data Warehouse para analytics de alta performance\n",
    "- **EMR**: Clusters Spark/Hadoop para procesamiento masivo\n",
    "\n",
    "**CuÃ¡ndo usar cada servicio:**\n",
    "\n",
    "| Caso de Uso | Servicio | RazÃ³n |\n",
    "|-------------|----------|-------|\n",
    "| TransformaciÃ³n <5 min, <10GB RAM | **Lambda** | Serverless, rÃ¡pido, econÃ³mico |\n",
    "| ETL batch, PySpark distribuido | **Glue** | Auto-scaling, sin servidores |\n",
    "| Queries exploratorias sobre S3 | **Athena** | Sin setup, paga por escaneo |\n",
    "| DW con TBs, queries complejas | **Redshift** | MPP, optimizado para analytics |\n",
    "| Custom Spark jobs, ML training | **EMR** | Control total, spot instances |\n",
    "| Real-time streaming | **Kinesis** | Managed Kafka-like service |\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **OrganizaciÃ³n de S3**:\n",
    "   - Usar particionamiento Hive-style (`year=2025/month=10/day=30/`)\n",
    "   - Formatos columnares (Parquet/ORC) para reducir costos Athena\n",
    "   - Lifecycle policies para migrar a Glacier\n",
    "\n",
    "2. **Lambda**:\n",
    "   - Idempotencia (mÃºltiples invocaciones no daÃ±an)\n",
    "   - DLQ para manejar errores\n",
    "   - Environment variables para configuraciÃ³n\n",
    "   - Layers para compartir dependencias\n",
    "\n",
    "3. **Glue**:\n",
    "   - DynamicFrame para datos semi-estructurados\n",
    "   - Glue Catalog como metastore centralizado\n",
    "   - Partition pruning para optimizar lectura\n",
    "   - Bookmark para procesamiento incremental\n",
    "\n",
    "4. **Athena**:\n",
    "   - Particionar datos agresivamente\n",
    "   - CTAS (Create Table As Select) para materializar resultados\n",
    "   - Workgroups para cost control por equipo\n",
    "\n",
    "5. **Seguridad**:\n",
    "   - IAM Roles (nunca hardcodear credenciales)\n",
    "   - S3 bucket policies (restringir por prefix)\n",
    "   - VPC endpoints para trÃ¡fico privado\n",
    "   - Encryption at-rest (S3-SSE) y in-transit (HTTPS)\n",
    "\n",
    "6. **Costos**:\n",
    "   - Spot instances para EMR (70-90% descuento)\n",
    "   - Reserved capacity para Glue/Redshift predictibles\n",
    "   - Comprimir datos (reduce storage + transfer)\n",
    "   - Monitoreo con AWS Cost Explorer\n",
    "\n",
    "**PrÃ³ximos Pasos:**\n",
    "1. âœ… Completar notebooks GCP y Azure para comparaciÃ³n\n",
    "2. âœ… Practicar con [AWS Free Tier](https://aws.amazon.com/free/)\n",
    "3. âœ… CertificaciÃ³n: [AWS Certified Data Analytics - Specialty](https://aws.amazon.com/certification/certified-data-analytics-specialty/)\n",
    "4. âœ… Explorar Step Functions para orquestaciÃ³n compleja\n",
    "\n",
    "**Happy data engineering en AWS! ğŸš€**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "Â© 2024-2025 - Data Engineering Modular Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§­ NavegaciÃ³n\n",
    "\n",
    "**â† Anterior:** [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "\n",
    "**Siguiente â†’:** [â˜ï¸ GCP para IngenierÃ­a de Datos: BigQuery, Cloud Storage, Dataflow y Composer â†’](03b_cloud_gcp.ipynb)\n",
    "\n",
    "**ğŸ“š Ãndice de Nivel Mid:**\n",
    "- [âš¡ Mid - 01. OrquestaciÃ³n de Pipelines con Apache Airflow](01_apache_airflow_fundamentos.ipynb)\n",
    "- [Streaming con Apache Kafka: Fundamentos](02_streaming_kafka.ipynb)\n",
    "- [â˜ï¸ AWS para IngenierÃ­a de Datos: S3, Glue, Athena y Lambda](03_cloud_aws.ipynb) â† ğŸ”µ EstÃ¡s aquÃ­\n",
    "- [â˜ï¸ GCP para IngenierÃ­a de Datos: BigQuery, Cloud Storage, Dataflow y Composer](03b_cloud_gcp.ipynb)\n",
    "- [â˜ï¸ Azure para IngenierÃ­a de Datos: ADLS, Synapse, Data Factory y Databricks](03c_cloud_azure.ipynb)\n",
    "- [ğŸ—„ï¸ Bases de Datos Relacionales y NoSQL: PostgreSQL y MongoDB](04_bases_datos_postgresql_mongodb.ipynb)\n",
    "- [â™»ï¸ DataOps y CI/CD para Pipelines de Datos](05_dataops_cicd.ipynb)\n",
    "- [ğŸŒ Conectores Avanzados: REST, GraphQL y SFTP](06_conectores_avanzados_rest_graphql_sftp.ipynb)\n",
    "- [ğŸ§© OptimizaciÃ³n SQL y Particionado de Datos](07_optimizacion_sql_particionado.ipynb)\n",
    "- [ğŸš€ Servicios de Datos con FastAPI](08_fastapi_servicios_datos.ipynb)\n",
    "- [ğŸ§ª Proyecto Integrador Mid 1: API â†’ DB â†’ Parquet con OrquestaciÃ³n](09_proyecto_integrador_1.ipynb)\n",
    "- [ğŸ”„ Proyecto Integrador Mid 2: Kafka â†’ Streaming â†’ Data Lake y Monitoreo](10_proyecto_integrador_2.ipynb)\n",
    "\n",
    "**ğŸ“ Otros Niveles:**\n",
    "- [Nivel Junior](../nivel_junior/README.md)\n",
    "- [Nivel Mid](../nivel_mid/README.md)\n",
    "- [Nivel Senior](../nivel_senior/README.md)\n",
    "- [Nivel GenAI](../nivel_genai/README.md)\n",
    "- [Negocio LATAM](../negocios_latam/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
