{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd878c8",
   "metadata": {},
   "source": [
    "# 🧪 Proyecto Integrador Mid 1: API → DB → Parquet con Orquestación\n",
    "\n",
    "Objetivo: construir un pipeline reproducible que extrae datos desde una API, valida y transforma, carga a una base de datos y exporta a Parquet, orquestado con Airflow.\n",
    "\n",
    "- Duración: 120–150 min\n",
    "- Dificultad: Media\n",
    "- Prerrequisitos: Airflow básico, SQL, validación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c884b",
   "metadata": {},
   "source": [
    "### 🎯 **Proyecto Integrador: Arquitectura End-to-End**\n",
    "\n",
    "**Objetivo del Proyecto:**  \n",
    "Implementar un pipeline de datos productivo que demuestre las mejores prácticas de Mid-level Data Engineering, integrando conceptos de los notebooks 01-08.\n",
    "\n",
    "**Arquitectura del Pipeline:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    ORCHESTRATION LAYER                       │\n",
    "│                    Apache Airflow DAG                        │\n",
    "│                  (Schedule: @daily, Retries: 2)              │\n",
    "└──────────────────────┬──────────────────────────────────────┘\n",
    "                       │\n",
    "       ┌───────────────┼───────────────┐\n",
    "       ▼               ▼               ▼\n",
    "   ┌────────┐    ┌─────────┐    ┌──────────┐\n",
    "   │EXTRACT │───>│VALIDATE │───>│TRANSFORM │\n",
    "   └────────┘    └─────────┘    └──────────┘\n",
    "       │              │               │\n",
    "       │              │               │\n",
    "   REST API      Pandera/GE     Normalization\n",
    "   (Retry +      Schema         + Enrichment\n",
    "   Backoff)      Validation\n",
    "       │              │               │\n",
    "       └──────────────┴───────────────┘\n",
    "                      │\n",
    "       ┌──────────────┼──────────────┐\n",
    "       ▼              ▼               ▼\n",
    "   ┌──────┐     ┌─────────┐    ┌──────────┐\n",
    "   │ LOAD │     │ EXPORT  │    │ MONITOR  │\n",
    "   └──────┘     └─────────┘    └──────────┘\n",
    "       │             │               │\n",
    "   PostgreSQL/   Parquet        Logging +\n",
    "   SQLite        Partitioned    Alerting\n",
    "   (UPSERT)      by Date\n",
    "```\n",
    "\n",
    "**Tecnologías Integradas:**\n",
    "\n",
    "1. **Airflow** (Notebook 01): Orquestación, scheduling, retry logic\n",
    "2. **API Consumption** (Notebook 06): HTTP retry, exponential backoff\n",
    "3. **Data Validation** (Notebook 05): Pandera schemas, quality gates\n",
    "4. **Database Load** (Notebook 04): SQLAlchemy UPSERT, idempotencia\n",
    "5. **Parquet Export** (Notebook 07): Particionamiento, columnar storage\n",
    "6. **DataOps** (Notebook 05): Testing, logging, monitoring\n",
    "\n",
    "**Principios Aplicados:**\n",
    "\n",
    "✅ **Idempotencia**: Re-ejecutar el DAG no duplica datos (UPSERT)  \n",
    "✅ **Resilience**: Exponential backoff + retries automáticos  \n",
    "✅ **Observability**: Structured logging + métricas  \n",
    "✅ **Quality**: Schema validation + data quality checks  \n",
    "✅ **Reproducibility**: Versionado de schemas + data lineage  \n",
    "✅ **Efficiency**: Particionamiento + formato columnar\n",
    "\n",
    "**Casos de Uso Reales:**\n",
    "\n",
    "- **FinTech**: Ingestión diaria de transacciones desde APIs bancarias\n",
    "- **E-commerce**: Sincronización de inventario desde ERPs\n",
    "- **Marketing**: Consolidación de métricas desde plataformas (Google Ads, Facebook)\n",
    "- **Healthcare**: Carga de registros médicos desde sistemas legacy\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9b1eb",
   "metadata": {},
   "source": [
    "## 1. Diseño del flujo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ef5d5",
   "metadata": {},
   "source": [
    "- Extract: API pública (REST) con paginación y reintentos.\n",
    "- Validate: esquema con Pandera/GE y controles de calidad (nulos, rangos).\n",
    "- Transform: normalización de tipos, enriquecimiento simple.\n",
    "- Load: upsert a DB (PostgreSQL/SQLite de demo).\n",
    "- Export: particionado por fecha a Parquet.\n",
    "- Orquestación: DAG diario; retries + alertas por email/logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a558a",
   "metadata": {},
   "source": [
    "### 📐 **Pipeline Design: Decisiones de Arquitectura**\n",
    "\n",
    "**1. EXTRACT - API Consumption Strategy:**\n",
    "\n",
    "**¿Por qué exponential backoff?**\n",
    "```python\n",
    "for i in range(max_retries):\n",
    "    if status == 429:  # Rate limit\n",
    "        wait = 2**i  # 1s, 2s, 4s, 8s, 16s\n",
    "        time.sleep(wait)\n",
    "```\n",
    "\n",
    "- API puede tener rate limits (ej: 100 req/min)\n",
    "- Reintentar inmediatamente → Baneado temporalmente\n",
    "- Backoff da tiempo al rate limit para resetearse\n",
    "\n",
    "**Alternativas consideradas:**\n",
    "- ❌ `requests` simple: Sin retry automático\n",
    "- ❌ `while True` sin límite: Infinite loop risk\n",
    "- ✅ `max_retries=5` con backoff: Balance entre persistencia y timeout\n",
    "\n",
    "**2. VALIDATE - Schema Enforcement:**\n",
    "\n",
    "**Pandera vs Great Expectations:**\n",
    "```python\n",
    "# Pandera: Code-first, lightweight\n",
    "schema = DataFrameSchema({\n",
    "    'API': Column(str, Check.str_length(min=1)),\n",
    "    'HTTPS': Column(bool)\n",
    "})\n",
    "df = schema.validate(df)  # Raises error si falla\n",
    "\n",
    "# Great Expectations: Config-first, enterprise\n",
    "expectation_suite = context.create_expectation_suite(\"api_suite\")\n",
    "df.expect_column_values_to_not_be_null(\"API\")\n",
    "results = df.validate()  # Devuelve report\n",
    "```\n",
    "\n",
    "**Decisión: Pandera para este proyecto**\n",
    "- ✅ Menos setup (no requiere Expectation Store)\n",
    "- ✅ Type hints integration\n",
    "- ❌ GE: Mejor para equipos grandes con governance\n",
    "\n",
    "**3. TRANSFORM - Data Normalization:**\n",
    "\n",
    "**Operaciones aplicadas:**\n",
    "```python\n",
    "df['ingestion_ts'] = pd.Timestamp.utcnow()  # Metadata de procesamiento\n",
    "df = df[columnas_relevantes].copy()          # Proyección\n",
    "df['API'] = df['API'].str.strip().str.title()  # Normalización\n",
    "```\n",
    "\n",
    "**¿Por qué agregar `ingestion_ts`?**\n",
    "- Debugging: Saber cuándo se procesó cada registro\n",
    "- Data freshness: Métricas de lag (event_time vs ingestion_time)\n",
    "- Auditoría: Compliance requirements\n",
    "\n",
    "**4. LOAD - Database Strategy:**\n",
    "\n",
    "**UPSERT vs INSERT:**\n",
    "```python\n",
    "# ❌ INSERT: Duplica datos si DAG se re-ejecuta\n",
    "df.to_sql('table', engine, if_exists='append')\n",
    "\n",
    "# ✅ UPSERT: Idempotente\n",
    "INSERT INTO table VALUES (...) \n",
    "ON CONFLICT (id) DO UPDATE SET ...\n",
    "```\n",
    "\n",
    "**SQLite limitation:** No tiene UPSERT nativo  \n",
    "**Workaround:** DELETE + INSERT en transacción\n",
    "\n",
    "```python\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(\"DELETE FROM apis WHERE API IN (:apis)\", apis=df['API'].tolist())\n",
    "    df.to_sql('apis', conn, if_exists='append', index=False)\n",
    "```\n",
    "\n",
    "**5. EXPORT - Parquet Partitioning:**\n",
    "\n",
    "**Estrategia de particionado:**\n",
    "```\n",
    "datasets/processed/pi1/\n",
    "├── year=2025/\n",
    "│   ├── month=10/\n",
    "│   │   ├── day=30/\n",
    "│   │   │   └── data.parquet\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- Athena/Spark: Partition pruning automático\n",
    "- Lifecycle: Eliminar particiones antiguas fácilmente\n",
    "- Compresión: Parquet ~10x más eficiente que CSV\n",
    "\n",
    "**Trade-off:** Small files problem  \n",
    "- Si muy pocas filas/día → Considerar agregación semanal\n",
    "- Target: 128-512 MB por archivo\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfcaa93",
   "metadata": {},
   "source": [
    "## 2. Componentes del pipeline (código reusable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee1800",
   "metadata": {},
   "source": [
    "### 🔧 **Componentes Reusables: Modularización**\n",
    "\n",
    "**Principio SOLID: Single Responsibility**\n",
    "\n",
    "Cada función tiene **una única responsabilidad**:\n",
    "\n",
    "```python\n",
    "def fetch_data(url):      # EXTRACT\n",
    "    \"\"\"Solo responsable de HTTP requests\"\"\"\n",
    "    pass\n",
    "\n",
    "def validate(df):         # VALIDATE\n",
    "    \"\"\"Solo valida schema\"\"\"\n",
    "    pass\n",
    "\n",
    "def transform(df):        # TRANSFORM\n",
    "    \"\"\"Solo transforma datos\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_db(df):          # LOAD\n",
    "    \"\"\"Solo escribe a DB\"\"\"\n",
    "    pass\n",
    "\n",
    "def export_parquet(df):   # EXPORT\n",
    "    \"\"\"Solo escribe Parquet\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Ventajas de modularización:**\n",
    "\n",
    "1. **Testability:**\n",
    "   ```python\n",
    "   def test_validate():\n",
    "       df = pd.DataFrame({'API': ['Test'], 'HTTPS': [True]})\n",
    "       result = validate(df)\n",
    "       assert len(result) == 1  # Unit test aislado\n",
    "   ```\n",
    "\n",
    "2. **Reusability:**\n",
    "   ```python\n",
    "   # Pipeline 1: API → DB\n",
    "   df = fetch_data()\n",
    "   df = validate(df)\n",
    "   load_db(df)\n",
    "   \n",
    "   # Pipeline 2: API → S3 (reutiliza fetch + validate)\n",
    "   df = fetch_data()\n",
    "   df = validate(df)\n",
    "   upload_to_s3(df)\n",
    "   ```\n",
    "\n",
    "3. **Debugging:**\n",
    "   - Error en `validate()` → Solo revisar función de validación\n",
    "   - vs Monolith: Error en línea 456 de 1000 líneas\n",
    "\n",
    "**Error Handling Pattern:**\n",
    "\n",
    "```python\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_data(url: str, max_retries: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch data from API with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        url: API endpoint\n",
    "        max_retries: Max retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        List of records\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If max retries exceeded\n",
    "        requests.HTTPError: For non-retriable errors (400, 401, 403)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                wait_time = 2 ** attempt\n",
    "                logger.warning(f\"Rate limited, waiting {wait_time}s (attempt {attempt+1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json().get('entries', [])\n",
    "            logger.info(f\"Fetched {len(data)} records from {url}\")\n",
    "            return data\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(f\"Timeout on attempt {attempt+1}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    raise RuntimeError(f\"Max retries ({max_retries}) exceeded\")\n",
    "```\n",
    "\n",
    "**Configuration Management:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    api_url: str = os.getenv('PI1_API_URL', 'https://api.example.com')\n",
    "    db_uri: str = os.getenv('PI1_DB_URI', 'sqlite:///data.db')\n",
    "    export_dir: str = os.getenv('PI1_EXPORT_DIR', './exports')\n",
    "    max_retries: int = int(os.getenv('PI1_MAX_RETRIES', '5'))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls):\n",
    "        \"\"\"Load config from environment\"\"\"\n",
    "        return cls()\n",
    "\n",
    "# Usage\n",
    "config = PipelineConfig.from_env()\n",
    "data = fetch_data(config.api_url, config.max_retries)\n",
    "```\n",
    "\n",
    "**Metrics & Observability:**\n",
    "\n",
    "```python\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def log_execution_time(step_name: str):\n",
    "    \"\"\"Context manager para medir tiempo de ejecución\"\"\"\n",
    "    start = time.time()\n",
    "    logger.info(f\"Starting {step_name}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        duration = time.time() - start\n",
    "        logger.info(f\"Completed {step_name} in {duration:.2f}s\")\n",
    "\n",
    "def run_pipeline():\n",
    "    metrics = {}\n",
    "    \n",
    "    with log_execution_time(\"EXTRACT\"):\n",
    "        data = fetch_data()\n",
    "        metrics['records_fetched'] = len(data)\n",
    "    \n",
    "    with log_execution_time(\"VALIDATE\"):\n",
    "        df = pd.DataFrame(data)\n",
    "        df = validate(df)\n",
    "        metrics['records_valid'] = len(df)\n",
    "    \n",
    "    with log_execution_time(\"LOAD\"):\n",
    "        rows_loaded = load_db(df)\n",
    "        metrics['records_loaded'] = rows_loaded\n",
    "    \n",
    "    logger.info(f\"Pipeline metrics: {metrics}\")\n",
    "    return metrics\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb811f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "from pandera import DataFrameSchema, Column, Check\n",
    "\n",
    "BASE_URL = os.getenv('PI1_API_URL', 'https://api.publicapis.org/entries')\n",
    "DB_URI = os.getenv('PI1_DB_URI', 'sqlite+pysqlite:///:memory:')\n",
    "EXPORT_DIR = os.getenv('PI1_EXPORT_DIR', 'datasets/processed/pi1/')\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_data(url=BASE_URL, max_retries=5):\n",
    "    for i in range(max_retries):\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(2**i)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.json().get('entries', [])\n",
    "    raise RuntimeError('Too many retries')\n",
    "\n",
    "def validate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    schema = DataFrameSchema({\n",
    "        'API': Column(str),\n",
    "        'HTTPS': Column(bool),\n",
    "        'Category': Column(str)\n",
    "    }, coerce=True)\n",
    "    return schema.validate(df)\n",
    "\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[['API','Description','Auth','HTTPS','Cors','Link','Category']].copy()\n",
    "    df['ingestion_ts'] = pd.Timestamp.utcnow()\n",
    "    return df\n",
    "\n",
    "def load_db(df: pd.DataFrame, uri=DB_URI):\n",
    "    engine = create_engine(uri, future=True)\n",
    "    df.to_sql('apis_publicas', engine, if_exists='append', index=False)\n",
    "    return len(df)\n",
    "\n",
    "def export_parquet(df: pd.DataFrame, out_dir=EXPORT_DIR):\n",
    "    out_path = os.path.join(out_dir, f\n",
    ")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "def run_pipeline():\n",
    "    items = fetch_data()\n",
    "    df = pd.DataFrame(items)\n",
    "    df = validate(df)\n",
    "    df = transform(df)\n",
    "    n = load_db(df)\n",
    "    fp = export_parquet(df)\n",
    "    return {'rows': n, 'parquet': fp}\n",
    "\n",
    "# Quick test local\n",
    "res = run_pipeline()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad402527",
   "metadata": {},
   "source": [
    "## 3. Orquestación con Airflow (DAG ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4090cb",
   "metadata": {},
   "source": [
    "### 🔀 **Airflow DAG: Orquestación Productiva**\n",
    "\n",
    "**Anatomía del DAG:**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-eng',\n",
    "    'depends_on_past': False,  # No depende de ejecución previa\n",
    "    'start_date': datetime(2025, 10, 1),\n",
    "    'email': ['data-team@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=1),  # Timeout global\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'pi1_api_to_db_parquet',\n",
    "    default_args=default_args,\n",
    "    description='Daily API ingestion pipeline',\n",
    "    schedule_interval='0 2 * * *',  # 2 AM daily (cron)\n",
    "    catchup=False,  # No backfill automático\n",
    "    max_active_runs=1,  # Solo 1 ejecución concurrente\n",
    "    tags=['production', 'api', 'daily']\n",
    ") as dag:\n",
    "    \n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_api_data',\n",
    "        python_callable=fetch_data,\n",
    "        op_kwargs={'url': '{{ var.value.api_url }}'},  # Airflow variable\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    validate_task = PythonOperator(\n",
    "        task_id='validate_schema',\n",
    "        python_callable=validate,\n",
    "        op_kwargs={'df': '{{ ti.xcom_pull(task_ids=\"extract_api_data\") }}'}\n",
    "    )\n",
    "    \n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_to_database',\n",
    "        python_callable=load_db\n",
    "    )\n",
    "    \n",
    "    export_task = PythonOperator(\n",
    "        task_id='export_to_parquet',\n",
    "        python_callable=export_parquet\n",
    "    )\n",
    "    \n",
    "    notify_success = EmailOperator(\n",
    "        task_id='notify_success',\n",
    "        to='data-team@example.com',\n",
    "        subject='Pipeline Success - {{ ds }}',\n",
    "        html_content='Pipeline completed successfully. Records: {{ ti.xcom_pull(...) }}',\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS\n",
    "    )\n",
    "    \n",
    "    notify_failure = EmailOperator(\n",
    "        task_id='notify_failure',\n",
    "        to='data-team@example.com',\n",
    "        subject='Pipeline Failed - {{ ds }}',\n",
    "        html_content='Pipeline failed. Check logs.',\n",
    "        trigger_rule=TriggerRule.ONE_FAILED\n",
    "    )\n",
    "    \n",
    "    # Dependencies\n",
    "    extract_task >> validate_task >> [load_task, export_task] >> notify_success\n",
    "    [extract_task, validate_task, load_task, export_task] >> notify_failure\n",
    "```\n",
    "\n",
    "**Conceptos Clave:**\n",
    "\n",
    "**1. Schedule Interval (Cron):**\n",
    "```\n",
    "'0 2 * * *'  = Daily at 2 AM\n",
    "'*/15 * * * *'  = Every 15 minutes\n",
    "'0 */4 * * *'  = Every 4 hours\n",
    "'@daily' = Alias for '0 0 * * *'\n",
    "```\n",
    "\n",
    "**2. Catchup:**\n",
    "```python\n",
    "catchup=False  # ✅ Recomendado para pipelines diarios\n",
    "# Si DAG pausado 7 días → solo ejecuta hoy al reactivar\n",
    "\n",
    "catchup=True   # ❌ Backfill automático\n",
    "# Si DAG pausado 7 días → ejecuta 7 runs al reactivar\n",
    "```\n",
    "\n",
    "**3. XCom (Cross-Communication):**\n",
    "```python\n",
    "def extract_data(**context):\n",
    "    data = fetch_from_api()\n",
    "    context['ti'].xcom_push(key='raw_data', value=data)\n",
    "    return len(data)\n",
    "\n",
    "def transform_data(**context):\n",
    "    data = context['ti'].xcom_pull(task_ids='extract_data', key='raw_data')\n",
    "    transformed = process(data)\n",
    "    return transformed\n",
    "```\n",
    "\n",
    "**Limitación:** XCom guarda en metadata DB (pickle)  \n",
    "**Max size:** ~48 KB  \n",
    "**Solución:** Para datasets grandes, pasar paths en vez de data\n",
    "\n",
    "**4. Trigger Rules:**\n",
    "```python\n",
    "TriggerRule.ALL_SUCCESS     # Default: Todos los padres exitosos\n",
    "TriggerRule.ALL_FAILED      # Todos fallaron\n",
    "TriggerRule.ONE_SUCCESS     # Al menos 1 exitoso\n",
    "TriggerRule.ONE_FAILED      # Al menos 1 falló\n",
    "TriggerRule.NONE_FAILED     # Ninguno falló (success o skipped)\n",
    "```\n",
    "\n",
    "**5. SLA (Service Level Agreement):**\n",
    "```python\n",
    "sla_miss_callback = lambda context: send_slack_alert(context)\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='critical_task',\n",
    "    python_callable=process,\n",
    "    sla=timedelta(hours=1),  # Alerta si tarda >1h\n",
    "    on_failure_callback=sla_miss_callback\n",
    ")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "✅ **Idempotencia**: DAG puede re-ejecutarse sin duplicar datos  \n",
    "✅ **Atomicidad**: Cada task es unidad atómica (success or fail)  \n",
    "✅ **Monitoreo**: Alertas en Slack/Email ante fallos  \n",
    "✅ **Timeouts**: `execution_timeout` previene hung tasks  \n",
    "✅ **Concurrency**: `max_active_runs=1` previene race conditions\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_code = r'''\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.email import send_email\n",
    "\n",
    "def run_pipeline_wrapper(**context):\n",
    "    from pi1_module import run_pipeline\n",
    "    res = run_pipeline()\n",
    "    return res\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'data-eng',\n",
    "  'retries': 2,\n",
    "  'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG('pi1_api_db_parquet', start_date=datetime(2025,10,1), schedule_interval='@daily', catchup=False, default_args=default_args) as dag:\n",
    "    t1 = PythonOperator(task_id='run_pipeline', python_callable=run_pipeline_wrapper)\n",
    "    t1\n",
    "'''\n",
    "print(dag_code.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b691c44",
   "metadata": {},
   "source": [
    "## 4. Validaciones y monitoreo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e9eb1",
   "metadata": {},
   "source": [
    "### 📊 **Data Quality: Validaciones y Monitoreo**\n",
    "\n",
    "**Data Quality Dimensions (6 C's):**\n",
    "\n",
    "1. **Completeness** (¿Están todos los datos?)\n",
    "2. **Consistency** (¿Valores coherentes?)\n",
    "3. **Conformity** (¿Cumplen formato?)\n",
    "4. **Accuracy** (¿Valores correctos?)\n",
    "5. **Integrity** (¿Relaciones válidas?)\n",
    "6. **Timeliness** (¿Datos actualizados?)\n",
    "\n",
    "**Implementación en el Pipeline:**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class DataQualityMetrics:\n",
    "    total_records: int\n",
    "    null_counts: Dict[str, int]\n",
    "    duplicate_count: int\n",
    "    validation_errors: List[str]\n",
    "    \n",
    "    @property\n",
    "    def completeness_score(self) -> float:\n",
    "        \"\"\"% de campos sin nulls\"\"\"\n",
    "        total_nulls = sum(self.null_counts.values())\n",
    "        total_fields = self.total_records * len(self.null_counts)\n",
    "        return (1 - total_nulls / total_fields) * 100 if total_fields > 0 else 0\n",
    "\n",
    "def validate_with_metrics(df: pd.DataFrame) -> tuple[pd.DataFrame, DataQualityMetrics]:\n",
    "    \"\"\"Valida y retorna métricas de calidad\"\"\"\n",
    "    \n",
    "    # 1. Schema validation (Pandera)\n",
    "    schema = DataFrameSchema({\n",
    "        'API': Column(str, Check.str_length(min_value=1)),\n",
    "        'HTTPS': Column(bool),\n",
    "        'Category': Column(str, Check.isin(['Animals', 'Business', 'Tech', ...]))\n",
    "    })\n",
    "    \n",
    "    errors = []\n",
    "    try:\n",
    "        validated_df = schema.validate(df)\n",
    "    except pa.errors.SchemaErrors as e:\n",
    "        errors.append(str(e))\n",
    "        validated_df = df\n",
    "    \n",
    "    # 2. Completeness check\n",
    "    null_counts = df.isnull().sum().to_dict()\n",
    "    \n",
    "    # 3. Duplicates\n",
    "    duplicates = df.duplicated(subset=['API']).sum()\n",
    "    \n",
    "    # 4. Custom business rules\n",
    "    if len(df) < 10:\n",
    "        errors.append(\"Too few records (expected >10)\")\n",
    "    \n",
    "    https_rate = df['HTTPS'].mean()\n",
    "    if https_rate < 0.5:\n",
    "        errors.append(f\"Low HTTPS rate: {https_rate:.1%} (expected >50%)\")\n",
    "    \n",
    "    metrics = DataQualityMetrics(\n",
    "        total_records=len(df),\n",
    "        null_counts=null_counts,\n",
    "        duplicate_count=duplicates,\n",
    "        validation_errors=errors\n",
    "    )\n",
    "    \n",
    "    return validated_df, metrics\n",
    "```\n",
    "\n",
    "**Alerting Strategy:**\n",
    "\n",
    "```python\n",
    "def check_quality_gates(metrics: DataQualityMetrics) -> bool:\n",
    "    \"\"\"Verifica thresholds de calidad\"\"\"\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    # Gate 1: Completeness\n",
    "    if metrics.completeness_score < 95:\n",
    "        alerts.append(f\"Completeness {metrics.completeness_score:.1f}% < 95%\")\n",
    "    \n",
    "    # Gate 2: Volume\n",
    "    if metrics.total_records < 100:\n",
    "        alerts.append(f\"Low volume: {metrics.total_records} records\")\n",
    "    \n",
    "    # Gate 3: Duplicates\n",
    "    dup_rate = metrics.duplicate_count / metrics.total_records\n",
    "    if dup_rate > 0.01:\n",
    "        alerts.append(f\"High duplicates: {dup_rate:.1%}\")\n",
    "    \n",
    "    # Gate 4: Validation errors\n",
    "    if metrics.validation_errors:\n",
    "        alerts.append(f\"Validation errors: {len(metrics.validation_errors)}\")\n",
    "    \n",
    "    if alerts:\n",
    "        send_slack_alert(\n",
    "            channel=\"#data-quality\",\n",
    "            message=f\"⚠️ Quality gates failed:\\n\" + \"\\n\".join(alerts),\n",
    "            severity=\"warning\"\n",
    "        )\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def run_pipeline_with_quality():\n",
    "    df = fetch_data()\n",
    "    df, metrics = validate_with_metrics(df)\n",
    "    \n",
    "    # Log metrics\n",
    "    logger.info(f\"Quality metrics: {metrics}\")\n",
    "    \n",
    "    # Check gates\n",
    "    if not check_quality_gates(metrics):\n",
    "        raise ValueError(\"Quality gates failed - aborting pipeline\")\n",
    "    \n",
    "    # Continue pipeline\n",
    "    load_db(df)\n",
    "    export_parquet(df)\n",
    "```\n",
    "\n",
    "**Monitoring Dashboard (Metrics to Track):**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "# Counters\n",
    "records_processed = Counter('pipeline_records_total', 'Total records processed')\n",
    "pipeline_failures = Counter('pipeline_failures_total', 'Pipeline failures')\n",
    "\n",
    "# Histograms (latency)\n",
    "extract_duration = Histogram('extract_duration_seconds', 'Extract step duration')\n",
    "transform_duration = Histogram('transform_duration_seconds', 'Transform duration')\n",
    "\n",
    "# Gauges (current state)\n",
    "data_freshness = Gauge('data_freshness_minutes', 'Minutes since last update')\n",
    "quality_score = Gauge('data_quality_score', 'Quality score 0-100')\n",
    "\n",
    "@extract_duration.time()\n",
    "def fetch_data():\n",
    "    data = requests.get(API_URL).json()\n",
    "    records_processed.inc(len(data))\n",
    "    return data\n",
    "\n",
    "# Expose metrics endpoint\n",
    "from prometheus_client import start_http_server\n",
    "start_http_server(8000)  # Metrics at http://localhost:8000/metrics\n",
    "```\n",
    "\n",
    "**Data Lineage (Tracking):**\n",
    "\n",
    "```python\n",
    "def track_lineage(df: pd.DataFrame, step: str):\n",
    "    \"\"\"Registra metadata de linaje\"\"\"\n",
    "    lineage = {\n",
    "        'timestamp': pd.Timestamp.utcnow(),\n",
    "        'step': step,\n",
    "        'records_count': len(df),\n",
    "        'columns': df.columns.tolist(),\n",
    "        'source': 'api.publicapis.org',\n",
    "        'pipeline_run_id': context['dag_run'].run_id\n",
    "    }\n",
    "    \n",
    "    # Guardar en metadata store (ej: PostgreSQL)\n",
    "    store_lineage(lineage)\n",
    "```\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc7cbe",
   "metadata": {},
   "source": [
    "- Añadir validaciones de calidad (conteo mínimo, no duplicados).\n",
    "- Alertar por email/Slack ante fallos o métricas fuera de rango.\n",
    "- Publicar artefactos (Parquet) con nombres idempotentes o versionados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
