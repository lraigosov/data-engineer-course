{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd878c8",
   "metadata": {},
   "source": [
    "# üß™ Proyecto Integrador Mid 1: API ‚Üí DB ‚Üí Parquet con Orquestaci√≥n\n",
    "\n",
    "Objetivo: construir un pipeline reproducible que extrae datos desde una API, valida y transforma, carga a una base de datos y exporta a Parquet, orquestado con Airflow.\n",
    "\n",
    "- Duraci√≥n: 120‚Äì150 min\n",
    "- Dificultad: Media\n",
    "- Prerrequisitos: Airflow b√°sico, SQL, validaci√≥n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9b1eb",
   "metadata": {},
   "source": [
    "## 1. Dise√±o del flujo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ef5d5",
   "metadata": {},
   "source": [
    "- Extract: API p√∫blica (REST) con paginaci√≥n y reintentos.\n",
    "- Validate: esquema con Pandera/GE y controles de calidad (nulos, rangos).\n",
    "- Transform: normalizaci√≥n de tipos, enriquecimiento simple.\n",
    "- Load: upsert a DB (PostgreSQL/SQLite de demo).\n",
    "- Export: particionado por fecha a Parquet.\n",
    "- Orquestaci√≥n: DAG diario; retries + alertas por email/logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfcaa93",
   "metadata": {},
   "source": [
    "## 2. Componentes del pipeline (c√≥digo reusable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb811f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "from pandera import DataFrameSchema, Column, Check\n",
    "\n",
    "BASE_URL = os.getenv('PI1_API_URL', 'https://api.publicapis.org/entries')\n",
    "DB_URI = os.getenv('PI1_DB_URI', 'sqlite+pysqlite:///:memory:')\n",
    "EXPORT_DIR = os.getenv('PI1_EXPORT_DIR', 'datasets/processed/pi1/')\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_data(url=BASE_URL, max_retries=5):\n",
    "    for i in range(max_retries):\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(2**i)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.json().get('entries', [])\n",
    "    raise RuntimeError('Too many retries')\n",
    "\n",
    "def validate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    schema = DataFrameSchema({\n",
    "        'API': Column(str),\n",
    "        'HTTPS': Column(bool),\n",
    "        'Category': Column(str)\n",
    "    }, coerce=True)\n",
    "    return schema.validate(df)\n",
    "\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[['API','Description','Auth','HTTPS','Cors','Link','Category']].copy()\n",
    "    df['ingestion_ts'] = pd.Timestamp.utcnow()\n",
    "    return df\n",
    "\n",
    "def load_db(df: pd.DataFrame, uri=DB_URI):\n",
    "    engine = create_engine(uri, future=True)\n",
    "    df.to_sql('apis_publicas', engine, if_exists='append', index=False)\n",
    "    return len(df)\n",
    "\n",
    "def export_parquet(df: pd.DataFrame, out_dir=EXPORT_DIR):\n",
    "    out_path = os.path.join(out_dir, f\n",
    ")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "def run_pipeline():\n",
    "    items = fetch_data()\n",
    "    df = pd.DataFrame(items)\n",
    "    df = validate(df)\n",
    "    df = transform(df)\n",
    "    n = load_db(df)\n",
    "    fp = export_parquet(df)\n",
    "    return {'rows': n, 'parquet': fp}\n",
    "\n",
    "# Quick test local\n",
    "res = run_pipeline()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad402527",
   "metadata": {},
   "source": [
    "## 3. Orquestaci√≥n con Airflow (DAG ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_code = r'''\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.email import send_email\n",
    "\n",
    "def run_pipeline_wrapper(**context):\n",
    "    from pi1_module import run_pipeline\n",
    "    res = run_pipeline()\n",
    "    return res\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'data-eng',\n",
    "  'retries': 2,\n",
    "  'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG('pi1_api_db_parquet', start_date=datetime(2025,10,1), schedule_interval='@daily', catchup=False, default_args=default_args) as dag:\n",
    "    t1 = PythonOperator(task_id='run_pipeline', python_callable=run_pipeline_wrapper)\n",
    "    t1\n",
    "'''\n",
    "print(dag_code.splitlines()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b691c44",
   "metadata": {},
   "source": [
    "## 4. Validaciones y monitoreo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc7cbe",
   "metadata": {},
   "source": [
    "- A√±adir validaciones de calidad (conteo m√≠nimo, no duplicados).\n",
    "- Alertar por email/Slack ante fallos o m√©tricas fuera de rango.\n",
    "- Publicar artefactos (Parquet) con nombres idempotentes o versionados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
