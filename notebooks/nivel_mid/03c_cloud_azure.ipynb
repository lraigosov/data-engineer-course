{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6907e164",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è Azure para Ingenier√≠a de Datos: ADLS, Synapse, Data Factory y Databricks\n",
    "\n",
    "Este notebook introduce el ecosistema de Microsoft Azure para Data Engineering, cubriendo almacenamiento con ADLS Gen2, transformaciones con Synapse Analytics y Azure Data Factory, y computaci√≥n distribuida con Azure Databricks.\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.)  \n",
    "**Nivel:** Mid  \n",
    "**Duraci√≥n:** 90-120 minutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b69b3a",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è RECORDATORIO IMPORTANTE\n",
    "\n",
    "### üö® NOTEBOOKS vs PRODUCCI√ìN\n",
    "\n",
    "Este curso usa notebooks para **ense√±anza**, pero en tu trabajo real:\n",
    "\n",
    "**‚ùå NO uses notebooks para pipelines en producci√≥n**\n",
    "\n",
    "**‚úÖ USA:**\n",
    "- Scripts Python modulares en `src/`\n",
    "- Azure Data Factory para orquestaci√≥n\n",
    "- Azure DevOps para CI/CD\n",
    "- Azure Functions para event-driven\n",
    "- ARM Templates o Bicep para IaC\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** LuisRai (Luis J. Raigoso V.) | ¬© 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54b7f0",
   "metadata": {},
   "source": [
    "## Requisitos y Notas de Ejecuci√≥n\n",
    "\n",
    "- Para ejecutar c√≥digo real de Azure necesitas:\n",
    "  - Suscripci√≥n Azure (free tier o pay-as-you-go)\n",
    "  - Azure CLI instalado (`az login`)\n",
    "  - Service Principal o Managed Identity configurado\n",
    "  - Recursos creados: Storage Account, Synapse Workspace, etc.\n",
    "- **Nunca subas credenciales al repositorio**\n",
    "- Usa Azure Key Vault para secrets\n",
    "- Este notebook muestra ejemplos ejecutables con Azure configurado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94066593",
   "metadata": {},
   "source": [
    "### ‚òÅÔ∏è **Microsoft Azure: Ecosistema Enterprise para Datos**\n",
    "\n",
    "**Stack Moderno de Azure para Data Engineering:**\n",
    "\n",
    "1. **Azure Data Lake Storage Gen2 (ADLS Gen2)**: Object storage optimizado\n",
    "   - Basado en Blob Storage con namespaces jer√°rquicos\n",
    "   - Compatible con HDFS (Hadoop File System)\n",
    "   - ACLs POSIX (permisos granulares)\n",
    "   - Pricing: ~$0.018/GB/mes (Hot tier)\n",
    "\n",
    "2. **Azure Synapse Analytics**: Data Warehouse + Analytics unificado\n",
    "   - SQL Pools (MPP dedicated, ex-Azure SQL DW)\n",
    "   - Serverless SQL (on-demand queries)\n",
    "   - Spark Pools (Apache Spark managed)\n",
    "   - Pipelines integrados (como Data Factory)\n",
    "   - Pricing: desde $1.20/hora (DW100c) o $5/TB escaneado (serverless)\n",
    "\n",
    "3. **Azure Data Factory (ADF)**: ETL/ELT orchestration\n",
    "   - 90+ conectores nativos\n",
    "   - Data Flows (visual transformations)\n",
    "   - Integration Runtime (on-prem connectivity)\n",
    "   - Pricing: por actividad ejecutada + data movement\n",
    "\n",
    "4. **Azure Databricks**: Apache Spark optimizado\n",
    "   - Runtime optimizado (Photon engine)\n",
    "   - Delta Lake integrado\n",
    "   - Notebooks colaborativos\n",
    "   - MLflow para ML lifecycle\n",
    "   - Pricing: DBU (Databricks Unit) + VM cost\n",
    "\n",
    "5. **Azure Functions**: Serverless compute event-driven\n",
    "   - Triggers: HTTP, Blob Storage, Event Hubs, Timer\n",
    "   - Runtimes: Python 3.7-3.11, .NET, Node.js, Java\n",
    "   - Consumption plan: paga solo por ejecuciones\n",
    "   - Pricing: primeras 1M ejecuciones gratis\n",
    "\n",
    "**Arquitectura de Referencia:**\n",
    "```\n",
    "Fuentes ‚Üí [Event Hubs] ‚Üí ADLS Gen2 (raw) ‚Üí [Data Factory/Databricks] \n",
    "                              ‚Üì\n",
    "                    ADLS Gen2 (curated) ‚Üí Synapse (anal√≠tica)\n",
    "                              ‚Üì\n",
    "                    Power BI (visualizaci√≥n)\n",
    "```\n",
    "\n",
    "**Ventajas de Azure para Datos:**\n",
    "- **Integraci√≥n Microsoft**: Active Directory, Office 365, Power BI\n",
    "- **H√≠brido**: Azure Arc para on-prem + cloud unificado\n",
    "- **Seguridad**: Compliance robusto (ISO, HIPAA, FedRAMP)\n",
    "- **Enterprise-grade**: SLA 99.9%, soporte 24/7\n",
    "\n",
    "**Comparaci√≥n con AWS/GCP:**\n",
    "\n",
    "| Servicio | Azure | AWS | GCP |\n",
    "|----------|-------|-----|-----|\n",
    "| Object Storage | ADLS Gen2 | S3 | Cloud Storage |\n",
    "| Data Warehouse | Synapse (dedicated) | Redshift | BigQuery |\n",
    "| DW Serverless | Synapse Serverless | Athena | BigQuery |\n",
    "| ETL | Data Factory | Glue | Dataflow |\n",
    "| Spark Managed | Databricks | EMR | Dataproc |\n",
    "| Streaming | Event Hubs | Kinesis | Pub/Sub |\n",
    "| Orchestration | Data Factory | Step Functions | Cloud Composer |\n",
    "| Functions | Azure Functions | Lambda | Cloud Functions |\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b431f",
   "metadata": {},
   "source": [
    "## 1. Azure Data Lake Storage Gen2 (ADLS Gen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a4297",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è **ADLS Gen2: Fundamentos**\n",
    "\n",
    "**¬øQu√© es ADLS Gen2?**\n",
    "\n",
    "ADLS Gen2 = Azure Blob Storage + Hierarchical Namespace + HDFS compatibility\n",
    "\n",
    "**Caracter√≠sticas Clave:**\n",
    "\n",
    "1. **Hierarchical Namespace (HNS):**\n",
    "   - Directorios reales (no prefixes como S3)\n",
    "   - Operaciones at√≥micas (rename directory instant√°neo)\n",
    "   - ACLs POSIX por directorio/archivo\n",
    "\n",
    "2. **Tiers de Almacenamiento:**\n",
    "   - **Hot**: Acceso frecuente (~$0.018/GB/mes)\n",
    "   - **Cool**: Acceso infrecuente (<30 d√≠as, $0.01/GB/mes)\n",
    "   - **Archive**: Almacenamiento a largo plazo ($0.002/GB/mes)\n",
    "\n",
    "3. **Security:**\n",
    "   - Azure AD authentication\n",
    "   - RBAC (Role-Based Access Control)\n",
    "   - ACLs (Access Control Lists)\n",
    "   - Encryption at rest (Microsoft-managed o customer-managed keys)\n",
    "\n",
    "**Organizacion Recomendada:**\n",
    "\n",
    "```\n",
    "container: data-lake\n",
    "‚îú‚îÄ‚îÄ raw/                     ‚Üê Datos crudos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sales/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025/10/30/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ customers/\n",
    "‚îú‚îÄ‚îÄ staging/                 ‚Üê Procesamiento\n",
    "‚îî‚îÄ‚îÄ curated/                 ‚Üê Datos limpios\n",
    "    ‚îú‚îÄ‚îÄ sales_aggregated/\n",
    "    ‚îî‚îÄ‚îÄ customer_360/\n",
    "```\n",
    "\n",
    "**Lifecycle Management:**\n",
    "```json\n",
    "{\n",
    "  \"rules\": [\n",
    "    {\n",
    "      \"name\": \"MoveToCool\",\n",
    "      \"enabled\": true,\n",
    "      \"type\": \"Lifecycle\",\n",
    "      \"definition\": {\n",
    "        \"actions\": {\n",
    "          \"baseBlob\": {\n",
    "            \"tierToCool\": {\n",
    "              \"daysAfterModificationGreaterThan\": 30\n",
    "            },\n",
    "            \"tierToArchive\": {\n",
    "              \"daysAfterModificationGreaterThan\": 90\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"filters\": {\n",
    "          \"blobTypes\": [\"blockBlob\"],\n",
    "          \"prefixMatch\": [\"raw/\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Operaciones con Python:**\n",
    "```python\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Autenticaci√≥n con Azure AD\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=\"https://mystorageaccount.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Crear filesystem (container)\n",
    "file_system_client = service_client.create_file_system(file_system=\"data-lake\")\n",
    "\n",
    "# Subir archivo\n",
    "file_client = file_system_client.get_file_client(\"raw/ventas/2025_10.csv\")\n",
    "with open(\"ventas.csv\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "# Listar archivos\n",
    "paths = file_system_client.get_paths(path=\"raw/\")\n",
    "for path in paths:\n",
    "    print(path.name)\n",
    "\n",
    "# Descargar archivo\n",
    "download = file_client.download_file()\n",
    "with open(\"downloaded.csv\", \"wb\") as f:\n",
    "    f.write(download.readall())\n",
    "```\n",
    "\n",
    "**ACLs POSIX:**\n",
    "```python\n",
    "# Establecer permisos\n",
    "from azure.storage.filedatalake import DataLakeDirectoryClient\n",
    "\n",
    "directory_client = file_system_client.get_directory_client(\"raw/sales\")\n",
    "acl = 'user::rwx,group::r-x,other::---'\n",
    "directory_client.set_access_control(acl=acl)\n",
    "\n",
    "# Permisos por usuario espec√≠fico\n",
    "acl = 'user::rwx,user:john@example.com:r--,group::r-x,other::---'\n",
    "directory_client.set_access_control(acl=acl)\n",
    "```\n",
    "\n",
    "**Costos:**\n",
    "- Storage Hot: $0.0184/GB/mes\n",
    "- Storage Cool: $0.01/GB/mes\n",
    "- Operations (read): $0.004 per 10,000\n",
    "- Operations (write): $0.065 per 10,000\n",
    "- Data transfer (egress): primeros 5GB gratis, luego $0.087/GB\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4287ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n (sin ejecutar sin Azure configurado)\n",
    "STORAGE_ACCOUNT = 'mydatalakestorage'\n",
    "CONTAINER_NAME = 'data-lake'\n",
    "TENANT_ID = 'your-tenant-id'\n",
    "SUBSCRIPTION_ID = 'your-subscription-id'\n",
    "\n",
    "print(f'üìã Storage Account: {STORAGE_ACCOUNT}')\n",
    "print(f'üì¶ Container: {CONTAINER_NAME}')\n",
    "print('‚ö†Ô∏è Requiere: az login o service principal configurado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9825429",
   "metadata": {},
   "source": [
    "### 1.1 Crear container y subir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥digo (descomentar con Azure configurado)\n",
    "'''\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd\n",
    "\n",
    "# Autenticaci√≥n\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{STORAGE_ACCOUNT}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Crear filesystem\n",
    "try:\n",
    "    file_system_client = service_client.create_file_system(file_system=CONTAINER_NAME)\n",
    "    print(f'‚úÖ Container {CONTAINER_NAME} creado')\n",
    "except Exception as e:\n",
    "    print(f'‚ÑπÔ∏è Container ya existe: {e}')\n",
    "    file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)\n",
    "\n",
    "# Subir CSV\n",
    "df = pd.read_csv('../../datasets/raw/ventas.csv')\n",
    "csv_string = df.to_csv(index=False)\n",
    "\n",
    "file_client = file_system_client.get_file_client(\"raw/ventas/ventas_2025_10.csv\")\n",
    "file_client.upload_data(csv_string, overwrite=True)\n",
    "print('üì§ Archivo subido a ADLS Gen2')\n",
    "'''\n",
    "print('C√≥digo de ejemplo listo para ejecutar con Azure configurado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a4ce6",
   "metadata": {},
   "source": [
    "## 2. Azure Synapse Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef1352",
   "metadata": {},
   "source": [
    "### üìä **Synapse Analytics: Analytics Platform Unificado**\n",
    "\n",
    "**¬øQu√© es Synapse?**\n",
    "\n",
    "Synapse unifica:\n",
    "- Data Warehouse (SQL Pools)\n",
    "- Big Data (Spark Pools)\n",
    "- Data Integration (Pipelines)\n",
    "- Visualization (Power BI integrado)\n",
    "\n",
    "**Componentes:**\n",
    "\n",
    "1. **SQL Pools (Dedicated):**\n",
    "   - MPP (Massively Parallel Processing)\n",
    "   - 60 distributions\n",
    "   - Columnstore indexes\n",
    "   - Pause/resume para ahorrar costos\n",
    "   - Pricing: DWU (Data Warehouse Units), desde $1.20/hora (DW100c)\n",
    "\n",
    "2. **Serverless SQL Pool:**\n",
    "   - On-demand queries sobre ADLS Gen2\n",
    "   - Sin infraestructura que administrar\n",
    "   - Standard SQL (T-SQL)\n",
    "   - Pricing: $5/TB escaneado\n",
    "\n",
    "3. **Spark Pools:**\n",
    "   - Apache Spark 3.x\n",
    "   - Auto-scaling (min-max nodes)\n",
    "   - Notebooks integrados (Python, Scala, SQL)\n",
    "   - Delta Lake support\n",
    "   - Pricing: por vCore-hora\n",
    "\n",
    "**Serverless SQL: Query ADLS directly**\n",
    "\n",
    "```sql\n",
    "-- External table sobre CSV en ADLS\n",
    "CREATE EXTERNAL DATA SOURCE adls_datasource\n",
    "WITH (\n",
    "    LOCATION = 'https://mystorageaccount.dfs.core.windows.net/data-lake'\n",
    ");\n",
    "\n",
    "CREATE EXTERNAL FILE FORMAT csv_format\n",
    "WITH (\n",
    "    FORMAT_TYPE = DELIMITEDTEXT,\n",
    "    FORMAT_OPTIONS (\n",
    "        FIELD_TERMINATOR = ',',\n",
    "        STRING_DELIMITER = '\"',\n",
    "        FIRST_ROW = 2\n",
    "    )\n",
    ");\n",
    "\n",
    "CREATE EXTERNAL TABLE ventas_external (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    producto_id INT,\n",
    "    cantidad INT,\n",
    "    total FLOAT,\n",
    "    fecha DATE\n",
    ")\n",
    "WITH (\n",
    "    LOCATION = '/raw/ventas/',\n",
    "    DATA_SOURCE = adls_datasource,\n",
    "    FILE_FORMAT = csv_format\n",
    ");\n",
    "\n",
    "-- Query (paga solo por datos escaneados)\n",
    "SELECT \n",
    "    cliente_id,\n",
    "    SUM(total) as total_ventas\n",
    "FROM ventas_external\n",
    "WHERE fecha >= '2025-10-01'\n",
    "GROUP BY cliente_id;\n",
    "```\n",
    "\n",
    "**Dedicated SQL Pool: Optimizaciones**\n",
    "\n",
    "```sql\n",
    "-- Tabla con distribuci√≥n HASH\n",
    "CREATE TABLE ventas (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    total DECIMAL(10,2),\n",
    "    fecha DATE\n",
    ")\n",
    "WITH (\n",
    "    DISTRIBUTION = HASH(cliente_id),\n",
    "    CLUSTERED COLUMNSTORE INDEX\n",
    ");\n",
    "\n",
    "-- CTAS (Create Table As Select) para cargar datos\n",
    "CREATE TABLE ventas_2025\n",
    "WITH (\n",
    "    DISTRIBUTION = HASH(cliente_id),\n",
    "    CLUSTERED COLUMNSTORE INDEX\n",
    ")\n",
    "AS\n",
    "SELECT *\n",
    "FROM ventas_external\n",
    "WHERE YEAR(fecha) = 2025;\n",
    "\n",
    "-- Estad√≠sticas (cr√≠tico para performance)\n",
    "CREATE STATISTICS stats_cliente ON ventas(cliente_id);\n",
    "CREATE STATISTICS stats_fecha ON ventas(fecha);\n",
    "```\n",
    "\n",
    "**Particionamiento:**\n",
    "```sql\n",
    "CREATE TABLE ventas_partitioned (\n",
    "    venta_id INT,\n",
    "    cliente_id INT,\n",
    "    total DECIMAL(10,2),\n",
    "    fecha DATE\n",
    ")\n",
    "WITH (\n",
    "    DISTRIBUTION = HASH(cliente_id),\n",
    "    PARTITION (fecha RANGE RIGHT FOR VALUES \n",
    "        ('2025-01-01', '2025-02-01', '2025-03-01')\n",
    "    )\n",
    ");\n",
    "```\n",
    "\n",
    "**PolyBase (bulk load desde ADLS):**\n",
    "```sql\n",
    "COPY INTO ventas\n",
    "FROM 'https://mystorageaccount.dfs.core.windows.net/data-lake/raw/ventas/*.csv'\n",
    "WITH (\n",
    "    FILE_TYPE = 'CSV',\n",
    "    FIELDTERMINATOR = ',',\n",
    "    ROWTERMINATOR = '\\\\n',\n",
    "    FIRSTROW = 2\n",
    ");\n",
    "```\n",
    "\n",
    "**Spark Pool con PySpark:**\n",
    "```python\n",
    "# Leer desde ADLS Gen2\n",
    "df = spark.read.csv(\n",
    "    \"abfss://data-lake@mystorageaccount.dfs.core.windows.net/raw/ventas/*.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Transformaciones\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "df_agg = df.groupBy(\"cliente_id\") \\\\\n",
    "    .agg(_sum(\"total\").alias(\"total_ventas\"))\n",
    "\n",
    "# Escribir como Delta Lake\n",
    "df_agg.write.format(\"delta\") \\\\\n",
    "    .mode(\"overwrite\") \\\\\n",
    "    .save(\"abfss://data-lake@mystorageaccount.dfs.core.windows.net/curated/sales_summary\")\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Pause dedicated pools cuando no se usan\n",
    "- Usar serverless para exploratory queries\n",
    "- Particionar tablas grandes\n",
    "- Comprimir datos (Parquet/Delta)\n",
    "- Result set caching (autom√°tico, 48h)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73201733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n Synapse\n",
    "SYNAPSE_WORKSPACE = 'mysynapseworkspace'\n",
    "DEDICATED_POOL = 'sqldw'\n",
    "SERVERLESS_ENDPOINT = f'{SYNAPSE_WORKSPACE}-ondemand.sql.azuresynapse.net'\n",
    "\n",
    "print(f'üìä Synapse Workspace: {SYNAPSE_WORKSPACE}')\n",
    "print(f'üíæ Dedicated Pool: {DEDICATED_POOL}')\n",
    "print(f'‚ö° Serverless: {SERVERLESS_ENDPOINT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4573f",
   "metadata": {},
   "source": [
    "### 2.1 Query serverless SQL desde Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74640f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de conexi√≥n a Synapse Serverless\n",
    "'''\n",
    "import pyodbc\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Obtener token Azure AD\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"https://database.windows.net/.default\")\n",
    "\n",
    "# Conexi√≥n con AAD authentication\n",
    "conn_string = f\"\"\"\n",
    "    Driver={{ODBC Driver 18 for SQL Server}};\n",
    "    Server={SERVERLESS_ENDPOINT};\n",
    "    Database=master;\n",
    "    Encrypt=yes;\n",
    "    TrustServerCertificate=no;\n",
    "    Connection Timeout=30;\n",
    "\"\"\"\n",
    "\n",
    "conn = pyodbc.connect(conn_string, attrs_before={\n",
    "    1256: token.token  # SQL_COPT_SS_ACCESS_TOKEN\n",
    "})\n",
    "\n",
    "# Ejecutar query\n",
    "cursor = conn.cursor()\n",
    "query = \"\"\"\n",
    "    SELECT TOP 10\n",
    "        cliente_id,\n",
    "        SUM(total) as total_ventas\n",
    "    FROM OPENROWSET(\n",
    "        BULK 'https://mystorageaccount.dfs.core.windows.net/data-lake/raw/ventas/*.csv',\n",
    "        FORMAT = 'CSV',\n",
    "        PARSER_VERSION = '2.0',\n",
    "        HEADER_ROW = TRUE\n",
    "    ) AS ventas\n",
    "    GROUP BY cliente_id\n",
    "    ORDER BY total_ventas DESC\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "for row in cursor:\n",
    "    print(f'Cliente {row[0]}: ${row[1]:.2f}')\n",
    "\n",
    "conn.close()\n",
    "'''\n",
    "print('C√≥digo Synapse Serverless con autenticaci√≥n Azure AD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54c90a",
   "metadata": {},
   "source": [
    "## 3. Azure Data Factory (ADF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5291cf6",
   "metadata": {},
   "source": [
    "### üîÑ **Azure Data Factory: ETL/ELT Orchestration**\n",
    "\n",
    "**Conceptos Core:**\n",
    "\n",
    "1. **Linked Services**: Conexiones a fuentes (ADLS, SQL, REST APIs)\n",
    "2. **Datasets**: Representaci√≥n de datos (CSV, Parquet, tablas)\n",
    "3. **Pipelines**: Flujo de actividades\n",
    "4. **Activities**: Tareas individuales (Copy, Data Flow, Notebook, etc.)\n",
    "5. **Triggers**: Ejecuci√≥n (Schedule, Tumbling Window, Event-based)\n",
    "\n",
    "**Actividades Comunes:**\n",
    "\n",
    "- **Copy Activity**: Mover datos entre 90+ conectores\n",
    "- **Data Flow**: Transformaciones visuales (similar a SSIS)\n",
    "- **Databricks Notebook**: Ejecutar notebooks Databricks\n",
    "- **Stored Procedure**: Ejecutar SQL en bases de datos\n",
    "- **Web Activity**: Llamadas HTTP/REST\n",
    "- **Azure Function**: Invocar functions\n",
    "\n",
    "**Pipeline JSON Example:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"CopyCSVtoADLS\",\n",
    "  \"properties\": {\n",
    "    \"activities\": [\n",
    "      {\n",
    "        \"name\": \"CopyVentas\",\n",
    "        \"type\": \"Copy\",\n",
    "        \"inputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"SourceCSV\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\n",
    "            \"referenceName\": \"SinkADLS\",\n",
    "            \"type\": \"DatasetReference\"\n",
    "          }\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"source\": {\n",
    "            \"type\": \"DelimitedTextSource\"\n",
    "          },\n",
    "          \"sink\": {\n",
    "            \"type\": \"DelimitedTextSink\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "      \"fechaEjecucion\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Data Flow (GUI Transform):**\n",
    "\n",
    "```\n",
    "Source (ADLS CSV) \n",
    "  ‚Üí Filter (total > 0) \n",
    "  ‚Üí Aggregate (SUM by cliente_id) \n",
    "  ‚Üí Sink (Synapse table)\n",
    "```\n",
    "\n",
    "**Integration Runtime:**\n",
    "\n",
    "- **Azure IR**: Cloud-to-cloud (managed por Azure)\n",
    "- **Self-hosted IR**: On-prem-to-cloud (instalas agent)\n",
    "- **Azure-SSIS IR**: SSIS packages en cloud\n",
    "\n",
    "**Parametrizaci√≥n:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"activities\": [\n",
    "    {\n",
    "      \"name\": \"DynamicCopy\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"parameters\": {\n",
    "            \"folderPath\": {\n",
    "              \"value\": \"@concat('raw/', formatDateTime(utcNow(), 'yyyy/MM/dd'))\",\n",
    "              \"type\": \"Expression\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Triggers:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"DailyTrigger\",\n",
    "  \"properties\": {\n",
    "    \"type\": \"ScheduleTrigger\",\n",
    "    \"typeProperties\": {\n",
    "      \"recurrence\": {\n",
    "        \"frequency\": \"Day\",\n",
    "        \"interval\": 1,\n",
    "        \"startTime\": \"2025-01-01T02:00:00Z\",\n",
    "        \"timeZone\": \"UTC\"\n",
    "      }\n",
    "    },\n",
    "    \"pipelines\": [\n",
    "      {\n",
    "        \"pipelineReference\": {\n",
    "          \"referenceName\": \"VentasETL\",\n",
    "          \"type\": \"PipelineReference\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Event-based Trigger (Blob created):**\n",
    "```json\n",
    "{\n",
    "  \"name\": \"BlobEventTrigger\",\n",
    "  \"properties\": {\n",
    "    \"type\": \"BlobEventsTrigger\",\n",
    "    \"typeProperties\": {\n",
    "      \"blobPathBeginsWith\": \"/data-lake/raw/ventas/\",\n",
    "      \"blobPathEndsWith\": \".csv\",\n",
    "      \"ignoreEmptyBlobs\": true,\n",
    "      \"events\": [\"Microsoft.Storage.BlobCreated\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Python SDK (crear pipeline programmatically):**\n",
    "\n",
    "```python\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory.models import *\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "adf_client = DataFactoryManagementClient(credential, SUBSCRIPTION_ID)\n",
    "\n",
    "# Crear Linked Service (ADLS Gen2)\n",
    "ls_adls = AzureBlobFSLinkedService(\n",
    "    url=f\"https://{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    ")\n",
    "\n",
    "adf_client.linked_services.create_or_update(\n",
    "    resource_group_name='my-rg',\n",
    "    factory_name='my-adf',\n",
    "    linked_service_name='ADLS_LS',\n",
    "    linked_service=ls_adls\n",
    ")\n",
    "\n",
    "# Crear dataset\n",
    "ds_adls = Dataset(\n",
    "    linked_service_name=LinkedServiceReference(reference_name='ADLS_LS'),\n",
    "    type='DelimitedText',\n",
    "    type_properties={\n",
    "        'location': {\n",
    "            'type': 'AzureBlobFSLocation',\n",
    "            'folderPath': 'raw/ventas'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "adf_client.datasets.create_or_update(\n",
    "    resource_group_name='my-rg',\n",
    "    factory_name='my-adf',\n",
    "    dataset_name='VentasCSV',\n",
    "    dataset=ds_adls\n",
    ")\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Usar par√°metros para reutilizaci√≥n\n",
    "- Logging con Azure Monitor\n",
    "- Manejo de errores con retry policies\n",
    "- Variables globales en Key Vault\n",
    "- CI/CD con Azure DevOps/GitHub Actions\n",
    "\n",
    "**Pricing:**\n",
    "- Orchestration: $1 per 1,000 activity runs\n",
    "- Data Movement: $0.25 per DIU-hour\n",
    "- Data Flow: $0.27 per vCore-hour\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e97447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de pipeline ADF definido en JSON\n",
    "adf_pipeline_example = '''\n",
    "{\n",
    "  \"name\": \"VentasDailyETL\",\n",
    "  \"properties\": {\n",
    "    \"activities\": [\n",
    "      {\n",
    "        \"name\": \"CopyFromSource\",\n",
    "        \"type\": \"Copy\",\n",
    "        \"inputs\": [\n",
    "          {\"referenceName\": \"SourceSQL\", \"type\": \"DatasetReference\"}\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\"referenceName\": \"SinkADLS\", \"type\": \"DatasetReference\"}\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"source\": {\"type\": \"SqlSource\"},\n",
    "          \"sink\": {\"type\": \"ParquetSink\"}\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"RunDatabricksNotebook\",\n",
    "        \"type\": \"DatabricksNotebook\",\n",
    "        \"dependsOn\": [\n",
    "          {\"activity\": \"CopyFromSource\", \"dependencyConditions\": [\"Succeeded\"]}\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"notebookPath\": \"/notebooks/transform_ventas\",\n",
    "          \"baseParameters\": {\n",
    "            \"date\": {\n",
    "              \"value\": \"@formatDateTime(utcNow(), 'yyyy-MM-dd')\",\n",
    "              \"type\": \"Expression\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"LoadToSynapse\",\n",
    "        \"type\": \"Copy\",\n",
    "        \"dependsOn\": [\n",
    "          {\"activity\": \"RunDatabricksNotebook\", \"dependencyConditions\": [\"Succeeded\"]}\n",
    "        ],\n",
    "        \"inputs\": [\n",
    "          {\"referenceName\": \"ProcessedADLS\", \"type\": \"DatasetReference\"}\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "          {\"referenceName\": \"SynapseTable\", \"type\": \"DatasetReference\"}\n",
    "        ],\n",
    "        \"typeProperties\": {\n",
    "          \"source\": {\"type\": \"ParquetSource\"},\n",
    "          \"sink\": {\n",
    "            \"type\": \"SqlDWSink\",\n",
    "            \"allowPolyBase\": true\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "      \"fechaInicio\": {\"type\": \"string\"},\n",
    "      \"fechaFin\": {\"type\": \"string\"}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(adf_pipeline_example)\n",
    "print('\\\\nüí° Deploy con: az datafactory pipeline create')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6c9ee",
   "metadata": {},
   "source": [
    "## 4. Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90505af",
   "metadata": {},
   "source": [
    "### ‚ö° **Azure Databricks: Spark Optimizado**\n",
    "\n",
    "**¬øQu√© es Databricks?**\n",
    "\n",
    "Databricks es una plataforma de an√°lisis unificada basada en Apache Spark, optimizada para Azure:\n",
    "\n",
    "- **Photon Engine**: Vectorized query engine (hasta 2x m√°s r√°pido)\n",
    "- **Delta Lake**: ACID transactions sobre data lakes\n",
    "- **Unity Catalog**: Governance centralizado\n",
    "- **MLflow**: ML lifecycle management\n",
    "- **Auto Loader**: Ingesta incremental eficiente\n",
    "\n",
    "**Componentes:**\n",
    "\n",
    "1. **Workspace**: Entorno colaborativo con notebooks\n",
    "2. **Cluster**: Conjunto de VMs con Spark runtime\n",
    "3. **Jobs**: Workloads programados\n",
    "4. **Delta Tables**: Tablas con ACID guarantees\n",
    "\n",
    "**Cluster Configuration:**\n",
    "\n",
    "```python\n",
    "# Cluster specs (via UI o API)\n",
    "{\n",
    "    \"cluster_name\": \"etl-cluster\",\n",
    "    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "    \"node_type_id\": \"Standard_DS3_v2\",\n",
    "    \"num_workers\": 2,\n",
    "    \"autoscale\": {\n",
    "        \"min_workers\": 2,\n",
    "        \"max_workers\": 8\n",
    "    },\n",
    "    \"spark_conf\": {\n",
    "        \"spark.databricks.delta.preview.enabled\": \"true\"\n",
    "    },\n",
    "    \"azure_attributes\": {\n",
    "        \"availability\": \"ON_DEMAND_AZURE\",\n",
    "        \"spot_bid_max_price\": -1\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Delta Lake:**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Escribir Delta table\n",
    "df = spark.read.csv(\n",
    "    \"abfss://data-lake@storage.dfs.core.windows.net/raw/ventas/*.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\") \\\\\n",
    "    .mode(\"overwrite\") \\\\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\\\n",
    "    .save(\"/mnt/delta/ventas\")\n",
    "\n",
    "# Leer Delta table\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/ventas\")\n",
    "\n",
    "# UPSERT (Merge)\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/ventas\")\n",
    "\n",
    "updates = spark.read.csv(\"new_data.csv\", header=True)\n",
    "\n",
    "deltaTable.alias(\"target\") \\\\\n",
    "    .merge(\n",
    "        updates.alias(\"source\"),\n",
    "        \"target.venta_id = source.venta_id\"\n",
    "    ) \\\\\n",
    "    .whenMatchedUpdate(set={\"total\": col(\"source.total\")}) \\\\\n",
    "    .whenNotMatchedInsertAll() \\\\\n",
    "    .execute()\n",
    "\n",
    "# Time Travel\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/mnt/delta/ventas\")\n",
    "df_yesterday = spark.read.format(\"delta\") \\\\\n",
    "    .option(\"timestampAsOf\", \"2025-10-29\") \\\\\n",
    "    .load(\"/mnt/delta/ventas\")\n",
    "\n",
    "# Optimizaci√≥n\n",
    "deltaTable.optimize().executeCompaction()\n",
    "deltaTable.vacuum(168)  # Cleanup files older than 7 days\n",
    "```\n",
    "\n",
    "**Auto Loader (Structured Streaming):**\n",
    "\n",
    "```python\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/ventas\")\n",
    "    .load(\"abfss://data-lake@storage.dfs.core.windows.net/raw/ventas/\")\n",
    ")\n",
    "\n",
    "# Transformaciones\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "df_processed = df_stream \\\\\n",
    "    .filter(col(\"total\") > 0) \\\\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "# Escribir stream a Delta\n",
    "query = (df_processed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/ventas\")\n",
    "    .start(\"/mnt/delta/ventas_stream\")\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Mount ADLS Gen2:**\n",
    "\n",
    "```python\n",
    "# Configurar Service Principal\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"<client-id>\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"<client-secret>\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/<tenant-id>/oauth2/token\"\n",
    "}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://data-lake@{STORAGE_ACCOUNT}.dfs.core.windows.net/\",\n",
    "    mount_point=\"/mnt/data-lake\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "# Usar mount\n",
    "df = spark.read.csv(\"/mnt/data-lake/raw/ventas/*.csv\")\n",
    "```\n",
    "\n",
    "**Notebook Widgets (parametrizaci√≥n):**\n",
    "\n",
    "```python\n",
    "# Crear widget\n",
    "dbutils.widgets.text(\"fecha\", \"2025-10-30\", \"Fecha de ejecuci√≥n\")\n",
    "\n",
    "# Leer valor\n",
    "fecha = dbutils.widgets.get(\"fecha\")\n",
    "print(f\"Procesando datos de {fecha}\")\n",
    "\n",
    "# Usar en paths din√°micos\n",
    "path = f\"/mnt/data-lake/raw/ventas/{fecha}/*.csv\"\n",
    "df = spark.read.csv(path)\n",
    "```\n",
    "\n",
    "**MLflow Tracking:**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mlflow.set_experiment(\"/experiments/churn-prediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "**Pricing:**\n",
    "- DBU (Databricks Unit): unidad de procesamiento\n",
    "  - Jobs Compute: $0.15/DBU\n",
    "  - All-Purpose Compute: $0.40/DBU\n",
    "- VM cost (Azure): seg√∫n tipo (Standard_DS3_v2 ~$0.15/hora)\n",
    "- Total = DBU cost + VM cost\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Usar Jobs Compute (m√°s barato) para workloads programados\n",
    "- Auto-termination de clusters inactivos\n",
    "- Spot VMs (hasta 90% descuento)\n",
    "- Photon acceleration (menos compute time)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa50183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de notebook Databricks\n",
    "databricks_notebook_example = '''\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Pipeline con Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Configuraci√≥n\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"fecha\", \"2025-10-30\")\n",
    "fecha = dbutils.widgets.get(\"fecha\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Ingesta desde ADLS\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "df_raw = spark.read.csv(\n",
    "    f\"/mnt/data-lake/raw/ventas/{fecha}/*.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"Registros le√≠dos: {df_raw.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Transformaciones\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df_clean = (df_raw\n",
    "    .filter(col(\"total\") > 0)\n",
    "    .filter(col(\"cliente_id\").isNotNull())\n",
    "    .withColumn(\"fecha_ingestion\", current_timestamp())\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Escribir a Delta Lake\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df_clean.write.format(\"delta\") \\\\\n",
    "    .mode(\"append\") \\\\\n",
    "    .partitionBy(\"fecha\") \\\\\n",
    "    .save(\"/mnt/delta/ventas\")\n",
    "\n",
    "print(\"‚úÖ Datos escritos a Delta Lake\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Validaci√≥n\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/ventas\")\n",
    "print(f\"Total registros en Delta: {df_delta.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %sql\n",
    "# MAGIC -- Query Delta table con SQL\n",
    "# MAGIC SELECT \n",
    "#     cliente_id,\n",
    "#     SUM(total) as total_ventas,\n",
    "#     COUNT(*) as num_ventas\n",
    "# MAGIC FROM delta.`/mnt/delta/ventas`\n",
    "# MAGIC WHERE fecha = current_date()\n",
    "# MAGIC GROUP BY cliente_id\n",
    "# MAGIC ORDER BY total_ventas DESC\n",
    "# MAGIC LIMIT 10\n",
    "'''\n",
    "\n",
    "print(databricks_notebook_example)\n",
    "print('\\\\nüí° Este notebook puede ser ejecutado desde ADF o Jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508e153",
   "metadata": {},
   "source": [
    "## 5. Azure Container Instances y Web Apps: Compute Flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a817ab",
   "metadata": {},
   "source": [
    "### üê≥ **Azure Container Instances (ACI) y Web Apps: Alternativas Serverless/Managed**\n",
    "\n",
    "**¬øPor qu√© Container Instances o Web Apps?**\n",
    "\n",
    "Azure Functions tiene l√≠mites (10 min timeout en consumption plan, 60 min en premium). Para cargas m√°s pesadas o personalizadas, Azure ofrece:\n",
    "\n",
    "1. **Azure Container Instances (ACI)**: Contenedores serverless sin K8s\n",
    "2. **Azure Web Apps**: PaaS para aplicaciones web (equivalente a App Engine GCP)\n",
    "\n",
    "**Comparaci√≥n:**\n",
    "\n",
    "| Aspecto | Azure Functions | Container Instances | Web Apps |\n",
    "|---------|----------------|---------------------|----------|\n",
    "| **Tiempo l√≠mite** | 10 min (consumption) | Sin l√≠mite | Sin l√≠mite |\n",
    "| **Lenguajes** | Python, .NET, Node, Java | Cualquiera (Docker) | Python, .NET, Node, PHP |\n",
    "| **Escalado** | Autom√°tico | Manual/KEDA | Auto-scale rules |\n",
    "| **Cold start** | ~1-2s | ~5-10s | Sin cold start |\n",
    "| **Pricing** | Pay-per-execution | Pay-per-second (CPU+RAM) | Pay-per-hour |\n",
    "| **Use case** | Event-driven r√°pido | Batch jobs, ETL largo | APIs, servicios 24/7 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Azure Container Instances (ACI)**\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Ejecuta contenedores Docker sin administrar infraestructura\n",
    "- Ideal para batch jobs, procesamiento largo, scripts personalizados\n",
    "- Facturaci√≥n por segundo (CPU + memoria)\n",
    "- Integraci√≥n con Virtual Networks\n",
    "\n",
    "**Arquitectura para Data:**\n",
    "\n",
    "```\n",
    "Scheduler ‚Üí ACI (ETL Python) ‚Üí ADLS Gen2 / Synapse\n",
    "              ‚Üì\n",
    "       CloudWatch Logs (monitoring)\n",
    "```\n",
    "\n",
    "**Ejemplo: ETL Python en ACI**\n",
    "\n",
    "**Dockerfile:**\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Instalar dependencias\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copiar c√≥digo\n",
    "COPY etl_script.py .\n",
    "\n",
    "# Comando de ejecuci√≥n\n",
    "CMD [\"python\", \"etl_script.py\"]\n",
    "```\n",
    "\n",
    "**etl_script.py:**\n",
    "```python\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    logger.info('Starting ETL process')\n",
    "    \n",
    "    # Autenticaci√≥n con Managed Identity\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=\"https://mystorageaccount.dfs.core.windows.net\",\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    # Leer datos\n",
    "    file_system_client = service_client.get_file_system_client(\"data-lake\")\n",
    "    file_client = file_system_client.get_file_client(\"raw/ventas/ventas.csv\")\n",
    "    \n",
    "    download = file_client.download_file()\n",
    "    df = pd.read_csv(io.BytesIO(download.readall()))\n",
    "    \n",
    "    logger.info(f'Loaded {len(df)} rows')\n",
    "    \n",
    "    # Transformaciones (proceso largo permitido)\n",
    "    df_clean = df.dropna()\n",
    "    df_clean['total'] = df_clean['total'].astype(float)\n",
    "    df_agg = df_clean.groupby('cliente_id')['total'].sum().reset_index()\n",
    "    \n",
    "    # Escribir resultados\n",
    "    output_client = file_system_client.get_file_client(\"curated/ventas_summary.csv\")\n",
    "    csv_content = df_agg.to_csv(index=False)\n",
    "    output_client.upload_data(csv_content, overwrite=True)\n",
    "    \n",
    "    logger.info(f'‚úÖ ETL completed: {len(df_agg)} rows')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Deploy a ACI:**\n",
    "```bash\n",
    "# Build y push a Azure Container Registry (ACR)\n",
    "az acr build --registry myregistry \\\\\n",
    "    --image sales-etl:v1 \\\\\n",
    "    --file Dockerfile .\n",
    "\n",
    "# Crear container instance\n",
    "az container create \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --name sales-etl-instance \\\\\n",
    "    --image myregistry.azurecr.io/sales-etl:v1 \\\\\n",
    "    --cpu 2 \\\\\n",
    "    --memory 4 \\\\\n",
    "    --restart-policy Never \\\\\n",
    "    --assign-identity [system] \\\\\n",
    "    --environment-variables STORAGE_ACCOUNT=mystorageaccount\n",
    "\n",
    "# Ver logs\n",
    "az container logs --resource-group my-rg --name sales-etl-instance\n",
    "```\n",
    "\n",
    "**Trigger con Logic Apps:**\n",
    "```json\n",
    "{\n",
    "  \"type\": \"Recurrence\",\n",
    "  \"recurrence\": {\n",
    "    \"frequency\": \"Day\",\n",
    "    \"interval\": 1,\n",
    "    \"startTime\": \"2025-01-01T02:00:00Z\"\n",
    "  },\n",
    "  \"actions\": {\n",
    "    \"Create_Container_Instance\": {\n",
    "      \"type\": \"ApiConnection\",\n",
    "      \"inputs\": {\n",
    "        \"host\": {\n",
    "          \"connection\": {\n",
    "            \"name\": \"@parameters('$connections')['aci']['connectionId']\"\n",
    "          }\n",
    "        },\n",
    "        \"method\": \"post\",\n",
    "        \"path\": \"/subscriptions/@{encodeURIComponent('sub-id')}/resourceGroups/@{encodeURIComponent('my-rg')}/providers/Microsoft.ContainerInstance/containerGroups/@{encodeURIComponent('sales-etl-instance')}\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Azure Web Apps (App Service)**\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- PaaS completamente administrado\n",
    "- Sin cold start (siempre caliente)\n",
    "- Auto-scaling basado en m√©tricas\n",
    "- Deployment slots (staging, production)\n",
    "\n",
    "**Ejemplo: API de Datos con Flask**\n",
    "\n",
    "**app.py:**\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=\"https://mystorageaccount.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "@app.route('/api/sales', methods=['GET'])\n",
    "def get_sales():\n",
    "    \"\"\"Endpoint para obtener ventas filtradas\"\"\"\n",
    "    customer_id = request.args.get('customer_id')\n",
    "    \n",
    "    # Leer desde ADLS\n",
    "    file_system_client = service_client.get_file_system_client(\"data-lake\")\n",
    "    file_client = file_system_client.get_file_client(\"curated/ventas.parquet\")\n",
    "    \n",
    "    download = file_client.download_file()\n",
    "    df = pd.read_parquet(io.BytesIO(download.readall()))\n",
    "    \n",
    "    if customer_id:\n",
    "        df = df[df['cliente_id'] == int(customer_id)]\n",
    "    \n",
    "    return jsonify(df.to_dict(orient='records'))\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return 'OK', 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "**Deploy a App Service:**\n",
    "```bash\n",
    "# Crear App Service Plan\n",
    "az appservice plan create \\\\\n",
    "    --name my-plan \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --sku B1 \\\\\n",
    "    --is-linux\n",
    "\n",
    "# Crear Web App\n",
    "az webapp create \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --plan my-plan \\\\\n",
    "    --name sales-api \\\\\n",
    "    --runtime \"PYTHON:3.10\" \\\\\n",
    "    --deployment-container-image-name myregistry.azurecr.io/sales-api:v1\n",
    "\n",
    "# Configurar auto-scaling\n",
    "az monitor autoscale create \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --resource sales-api \\\\\n",
    "    --resource-type Microsoft.Web/sites \\\\\n",
    "    --name autoscale-rule \\\\\n",
    "    --min-count 1 \\\\\n",
    "    --max-count 10 \\\\\n",
    "    --count 2\n",
    "\n",
    "# Agregar regla: escalar si CPU > 70%\n",
    "az monitor autoscale rule create \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --autoscale-name autoscale-rule \\\\\n",
    "    --condition \"Percentage CPU > 70 avg 5m\" \\\\\n",
    "    --scale out 1\n",
    "```\n",
    "\n",
    "**Pricing:**\n",
    "- **ACI**: $0.0000012 por vCPU-segundo + $0.0000001 per GB-segundo\n",
    "  - Ejemplo: 2 vCPU, 4GB, 30 min/d√≠a = ~$3/mes\n",
    "- **Web Apps**: Desde $13/mes (B1 Basic plan)\n",
    "  - Sin cold start, siempre activo\n",
    "\n",
    "**Cu√°ndo usar cada uno:**\n",
    "- **Functions**: Event-driven r√°pido (<10 min)\n",
    "- **ACI**: Batch jobs, ETL largo, scripts one-time\n",
    "- **Web Apps**: APIs 24/7, servicios con SLA estricto\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff0cb6",
   "metadata": {},
   "source": [
    "## 6. Azure Functions para Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672753e7",
   "metadata": {},
   "source": [
    "### ‚ö° **Azure Functions: Event-Driven Data Processing**\n",
    "\n",
    "**Triggers para Data Engineering:**\n",
    "\n",
    "1. **Blob Storage Trigger**: Procesar archivos al subir\n",
    "2. **Event Hubs Trigger**: Streaming events\n",
    "3. **Timer Trigger**: Cron jobs\n",
    "4. **HTTP Trigger**: REST APIs\n",
    "\n",
    "**Ejemplo: Procesar CSV al subir a ADLS**\n",
    "\n",
    "```python\n",
    "# function_app.py\n",
    "import azure.functions as func\n",
    "import logging\n",
    "import pandas as pd\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "app = func.FunctionApp()\n",
    "\n",
    "@app.blob_trigger(\n",
    "    arg_name=\"myblob\", \n",
    "    path=\"data-lake/raw/{name}\",\n",
    "    connection=\"AzureWebJobsStorage\"\n",
    ")\n",
    "def process_csv(myblob: func.InputStream):\n",
    "    logging.info(f\"Processing blob: {myblob.name} ({myblob.length} bytes)\")\n",
    "    \n",
    "    # Skip if not CSV\n",
    "    if not myblob.name.endswith('.csv'):\n",
    "        logging.info(\"Skipping non-CSV file\")\n",
    "        return\n",
    "    \n",
    "    # Read CSV\n",
    "    content = myblob.read()\n",
    "    df = pd.read_csv(pd.io.common.BytesIO(content))\n",
    "    \n",
    "    # Validations\n",
    "    required_cols = ['venta_id', 'cliente_id', 'total']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logging.error(f\"Missing columns: {required_cols}\")\n",
    "        raise ValueError(\"Invalid schema\")\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = df.dropna(subset=['total'])\n",
    "    df_clean['total'] = df_clean['total'].astype(float)\n",
    "    df_clean = df_clean[df_clean['total'] > 0]\n",
    "    \n",
    "    logging.info(f\"Cleaned {len(df_clean)} rows (from {len(df)} original)\")\n",
    "    \n",
    "    # Write to curated zone\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=\"https://mystorageaccount.dfs.core.windows.net\",\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    file_system_client = service_client.get_file_system_client(\"data-lake\")\n",
    "    \n",
    "    # Output path\n",
    "    output_path = myblob.name.replace('/raw/', '/curated/').replace('.csv', '_clean.csv')\n",
    "    file_client = file_system_client.get_file_client(output_path)\n",
    "    \n",
    "    csv_output = df_clean.to_csv(index=False)\n",
    "    file_client.upload_data(csv_output, overwrite=True)\n",
    "    \n",
    "    logging.info(f\"‚úÖ Written to {output_path}\")\n",
    "```\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "azure-functions\n",
    "azure-storage-file-datalake\n",
    "azure-identity\n",
    "pandas\n",
    "```\n",
    "\n",
    "**Event Hubs Trigger (Streaming):**\n",
    "\n",
    "```python\n",
    "@app.event_hub_message_trigger(\n",
    "    arg_name=\"events\",\n",
    "    event_hub_name=\"sales-events\",\n",
    "    connection=\"EventHubConnection\"\n",
    ")\n",
    "@app.cosmos_db_output(\n",
    "    arg_name=\"outputDocument\",\n",
    "    database_name=\"SalesDB\",\n",
    "    container_name=\"Events\",\n",
    "    connection=\"CosmosDBConnection\"\n",
    ")\n",
    "def process_stream(events: func.EventHubEvent, outputDocument: func.Out[func.Document]):\n",
    "    for event in events:\n",
    "        logging.info(f\"Processing event: {event.get_body().decode('utf-8')}\")\n",
    "        \n",
    "        # Parse event\n",
    "        import json\n",
    "        data = json.loads(event.get_body().decode('utf-8'))\n",
    "        \n",
    "        # Enrich\n",
    "        data['processed_timestamp'] = event.enqueued_time.isoformat()\n",
    "        \n",
    "        # Write to Cosmos DB\n",
    "        outputDocument.set(func.Document.from_dict(data))\n",
    "```\n",
    "\n",
    "**Timer Trigger (Daily Job):**\n",
    "\n",
    "```python\n",
    "@app.timer_trigger(\n",
    "    arg_name=\"timer\",\n",
    "    schedule=\"0 0 2 * * *\",  # Daily at 2 AM\n",
    "    run_on_startup=False\n",
    ")\n",
    "def daily_aggregation(timer: func.TimerRequest):\n",
    "    logging.info(\"Running daily aggregation\")\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Call Data Factory pipeline\n",
    "    from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    \n",
    "    credential = DefaultAzureCredential()\n",
    "    adf_client = DataFactoryManagementClient(credential, SUBSCRIPTION_ID)\n",
    "    \n",
    "    run_response = adf_client.pipelines.create_run(\n",
    "        resource_group_name='my-rg',\n",
    "        factory_name='my-adf',\n",
    "        pipeline_name='VentasDailyETL',\n",
    "        parameters={'fecha': yesterday}\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Pipeline run ID: {run_response.run_id}\")\n",
    "```\n",
    "\n",
    "**Deploy con Azure CLI:**\n",
    "\n",
    "```bash\n",
    "# Crear Function App\n",
    "az functionapp create \\\\\n",
    "    --resource-group my-rg \\\\\n",
    "    --consumption-plan-location eastus \\\\\n",
    "    --runtime python \\\\\n",
    "    --runtime-version 3.10 \\\\\n",
    "    --functions-version 4 \\\\\n",
    "    --name my-data-functions \\\\\n",
    "    --storage-account mystorageaccount\n",
    "\n",
    "# Deploy c√≥digo\n",
    "func azure functionapp publish my-data-functions\n",
    "```\n",
    "\n",
    "**Durable Functions (Orchestration):**\n",
    "\n",
    "```python\n",
    "import azure.functions as func\n",
    "import azure.durable_functions as df\n",
    "\n",
    "# Orchestrator\n",
    "def orchestrator_function(context: df.DurableOrchestrationContext):\n",
    "    # Step 1: Validate\n",
    "    valid = yield context.call_activity('validate_data', context.get_input())\n",
    "    \n",
    "    if not valid:\n",
    "        return \"Validation failed\"\n",
    "    \n",
    "    # Step 2: Transform (parallel)\n",
    "    tasks = [\n",
    "        context.call_activity('transform_sales', None),\n",
    "        context.call_activity('transform_customers', None)\n",
    "    ]\n",
    "    yield context.task_all(tasks)\n",
    "    \n",
    "    # Step 3: Load\n",
    "    result = yield context.call_activity('load_to_warehouse', None)\n",
    "    \n",
    "    return result\n",
    "\n",
    "main = df.Orchestrator.create(orchestrator_function)\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Idempotent functions (rerun-safe)\n",
    "- Application Insights para monitoring\n",
    "- Key Vault para secrets\n",
    "- Consumption plan para sporadic workloads\n",
    "- Premium plan para VNet integration\n",
    "\n",
    "**Pricing:**\n",
    "- Consumption: $0.20 per 1M executions + $0.000016/GB-s\n",
    "- Premium: desde $150/mes (dedicated instances)\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e85771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo completo de Azure Function\n",
    "azure_function_complete = '''\n",
    "# function_app.py\n",
    "import azure.functions as func\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "app = func.FunctionApp()\n",
    "\n",
    "@app.blob_trigger(\n",
    "    arg_name=\"inputBlob\",\n",
    "    path=\"data-lake/raw/ventas/{name}\",\n",
    "    connection=\"AzureWebJobsStorage\"\n",
    ")\n",
    "@app.blob_output(\n",
    "    arg_name=\"outputBlob\",\n",
    "    path=\"data-lake/curated/ventas/{name}\",\n",
    "    connection=\"AzureWebJobsStorage\"\n",
    ")\n",
    "def validate_and_move(inputBlob: func.InputStream, outputBlob: func.Out[bytes]):\n",
    "    \"\"\"\n",
    "    Valida CSV y mueve a curated zone si es v√°lido.\n",
    "    \"\"\"\n",
    "    logging.info(f\"üîç Validating: {inputBlob.name}\")\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "        from io import BytesIO\n",
    "        \n",
    "        # Read CSV\n",
    "        content = inputBlob.read()\n",
    "        df = pd.read_csv(BytesIO(content))\n",
    "        \n",
    "        # Validations\n",
    "        errors = []\n",
    "        \n",
    "        if 'venta_id' not in df.columns:\n",
    "            errors.append(\"Missing venta_id column\")\n",
    "        \n",
    "        if 'total' not in df.columns:\n",
    "            errors.append(\"Missing total column\")\n",
    "        else:\n",
    "            if df['total'].isnull().sum() > 0:\n",
    "                errors.append(f\"{df['total'].isnull().sum()} null values in total\")\n",
    "        \n",
    "        if errors:\n",
    "            logging.error(f\"‚ùå Validation failed: {errors}\")\n",
    "            # Write to DLQ (dead letter queue)\n",
    "            return\n",
    "        \n",
    "        # Clean\n",
    "        df_clean = df[df['total'] > 0]\n",
    "        \n",
    "        # Add metadata\n",
    "        df_clean['validated_at'] = datetime.utcnow().isoformat()\n",
    "        \n",
    "        # Write to output\n",
    "        csv_output = df_clean.to_csv(index=False).encode('utf-8')\n",
    "        outputBlob.set(csv_output)\n",
    "        \n",
    "        logging.info(f\"‚úÖ Validated {len(df_clean)} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"üí• Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# requirements.txt:\n",
    "# azure-functions\n",
    "# pandas\n",
    "'''\n",
    "\n",
    "print(azure_function_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b02a01",
   "metadata": {},
   "source": [
    "## 7. Comparaci√≥n: Azure vs AWS vs GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd76992",
   "metadata": {},
   "source": [
    "### üîÑ **Multi-Cloud: Deep Comparison**\n",
    "\n",
    "| Categor√≠a | Azure | AWS | GCP |\n",
    "|-----------|-------|-----|-----|\n",
    "| **Object Storage** | ADLS Gen2 (HNS + POSIX) | S3 (prefixes) | Cloud Storage (strong consistency) |\n",
    "| **DW Dedicated** | Synapse dedicated ($1.20/h) | Redshift ($0.25/h node) | BigQuery flat-rate ($2K/mo) |\n",
    "| **DW Serverless** | Synapse serverless ($5/TB) | Athena ($5/TB) | BigQuery on-demand ($5/TB) |\n",
    "| **Spark Managed** | Databricks (DBU model) | EMR ($0.27/h + EC2) | Dataproc ($0.01/vCPU-hour + Compute) |\n",
    "| **ETL Visual** | Data Factory | Glue Studio | Dataflow |\n",
    "| **Streaming** | Event Hubs + Stream Analytics | Kinesis + Lambda | Pub/Sub + Dataflow |\n",
    "| **Notebooks** | Synapse + Databricks | SageMaker + EMR | Vertex AI Workbench |\n",
    "| **Governance** | Purview | Lake Formation | Dataplex |\n",
    "| **ML Platform** | Azure ML | SageMaker | Vertex AI |\n",
    "| **BI Native** | Power BI | QuickSight | Looker |\n",
    "\n",
    "**Fortalezas por Cloud:**\n",
    "\n",
    "**Azure:**\n",
    "- ‚úÖ **H√≠brido**: Azure Arc, ExpressRoute (on-prem + cloud seamless)\n",
    "- ‚úÖ **Microsoft Stack**: AD, Office 365, Power BI integration\n",
    "- ‚úÖ **Databricks**: Partnership m√°s profunda que AWS\n",
    "- ‚úÖ **Synapse**: Unified analytics (DW + Spark + Pipelines)\n",
    "- ‚ùå M√°s complejo que AWS/GCP\n",
    "- ‚ùå Documentaci√≥n dispersa\n",
    "\n",
    "**AWS:**\n",
    "- ‚úÖ **Madurez**: Servicios battle-tested desde 2006\n",
    "- ‚úÖ **Amplitud**: 200+ servicios, m√°s connectors\n",
    "- ‚úÖ **Comunidad**: Mayor adoption, m√°s recursos\n",
    "- ‚úÖ **Compliance**: FedRAMP High, HIPAA, PCI DSS\n",
    "- ‚ùå Pricing complejo\n",
    "- ‚ùå Menos cohesi√≥n entre servicios\n",
    "\n",
    "**GCP:**\n",
    "- ‚úÖ **BigQuery**: Mejor DW serverless del mercado\n",
    "- ‚úÖ **Kubernetes**: GKE l√≠der (creadores de K8s)\n",
    "- ‚úÖ **ML/AI**: TensorFlow nativo, TPUs, Vertex AI\n",
    "- ‚úÖ **Pricing**: M√°s simple y transparente\n",
    "- ‚ùå Menos enterprise features\n",
    "- ‚ùå Menor market share (~10%)\n",
    "\n",
    "**Cu√°ndo elegir Azure:**\n",
    "\n",
    "1. **Organizaci√≥n Microsoft-centric:**\n",
    "   - Active Directory para autenticaci√≥n\n",
    "   - Office 365, Teams, SharePoint integrados\n",
    "   - Windows workloads cr√≠ticos\n",
    "\n",
    "2. **H√≠brido on-prem + cloud:**\n",
    "   - Azure Stack para on-prem\n",
    "   - ExpressRoute para conectividad dedicada\n",
    "   - Arc para management unificado\n",
    "\n",
    "3. **Databricks-heavy:**\n",
    "   - Partnership Azure-Databricks m√°s estrecha\n",
    "   - Unity Catalog (governance)\n",
    "   - Delta Lake performance optimizado\n",
    "\n",
    "4. **Compliance estricto:**\n",
    "   - GDPR, HIPAA, ISO certifications\n",
    "   - Gobierno/Finanzas (common en Azure)\n",
    "\n",
    "**Arquitectura Multi-Cloud:**\n",
    "\n",
    "```\n",
    "On-Premise (SQL Server)\n",
    "    ‚Üì [ExpressRoute]\n",
    "Azure Data Factory (ingesta)\n",
    "    ‚Üì\n",
    "ADLS Gen2 (landing zone)\n",
    "    ‚Üì\n",
    "Azure Databricks (transform) ‚Üí Delta Lake\n",
    "    ‚Üì\n",
    "Synapse Analytics (serving) ‚Üí Power BI\n",
    "    ‚Üì [Archive]\n",
    "AWS S3 Glacier (long-term storage)\n",
    "```\n",
    "\n",
    "**IaC Comparison:**\n",
    "\n",
    "```hcl\n",
    "# Terraform (multi-cloud)\n",
    "resource \"azurerm_storage_account\" \"example\" {\n",
    "  name                     = \"mystorageaccount\"\n",
    "  resource_group_name      = azurerm_resource_group.example.name\n",
    "  location                 = \"eastus\"\n",
    "  account_tier             = \"Standard\"\n",
    "  account_replication_type = \"LRS\"\n",
    "  is_hns_enabled           = true  # ADLS Gen2\n",
    "}\n",
    "\n",
    "# vs AWS\n",
    "resource \"aws_s3_bucket\" \"example\" {\n",
    "  bucket = \"my-bucket\"\n",
    "  acl    = \"private\"\n",
    "}\n",
    "\n",
    "# vs GCP\n",
    "resource \"google_storage_bucket\" \"example\" {\n",
    "  name     = \"my-bucket\"\n",
    "  location = \"US\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Comparison (example workload):**\n",
    "\n",
    "| Resource | Azure | AWS | GCP |\n",
    "|----------|-------|-----|-----|\n",
    "| Storage 1TB | $18/mo | $23/mo | $20/mo |\n",
    "| DW Serverless 10TB | $50 | $50 | $50 |\n",
    "| Spark (100 vCore-h) | $150 + DBU | $80 + EC2 | $10 + Compute |\n",
    "| Orchestration | $100 | $120 | $80 |\n",
    "| **Total** | **~$318** | **~$273** | **~$160** |\n",
    "\n",
    "*Nota: Precios aproximados, var√≠an por region/commitment*\n",
    "\n",
    "---\n",
    "**Autor:** Luis J. Raigoso V. (LJRV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79caff51",
   "metadata": {},
   "source": [
    "## 8. Ejercicios Pr√°cticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78adf50",
   "metadata": {},
   "source": [
    "### üìù **Ejercicios**\n",
    "\n",
    "1. **ADLS Gen2 + Lifecycle:**\n",
    "   - Crear Storage Account con HNS habilitado\n",
    "   - Subir CSVs a `raw/`\n",
    "   - Configurar lifecycle policy (Cool a 30d, Archive a 90d)\n",
    "   - Implementar ACLs POSIX por directorio\n",
    "\n",
    "2. **Synapse Serverless:**\n",
    "   - Crear external table sobre CSVs en ADLS\n",
    "   - Query con OPENROWSET\n",
    "   - Particionar por fecha\n",
    "   - Optimizar query (costo < $0.01)\n",
    "\n",
    "3. **Data Factory Pipeline:**\n",
    "   - Copy Activity: SQL ‚Üí ADLS (Parquet)\n",
    "   - Data Flow: Filter + Aggregate\n",
    "   - Trigger: Event-based (blob created)\n",
    "   - Parametrizar con `@pipeline().parameters.date`\n",
    "\n",
    "4. **Azure Function:**\n",
    "   - Blob trigger en `raw/`\n",
    "   - Validar schema CSV\n",
    "   - Si v√°lido ‚Üí mover a `curated/`\n",
    "   - Si inv√°lido ‚Üí escribir a `errors/` con log\n",
    "\n",
    "5. **Databricks Delta Lake:**\n",
    "   - Mount ADLS Gen2\n",
    "   - Escribir Delta table particionada\n",
    "   - UPSERT con merge\n",
    "   - Time travel (query versi√≥n anterior)\n",
    "\n",
    "**Recursos:**\n",
    "- [Azure Free Account](https://azure.microsoft.com/free/) ($200 cr√©dito)\n",
    "- [Microsoft Learn](https://learn.microsoft.com/azure/data-factory/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training/home)\n",
    "- Certificaci√≥n: [DP-203 Data Engineering on Azure](https://learn.microsoft.com/certifications/exams/dp-203)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5e41a",
   "metadata": {},
   "source": [
    "## 9. Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad16a4c",
   "metadata": {},
   "source": [
    "### üéØ **Key Takeaways**\n",
    "\n",
    "**Azure Strengths para Data Engineering:**\n",
    "\n",
    "1. **Synapse Analytics unifica todo:**\n",
    "   - DW + Spark + Pipelines en un solo servicio\n",
    "   - Elimina silos entre equipos SQL/Spark\n",
    "   - Cost optimization con pause/resume\n",
    "\n",
    "2. **ADLS Gen2 es superior para Big Data:**\n",
    "   - Hierarchical namespace (operaciones at√≥micas)\n",
    "   - ACLs POSIX (security granular)\n",
    "   - HDFS compatible (Spark/Hadoop native)\n",
    "\n",
    "3. **Databricks + Azure:**\n",
    "   - Unity Catalog (governance)\n",
    "   - Delta Lake ACID guarantees\n",
    "   - Photon engine (performance)\n",
    "\n",
    "4. **Integraci√≥n Microsoft:**\n",
    "   - Power BI nativo\n",
    "   - Azure DevOps para CI/CD\n",
    "   - Active Directory SSO\n",
    "\n",
    "**Limitaciones:**\n",
    "\n",
    "- Curva de aprendizaje pronunciada\n",
    "- Docs a veces inconsistentes\n",
    "- Menos \"cool factor\" que GCP\n",
    "- Pricing puede ser opaco\n",
    "\n",
    "**Pr√≥ximos Pasos:**\n",
    "\n",
    "1. **Crear cuenta Azure** (free tier $200 cr√©dito por 30 d√≠as)\n",
    "2. **Completar labs:**\n",
    "   - [Microsoft Learn DP-203](https://learn.microsoft.com/training/paths/data-engineering-with-azure-data-factory/)\n",
    "   - [Databricks Academy](https://www.databricks.com/learn/training/lakehouse-fundamentals)\n",
    "3. **Certificaci√≥n:** DP-203 (Azure Data Engineer Associate)\n",
    "4. **Explorar:** Unity Catalog, Purview (governance)\n",
    "\n",
    "**Comparaci√≥n Final:**\n",
    "\n",
    "- **AWS**: M√°s servicios, mayor comunidad, maduro\n",
    "- **GCP**: BigQuery excelente, ML/AI l√≠der, pricing simple\n",
    "- **Azure**: H√≠brido, Microsoft stack, enterprise-grade\n",
    "\n",
    "**Elecci√≥n depende de:**\n",
    "- Stack actual (Microsoft ‚Üí Azure, Google ‚Üí GCP)\n",
    "- On-prem requirements (Azure Arc wins)\n",
    "- Team skills (SQL ‚Üí Synapse, Spark ‚Üí Databricks)\n",
    "- Budget (GCP suele ser m√°s barato)\n",
    "\n",
    "**Happy data engineering en Azure! üöÄ**\n",
    "\n",
    "---\n",
    "**Autor Final:** LuisRai (Luis J. Raigoso V.)  \n",
    "¬© 2024-2025 - Data Engineering Modular Course"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
