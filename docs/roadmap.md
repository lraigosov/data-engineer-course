# üó∫Ô∏è Roadmap del Curso - Ingenier√≠a de Datos

**Autor**: Luis J. Raigoso V. (LJRV) - [@lraigosov](https://github.com/lraigosov)

---

## üìä Visi√≥n General

Este roadmap detalla la progresi√≥n completa del curso, desde fundamentos hasta nivel Senior + GenAI. El curso consta de **40 notebooks completados (100%)**, organizados en 4 niveles de especializaci√≥n.

---

## üéØ Estructura del Curso

| Nivel         | Notebooks | Estado | Duraci√≥n Estimada |
|---------------|-----------|--------|-------------------|
| **Junior**    | 10        | ‚úÖ 100% | 8-10 semanas      |
| **Mid**       | 10        | ‚úÖ 100% | 10-12 semanas     |
| **Senior**    | 10        | ‚úÖ 100% | 10-12 semanas     |
| **GenAI**     | 10        | ‚úÖ 100% | 6-8 semanas       |
| **Negocio LATAM** | 11    | ‚úÖ 100% | 4-6 semanas       |
| **TOTAL**     | **51**    | **‚úÖ 100%** | **42-52 semanas** |

---

## üìò Nivel Junior - Fundamentos (8-10 semanas)

### Notebooks Completados (10/10)
### Notebooks Completados (10/10)

1. **01_introduccion_ingenieria_datos.ipynb** ‚úÖ
   - Rol del Data Engineer vs Data Scientist vs Data Analyst
   - Ecosistema de herramientas y tecnolog√≠as
   - Primer pipeline conceptual
   
2. **02_python_manipulacion_datos.ipynb** ‚úÖ
   - Estructuras de datos en Python (listas, diccionarios, sets)
   - Manejo de archivos CSV, JSON, XML
   - Comprensiones y generadores
   - Manejo de errores y excepciones

3. **03_pandas_fundamentos.ipynb** ‚úÖ
   - DataFrames y Series
   - Lectura y escritura de m√∫ltiples formatos
   - Indexaci√≥n y selecci√≥n de datos
   - Operaciones b√°sicas de manipulaci√≥n

4. **04_sql_basico.ipynb** ‚úÖ
   - SELECT, WHERE, JOIN, GROUP BY
   - Subconsultas y CTEs
   - SQLite y conexi√≥n desde Python
   - Modelado b√°sico de datos

5. **05_limpieza_datos.ipynb** ‚úÖ
   - Detecci√≥n y manejo de valores nulos
   - Identificaci√≥n de duplicados
   - Conversi√≥n de tipos de datos
   - Normalizaci√≥n y estandarizaci√≥n

6. **06_visualizacion_datos.ipynb** ‚úÖ
   - Matplotlib para gr√°ficos b√°sicos
   - Seaborn para visualizaciones estad√≠sticas
   - Tipos de gr√°ficos apropiados por caso de uso
   - Storytelling con datos

7. **07_git_control_versiones.ipynb** ‚úÖ
   - Git b√°sico: init, add, commit, push, pull
   - Branches y merge workflows
   - .gitignore para proyectos de datos
   - Colaboraci√≥n en equipo con GitHub

8. **08_apis_web_scraping.ipynb** ‚úÖ
   - Consumo de APIs REST con requests
   - Autenticaci√≥n y rate limiting
   - Web scraping √©tico con BeautifulSoup
   - Manejo de datos JSON y XML

9. **09_proyecto_integrador_1.ipynb** ‚úÖ
   - ETL completo: Extraer desde API
   - Transformar con Pandas
   - Cargar a SQLite

10. **10_proyecto_integrador_2.ipynb** ‚úÖ
    - Pipeline end-to-end con m√∫ltiples fuentes
    - Limpieza, validaci√≥n y almacenamiento
    - Visualizaciones y reporting

### Objetivos de Aprendizaje Junior
- ‚úÖ Comprender el rol y ecosistema del Data Engineer
- ‚úÖ Dominar Python para manipulaci√≥n de datos
- ‚úÖ Trabajar eficientemente con Pandas y NumPy
- ‚úÖ Fundamentos de SQL y bases de datos relacionales
- ‚úÖ Control de versiones con Git y GitHub
- ‚úÖ Construir pipelines ETL b√°sicos
- ‚úÖ Visualizaci√≥n de datos para insights

---

## ÔøΩ Nivel Mid - Pipelines y Cloud (10-12 semanas)

### Notebooks Completados (10/10)

### Notebooks Completados (10/10)

1. **01_apache_airflow_fundamentos.ipynb** ‚úÖ
   - Conceptos de DAGs, Tasks, Operators
   - Instalaci√≥n y configuraci√≥n local
   - Primer DAG simple con dependencies
   - Web UI y monitoreo

2. **02_streaming_kafka.ipynb** ‚úÖ
   - Fundamentos de Kafka (topics, partitions, brokers)
   - Productores y consumidores con Python
   - Esquemas y serializaci√≥n (Avro/JSON)
   - Patrones de consumo y reintentos

3. **03_cloud_aws.ipynb** ‚úÖ
   - S3 como data lake
   - AWS Glue (PySpark) para transformaciones
   - Athena para consultas SQL sobre S3
   - Lambda y EventBridge para triggers

3b. **03b_cloud_gcp.ipynb** ‚úÖ üÜï
   - Cloud Storage con lifecycle management
   - BigQuery (serverless + optimizaci√≥n)
   - Dataflow con Apache Beam
   - Cloud Composer (Airflow managed)
   - Cloud Functions event-driven

3c. **03c_cloud_azure.ipynb** ‚úÖ üÜï
   - ADLS Gen2 (hierarchical namespaces + ACLs POSIX)
   - Azure Synapse Analytics (dedicated + serverless SQL + Spark)
   - Azure Data Factory (ETL/ELT orchestration)
   - Azure Databricks (Delta Lake + Photon engine)
   - Azure Functions (m√∫ltiples triggers)

4. **04_bases_datos_postgresql_mongodb.ipynb** ‚úÖ
   - Conexi√≥n con SQLAlchemy (PostgreSQL) y PyMongo
   - Modelado relacional vs documental
   - √çndices y optimizaci√≥n de queries
   - Estrategias de carga (batch, upsert, idempotencia)

5. **05_dataops_cicd.ipynb** ‚úÖ
   - Pruebas de datos con Great Expectations y Pandera
   - Pre-commit hooks (black, isort, flake8, pytest)
   - GitHub Actions para CI/CD
   - Data quality en pipelines

6. **06_conectores_avanzados_rest_graphql_sftp.ipynb** ‚úÖ
   - REST API con paginaci√≥n, backoff y validaci√≥n
   - GraphQL para consultas flexibles
   - SFTP con Paramiko (opcional)
   - Patrones de retry y error handling

7. **07_optimizacion_sql_particionado.ipynb** ‚úÖ
   - EXPLAIN/ANALYZE para an√°lisis de queries
   - √çndices y selectividad
   - Particionado (PostgreSQL y data lakes)
   - Pruning y almacenamiento columnar

8. **08_fastapi_servicios_datos.ipynb** ‚úÖ
   - FastAPI + Pydantic para CRUD de datos
   - Endpoints RESTful para ventas
   - Cach√© con Redis (opcional)
   - Testing con requests

9. **09_proyecto_integrador_1.ipynb** ‚úÖ
   - Pipeline: API ‚Üí validaci√≥n ‚Üí DB ‚Üí Parquet
   - Orquestaci√≥n con Airflow (DAG completo)
   - Alertas y monitoreo b√°sico

10. **10_proyecto_integrador_2.ipynb** ‚úÖ
    - Pipeline streaming: Kafka ‚Üí validaci√≥n ‚Üí enriquecimiento ‚Üí Parquet
    - Idempotencia con checkpoints
    - M√©tricas y consumidor/productor con simulaci√≥n

### Objetivos de Aprendizaje Mid
- ‚úÖ Orquestaci√≥n de pipelines con Apache Airflow
- ‚úÖ Procesamiento de streaming con Kafka
- ‚úÖ **Cloud multi-proveedor (AWS, GCP, Azure) con notebooks pr√°cticos**
- ‚úÖ Bases de datos relacionales (PostgreSQL) y NoSQL (MongoDB)
- ‚úÖ DataOps: testing, CI/CD, calidad de datos
- ‚úÖ Conectores avanzados (REST, GraphQL, SFTP)
- ‚úÖ Optimizaci√≥n de SQL y particionado
- ‚úÖ Servicios de datos con FastAPI
- ‚úÖ Proyectos integradores end-to-end

---

## üöÄ Nivel Senior - Arquitectura y Gobernanza (10-12 semanas)

### Notebooks Completados (10/10)

1. **01_data_governance_calidad.ipynb** ‚úÖ
   - Framework DAMA-DMBOK
   - Dimensiones de calidad de datos (exactitud, completitud, etc.)
   - Data Lineage y trazabilidad
   - Implementaci√≥n de validaciones autom√°ticas

2. **02_lakehouse_delta_iceberg.ipynb** ‚úÖ
   - Principios de Data Lakehouse
   - Cat√°logos de datos (Hive, Glue, Unity)
   - Pr√°ctica con Parquet particionado
   - Referencia a Delta Lake e Iceberg

3. **03_spark_streaming.ipynb** ‚úÖ
   - Structured Streaming con PySpark
   - Ventanas temporales y watermarks
   - Estado y agregaciones incrementales
   - Integraci√≥n con cat√°logos

4. **04_arquitecturas_modernas.ipynb** ‚úÖ
   - Arquitectura Lambda (batch + stream)
   - Arquitectura Kappa (solo stream)
   - Arquitectura Delta (lakehouse)
   - Data Mesh: paradigma organizacional
   - Comparaci√≥n y casos de uso

5. **05_ml_pipelines_feature_stores.ipynb** ‚úÖ
   - Pipelines de datos para ML (ETL ‚Üí features ‚Üí training)
   - Feature stores (Feast, Tecton)
   - MLOps: versionado, reentrenamiento, monitoreo
   - Integraci√≥n con Airflow y Spark

6. **06_cost_optimization_finops.ipynb** ‚úÖ
   - FinOps: visibilidad, accountability, optimizaci√≥n
   - Estrategias de optimizaci√≥n (compute, storage, networking)
   - M√©tricas clave (USD/TB, utilizaci√≥n)
   - Alertas de presupuesto

7. **07_seguridad_compliance.ipynb** ‚úÖ
   - IAM y principio de m√≠nimo privilegio
   - Cifrado at-rest e in-transit
   - Compliance (GDPR, HIPAA, SOC2)
   - Auditor√≠a y logging de seguridad

8. **08_observabilidad_linaje.ipynb** ‚úÖ
   - Logs estructurados y centralizaci√≥n
   - M√©tricas (Prometheus, CloudWatch)
   - Trazas distribuidas
   - Linaje de datos (OpenLineage, DataHub)
   - SLOs y dashboards

9. **09_proyecto_integrador_1.ipynb** ‚úÖ
   - Plataforma completa: governance + lakehouse + orquestaci√≥n
   - Streaming + batch con observabilidad
   - Compliance y seguridad integrados
   - Checklist de 18 componentes

10. **10_proyecto_integrador_2.ipynb** ‚úÖ
    - Data Mesh multi-dominio con feature store
    - Gobernanza federada y data products
    - ML training con features cross-domain
    - Arquitectura descentralizada

### Objetivos de Aprendizaje Senior
- ‚úÖ Arquitecturas modernas (Lambda, Kappa, Delta, Data Mesh)
- ‚úÖ Data Governance completo (DAMA-DMBOK)
- ‚úÖ Data Lakehouse con Delta Lake e Iceberg
- ‚úÖ Stream processing avanzado con Spark Streaming
- ‚úÖ ML Pipelines y Feature Stores (MLOps)
- ‚úÖ Cost Optimization y FinOps en cloud
- ‚úÖ Seguridad y compliance (GDPR, HIPAA, SOC2)
- ‚úÖ Observabilidad y linaje de datos
- ‚úÖ Liderazgo t√©cnico y dise√±o de plataformas enterprise

---

## ü§ñ Nivel GenAI - IA Generativa para Datos (6-8 semanas)

### Notebooks Completados (10/10)

0. **00_comparacion_openai_gemini.ipynb** ‚úÖ
   - Comparaci√≥n detallada OpenAI vs Google Gemini
   - Pricing, capacidades, l√≠mites
   - Casos de uso espec√≠ficos para datos
   - Setup y configuraci√≥n de ambas APIs

1. **01_fundamentos_llms_prompting.ipynb** ‚úÖ
   - Fundamentos de Large Language Models
   - Prompt engineering para tareas de datos
   - Few-shot learning y chain-of-thought
   - Best practices y limitaciones

2. **02_generacion_sql_nl2sql.ipynb** ‚úÖ
   - Text-to-SQL con LLMs
   - Validaci√≥n y sanitizaci√≥n de queries
   - Manejo de esquemas complejos
   - Errores comunes y correcciones

3. **03_generacion_codigo_etl.ipynb** ‚úÖ
   - Generaci√≥n autom√°tica de c√≥digo ETL
   - Templates y patrones reutilizables
   - Validaci√≥n de c√≥digo generado
   - De descripci√≥n en lenguaje natural a pipeline ejecutable

4. **04_rag_documentacion_datos.ipynb** ‚úÖ
   - Retrieval-Augmented Generation (RAG)
   - Chatbot sobre documentaci√≥n t√©cnica
   - Vector stores (ChromaDB, FAISS)
   - B√∫squeda sem√°ntica en cat√°logos de datos

5. **05_embeddings_similitud_datos.ipynb** ‚úÖ
   - Embeddings para representaci√≥n de datos
   - B√∫squeda por similitud sem√°ntica
   - Deduplicaci√≥n inteligente de registros
   - Clustering y clasificaci√≥n con embeddings

6. **06_agentes_automatizacion.ipynb** ‚úÖ
   - LangGraph para agentes aut√≥nomos
   - Agentes para automatizaci√≥n de tareas de datos
   - Herramientas (tools) personalizadas
   - Loops de decisi√≥n y memoria

7. **07_calidad_validacion_llm.ipynb** ‚úÖ
   - Validaci√≥n de calidad de datos con LLMs
   - Detecci√≥n de anomal√≠as sem√°nticas
   - Sugerencias de limpieza autom√°tica
   - Integraci√≥n con Great Expectations

8. **08_sintesis_aumento_datos.ipynb** ‚úÖ
   - Generaci√≥n de datos sint√©ticos con LLMs
   - Data augmentation para ML
   - Preservaci√≥n de distribuciones estad√≠sticas
   - Anonimizaci√≥n inteligente

9. **09_proyecto_integrador_1.ipynb** ‚úÖ
   - Chatbot empresarial sobre datos
   - RAG + Text-to-SQL integrados
   - Dashboard conversacional
   - Interface con Gradio/Streamlit

10. **10_proyecto_integrador_2.ipynb** ‚úÖ
    - Plataforma self-service con GenAI
    - Generaci√≥n autom√°tica de pipelines
    - Documentaci√≥n auto-generada
    - Agentes de mantenimiento y alertas inteligentes

### Objetivos de Aprendizaje GenAI
- ‚úÖ Fundamentos de LLMs y prompt engineering
- ‚úÖ Text-to-SQL para consultas en lenguaje natural
- ‚úÖ Generaci√≥n autom√°tica de c√≥digo ETL
- ‚úÖ RAG para chatbots sobre documentaci√≥n
- ‚úÖ Embeddings para b√∫squeda sem√°ntica
- ‚úÖ Agentes aut√≥nomos con LangGraph
- ‚úÖ Validaci√≥n de calidad con LLMs
- ‚úÖ Generaci√≥n de datos sint√©ticos
- ‚úÖ Integraci√≥n LLMs en pipelines de producci√≥n
- ‚úÖ Costos y optimizaci√≥n de APIs de IA


---

## üìà Nivel Negocio LATAM - Estrategia y Sectores (4-6 semanas)

### Notebooks Completados (11/11)

1. **01_estrategia_datos_latam.ipynb** ‚úÖ
   - Marco conceptual: Rol de ingenier√≠a de datos en estrategia corporativa
   - Caracter√≠sticas estructurales (confiabilidad, escalabilidad, gobernanza)
   - Ventajas estrat√©gicas (velocidad decisi√≥n, calidad, reducci√≥n riesgo)
   - Resultados medibles (ciclos reporte, detecci√≥n temprana, auditor√≠as)
   - Diagn√≥stico de madurez con visualizaci√≥n

2. **02_retail_consumo_masivo.ipynb** ‚úÖ
   - Caso de uso: Pipeline OSA (On-Shelf Availability)
   - Impacto: 92%‚Üí96% OSA, detecci√≥n 48h‚Üí2h, ahorro $1.8M/a√±o
   - Ejercicio: Validaci√≥n calidad datos (nulls, negativos, cobertura)
   - Visualizaci√≥n Plotly: Top 5 SKUs

3. **03_finanzas_banca.ipynb** ‚úÖ
   - Caso de uso: Detecci√≥n fraude streaming (Kafka + feature store)
   - Impacto: 24h‚Üí<200ms latencia, 65%‚Üí89% detecci√≥n, ahorro $3.2M/a√±o
   - Ejercicio: Feature engineering para riesgo (horas nocturnas, montos altos)
   - Visualizaci√≥n: Distribuci√≥n ECL (Expected Credit Loss)

4. **04_salud_farmaceutico.ipynb** ‚úÖ
   - Caso de uso: Interoperabilidad HL7/FHIR en emergencias
   - Impacto: 45‚Üí28 min espera, ahorro $800k/a√±o
   - Ejercicio: Pseudonymizaci√≥n con SHA256 para PII
   - Visualizaci√≥n: Box plot tiempos espera

5. **05_energia_recursos_naturales.ipynb** ‚úÖ
   - Caso de uso: Mantenimiento predictivo IoT/SCADA
   - Impacto: OEE 0.82‚Üí0.91, stops 12‚Üí2/a√±o, ahorro $4.5M/a√±o
   - Ejercicio: Detecci√≥n anomal√≠as con rolling mean + 3-sigma
   - Visualizaci√≥n: Time-series con umbral temperatura

6. **06_telecomunicaciones.ipynb** ‚úÖ
   - Caso de uso: Churn reduction con CDR streaming
   - Impacto: Churn 2.5%‚Üí1.4%, reacci√≥n 24h‚Üí15min, LTV salvado $6.8M
   - Ejercicio: Pipeline features churn (uso bajo, quejas altas)
   - Visualizaci√≥n: Probabilidad churn por segmento

7. **07_industria_manufactura.ipynb** ‚úÖ
   - Caso de uso: SPC (Statistical Process Control) + visi√≥n computacional
   - Impacto: Scrap 8%‚Üí2.8%, OEE 0.78‚Üí0.88, ahorro $6.2M/a√±o
   - Ejercicio: C√°lculo l√≠mites control 3-sigma, detecci√≥n out-of-control
   - Visualizaci√≥n: OEE por turno

8. **08_logistica_transporte.ipynb** ‚úÖ
   - Caso de uso: Routing optimization con telemetr√≠a GPS
   - Impacto: OTIF 88%‚Üí96.5%, costo $45‚Üí$38/entrega, ahorro $3.2M/a√±o
   - Ejercicio: C√°lculo OTIF y costos log√≠sticos
   - Visualizaci√≥n: Histograma ETA vs real

9. **09_agro_alimentos.ipynb** ‚úÖ
   - Caso de uso: Agricultura precisi√≥n con sat√©lite Sentinel-2
   - Impacto: Yield 3.2‚Üí3.6 ton/ha (+12.5%), p√©rdidas 15%‚Üí7%, revenue +$1.8M
   - Ejercicio: An√°lisis variabilidad intra-lote con NDVI
   - Visualizaci√≥n: Scatter NDVI vs yield con OLS

10. **10_sector_publico_gobierno.ipynb** ‚úÖ
    - Caso de uso: Interoperabilidad entre sistemas gubernamentales
    - Impacto: Tr√°mites 12‚Üí4.2 d√≠as, satisfacci√≥n 38%‚Üí74%, fraude +120% detecci√≥n
    - Ejercicio: Simulaci√≥n interoperabilidad padron/impuestos
    - Visualizaci√≥n: Box plot tiempos procedimientos

### Componentes T√©cnicos
- ‚úÖ **Pipelines de producci√≥n**:
  - `scripts/pipelines/retail/pipeline_retail.py` (CLI con Click)
  - `scripts/pipelines/manufactura/pipeline_manufactura.py` (CLI con Click)
- ‚úÖ **Tests unitarios**: 18 passing (`test_pipeline_retail.py`, `test_pipeline_manufactura.py`)
- ‚úÖ **Visualizaciones**: 10 gr√°ficos interactivos Plotly
- ‚úÖ **Contratos de datos**: SLOs, ownership, ROI por sector

### Objetivos de Aprendizaje Negocio LATAM
- ‚úÖ Comprender rol estrat√©gico de ingenier√≠a de datos en corporaciones
- ‚úÖ Mapear OKRs de negocio a capacidades t√©cnicas de datos
- ‚úÖ Cuantificar impacto econ√≥mico de decisiones de ingenier√≠a
- ‚úÖ Dise√±ar pipelines con m√©tricas de negocio (OSA, OEE, OTIF, churn)
- ‚úÖ Implementar validaci√≥n y calidad alineada a SLOs
- ‚úÖ Aplicar patrones sectoriales (retail, finanzas, salud, energ√≠a, telco, manufactura, log√≠stica, agro, gobierno)
- ‚úÖ Construir contratos de datos con propietarios y responsabilidades
- ‚úÖ Traducir requerimientos de negocio a arquitecturas de datos

---

## üìä Resumen Ejecutivo del Curso

### Estad√≠sticas Generales
- **Total notebooks**: 51 ‚úÖ (100% completados)
- **L√≠neas de c√≥digo**: ~20,000+ (estimado)
- **Tecnolog√≠as cubiertas**: 50+ herramientas y frameworks
- **Proyectos integradores**: 8 (2 por nivel t√©cnico)
- **Duraci√≥n total**: 42-52 semanas (10-13 meses a tiempo parcial)

### Cobertura Tecnol√≥gica

**Lenguajes y Core:**
- Python (avanzado), SQL (intermedio-avanzado), Bash

**Data Processing:**
- pandas, numpy, polars, dask, PySpark

**Bases de Datos:**
- PostgreSQL, SQLite, MongoDB, Redis, DuckDB

**Orquestaci√≥n:**
- Apache Airflow, Prefect, Dagster

**Streaming:**
- Apache Kafka, Spark Streaming

**Cloud (Multi-Cloud):**
- **AWS**: S3, Glue, Athena, Lambda, EMR, Redshift
- **GCP**: Cloud Storage, BigQuery, Dataflow, Composer, Cloud Functions
- **Azure**: ADLS Gen2, Synapse, Data Factory, Databricks, Azure Functions

**Data Quality:**
- Great Expectations, Pandera, Pydantic

**GenAI:**
- OpenAI GPT-4/3.5, Google Gemini Pro/Flash
- LangChain, LangGraph, ChromaDB
- RAG, embeddings, agentes aut√≥nomos

**MLOps:**
- Feast, Tecton (feature stores), MLflow

**Observability:**
- Prometheus, OpenLineage, DataHub

---

## üéì Certificaciones Recomendadas Post-Curso

### Cloud Certifications
- **AWS**: AWS Certified Data Analytics - Specialty
- **GCP**: Google Cloud Professional Data Engineer
- **Azure**: Microsoft Certified: Azure Data Engineer Associate (DP-203)

### Vendor-Specific
- **Databricks**: Databricks Certified Data Engineer Associate/Professional
- **Snowflake**: SnowPro Core Certification
- **Apache**: Airflow Fundamentals (Astronomer)

### General
- **DAMA**: CDMP (Certified Data Management Professional)

---

## üöÄ Siguientes Pasos Despu√©s del Curso

1. **Construir Portfolio**:
   - Subir proyectos a GitHub con documentaci√≥n
   - Blog posts explicando soluciones t√©cnicas
   - Contribuir a proyectos open source

2. **Networking**:
   - Participar en meetups de data engineering
   - Conferencias: Data Council, Kafka Summit, Spark Summit
   - LinkedIn: conectar con profesionales del √°rea

3. **Especializaci√≥n**:
   - Elegir un nicho (streaming, ML, cloud, governance)
   - Certificaciones espec√≠ficas
   - Profundizar en tecnolog√≠as clave

4. **Buscar Empleo**:
   - Actualizar CV con proyectos del curso
   - Preparar para entrevistas t√©cnicas
   - Target empresas con stack similar al aprendido

---

Este roadmap refleja el estado **100% completo** del curso de Ingenier√≠a de Datos. ¬°Felicitaciones por embarcarte en este viaje! üéâ